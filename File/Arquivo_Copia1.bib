@INPROCEEDINGS{9849793,
author={Chen, Bingquan and Nie, Guojian and Jiang, Shixin and Hu, Ning},
booktitle={2022 4th International Conference on Advances in Computer Technology, Information Science and Communications (CTISC)},
title={Research on the Big Data-based Product Quality Data Package Construction and Application},
year={2022},
volume={},
number={},
pages={1-6},
abstract={In the new environment of intelligent manufacturing, enterprise quality data has increased exponentially. How to manage, utilize, mine and analyze quality data has become a key issue in modern quality management. This article expands the definition of the product quality data package in the intelligent manufacturing environment, and proposes a big data-based product quality data package construction and management solution, gives a quality data fusion method based on business decision, outlines the application of quality data package. Finally, a chip manufacturing company was used to verify the feasibility of the product quality data package construction and management plan.},
keywords={Information science;Data integration;Companies;Reliability theory;Big Data;Product design;Quality assessment;quality data package;big data;intelligent manufacturing;data fusion},
doi={10.1109/CTISC54888.2022.9849793},
ISSN={},
month={April},}
@INPROCEEDINGS{9245455,
author={Loetpipatwanich, Sakda and Vichitthamaros, Preecha},
booktitle={2020 1st International Conference on Big Data Analytics and Practices (IBDAP)},
title={Sakdas: A Python Package for Data Profiling and Data Quality Auditing},
year={2020},
volume={},
number={},
pages={1-4},
abstract={Data Profiling and data quality management become a more significant part of data engineering, which an essential part of ensuring that the system delivers quality information to users. In the last decade, data quality was considered to need more managing. Especially in the big data era that the data comes from many sources, many data types, and an enormous amount. Thus it makes the managing of data quality is more difficult and complicated. The traditional system was unable to respond as needed. The data quality managing software for big data was developed but often found in a high-priced, difficult to customize as needed, and mostly provide as GUI, which is challenging to integrate with other systems. From this problem, we have developed an opensource package for data quality managing. By using Python programming language, Which is a programming language that is widely used in the scientific and engineering field today. Because it is a programming language that is easy to read syntax, small, and has many additional packages to integrate. The software developed here is called “Sakdas” this package has been divided into three parts. The first part deals with data profiling provide a set of data analyses to generate a data profile, and this profile will help to define the data quality rules. The second part deals with data quality auditing that users can set their own data quality rules for data quality measurement. The final part deals with data visualizing that provides data profiling and data auditing report to improve the data quality. The results of the profiling and auditing services, the user can specify both the form of a report for self-review. Or in the form of JSON for use in post-process automation.},
keywords={Data integrity;Pipelines;Data visualization;Big Data;Syntactics;Software;Python;Data Quality Management;Data Profiling;Data Quality Auditing;Python Package;Data Pipeline},
doi={10.1109/IBDAP50342.2020.9245455},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{8432043,
author={Pan, Xing and Zhang, Manli and Chen, Xi},
booktitle={2018 IEEE International Conference on Software Quality, Reliability and Security Companion (QRS-C)},
title={A Method of Quality Improvement Based on Big Quality Warranty Data Analysis},
year={2018},
volume={},
number={},
pages={643-644},
abstract={Quality warranty data includes big data of product use and customer services, which is foundation of product quality and reliability improvement. This paper presents a method of quality warranty data analysis, which is based on the big data analysis technology. By means of the method of association rules mining, it distinguishes the association rules of failure modes while feeding back the information to the process of product design, production, and usage. To achieve product fault location and fault disposal, the key factors such as fault type and fault cause are analyzed. Meanwhile, this paper adopted the principles of PDCA circulation to propose a procedure of product quality improvement. The quality improvement procedure based on quality warranty data analysis provides a comprehensive and systematic quality improvement for different stages and different types of products. Finally, a case study of household appliances in China is given to illustrate the method.},
keywords={Warranties;Data mining;Product design;Quality assessment;Databases;Big Data;Reliability engineering;quality warranty data;big data analysis;association rules;quality improvement;PDCA},
doi={10.1109/QRS-C.2018.00115},
ISSN={},
month={July},}
@INPROCEEDINGS{8725668,
author={Jin, Li and Haosong, Li and Zhongping, Xu and Ting, Wang and Shuai, Wang and Yutong, Wei and Dongliang, Hu and Chunting, Kang and Jia, Wu and Dan, Su},
booktitle={2019 IEEE 4th International Conference on Cloud Computing and Big Data Analysis (ICCCBDA)},
title={Research on Wide-area Distributed Power Quality Data Fusion Technology of Power Grid},
year={2019},
volume={},
number={},
pages={185-188},
abstract={With the advancement of the "big operation" system construction, the online monitoring system for power quality has been integrated, and various power quality data have been incorporated into relevant organizations for unified management. Power quality management has a larger range of data, more types, and higher frequency. It needs to realize the unified storage management and efficient access of massive heterogeneous power quality data for the characteristics of data applications and the collection and aggregation of these effective data. This paper proposes a new type of grid wide-area distributed power quality data integration architecture, which is designed for multi-source, heterogeneous, distributed data integration technology and wide-area distributed data storage technology to solve the big data source problem and realize the sharing of power quality data information of the whole network.},
keywords={Power quality;Data integration;Distributed databases;Monitoring;Power grids;Computer architecture;Data models;Power Quality;Wide Area Distribution;Data Integration},
doi={10.1109/ICCCBDA.2019.8725668},
ISSN={},
month={April},}
@INPROCEEDINGS{8605945,
author={Taleb, Ikbal and Serhani, Mohamed Adel and Dssouli, Rachida},
booktitle={2018 International Conference on Innovations in Information Technology (IIT)},
title={Big Data Quality Assessment Model for Unstructured Data},
year={2018},
volume={},
number={},
pages={69-74},
abstract={Big Data has gained an enormous momentum the past few years because of the tremendous volume of generated and processed Data from diverse application domains. Nowadays, it is estimated that 80% of all the generated data is unstructured. Evaluating the quality of Big data has been identified to be essential to guarantee data quality dimensions including for example completeness, and accuracy. Current initiatives for unstructured data quality evaluation are still under investigations. In this paper, we propose a quality evaluation model to handle quality of Unstructured Big Data (UBD). The later captures and discover first key properties of unstructured big data and its characteristics, provides some comprehensive mechanisms to sample, profile the UBD dataset and extract features and characteristics from heterogeneous data types in different formats. A Data Quality repository manage relationships between Data quality dimensions, quality Metrics, features extraction methods, mining methodologies, data types and data domains. An analysis of the samples provides a data profile of UBD. This profile is extended to a quality profile that contains the quality mapping with selected features for quality assessment. We developed an UBD quality assessment model that handles all the processes from the UBD profiling exploration to the Quality report. The model provides an initial blueprint for quality estimation of unstructured Big data. It also, states a set of quality characteristics and indicators that can be used to outline an initial data quality schema of UBD.},
keywords={Big Data;Data integrity;Data mining;Feature extraction;Data models;Measurement;Quality assessment;Big Data;Data Quality;Unstructured Data;Quality of Unstructured Big Data},
doi={10.1109/INNOVATIONS.2018.8605945},
ISSN={2325-5498},
month={Nov},}
@INPROCEEDINGS{8029366,
author={Taleb, Ikbal and Serhani, Mohamed Adel},
booktitle={2017 IEEE International Congress on Big Data (BigData Congress)},
title={Big Data Pre-Processing: Closing the Data Quality Enforcement Loop},
year={2017},
volume={},
number={},
pages={498-501},
abstract={In the Big Data Era, data is the core for any governmental, institutional, and private organization. Efforts were geared towards extracting highly valuable insights that cannot happen if data is of poor quality. Therefore, data quality (DQ) is considered as a key element in Big data processing phase. In this stage, low quality data is not penetrated to the Big Data value chain. This paper, addresses the data quality rules discovery (DQR) after the evaluation of quality and prior to Big Data pre-processing. We propose a DQR discovery model to enhance and accurately target the pre-processing activities based on quality requirements. We defined, a set of pre-processing activities associated with data quality dimensions (DQD's) to automatize the DQR generation process. Rules optimization are applied on validated rules to avoid multi-passes pre-processing activities and eliminates duplicate rules. Conducted experiments showed an increased quality scores after applying the discovered and optimized DQR's on data.},
keywords={Big Data;Optimization;Data models;Quality assessment;Big Data;Data Quality Evaluation;Data Quality Rules Discovery;Big Data Pre-Processing},
doi={10.1109/BigDataCongress.2017.73},
ISSN={},
month={June},}
@INPROCEEDINGS{8386521,
author={Hongxun, Tian and Honggang, Wang and Kun, Zhou and Mingtai, Shi and Haosong, Li and Zhongping, Xu and Taifeng, Kang and Jin, Li and Yaqi, Cai},
booktitle={2018 IEEE 3rd International Conference on Cloud Computing and Big Data Analysis (ICCCBDA)},
title={Data quality assessment for on-line monitoring and measuring system of power quality based on big data and data provenance theory},
year={2018},
volume={},
number={},
pages={248-252},
abstract={Currently, on-line monitoring and measuring system of power quality has accumulated a huge amount of data. In the age of big data, those data integrated from various systems will face big data application problems. This paper proposes a data quality assessment system method for on-line monitoring and measuring system of power quality based on big data and data provenance to assess integrity, redundancy, accuracy, timeliness, intelligence and consistency of data set and single data. Specific assessment rule which conforms to the situation of on-line monitoring and measuring system of power quality will be devised to found data quality problems. Thus it will provide strong data support for big data application of power quality.},
keywords={Data integrity;Power quality;Monitoring;Power measurement;Redundancy;Big Data;Business;power qualiy;data quality;big data;data provenance;data assessment},
doi={10.1109/ICCCBDA.2018.8386521},
ISSN={},
month={April},}
@INPROCEEDINGS{9742286,
author={Wu, Xiangwei and Yang, Hongqi and Liu, Yuke and Nie, Guojia and Yang, Lihao and Yang, Yun},
booktitle={2021 2nd International Conference on Electronics, Communications and Information Technology (CECIT)},
title={An Intelligent Selection Method Based On Electronic Component Quality Data System},
year={2021},
volume={},
number={},
pages={218-223},
abstract={Aiming at the difficulty of electronic component quality data management and application, and the lack of data system and application methods required for data management in selection scenarios, this paper proposes an intelligent selection method based on electronic component quality data system, uses Bi-LSTM-ATT model for entity identification, and identifies data association based on entity relationship. By calculating the Tanimoto coefficient, the intelligent matching and push of similar products and substitute products are realized, and the intellectualization of component selection is fully supported. Finally, taking the scenario of fast switching diode selection as an example, the feasibility of the method proposed in this paper is verified, which provides a model for the intelligent application of quality data resources.},
keywords={Switches;Data systems;Data models;Information and communication technology;component;quality data;data system;intelligent selection},
doi={10.1109/CECIT53797.2021.00045},
ISSN={},
month={Dec},}
@INPROCEEDINGS{8078781,
author={Ning, Xiuli and Xu, Yingcheng and Gao, Xiaohong and Li, Ying},
booktitle={2017 IEEE 2nd International Conference on Big Data Analysis (ICBDA)},
title={Missing data of quality inspection imputation algorithm base on stacked denoising auto-encoder},
year={2017},
volume={},
number={},
pages={84-88},
abstract={Analyzing and processing big data of quality inspection is the key factor in ensuring product quality and People's property security. Big data of quality inspection collected by social network and E-commerce is missing in most cases. And the incompleteness of data brings huge challenge for analyzing and processing. Therefore, the algorithm of data filling based on stacked denoising auto-encoder is proposed in this text. As the experiment shows that the algorithm proposed in this text is effective in dealing with big data of quality inspection.},
keywords={Filling;Algorithm design and analysis;Noise reduction;Training;Clustering algorithms;Inspection;Big Data;Big data of quality inspection;Stacked denoising auto-encoder;Filling algorithm},
doi={10.1109/ICBDA.2017.8078781},
ISSN={},
month={March},}
@INPROCEEDINGS{8332632,
author={Liu, He and Huang, Fupeng and Li, Han and Liu, Weiwei and Wang, Tongxun},
booktitle={2017 14th Web Information Systems and Applications Conference (WISA)},
title={A Big Data Framework for Electric Power Data Quality Assessment},
year={2017},
volume={},
number={},
pages={289-292},
abstract={Since a low-quality data may influence the effectiveness and reliability of applications, data quality is required to be guaranteed. Data quality assessment is considered as the foundation of the promotion of data quality, so it is essential to access the data quality before any other data related activities. In the electric power industry, more and more electric power data is continuously accumulated, and many electric power applications have been developed based on these data. In China, the power grid has many special characteristic, traditional big data assessment frameworks cannot be directly applied. Therefore, a big data framework for electric power data quality assessment is proposed. Based on big data techniques, the framework can accumulate both the real-time data and the history data, provide an integrated computation environment for electric power big data assessment, and support the storage of different types of data.},
keywords={Big Data;Data integrity;Power grids;History;Real-time systems;Sensors;data quality;electric power data;data quality assessment;big data;framework},
doi={10.1109/WISA.2017.29},
ISSN={},
month={Nov},}
@INPROCEEDINGS{9148352,
author={Peng, Zhibin and Chen, Yuefeng and Zhang, Zehong and Qiu, Queling and Han, Xiaoqiang},
booktitle={2020 International Conference on Computer Information and Big Data Applications (CIBDA)},
title={Implementation of Water Quality Management Platform for Aquaculture Based on Big Data},
year={2020},
volume={},
number={},
pages={70-74},
abstract={In order to ensure the quality and quantity of aquaculture, aquaculture farmers need to grasp the water quality in time. However, most farmers have to collect water quality data manually at present, and cannot store and reuse that information rapidly. This paper aims to use SpringBoot framework and JPA framework to build a big data platform of acquisition automation and visualization, which realizes the data analysis and display of heterogeneous water quality and breeding information. The platform can make the water quality prediction and real-time warning. Meanwhile, it realizes the management of robots, users and breeding experts. The application of this platform will bring better social benefits to aquaculture farmers.},
keywords={Data visualization;Aquaculture;Data mining;Big Data;Neural networks;Data models;Predictive models;aquaculture;big data;water quality warning;data visualization},
doi={10.1109/CIBDA50819.2020.00024},
ISSN={},
month={April},}
@INPROCEEDINGS{9378148,
author={O’Shea, Enda and Khan, Rafflesia and Breathnach, Ciara and Margaria, Tiziana},
booktitle={2020 IEEE International Conference on Big Data (Big Data)},
title={Towards Automatic Data Cleansing and Classification of Valid Historical Data An Incremental Approach Based on MDD},
year={2020},
volume={},
number={},
pages={1914-1923},
abstract={The project Death and Burial Data: Ireland 1864-1922 (DBDIrl) examines the relationship between historical death registration data and burial data to explore the history of power in Ireland from 1864 to 1922. Its core Big Data arises from historical records from a variety of heterogeneous sources, some aspects are pre-digitized and machine readable. A huge data set (over 4 million records in each source) and its slow manual enrichment (ca 7,000 records processed so far) pose issues of quality, scalability, and creates the need for a quality assurance technology that is accessible to non-programmers. An important goal for the researcher community is to produce a reusable, high-level quality assurance tool for the ingested data that is domain specific (historic data), highly portable across data sources, thus independent of storage technology.This paper outlines the step-wise design of the finer granular digital format, aimed for storage and digital archiving, and the design and test of two generations of the techniques, used in the first two data ingestion and cleaning phases.The first small scale phase was exploratory, based on metadata enrichment transcription to Excel, and conducted in parallel with the design of the final digital format and the discovery of all the domain-specific rules and constraints for the syntax and semantic validity of individual entries. Excel embedded quality checks or database-specific techniques are not adequate due to the technology independence requirement. This first phase produced a Java parser with an embedded data cleaning and evaluation classifier, continuously improved and refined as insights grew. The next, larger scale phase uses a bespoke Historian Web Application that embeds the Java validator from the parser, as well as a new Boolean classifier for valid and complete data assurance built using a Model-Driven Development technique that we also describe. This solution enforces property constraints directly at data capture time, removing the need for additional parsing and cleaning stages. The new classifier is built in an easy to use graphical technology, and the ADD-Lib tool it uses is a modern low-code development environment that auto-generates code in a large number of programming languages. It thus meets the technology independence requirement and historians are now able to produce new classifiers themselves without being able to program. We aim to infuse the project with computational and archival thinking in order to produce a robust data set that is FAIR compliant (Free Accessible Inter-operable and Re-useable).},
keywords={Java;Quality assurance;Semantics;Big Data;Tools;Syntactics;Cleaning;Data Collection;Data Analytics;Model-Driven Development;Historical Data;Data Parsing;Data Cleaning;Ethics;Data Assurance;Data Quality},
doi={10.1109/BigData50022.2020.9378148},
ISSN={},
month={Dec},}
@INPROCEEDINGS{9332030,
author={Sulistyo, Haidar Alvinanda and Kusumasari, Tien Febrianti and Alam, Ekky Novriza},
booktitle={2020 3rd International Conference on Information and Communications Technology (ICOIACT)},
title={Implementation of Data Cleansing Null Method for Data Quality Management Dashboard using Pentaho Data Integration},
year={2020},
volume={},
number={},
pages={12-16},
abstract={Data is a collection of facts or information collected from various sources that are dirty and will affect the quality of decision-making in an organization. Data cleansing ensures that the data is correct, useable, and consistent. Data may be incomplete, inaccurate, or has the wrong format and needs to be corrected or deleted. Data cleansing processing can improve the quality of the data significantly. The data cleansing processing requires to create useful quality data that provides significant benefits for the recipient. The availability of data is crucial in an organization to develop competent, valid, and trustworthy decisions. The null or blank field in data is one of many problems to maintain data quality management in an organization, especially in Indonesian government agencies. The brand registration number permits contain many blank fields, including the complete data needed for the next step processing. Therefore, to solve the amount of blank data, this research will discuss the design and implementation of the data cleansing null method using Pentaho Data Integration (PDI). The result will be implemented to the data quality management (DQM) dashboard using the laravel framework and MySQL as a DBMS.},
keywords={Data integrity;Government;Decision making;Data integration;Information and communication technology;Data cleansing;Data quality;Data quality management;Null cleansing;Pentaho data integration},
doi={10.1109/ICOIACT50329.2020.9332030},
ISSN={},
month={Nov},}
@INPROCEEDINGS{9322890,
author={Vasiliev, Victor A. and Aleksandrova, Svetlana V.},
booktitle={2020 International Conference Quality Management, Transport and Information Security, Information Technologies (IT&QM&IS)},
title={The Prospects for the Creation of a Digital Quality Management System DQMS},
year={2020},
volume={},
number={},
pages={3-5},
abstract={The development of digital technologies can give a new impetus to the development of quality management (QM). The development of new approaches based on the integration of quality management methods and digital technologies creates prerequisites for the digital transformation of the entire product lifecycle. The difficulty of creating an effective quality management system using digital technologies is not only in the absence of specialists in two areas of knowledge simultaneously, but also in the lack of integration of modern quality management methods with existing software products. In most ready-made solutions, quality management is limited to controlling process parameters and product quality. Automatic registration of process parameters with real-time data analysis should be additionally enabled in DQMS. This will allow you to organize monitoring and control of processes at each automated workplace. The accumulated analysis results will help you make decisions in difficult situations. A set of processes with digital control and analysis ensures quality assurance at all stages of the product lifecycle.},
keywords={Quality management;Process control;Digital transformation;Information technology;Task analysis;Information security;Companies;digital technologies;quality;quality management;digital quality management system DQMS;aerospace;life cycle;digital transformation;Big data;control of technological processes;Internet of things},
doi={10.1109/ITQMIS51053.2020.9322890},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{4424094,
author={Vatra, Fanica and Poida, Ana and Stanescu, Carmen},
booktitle={2007 9th International Conference on Electrical Power Quality and Utilisation},
title={Data system for the monitoring of power quality in the transmission substations supplying big consumers},
year={2007},
volume={},
number={},
pages={1-5},
abstract={During 2006, at CN Transelectrica - OMEPA Branch request, ISPE - Power Systems Department has conceived the designing documentations (Feasibility Study and Tender Documents) for “Power Quality Analyzing System at the big consumers”. The present paper reports the purpose and technical endowment proposed by ISPE for “Power Quality Monitoring and Analyzing System” that will be developed at OMEPA.},
keywords={Data systems;Monitoring;Power quality;Substations;Power measurement;Electric variables measurement;Data analysis;Current measurement;Frequency measurement;Data acquisition;power quality;data acquisition;monitoring;data system;big consumers},
doi={10.1109/EPQU.2007.4424094},
ISSN={2150-6655},
month={Oct},}
@INPROCEEDINGS{9422032,
author={Mao, Yifan and Huang, Shasha and Cui, Shuo and Wang, HaiFeng and Zhang, Junyan and Ding, Wenhao},
booktitle={2020 2nd International Conference on Information Technology and Computer Application (ITCA)},
title={Multi dimensional data distribution monitoring based on OLAP},
year={2020},
volume={},
number={},
pages={298-302},
abstract={With the rapid development of the Internet, society is gradually entering the information age, and various data in enterprises have become the most important strategic core resources of all enterprises. The operation and decision-making of enterprises all require a large amount of data analysis. Nowadays, many companies do not pay enough attention to the monitoring of data asset distribution. In addition, various internal systems such as financial management and ERP systems are relatively independent. Each system has its own data organization standard, which makes it difficult to conduct a unified management of data. This also directly leads to the one-sided and subjective problem of enterprise managers' distribution of data assets. With the construction of the data center of each enterprise, the data of each system is aggregated to the center through data integration technology. Therefore, all enterprises need to build a multi-dimensional data distribution monitoring model around data links to comprehensively monitor the status of various data distributions across the company's entire network, and improve data service capabilities and sharing capabilities as well as the company's operational capabilities. This article uses OLAP technology to construct a multi-dimensional data distribution monitoring model for the data link in the process of power enterprise data integration. This article first selects the dimensions and metrics that need to be monitored in the multidimensional data, and then constructs the conceptual model, logical model and physical model of the multidimensional data using on line analytical processing technology. Finally, an example analysis of OLAP system architecture based on B/S structure is realized. The overall data distribution of the enterprise can be grasped by analyzing the various dimensions of the data link, such as System type, location distribution, and time.},
keywords={Measurement;Analytical models;Standards organizations;Data integration;Systems architecture;Financial management;Information age;data link;OLAP;multidimensional data model},
doi={10.1109/ITCA52113.2020.00070},
ISSN={},
month={Dec},}
@INPROCEEDINGS{8109169,
author={Pastorello, Gilberto and Gunter, Dan and Chu, Housen and Christianson, Danielle and Trotta, Carlo and Canfora, Eleonora and Faybishenko, Boris and Cheah, You-Wei and Beekwilder, Norm and Chan, Stephen and Dengel, Sigrid and Keenan, Trevor and O'Brien, Fianna and Elbashandy, Abdelrahman and Poindexter, Cristina and Humphrey, Marty and Papale, Dario and Agarwal, Deb},
booktitle={2017 IEEE 13th International Conference on e-Science (e-Science)},
title={Hunting Data Rogues at Scale: Data Quality Control for Observational Data in Research Infrastructures},
year={2017},
volume={},
number={},
pages={446-447},
abstract={Data quality control is one of the most time consuming activities within Research Infrastructures (RIs), especially when involving observational data and multiple data providers. In this work we report on our ongoing development of data rogues, a scalable approach to manage data quality issues for observational data within RIs. The motivation for this work started with the creation of the FLUXNET2015 dataset, which includes carbon, water, and energy fluxes plus micrometeorological and ancillary data measured in over 200 sites around the world. To create an uniform dataset, including derived data products, extensive work on data quality control was needed. The unpredictable nature of observational data quality issues makes the automation of data quality control inherently difficult. Developed based on this experience, the data rogues methodology allows for increased automation of quality control activities by systematically identifying, cataloging, and documenting implementations of solutions to data issues. We believe this methodology can be extended and applied to others domains and types of data, making the automation of data quality control a more tractable problem.},
keywords={Quality control;Automation;Legged locomotion;Carbon;Soil;Ecosystems;data quality;quality assurance;quality control;observational data;research infrastructures},
doi={10.1109/eScience.2017.64},
ISSN={},
month={Oct},}
@INPROCEEDINGS{8674323,
author={Sabtiana, Rela and Yudhoatmojo, Satrio Baskoro and Hidayanto, Achmad Nizar},
booktitle={2018 6th International Conference on Cyber and IT Service Management (CITSM)},
title={Data Quality Management Maturity Model: A Case Study in BPS-Statistics of Kaur Regency, Bengkulu Province, 2017},
year={2018},
volume={},
number={},
pages={1-4},
abstract={Data are widely used in an organization not only for operation but also for strategic level use. Poor data quality can have negative impact for an organization such as poor decision making and planning. Therefore, data quality management becomes an issue growing today not only to the academic but also professional communities. Based on this issue, this paper presents and analyzes a case study developed in a governmental agency, BPS-Statistics of Kaur Regency. For analysis, a data quality maturity model is used to measure the implementation of data quality management in the organization. The results show that for the dimension of `Data quality expectations' is at a maturity of 4.25. `Data quality protocol' is at a maturity of 3.50. `Policies' reaches a maturity of 3.67. `Data quality protocol' and `Data standard' are at a maturity of 4.42. `Data governance' is at a maturity of 3.00. `Technology' is at a maturity 3.17. `Performance management' is at a maturity of 3.33. However, this also implies that implementing these particular dimensions will lead to a direct increase in overall maturity.},
keywords={Data integrity;Data models;Organizations;Standards organizations;Capability maturity model;Protocols;data quality;data quality management;maturity model;data quality maturity model},
doi={10.1109/CITSM.2018.8674323},
ISSN={},
month={Aug},}
@INPROCEEDINGS{8078796,
author={HongJu, Xiao and Fei, Wang and FenMei, Wang and XiuZhen, Wang},
booktitle={2017 IEEE 2nd International Conference on Big Data Analysis (ICBDA)},
title={Some key problems of data management in army data engineering based on big data},
year={2017},
volume={},
number={},
pages={149-152},
abstract={This paper analyzed the challenges of data management in army data engineering, such as big data volume, data heterogeneous, high rate of data generation and update, high time requirement of data processing, and widely separated data sources. We discussed the disadvantages of traditional data management technologies to deal with these problems. We also highlighted the key problems of data management in army data engineering including data integration, data analysis, representation of data analysis results, and evaluation of data quality.},
keywords={Data engineering;Data analysis;Data integration;Big Data;Distributed databases;Data models;Uncertainty;army data engineering;data management;data integration;data analysis;representation of data analysis results;data quality},
doi={10.1109/ICBDA.2017.8078796},
ISSN={},
month={March},}
@INPROCEEDINGS{9006358,
author={Kumar, Jitendra and Crow, Michael C. and Devarakonda, Ranjeet and Giansiracusa, Michael and Guntupally, Kavya and Olatt, Joseph V. and Price, Zach and Shanafield, Harold A. and Singh, Alka},
booktitle={2019 IEEE International Conference on Big Data (Big Data)},
title={Provenance–aware workflow for data quality management and improvement for large continuous scientific data streams},
year={2019},
volume={},
number={},
pages={3260-3266},
abstract={Data quality assessment, management and improvement is an integral part of any big data intensive scientific research to ensure accurate, reliable, and reproducible scientific discoveries. The task of maintaining the quality of data, however, is non-trivial and poses a challenge for a program like the Department of Energy's Atmospheric Radiation Measurement (ARM) that collects data from hundreds of instruments across the world, and distributes thousands of streaming data products that are continuously produced in near-real-time for an archive 1.7 Petabyte in size and growing. In this paper, we present a computational data processing workflow to address the data quality issues via an easy and intuitive web-based portal that allows reporting of any quality issues for any site, facility or instruments at a granularity down to individual variables in the data files. This portal allows instrument specialists and scientists to provide corrective actions in the form of symbolic equations. A parallel processing framework applies the data improvement to a large volume of data in an efficient, parallel environment, while optimizing data transfer and file I/O operations; corrected files are then systematically versioned and archived. A provenance tracking module tracks and records any change made to the data during its entire life cycle which are communicated transparently to the scientific users. Developed in Python using open source technologies, this software architecture enables fast and efficient management and improvement of data in an operational data center environment.},
keywords={Data integrity;Instruments;Atmospheric measurements;Dictionaries;Big Data;Portals;scientific data workflows;data quality;provenance;atmospheric science},
doi={10.1109/BigData47090.2019.9006358},
ISSN={},
month={Dec},}
@INPROCEEDINGS{5768635,
author={Jihong, Shan and Yalang, Mao and Yi, Sun},
booktitle={2011 International Conference on Consumer Electronics, Communications and Networks (CECNet)},
title={Data model for product life cycle quality control mapping with product structure geometrical model},
year={2011},
volume={},
number={},
pages={2082-2085},
abstract={This paper focus on the technique of learning the knowledge on the design quality and manufacture quality. The characteristic of quality control are illustrated, such as status, process, structure etc, and a lot of feature of the quality control information which consist of geometrical structure model, manufacture technique, detect, fault diagnose and data analysis were presented. Then an approach of the mapping between the product quality control information to the components geometrical model is put out, which can be implemented to optimal the product design, manufacture and assembly in quality control. Finally a prototype system was designed based on the data model.},
keywords={Quality control;Data models;Unified modeling language;Solid modeling;Product design;Design automation;quality control;data model;structure mapping model},
doi={10.1109/CECNET.2011.5768635},
ISSN={},
month={April},}
@INPROCEEDINGS{7584971,
author={Serhani, Mohamed Adel and El Kassabi, Hadeel T. and Taleb, Ikbal and Nujum, Alramzana},
booktitle={2016 IEEE International Congress on Big Data (BigData Congress)},
title={An Hybrid Approach to Quality Evaluation across Big Data Value Chain},
year={2016},
volume={},
number={},
pages={418-425},
abstract={While the potential benefits of Big Data adoption are significant, and some initial successes have already been realized, there remain many research and technical challenges that must be addressed to fully realize this potential. The Big Data processing, storage and analytics, of course, are major challenges that are most easily recognized. However, there are additional challenges related for instance to Big Data collection, integration, and quality enforcement. This paper proposes a hybrid approach to Big Data quality evaluation across the Big Data value chain. It consists of assessing first the quality of Big Data itself, which involve processes such as cleansing, filtering and approximation. Then, assessing the quality of process handling this Big Data, which involve for example processing and analytics process. We conduct a set of experiments to evaluate Quality of Data prior and after its pre-processing, and the Quality of the pre-processing and processing on a large dataset. Quality metrics have been measured to access three Big Data quality dimensions: accuracy, completeness, and consistency. The results proved that combination of data-driven and process-driven quality evaluation lead to improved quality enforcement across the Big Data value chain. Hence, we recorded high prediction accuracy and low processing time after we evaluate 6 well-known classification algorithms as part of processing and analytics phase of Big Data value chain.},
keywords={Big data;Metadata;Measurement;Quality assessment;Quality of service;Unified modeling language;Big Data;Quality assessment;Metadata;Quality metrics;quality Metadata;Quality of process;Hybrid quality assessment},
doi={10.1109/BigDataCongress.2016.65},
ISSN={},
month={June},}
@INPROCEEDINGS{8532518,
author={Burkhardt, Andrew and Berryman, Sheila and Brio, Ashley and Ferkau, Susan and Hubner, Gloria and Lynch, Kevin and Mittman, Susan and Sonderer, Kathy},
booktitle={2018 IEEE AUTOTESTCON},
title={Measuring Manufacturing Test Data Analysis Quality},
year={2018},
volume={},
number={},
pages={1-6},
abstract={Manufacturing test data volumes are constantly increasing. While there has been extensive focus in the literature on big data processing, less focus has existed on data quality, and considerably less focus has been placed specifically on manufacturing test data quality. This paper presents a fully automated test data quality measurement developed by the authors to facilitate analysis of manufacturing test operations, resulting in a single number used to compare manufacturing test data quality across programs and factories, and focusing effort cost-effectively. The automation enables program and factory users to see, understand, and improve their test data quality directly. Immediate improvements in test data quality speed manufacturing test operation analysis, reducing elapsed time and overall spend in test operations. Data quality has significant financial impacts to businesses [1]. While manufacturing cost models are well understood, data quality cost models are less well understood (see Eppler & Helfert [2] who review manufacturing cost models and create a taxonomy for data quality costs). Kim & Choi [3] discuss measuring data quality costs, and a rudimentary data quality cost calculation is described in [4]. Haug et al. [5] describe a classification of costs for poor data quality, and while they do not provide a cost calculation, they do define optimality for data quality. Laranjeiro et al. [6] have a recent survey of poor data quality classification. Ge & Helfert [7] extend the work in [2], and provide an updated review of data quality costs. Test data is specifically addressed in the context of data processing in [8]. Big data quality efforts are reviewed in [9, 10]. Data quality metrics are discussed in [11], and requirements for data quality metrics are identified in [12]. Data inconsistencies are detailed in [13], while categorical data inconsistencies are explained in [14]. In the current work, manufacturing test data quality is directly correlated to the speed of manufacturing test operations analysis. A measurement for manufacturing test data quality indicates the speed at which analysis can be performed, and increases in the test data quality score have precipitated increases in the speed of analysis, described herein.},
keywords={Data integrity;Manufacturing;Measurement;Decision making;Production facilities;Data models;manufacturing test;data quality;test data quality;cost of data quality},
doi={10.1109/AUTEST.2018.8532518},
ISSN={1558-4550},
month={Sep.},}
@INPROCEEDINGS{7816918,
author={Taleb, Ikbal and Kassabi, Hadeel T. El and Serhani, Mohamed Adel and Dssouli, Rachida and Bouhaddioui, Chafik},
booktitle={2016 Intl IEEE Conferences on Ubiquitous Intelligence & Computing, Advanced and Trusted Computing, Scalable Computing and Communications, Cloud and Big Data Computing, Internet of People, and Smart World Congress (UIC/ATC/ScalCom/CBDCom/IoP/SmartWorld)},
title={Big Data Quality: A Quality Dimensions Evaluation},
year={2016},
volume={},
number={},
pages={759-765},
abstract={Data is the most valuable asset companies are proud of. When its quality degrades, the consequences are unpredictable, can lead to complete wrong insights. In Big Data context, evaluating the data quality is challenging, must be done prior to any Big data analytics by providing some data quality confidence. Given the huge data size, its fast generation, it requires mechanisms, strategies to evaluate, assess data quality in a fast, efficient way. However, checking the Quality of Big Data is a very costly process if it is applied on the entire data. In this paper, we propose an efficient data quality evaluation scheme by applying sampling strategies on Big data sets. The Sampling will reduce the data size to a representative population samples for fast quality evaluation. The evaluation targeted some data quality dimensions like completeness, consistency. The experimentations have been conducted on Sleep disorder's data set by applying Big data bootstrap sampling techniques. The results showed that the mean quality score of samples is representative for the original data, illustrate the importance of sampling to reduce computing costs when Big data quality evaluation is concerned. We applied the Quality results generated as quality proposals on the original data to increase its quality.},
keywords={Big data;Measurement;Feature extraction;Quality assessment;Social network services;Companies;Context;Big Data;data quality dimensions;data quality evaluation;Big data sampling},
doi={10.1109/UIC-ATC-ScalCom-CBDCom-IoP-SmartWorld.2016.0122},
ISSN={},
month={July},}
@INPROCEEDINGS{9323615,
author={Han, Weiguo and Jochum, Matthew},
booktitle={IGARSS 2020 - 2020 IEEE International Geoscience and Remote Sensing Symposium},
title={A Machine Learning Approach for Data Quality Control of Earth Observation Data Management System},
year={2020},
volume={},
number={},
pages={3101-3103},
abstract={In the big data era, innovative technologies like cloud computing, artificial intelligence, and machine learning are increasingly utilized in the large-scale data management systems of many industry sectors to make them more scalable and intelligent. Applying them to automate and optimize earth observation data management is a hot topic. To improve data quality control mechanisms, a machine learning method in combination with built-in quality rules is presented in this paper to evolve processes around data quality and enhance management of earth observation data. The rules of quality check are set up to detect the common issues, including data completeness, data latency, bad data, and data duplication, and the machine learning model is trained, tested, and deployed to address these quality issues automatically and reduce manual efforts.},
keywords={Data integrity;Machine learning;Earth;Satellites;Big Data;Process control;Monitoring;Big Data;Machine Learning;Earth Observation Data;Data management;Data Quality;Random Forest},
doi={10.1109/IGARSS39084.2020.9323615},
ISSN={2153-7003},
month={Sep.},}
@INPROCEEDINGS{9599117,
author={West, Nikolai and Gries, Jonas and Brockmeier, Carina and Göbel, Jens C. and Deuse, Jochen},
booktitle={2021 IEEE 22nd International Conference on Information Reuse and Integration for Data Science (IRI)},
title={Towards integrated Data Analysis Quality: Criteria for the application of Industrial Data Science},
year={2021},
volume={},
number={},
pages={131-138},
abstract={The application of Industrial Data Science in context of connected Smart Products requires modeling and structuring data for its design, development and use. Especially for Smart Products, a comprehensive handling of data quality is mandatory, because of their interdisciplinary character and broad range of heterogeneous stakeholders covering the entire product lifecycle. The overall goal of data preparation is to provide high-quality data for application and evaluation by users. Established process models for industrial data analysis often treat the specification and assurance of data quality as a single-point activity with a defined conclusion. Providing end-to-end data quality has received little attention in the field of industrial data analytics. In this paper, we will (1) structure four distinct phases for ensuring end-to-end data quality along data analytics activities, (2) define a set of criteria and measures for meeting and quantifying data quality requirements based on established criteria, and (3) provide a step-by-step model for establishing and maintaining high Data Quality for Industrial Data Science applications. The quality criteria aim to identify pointwise and continuous actions during the data analysis process. Such criteria target a shared responsibility for maintaining data quality during analyses between analyst and user. The developed model provides an actionable approach for assessing and ensuring the requirements of Data Analysis Quality.},
keywords={Analytical models;Phase measurement;Data integrity;Data acquisition;Production;Data science;Data models;Data quality;data analysis quality;industrial data science;quality management;quality assurance;data quality criteria},
doi={10.1109/IRI51335.2021.00024},
ISSN={},
month={Aug},}
@INPROCEEDINGS{6842887,
author={Stander, Tiaan and Rens, Johan},
booktitle={2014 16th International Conference on Harmonics and Quality of Power (ICHQP)},
title={Quality of supply data mining},
year={2014},
volume={},
number={},
pages={44-48},
abstract={Extracting useful network management information from a large volume of QoS data obtained all over a network can be simplified by innovative data mining techniques. The need for QoS expertise is reduced as interactive visualization by brushing and linking of datasets reveals interrelation of parameters. Data contextualization by annotated data can aid the assessment on the global level of compatibility between supply and use conditions. Data dashboards can further simplify the analysis of QoS data by recognizing the network connectivity of different sites, seasonal effects and direction of voltage waveform events.},
keywords={Quality of service;Data mining;Data visualization;Standards;Visualization;Voltage fluctuations;Joining processes;QoS;PQ;data mining;SQL;QoS reporting;compliance to compatibility;network risk management;interactive data visualization;contextualization;data dashboards},
doi={10.1109/ICHQP.2014.6842887},
ISSN={2164-0610},
month={May},}
@INPROCEEDINGS{5557442,
author={Man, Yuan and Wei, Liu and Gang, Huang and Juntao, Gao},
booktitle={2010 Third International Symposium on Electronic Commerce and Security},
title={A Noval Data Quality Controlling and Assessing Model Based on Rules},
year={2010},
volume={},
number={},
pages={29-32},
abstract={As a resource, data is the base for information construction and application. According to the principle of “garbage in and garbage out”, it needs us to ensure data reliability, no errors and accurately reflect the real situation to support the right decisions. However, due to various reasons, it leads to poor quality of dirty data in existing system business, while the dirty data is an important factor which affects right decisions. For the above, in this paper, a metadata-based data quality rule base is created for improving traditional quality control model, a more practical application of the weighted assessment algorithm is proposed and a three-tier data quality assessment system model is constructed based on the study of definition and classification of quality, assessment algorithm, metadata and the control theory. This model is confirmed to achieve comprehensive quality of data management and control in oilfield practical applications.},
keywords={Data models;Databases;Quality assessment;Accuracy;Dictionaries;Quality control;data;data quality;metadata;rule base},
doi={10.1109/ISECS.2010.15},
ISSN={},
month={July},}
@INPROCEEDINGS{9671672,
author={Poon, Lex and Farshidi, Siamak and Li, Na and Zhao, Zhiming},
booktitle={2021 IEEE International Conference on Big Data (Big Data)},
title={Unsupervised Anomaly Detection in Data Quality Control},
year={2021},
volume={},
number={},
pages={2327-2336},
abstract={Data is one of the most valuable assets of an organization and has a tremendous impact on its long-term success and decision-making processes. Typically, organizational data error and outlier detection processes perform manually and reactively, making them time-consuming and prone to human errors. Additionally, rich data types, unlabeled data, and increased volume have made such data more complex. Accordingly, an automated anomaly detection approach is required to improve data management and quality control processes. This study introduces an unsupervised anomaly detection approach based on models comparison, consensus learning, and a combination of rules of thumb with iterative hyper-parameter tuning to increase data quality. Furthermore, a domain expert is considered a human in the loop to evaluate and check the data quality and to judge the output of the unsupervised model. An experiment has been conducted to assess the proposed approach in the context of a case study. The experiment results confirm that the proposed approach can improve the quality of organizational data and facilitate anomaly detection processes.},
keywords={Data integrity;Decision making;Process control;Quality control;Organizations;Big Data;Data models;data quality control;data quality assessment;unsupervised learning;anomaly detection;automated data quality control},
doi={10.1109/BigData52589.2021.9671672},
ISSN={},
month={Dec},}
@INPROCEEDINGS{6021598,
author={Idris, Norizam and Ahmad, Kamsuriah},
booktitle={Proceedings of the 2011 International Conference on Electrical Engineering and Informatics},
title={Managing Data Source quality for data warehouse in manufacturing services},
year={2011},
volume={},
number={},
pages={1-6},
abstract={Data quality and Data Source Management is one of the key success factors for data warehouse project. Many data warehouse projects fail due to poor quality of the data. It is believed that the problems can be fixed later and because of that, a lot of time will be spent to fix the error. If low quality data fed into the data warehouse system, the result will be not accurate if these data are used in the decision making. Many data warehouse and business intelligence projects failure are due to wrong or low quality data. Therefore this paper will underpins several aspects such as Total Data Quality Management (TDQM), ISO 9001:2008 and Quality Management System (QMS) in order to address data quality problems in the early stage and find out the best procedure to manage the data sources. To find a standard procedure in managing data source base on ISO 9001:2008 standard, process in managing data source is identified and a compared to the ISO 9001:2008 Quality Management System (QMS) requirements. As a result, this process is viewed as a kind of production process and relate to the concepts of quality management known from the manufacturing and service domain. More precisely, a high quality management system in managing data source is proposed. This system is based on ISO 9001:2008 standard and hopes it can help organizations in implementing and operating quality management system. By using ISO 9001:2008 framework to the process of managing data source, this approach will be similar to the manufacturing concept that has an added advantage when compared to traditional approaches in managing data source.},
keywords={ISO standards;Quality management;Data warehouses;Organizations;Planning;Data models;Standards organizations;Data Quality;Data Source;Quality Management System;Data Warehouse;ISO 9001:2008},
doi={10.1109/ICEEI.2011.6021598},
ISSN={2155-6830},
month={July},}
@INPROCEEDINGS{5540836,
author={Han Hongke and Qi Linhai},
booktitle={2010 International Conference On Computer Design and Applications},
title={Application and research of multidimensional data analysis in power quality},
year={2010},
volume={1},
number={},
pages={V1-390-V1-393},
abstract={To extract potentially useful information from a large number of power quality monitoring data, this article carries out the analysis of power quality data by the use of multi-dimensional data analysis from multiple perspectives. Considering the data mining in data pretreatment, disturbance recognition related applications, then analyzes and builds a power quality cube. Through the cube analysis, the manager can have a comprehensive and detailed understanding of the trends of power quality data in order to operate in the power to make scientific decision-making.},
keywords={Multidimensional systems;Data analysis;Power quality;Monitoring;Voltage;Power engineering computing;Data mining;Energy management;Quality management;Frequency;Multidimensional Data Analysi;Power Quality;MS Analysis Services},
doi={10.1109/ICCDA.2010.5540836},
ISSN={},
month={June},}
@INPROCEEDINGS{8258221,
author={Hee, Kim},
booktitle={2017 IEEE International Conference on Big Data (Big Data)},
title={Is data quality enough for a clinical decision?: Apply machine learning and avoid bias},
year={2017},
volume={},
number={},
pages={2612-2619},
abstract={This paper provides a practical guideline for the assurance and (re-)usage of clinical data. It proposes a process which aims to provide a systematic data quality assurance even without involving a medical domain expert. Especially when (re-)using clinical data, data quality is an important topic because clinical data are not purposely collected. Therefore, data driven conclusions might be false, because a given dataset is not representative. These false data driven conclusions could even harm the life of patients. Thus, all researchers should adhere to some basic principles that can prevent false conclusions. Twelve empirical experiments were conducted in order to prove that my process is able to assure data quality with respect to the descriptive and predictive analysis. Descriptive results obtained by applying stratified sampling are conflicting in four out of nine population inputs. Sampling is carried based on the top ranked feature drawn by the Contextual Data Quality Assurance (CDQA). Between datasets these features are confirmed by the Mutual Data Quality Assurance (MDQA). Stratified sampled inputs improve predictive results compared to raw data. Both Area Under the Curve (AUC) scores and accuracy scores increase by three percent.},
keywords={Data mining;Medical diagnostic imaging;Currencies;Tools;Medical services;Measurement;data quality;decision quality;healthcare;clinical data;EHRs;data quality assurance;stratified sampling;bias},
doi={10.1109/BigData.2017.8258221},
ISSN={},
month={Dec},}
@INPROCEEDINGS{8465410,
author={Ogudo, Kingsley A. and Nestor, Dahj Muwawa Jean},
booktitle={2018 International Conference on Advances in Big Data, Computing and Data Communication Systems (icABCD)},
title={Modeling of an Efficient Low Cost, Tree Based Data Service Quality Management for Mobile Operators Using in-Memory Big Data Processing and Business Intelligence use Cases},
year={2018},
volume={},
number={},
pages={1-8},
abstract={Network Operators are shifting their business interest towards Data services in a geometric progression manner, as Data services is becoming the major source of Telco revenue. The wide use of Data platforms; such as WhatsApp, Skype, Hangout and other Over the Top (OTT) voice applications over the traditional voice services is a clear indication that Network Operators need to adjust their business model and needs. And couple with the adoption of Smartphones usage which grows continuously year by year, this means more subscribers to manage, large amount of transactions generated, more network resources to be added and evidently more human technical expertise required to ensure good service quality. That has led to high investment on Robust Service Quality Management (SQM) and Customer Experience Management (CEM) to stay competitive in the market. The high investment is justified by the integration of Big Data Solutions, Machine Learning capabilities and good visualization of insight data. However, the Return on Investment (ROI) of the expensive systems are not as conspicuous as the provided functionalities and business rules. Therefore, in this paper an efficient model for low cost SQM system is presented, exploring the advantages of In-Memory Big Data processing and low cost business Intelligence tools to showcase how a good Service Quality Management can be implemented with no big investment.},
keywords={Quality of service;Big Data;Business intelligence;Structured Query Language;Tools;Sparks;Service Quality Management;In-Memory Big Data;Business Intelligence;Service Quality Index;Over The Top Application (OTT);Data Traffic and ROI},
doi={10.1109/ICABCD.2018.8465410},
ISSN={},
month={Aug},}
@INPROCEEDINGS{7404600,
author={Abeysirigunawardena, Dilumie and Jeffries, Marlene and Morley, Michael G. and Bui, Alice O.V. and Hoeberechts, Maia},
booktitle={OCEANS 2015 - MTS/IEEE Washington},
title={Data quality control and quality assurance practices for Ocean Networks Canada observatories},
year={2015},
volume={},
number={},
pages={1-8},
abstract={Cabled observatory installations permit the acquisition of large volumes of continuous, high-resolution data from in-situ instruments. This type of data acquisition presents new challenges and opportunities in the development of data quality assurance and quality control (QAQC) measures. Ocean Networks Canada (ONC) operates the world-leading NEPTUNE and VENUS cabled ocean observing systems in the NE Pacific, and a small seafloor observatory in the Canadian Arctic. ONC collects high-resolution, real-time data on physical, chemical, biological, and geological aspects of the ocean over long time periods, supporting research on complex earth and ocean processes with innovative methods. High quality research depends on high quality data, which in turn depends on robust data quality control practices. For the data to be useful to potential end users, they must be qualified under accepted international standards with additional metadata pertaining to methods of measurement, instrument calibrations, and subsequent data processing included. Ocean Network Canada's QAQC methodology presented here has been developed with the key objectives of maintaining consistency within a single data set and within a collection of data sets. The QAQC model also ensures that the end user has sufficient information on the quality and errors of the data to assess its suitability for their use. The data QAQC procedures and tools have the capability to associate distinctly different but related types of information with data to provide a systematic and timely examination of the measurements. Efforts have been taken to develop efficient and accurate data QAQC techniques and tools to ensure quality data delivery to the end users in a timely manner. The large volume of data coming from extremely complex, diverse, and unpredictable ocean environments has resulted in many challenges as well as opportunities to develop efficient and informative tools for data QAQC at ONC. This paper describes the current and future steps that ONC is undertaking to ensure that data delivered by the observatories are of high quality, easily accessible, and reliable.},
keywords={Instruments;Quality control;Oceans;Real-time systems;Quality assurance;Manuals;Data models;Data Quality Assurance;Data Quality Control;Automated data quality control;Manual data quality control;Cabled Ocean observatory;data assessment annotations;workflow},
doi={10.23919/OCEANS.2015.7404600},
ISSN={},
month={Oct},}
@INBOOK{9821441,
author={Kenett, Ron S. and Shmueli, Galit},
booktitle={Information Quality: The Potential of Data and Analytics to Generate Knowledge},
title={Quality of goal, data quality, and analysis quality},
year={2017},
volume={},
number={},
pages={18-30},
abstract={This chapter examines quality in terms of these information quality (InfoQ) components: quality of the analysis goal, data quality, analysis quality and quality of utility. Although the quality of each of the individual components affects InfoQ, it is the combination of the four that determines the level of InfoQ. The chapter aims to help the reader understand the difference between the quality of a single component and that of InfoQ. In a sense, we build here on the seminal work of Jeffreys, de Finetti, Ramsey, Savage Luce, Raiffa, Shlaifer, Fishburn, and many others who developed decision theory over 75 years ago, integrating data‐driven inference and modeling with decision making. Our updated approach to InfoQ is predicated on modern technologies that provide online access to structured and unstructured data with advanced computational and visualization capabilities. In this context, InfoQ needs to be viewed in a wide context where data is transformed into insights that drive focused and adapted responses. This chapter provides the foundation for an expanded approach to InfoQ.},
keywords={Data integrity;Time measurement;Data analysis;Cleaning;Task analysis;Stakeholders;Medical services},
doi={10.1002/9781118890622.ch2},
ISSN={},
publisher={Wiley},
isbn={9781118890646},
url={https://ieeexplore.ieee.org/document/9821441},}
@INPROCEEDINGS{7364060,
author={Nobles, Alicia L. and Vilankar, Ketki and Wu, Hao and Barnes, Laura E.},
booktitle={2015 IEEE International Conference on Big Data (Big Data)},
title={Evaluation of data quality of multisite electronic health record data for secondary analysis},
year={2015},
volume={},
number={},
pages={2612-2620},
abstract={Currently, a large amount of data is amassed in electronic health records (EHRs). However, EHR systems are largely information silos, that is, uses of these systems are often confined to management of patient information and analytics specific to a clinician's practice. A growing trend in healthcare is combining multiple databases to support epidemiological research. The College Health Surveillance Network is the first national data warehouse containing EHR data from 31 different student health centers. Each member university contributes to the data warehouse by uploading select EHR data including patient demographics, diagnoses, and procedures to a common server on a monthly basis. In this paper, we focus on the data quality dimensions from a subsample of the data comprised of over 5.7 million patient visits for approximately 980,000 patients with 4,465 unique diagnoses from 23 of those universities. We examine the data for measures of completeness, consistency, and availability for secondary use for epidemiological research. Additionally, clinical documentation practices and EHR vendor were evaluated as potential contributors to data quality. We found that overall about 70% of the data in the data warehouse is available for secondary use, and identified clinical documentation practices that are correlated to a reduction in data quality. This suggests that automated quality control and proactive clinical documentation support could reduce ad-hoc data cleaning needs resulting in greater data availability for secondary use.},
keywords={Documentation;Data warehouses;Medical services;Big data;Databases;Cleaning;Medical diagnostic imaging;electronic health records;data quality;big data;multiple data vendors;metrics},
doi={10.1109/BigData.2015.7364060},
ISSN={},
month={Oct},}
@INPROCEEDINGS{8457745,
author={Taleb, Ikbal and Serhani, Mohamed Adel and Dssouli, Rachida},
booktitle={2018 IEEE International Congress on Big Data (BigData Congress)},
title={Big Data Quality: A Survey},
year={2018},
volume={},
number={},
pages={166-173},
abstract={With the advances in communication technologies and the high amount of data generated, collected, and stored, it becomes crucial to manage the quality of this data deluge in an efficient and cost-effective way. The storage, processing, privacy and analytics are the main keys challenging aspects of Big Data that require quality evaluation and monitoring. Quality has been recognized by the Big Data community as an essential facet of its maturity. Yet, it is a crucial practice that should be implemented at the earlier stages of its lifecycle and progressively applied across the other key processes. The earlier we incorporate quality the full benefit we can get from insights. In this paper, we first identify the key challenges that necessitates quality evaluation. We then survey, classify and discuss the most recent work on Big Data management. Consequently, we propose an across-the-board quality management framework describing the key quality evaluation practices to be conducted through the different Big Data stages. The framework can be used to leverage the quality management and to provide a roadmap for Data scientists to better understand quality practices and highlight the importance of managing the quality. We finally, conclude the paper and point to some future research directions on quality of Big Data.},
keywords={Big Data;Data integrity;Quality management;Data visualization;Databases;Sensors;Social network services;Big Data, Data Quality, Quality Management framework, Quality of Big Data},
doi={10.1109/BigDataCongress.2018.00029},
ISSN={},
month={July},}
@INPROCEEDINGS{8276745,
author={Micic, Natasha and Neagu, Daniel and Campean, Felician and Zadeh, Esmaeil Habib},
booktitle={2017 IEEE International Conference on Internet of Things (iThings) and IEEE Green Computing and Communications (GreenCom) and IEEE Cyber, Physical and Social Computing (CPSCom) and IEEE Smart Data (SmartData)},
title={Towards a Data Quality Framework for Heterogeneous Data},
year={2017},
volume={},
number={},
pages={155-162},
abstract={Every industry has significant data output as a product of their working process, and with the recent advent of big data mining and integrated data warehousing it is the case for a robust methodology for assessing the quality for sustainable and consistent processing. In this paper a review is conducted on Data Quality (DQ) in multiple domains in order to propose connections between their methodologies. This critical review suggests that within the process of DQ assessment of heterogeneous data sets, not often are they treated as separate types of data in need of an alternate data quality assessment framework. We discuss the need for such a directed DQ framework and the opportunities that are foreseen in this research area and propose to address it through degrees of heterogeneity.},
keywords={Data integrity;Measurement;Warranties;Metadata;Cleaning;Big Data;Heterogeneous Data Sets;Data Quality;Metadata;Data Cleaning;Data Quality Assessment},
doi={10.1109/iThings-GreenCom-CPSCom-SmartData.2017.28},
ISSN={},
month={June},}
@INPROCEEDINGS{5360632,
author={Su, Ying and Peng, Gongqian and Jin, Zhanming},
booktitle={2009 Sixth International Conference on Fuzzy Systems and Knowledge Discovery},
title={Assuring the Information Quality in Data Mining for a Finance Company},
year={2009},
volume={5},
number={},
pages={188-191},
abstract={This paper describes a information quality assurance exercise undertaken for a finance company as part of a larger project in auto finance marketing. A methodology to estimate the effects of data accuracy, completeness and consistency on the data aggregate functions count, sum and average is presented. This methodology should be of specific interest to quality assurance practitioners for projects that harvest warehouse data for decision support to the management. The assessment comprised ten checks in three broad categories, to ensure the quality of information collected over 1103 attributes. The assessment discovered four critical gaps in the data that had to be corrected before the data could be transitioned to the analysis phase.},
keywords={Data mining;Finance;Quality assurance;Information analysis;Resource management;Quality management;Project management;Quality assessment;Frequency;Fuzzy systems;Auto finance marketing;data mining;data warehouse;quality assessment;information quality},
doi={10.1109/FSKD.2009.842},
ISSN={},
month={Aug},}
@INPROCEEDINGS{7077304,
author={Panahy, Payam Hassany Shariat and Sidi, Fatimah and Affendey, Lilly Suriani and Jabar, Marzanah A.},
booktitle={2014 4th World Congress on Information and Communication Technologies (WICT 2014)},
title={The impact of data quality dimensions on business process improvement},
year={2014},
volume={},
number={},
pages={70-73},
abstract={Guaranteeing high data quality level is an important issue to increase the efficiency of the business processes. In fact, poor data quality produces wrong information, which leads to the failure of the business process improvement. Identifying data quality problems has positive impact on overall effectiveness and efficiency of the process improvement. In fact, improving data quality often requires modifying business process enriching them with the most improvement activities. Such activities depend and change based on the data quality dimensions. In this paper, we focus on review of the impact of data quality dimensions on business process improvement in order to support managers to facilitate the implementation of process improvement. The evaluations of this research will use to refine and extend knowledge of relationship between data quality dimensions and business process improvement.},
keywords={Information systems;Organizations;Accuracy;Quality assessment;Correlation;Data mining;Data Quality;Data quality Dimension;Business Process Improvement;Information System;Data Quality Problem},
doi={10.1109/WICT.2014.7077304},
ISSN={},
month={Dec},}
@INPROCEEDINGS{5593829,
author={Xiaoping, Zhao and Zhenwang, Dong and Chao, Luo},
booktitle={2010 5th International Conference on Computer Science & Education},
title={Integrative system structure of quality data acquisition, monitoring and management oriented to cotton-spinning enterprise},
year={2010},
volume={},
number={},
pages={1939-1942},
abstract={Traditionally, for a cotton spinning corporation, its quality data acquisition, analysis, and monitoring were separated. In order to improve the quality management of the enterprise, this paper completes and depicts the framework construction of integrative system of quality data acquisition, analysis, and monitoring oriented to the cotton spinning corporations. It also elaborates the technology of quality data acquisition, analysis, and monitoring in the process of production management of this enterprise. The executive results shows that this system has enhanced the quality awareness, significantly increased the products quality, and reinforced the core competitiveness of the enterprise.},
keywords={Cotton;Monitoring;Data acquisition;Production;Raw materials;Quality management;Spinning;Cotton Spinning Corporation;Quality Data Acquisition;Monitoring and Management;Integration},
doi={10.1109/ICCSE.2010.5593829},
ISSN={},
month={Aug},}
@INPROCEEDINGS{9026017,
author={Wenxue, Chi and Yong, Zhang and Xiaochun, Zhang and Junshan, Jing and Peng, Yan},
booktitle={2019 International Conference on Meteorology Observations (ICMO)},
title={Analysis of Quality Control Effect of Reactive Gas Observation Data Based on Multiple Data Quality Control Methods},
year={2019},
volume={},
number={},
pages={1-4},
abstract={The quality control of observation data is an important link in the entire meteorological observation service system chain. The reactive gas observation is different from the observation of other meteorological elements in general, especially the special operations such as periodic zero / span inspection and multi-point calibration involved in instrument observation, and the quality control corresponding to its data is also special character. The main purpose of this article is to find the most effective quality control method for quality control of reactive gas observation data. Six types of quality control methods are used to perform quality control on reactive gas observation data, including extreme value check, triple standard deviation method, zero-span identification method, internal consistency check, time change check, and comprehensive identification check. By comprehensively identifying the data quality control result identification code given by each method, the quality control status of the data is finally determined. The quality control data used are the observation data of Beijing Shangdianzi atmospheric background station in 2018. Conclusion: A comparison of longsequence, multi-site analysis of the data after quality control using 6 methods and the data processed by a single method can draw conclusions. The multi-method comprehensive quality control results are superior to a single traditional meteorological observation data quality control method, which will cause fewer false judgments on the data and ensure the best quality control results of the observation data.},
keywords={Quality control;Inspection;Process control;Standards;Instruments;Atmosphere;Data integrity;Quality Control;Reactive Gas; Control Method;Quality Control, Reactive Gas;Control Method},
doi={10.1109/ICMO49322.2019.9026017},
ISSN={},
month={Dec},}
@INPROCEEDINGS{9642876,
author={Efimova, Olga V. and Igolnikov, Boris V. and Isakov, Michail P. and Dmitrieva, Elizaveta I.},
booktitle={2021 International Conference on Quality Management, Transport and Information Security, Information Technologies (IT&QM&IS)},
title={Data Quality and Standardization for Effective Use of Digital Platforms},
year={2021},
volume={},
number={},
pages={282-285},
abstract={Functioning of a company is largely based on digital technologies that form digital platforms - a set of products that implements innovative business models based on modern technologies, new ways of interaction with clients, contractors and partners. The purpose of the paper is to form metrics of data quality when using digital platforms. Studies of data quality metrics in various types of business and digital ecosystems made it possible to form a data quality scale capable of assessing the effectiveness of the role model, competency profiles of participants in digital ecosystems, and the sufficiency of resources. It is also necessary to evaluate the simplicity, efficiency and discipline of data quality control and management processes in order to eliminate the loss of digital trust of users. The paper focuses on the organizational model of data quality management based on the quality metrics system.},
keywords={Measurement;Data integrity;Biological system modeling;Ecosystems;Standards organizations;Process control;Companies;data quality;digital platforms;standards;metrics},
doi={10.1109/ITQMIS53292.2021.9642876},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{9260067,
author={Wong, Ka Yee. and Wong, Raymond K.},
booktitle={2020 IEEE 7th International Conference on Data Science and Advanced Analytics (DSAA)},
title={Big Data Quality Prediction on Banking Applications: Extended Abstract},
year={2020},
volume={},
number={},
pages={791-792},
abstract={Big data has been transformed into knowledge by information systems to add value in businesses. Enterprises relying on it benefit from risk management to a certain extent. The value, however, depends on the quality of data. The quality needs to be verified before any use of the data. Specifically, measuring the quality by simulating the real life situation and even forecast it accurately turns into a hot topic. In recent years, there have been numerous researches on the measurement and assessment of data quality. These are yet to utilize a scientific computational method for the measurement and prediction. Current methods either fail to make an accurate prediction or do not consider the correlation and time sequence factors of the data. To address this, we design a model to extend machine learning technique to business applications predicting this. Firstly, we implement the model to detect data noises from a risk dataset according to an international data quality standard from banking industry and then estimate their impacts with Gaussian and Bayesian methods. Secondly, we direct sequential learning in multiple deep neural networks for the prediction with an attention mechanism. The model is experimented with various network methodologies to show the predictive power of machine learning technique and is evaluated by validation data to confirm the model effectiveness. The model is scalable to apply to any industries utilizing big data other than the banking industry.},
keywords={Big Data;Data models;Banking;Neural networks;Industries;Data integrity;Computational modeling;Big Data;Data Noise;Data Quality;Prediction and Machine Learning},
doi={10.1109/DSAA49011.2020.00119},
ISSN={},
month={Oct},}
@INPROCEEDINGS{7840906,
author={Angryk, Rafal A. and Galarus, Douglas E.},
booktitle={2016 IEEE International Conference on Big Data (Big Data)},
title={The SMART approach to comprehensive quality assessment of site-based spatial-temporal data},
year={2016},
volume={},
number={},
pages={2636-2645},
abstract={There is a need for comprehensive solutions to address the challenges of spatio-temporal data quality assessment. Emphasis is often placed on the quality assessment of individual observations from sensors but not on the sensors themselves nor upon site metadata such as location and timestamps. The focus of this paper is on the development and evaluation of a representative and comprehensive, interpolation-based methodology for assessment of spatio-temporal data quality. We call our method the SMART method, short for Simple Mappings for the Approximation and Regression of Time series. When applied to a real-world, meteorological data set, we show that our method outperforms standard interpolators and we identify numerous problematic sites that otherwise would not have been flagged as bad. We further identify sites for which metadata is incorrect. We believe that there are many problems with real data sets like these and, in the absence of an approach like ours, these problems have largely gone unidentified. Our results bring into question the validity of provider-based quality control indicators. In addition to providing a comprehensive solution, our approach is novel for the simple but effective way that it accounts for spatial and temporal variation.},
keywords={Sensors;Quality control;Metadata;Meteorology;Interpolation;Quality assessment;Time series analysis;data quality;data stream processing;spatial-temporal data;quality control;interpolation},
doi={10.1109/BigData.2016.7840906},
ISSN={},
month={Dec},}
@INPROCEEDINGS{7364065,
author={Rao, Dhana and Gudivada, Venkat N and Raghavan, Vijay V.},
booktitle={2015 IEEE International Conference on Big Data (Big Data)},
title={Data quality issues in big data},
year={2015},
volume={},
number={},
pages={2654-2660},
abstract={Though the issues of data quality trace back their origin to the early days of computing, the recent emergence of Big Data has added more dimensions. Furthermore, given the range of Big Data applications, potential consequences of bad data quality can be for more disastrous and widespread. This paper provides a perspective on data quality issues in the Big Data context. it also discusses data integration issues that arise in biological databases and attendant data quality issues.},
keywords={Databases;Big data;Measurement;Context;Cleaning;Biology;Computers;Data quality;big data;biological data;information quality},
doi={10.1109/BigData.2015.7364065},
ISSN={},
month={Oct},}
@INPROCEEDINGS{8085513,
author={Li, Yuqian and Li, Peng and Zhu, Feng and Wang, Ruchuan},
booktitle={2017 12th International Conference on Computer Science and Education (ICCSE)},
title={Design of higher education quality monitoring and evaluation platform based on big data},
year={2017},
volume={},
number={},
pages={337-342},
abstract={Through the continuous collection and in-depth analysis of the quality monitoring data of colleges and universities, we combine the efficiency processing of big data and data evaluation, monitor the status of higher education normally, and construct a higher education quality monitoring and evaluation platform based on Spark. This platform is teaching centered with schools as its basis, including subsystems of data acquisition, data analysis, machine learning, data storage, data analysis and other areas. Through the application of the higher education quality monitoring platform, we can understand the current situation of the development of higher education scientifically, and provide the basis for the macro-decision of education administration department.},
keywords={Education;Monitoring;Big Data;Data mining;Servers;Memory;Indexes;big data;monitoring and evaluation;system design},
doi={10.1109/ICCSE.2017.8085513},
ISSN={2473-9464},
month={Aug},}
@INPROCEEDINGS{8862267,
author={Juneja, Ashish and Das, Nripendra Narayan},
booktitle={2019 International Conference on Machine Learning, Big Data, Cloud and Parallel Computing (COMITCon)},
title={Big Data Quality Framework: Pre-Processing Data in Weather Monitoring Application},
year={2019},
volume={},
number={},
pages={559-563},
abstract={Big Data has become an imminent part of all industries and business sectors today. All organizations in any sector like energy, banking, retail, hardware, networking, etc all generate huge quantum of heterogenous data which if mined, processed and analyzed accurately can reveal immensely useful patterns for business heads to apply to generate and grow their businesses. Big Data helps in acquiring, processing and analyzing large amounts of heterogeneous data to derive valuable results. Quality of information is affected by size, speed and format in which data is generated. Hence, Quality of Big Data is of great relevance and importance. We propose addressing various aspects of the raw data to improve its quality in the pre-processing stage, as the raw data may not usable as-is. We are exploring process like Cleansing to fix as much data as feasible, Noise filters to remove bad data, as well sub-processes for Integration and Filtering along with Data Transformation/Normalization. We evaluate and profile the Big Data during acquisition stage, which is adapted to expectations to avoid cost overheads later while also improving and leading to accurate data analysis. Hence, it is imperative to improve Data quality even it is absorbed and utilized in an industry's Big Data system. In this paper, we propose a Pre-Processing Framework to address quality of data in a weather monitoring and forecasting application that also takes into account global warming parameters and raises alerts/notifications to warn users and scientists in advance.},
keywords={Big Data;Data integrity;Meteorology;Monitoring;Data mining;Organizations;Big Data;Big Data Quality;Data Quality;preprocessing;pre-processing},
doi={10.1109/COMITCon.2019.8862267},
ISSN={},
month={Feb},}
@ARTICLE{8825534,
author={Hu, Chuanmin and Barnes, Brian B. and Feng, Lian and Wang, Menghua and Jiang, Lide},
journal={IEEE Geoscience and Remote Sensing Letters},
title={On the Interplay Between Ocean Color Data Quality and Data Quantity: Impacts of Quality Control Flags},
year={2020},
volume={17},
number={5},
pages={745-749},
abstract={Nearly all calibration/validation activities for the satellite ocean color missions have focused on data quality to produce data products of the highest quality (i.e., science quality) for climate-related research. Little attention, however, has been paid to data quantity, particularly on how data quality control during data processing impacts downstream data quality and data quantity. In this letter, we attempt to fill this knowledge gap using measurements from the Visible Infrared Imaging Radiometer Suite (VIIRS) onboard the Suomi National Polar-orbiting Partnership (SNPP). For this sensor, the same level-1B data are processed independently using different quality control methods by NASA and NOAA, respectively, allowing for an in-depth evaluation of the interplay between data quantity and quality. The results indicate that the methods to identify stray light and sun glint are the two primary quality control procedures affecting data quantity, where the criteria for flagging pixels “contaminated” by stray light and sun glint may be relaxed in the NASA ocean color data processing to increase data quantity without compromising data quality.},
keywords={Oceans;Image color analysis;Data integrity;Quality control;Sun;Sea measurements;NASA;Calibration;data quality;data quantity;level-2 flags stray light;ocean color;quality control;remote sensing;sun glint;uncertainty;validation},
doi={10.1109/LGRS.2019.2936220},
ISSN={1558-0571},
month={May},}
@INPROCEEDINGS{8465129,
author={Juddoo, Suraj and George, Carlisle},
booktitle={2018 International Conference on Advances in Big Data, Computing and Data Communication Systems (icABCD)},
title={Discovering Most Important Data Quality Dimensions Using Latent Semantic Analysis},
year={2018},
volume={},
number={},
pages={1-6},
abstract={Big Data quality is a field which is emerging. Many authors nowadays agree that data quality is still very relevant, even for Big Data uses. However, there is a lack of frameworks or guidelines about how to carry out those big data quality initiatives. The starting point of any data quality work is to determine the properties of data quality, termed as data quality dimensions (DQDs). Even those dimensions lack precise rigour in terms of definition from existing literature. This current research aims to contribute towards identifying the most important DQDs for big data in the health industry. It is a continuation of a previous work, which already identified five most important DQDs, using a human judgement based technique known as inner hermeneutic cycle. To remove potential bias coming from the human judgement aspect, this research uses the same set of literature but applies a statistical technique known to extract knowledge from a set of documents known as latent semantic analysis. The results confirm only 2 similar most important DQDs, namely accuracy and completeness.},
keywords={Data integrity;Big Data;Semantics;Industries;Bibliographies;Libraries;Production;Big Data;data quality;health data;data quality dimensions;latent semantic analysis},
doi={10.1109/ICABCD.2018.8465129},
ISSN={},
month={Aug},}
@INPROCEEDINGS{9006294,
author={Arruda, Darlan and Madhavji, Nazim H.},
booktitle={2019 IEEE International Conference on Big Data (Big Data)},
title={QualiBD: A Tool for Modelling Quality Requirements for Big Data Applications},
year={2019},
volume={},
number={},
pages={5977-5979},
abstract={The development of Big Data applications is not well-explored, to our knowledge. Embracing Big Data in system building, questions arise as to how to elicit, specify, analyse, model, and document Big Data quality requirements. In our ongoing research, we explore a requirements modelling language for Big Data software applications. In this paper, we introduce QualiBD, a modelling tool that implements the proposed goal-oriented requirements language that facilitates the modelling of Big Data quality requirements.},
keywords={Tools;Data models;Containers;Software;Real-time systems;Big Data applications;Big Data Applications;Quality Requirements;Big Data Goal-oriented Requirements Language;Requirements Modelling Tool},
doi={10.1109/BigData47090.2019.9006294},
ISSN={},
month={Dec},}
@ARTICLE{8509662,
author={},
journal={IEEE P1159.3/D19, October 2018},
title={IEEE Draft Recommended Practice for Power Quality Data Interchange Format (PQDIF)},
year={2018},
volume={},
number={},
pages={1-238},
abstract={A file format suitable for exchanging power quality related measurement and simulation data in a vendor independent manner is defined in this recommended practice. The format is designed to represent all power quality phenomena identified in IEEE Std 1159-2009, IEEE Recommended Practice on Monitoring Electric Power Quality, other power related measurement data, and is extensible to other data types as well. The recommended file format utilizes a highly compressed storage scheme to help reduce disk space and transmission times. The utilization of Globally Unique Identifiers (GUID) to represent each element in the file permits the format to be extensible without the need for a central registration authority.},
keywords={IEEE Standards;Power quality;Protocols;Data integration;data interchange;file format;IEEE 1159.3;measurement;monitoring;power quality;PQDIF},
doi={},
ISSN={},
month={Oct},}
@INPROCEEDINGS{7799648,
author={Xia, Wenze and Xu, Zhuoming and Wei, Jie and Tian, Haimei},
booktitle={2016 13th Web Information Systems and Applications Conference (WISA)},
title={DQFIRD: Towards Data Quality-Based Filtering and Ranking of Datasets for Data Portals},
year={2016},
volume={},
number={},
pages={18-23},
abstract={The Data on the Web Best Practices Working Group, as part of W3C Data Activity, is standardizing the Data Quality Vocabulary (DQV) for expressing data quality of datasets published on the Web. By exploiting such DQV-based quality metadata associated to the datasets in a data portal, data consumers can achieve data quality-based filtering and ranking of datasets on the portal's conventional search results to obtain desired datasets with high data-quality. Despite the significant progress in standardization, there is a lack of systematic research on approaches and tools for data quality-based filtering and ranking of Web published datasets. This paper therefore proposes a generic software framework for Data Quality-based Filtering and Ranking of Datasets (DQFIRD) in data portals. DQFIRD adopts faceted search (or faceted exploration) techniques to filter the search results of a data portal based on quality metadata about the resulting datasets, and then ranks the filtered datasets according to numeric values of quality measurements in the metadata. We designed the main algorithms of DQFIRD and implemented a prototype of DQFIRD using Java and Jena API. Furthermore, we used the prototype to conduct case study experiments and time efficiency test on the Faceted Taxonomy Materialization (FTM) algorithm, the most time-consuming online operation algorithm in DQFIRD. The results indicate that the proposed DQFIRD approach is implementable and effective, and it has low time complexity because the run-time of the FTM algorithm exhibits approximately a linear growth rate as the size of the relevant dataset quality metadata increases.},
keywords={Visualization;User interfaces;Data models;Resource description framework;W3C;Prototypes;Context;data quality-based filtering and ranking;datasets;faceted search;Data Quality Vocabulary (DQV);quality metadata;data portal},
doi={10.1109/WISA.2016.14},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{9148148,
author={Bai, Zhongxian and Zhuo, Rongqing},
booktitle={2020 International Conference on Computer Information and Big Data Applications (CIBDA)},
title={Quality Management of Crowd Sensing Data Based on Machine Learning},
year={2020},
volume={},
number={},
pages={185-188},
abstract={Recently, research on crowd sensing data quality management is a new subject area developed based on wireless sensor network related concepts. Crowd sensing data has also experienced many links in the process of network propagation, so it is inevitable that there are abnormal data in the database. Therefore, how to filter these unreliable data to get more real data is particularly important. This paper takes somatosensory temperature as an example, solves the problem of calculating the similarity of unequal long-term sequences by using DTW technology, and then clusters and compares the data in the database to find out the abnormal data. Thereby, the accuracy of the somatosensory temperature database is improved, and relevant users can obtain more accurate information. The experimental results show that when the minimum DTW value exceeds the threshold t = 0.7, the more the number of simulation sequences, the more stable the accuracy rate, and the faster the growth rate of the running time.},
keywords={Time series analysis;Temperature sensors;Mobile handsets;Temperature distribution;Databases;Data models;Machine Learning;Crowd Sensing;Big Data Analysis;Abnormal Data Detection Management;Clustering Method},
doi={10.1109/CIBDA50819.2020.00049},
ISSN={},
month={April},}
@INPROCEEDINGS{9006187,
author={Shrivastava, Shrey and Patel, Dhaval and Bhamidipaty, Anuradha and Gifford, Wesley M. and Siegel, Stuart A. and Ganapavarapu, Venkata Sitaramagiridharganesh and Kalagnanam, Jayant R.},
booktitle={2019 IEEE International Conference on Big Data (Big Data)},
title={DQA: Scalable, Automated and Interactive Data Quality Advisor},
year={2019},
volume={},
number={},
pages={2913-2922},
abstract={Fueled with growth in the fields of Internet of Things (IoT) and Big Data, data has become one of the most valuable assets in today's world. While we are leveraging this data for analyzing complex systems using machine learning and deep learning, a considerable amount of time and effort is spent on addressing data quality issues. If undetected, data quality issues can cause large deviations in the analysis, misleading data scientists. To ease the effort of identifying and addressing data quality challenges, we introduce DQA, a scalable, automated and interactive data quality advisor. In this paper, we describe the DQA framework, provide detailed description of its components and the benefits of integrating it in a data science process. We propose a programmatic approach for implementing the data quality framework which automatically generates dynamic executable graphs for performing data validations fine-tuned for a given dataset. We discuss the use of DQA to build a library of validation checks common to many applications. We provide insight into how DQA addresses many persistence and usability issues which currently make data cleaning a laborious task for data scientists. Finally, we provide a case study of how DQA is implemented in a realworld system and describe the benefits realized.},
keywords={Data integrity;Machine learning;Pipelines;Cleaning;Libraries;Buildings;Data quality;machine learning;data cleaning;scalability;automation;data science},
doi={10.1109/BigData47090.2019.9006187},
ISSN={},
month={Dec},}
@INPROCEEDINGS{7100100,
author={Estler, Manfred and Hilpert, Ralf and Kiupel, Niels and Soravia, Sergio},
booktitle={1999 European Control Conference (ECC)},
title={Using data mining methods for improvement of product and process quality},
year={1999},
volume={},
number={},
pages={4824-4829},
abstract={Recently, data analysis methods have been successfully employed in various areas of industry. With their help analysts gain valuable information and knowledge about business and technical processes. In addition to established and proven statistical methods, new data analysis techniques hiding behind catchwords such as Data Mining, Knowledge Discovery in Databases, and Computational Intelligence are increasingly used. These new approaches are primarily characterized by their ability to find conspicuous patterns in databases almost on their own. In a particular way, they support the general goal to detect existing and possibly still hidden information in databases. In chemical industry the information obtained is mainly meant to support the design of products and production processes, as well as the development of intelligent process management strategies.},
keywords={Data analysis;Databases;Statistical analysis;Data mining;Quality assessment;Product design;Safety;Exploratory data analysis;data mining;statistical methods;independence analysis;data preprocessing},
doi={10.23919/ECC.1999.7100100},
ISSN={},
month={Aug},}
@INPROCEEDINGS{9073586,
author={Khaleel, Majida Yaseen and Hamad, Murtadha M.},
booktitle={2019 12th International Conference on Developments in eSystems Engineering (DeSE)},
title={Data Quality Management for Big Data Applications},
year={2019},
volume={},
number={},
pages={357-362},
abstract={Currently, as a result of the continuous increase of data, one of the key issues is the development of systems and applications to deal with storage, management and processing of big numbers of data. These data are found in unstructured ways. Data management with traditional approaches is inappropriate because of the large and complex data sizes. Hadoop is a suitable solution for the continuous increase in data sizes. The important characteristics of the Hadoop are distributed processing, high storage space, and easy administration. Hadoop is better known for distributed file systems. In this paper, we have proposed techniques and algorithms that deal with big data including data collecting, data preprocessing, algorithms for data cleaning, A Technique for Converting Unstructured Data to Structured Data using metadata, distributed data file system (fragmentation algorithm) and Quality assurance algorithms by using the model is the statistical model to evaluate the highest educational institutions. We concluded that Metadata accelerates query response required and facilitates query execution, metadata will be content for reports, fields and descriptions. Total time access for three complex queries in distributed processing it is 00: 03: 00 per second while in nondistributed processing it is at 00: 15: 77 per second, average is approximately five minutes per second. Quality assurance note values (T-test) is 0.239 and values (T-dis) is 1.96, as a result of dealing with scientific sets and humanities sets. In the comparison law, it can be deduced that if the t-test is smaller than the t-dis; so there is no difference between the mean of the scientific and humanities samples, the values of C.V for both scientific is (8.585) and humanities sets is (7.427), using the law of homogeneity know whether any sets are more homogeneous whenever the value of a small C.V was more homogeneous however the humanity set is more homogeneity.},
keywords={Distributed databases;Metadata;Big Data;Standards;File systems;Data integrity;Big Data;data quality;unstructured Data Distributed data file system;and statistical model.},
doi={10.1109/DeSE.2019.00072},
ISSN={2161-1351},
month={Oct},}
@INPROCEEDINGS{8258380,
author={Fu, Qian and Easton, John M.},
booktitle={2017 IEEE International Conference on Big Data (Big Data)},
title={Understanding data quality: Ensuring data quality by design in the rail industry},
year={2017},
volume={},
number={},
pages={3792-3799},
abstract={The railways worldwide are increasingly looking to the integration of their data resources coupled with advanced analytics to enhance traffic management, to provide new insights on the health of infrastructure assets, to provide soft linkages to other transport modes, and ultimately to enable them to better serve their customers. As in many industrial sectors, over the past decade the rail industry has been investing heavily in sensing technologies that record every aspect of the operation of the railway network. However, as any data scientist knows, it does not matter how good an algorithm is, if you put rubbish in, you get rubbish out; and as the traditional industry model of working with data only within the system that it was collected by becomes increasingly fragile, the industry is discovering that it knows less than it thought about the data it is gathering. When coupled with legacy data resources of unknown accuracy, such as design diagrams for assets that in many cases are decades old, the rail industry now faces a crisis in which its data may become essentially worthless due to a poor understanding of the quality of its data. This paper reports the findings of the first phase of a three-phase systematic review of literature about how data quality can be managed and evaluated in the rail domain. It begins by discussing why data quality matters in a rail context, before going on to define the quality, introduce and expand the concept of a data quality schema.},
keywords={Industries;Rails;Data models;Rail transportation;Systematics;Decision making;data quality;rail;quality by design;data quality schema},
doi={10.1109/BigData.2017.8258380},
ISSN={},
month={Dec},}
@INPROCEEDINGS{9754682,
author={Peng, Sha and Tian, Zhuxiao and Siqin, Zhuoya and Xu, Xiaomin},
booktitle={2021 IEEE 7th International Conference on Cloud Computing and Intelligent Systems (CCIS)},
title={Construction of Data Quality Evaluation Index for Manufacturing Multi-value Chain Collaborative Data Space Based on the Whole Life Cycle of Data},
year={2021},
volume={},
number={},
pages={315-323},
abstract={As a new data management model, data space can effectively manage a large amount of multi-heterogeneous dynamic data, but the construction of data space often needs to be based on accurate and scientific original data and to obtain valuable information in data, which poses a challenge to the data quality control of the whole life cycle of data, so it is especially important to evaluate the data quality. By analyzing the synergistic effect of multi-value chain in manufacturing industry and combining the dynamic system of the whole life cycle of data, the data quality evaluation index system is proposed from three aspects of data provider, data space construction and data user, combining four levels of data itself, technology, data flow layer and data management. Through the construction of AHP-TOPSIS data quality evaluation model, AHP is used to determine the index weight, TOPSIS is used to calculate the ideal solution and relative closeness degree, and the evaluation results are obtained. Through the application analysis of examples, quantitative evaluation of data quality, the construction, access and mining of multi-value chain collaborative data space can provide practical experience.},
keywords={Manufacturing industries;Data integrity;Computational modeling;Collaboration;Organizations;Aerospace electronics;Data models;manufacturing industry;whole life cycle;data space;data quality},
doi={10.1109/CCIS53392.2021.9754682},
ISSN={},
month={Nov},}
@INPROCEEDINGS{7823519,
author={Li, Tao and He, Yihai and Zhu, Chunling},
booktitle={2016 International Conference on Industrial Informatics - Computing Technology, Intelligent Technology, Industrial Information Integration (ICIICII)},
title={Big Data Oriented Macro-Quality Index Based on Customer Satisfaction Index and PLS-SEM for Manufacturing Industry},
year={2016},
volume={},
number={},
pages={181-186},
abstract={The aim of this paper was to develop a novel macro-quality index driven by big quality data regarding the macro-quality management demands of manufacturing enterprises in Industry 4.0. Firstly, the connotation of big data of macro-quality management in manufacturing industry is expounded, which is the collection of the quality data in product lifecycle including the product quality data, process quality data and organizational ability data. Secondly, referring to the customer satisfaction index theory, a new big data oriented macro-quality index computation model based on the partial least square-structural equation modeling(PLS-SEM) theory is proposed, and the partial least square(PLS) is adopted to estimate the path parameters. Finally, a case study of macro-quality situation evaluation for the manufacturing industry of a city in China is presented. The final result shows that the proposed macro-quality index model is applicable and predictable.},
keywords={Indexes;Mathematical model;Data models;Manufacturing industries;Computational modeling;Big data;Customer satisfaction;macro-quality evaluation;big data;macro-quality index;customer satisfaction index;PLS-SEM},
doi={10.1109/ICIICII.2016.0052},
ISSN={},
month={Dec},}
@INPROCEEDINGS{5228165,
author={Bing Chen and Xuchu Weng and Beizhan Wang and Xueqin Hu},
booktitle={2009 4th International Conference on Computer Science & Education},
title={Analysis and solution of data quality in data warehouse of Chinese materia medica},
year={2009},
volume={},
number={},
pages={823-827},
abstract={The data quality problem often be ignored in the process of data warehouse construction and utilization. In order to avoid the phenomenon of “garbage in, garbage out” which will influence the decision-making, the problem of data quality must be paid great attention. In this paper, we took the Chinese materia medica (Cmm) data warehouse in China Academy of Chinese Medical Sciences (CACMS) as example. We analyzed the data quality problems in the process of its construction, cited the reasons for bad data quality, and gave the key elements and their relations for data quality analysis and assessment. At the end of article, we proposed a series of assessment standards and solution scheme to improve the data quality in Cmm data warehouse, and carried out in practice to prove it.},
keywords={Data warehouses;Data mining;Computer science;Computer science education;Data analysis;Medical services;Laboratories;Data conversion;Artificial intelligence;Software quality;data warehouse;data quality;metadata},
doi={10.1109/ICCSE.2009.5228165},
ISSN={},
month={July},}
@INPROCEEDINGS{7923744,
author={Song, Yuhang and Xu, Mai and Li, Shengxi},
booktitle={2017 Data Compression Conference (DCC)},
title={Watching Videos with Certain and Constant Quality: PID-Based Quality Control Method},
year={2017},
volume={},
number={},
pages={461-461},
abstract={In video coding, compressed videos with certain and constant quality can ensure quality of experience (QoE). To this end, we propose in this paper a novel PID-based quality control (PQC) method for video coding. Specifically, a formulation is modelled to control quality of video coding with two objectives: minimizing control error and quality fluctuation. Then, we apply the Laplace domain analysis to model the relationship between quantization parameter (QP) and control error in this formulation. Given the relationship between QP and control error, we propose a solution to the PQC formulation, such that videos can be compressed at certain and constant quality. Finally, experimental results show that our PQC method is effective in both control accuracy and quality fluctuation.},
keywords={Quality control;Video coding;Videos;Data compression;Encoding;Video sequences;video coding;quality of experience;PID-based quality control},
doi={10.1109/DCC.2017.57},
ISSN={2375-0359},
month={April},}
@INPROCEEDINGS{7761054,
author={Thomas, Julie},
booktitle={OCEANS 2016 MTS/IEEE Monterey},
title={Wave data analysis and Quality Control challenges},
year={2016},
volume={},
number={},
pages={1-7},
abstract={Since its inception in 1975, quality control has been at the forefront of the Coastal Data Information Program (CDIP). During the early days, with few pressure sensor stations deployed (Imperial Beach and Ocean Beach, CA), timely Quality Assurance/Quality Control (QA/QC) checks were done manually. However, as the program grew, it became evident that in order to maintain high quality standards commensurate with real time data dissemination, it would be necessary to develop automated quality control checks designed to accept qualified data and reject sets that fail to meet minimum standards. Both categories were archived and are available, in the public domain, for further manual or machine processing and inspection.},
keywords={Quality control;Standards;Sea measurements;Real-time systems;Data analysis;Oceanography;Oceans},
doi={10.1109/OCEANS.2016.7761054},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{9590700,
author={Du, Jinming},
booktitle={2021 IEEE 4th International Conference on Information Systems and Computer Aided Education (ICISCAE)},
title={Research on the Construction of Educational Data Quality Model Based on Multiple Constraints Model},
year={2021},
volume={},
number={},
pages={363-367},
abstract={With the development of Internet and information technology, data has become an important asset related to the development prospects of society and all walks of life. At present, there are many quality problems in the use of educational data, which has brought great obstacles to exerting the value of educational data. Only by using scientific statistical methods, obtaining real, objective, comprehensive, scientific and effective basic data, and carrying out systematic and comprehensive analysis on the obtained data, can we give full play to its command and decision-making role. The development of big data and artificial intelligence technology provides new ideas for the analysis and evaluation of educational data quality, and is committed to restoring the overall picture of the education system and promoting the change of regional educational ecology. In this paper, an educational data quality analysis model based on multiple constraint model is proposed, which classifies the data in the database, divides the information in the database into several different categories according to the data characteristics, and establishes a quality management system for educational data in universities, so as to effectively improve the quality of educational data.},
keywords={Training;Analytical models;Systematics;Databases;Data integrity;Biological system modeling;Education;Educational data;Data quality;Statistics},
doi={10.1109/ICISCAE52414.2021.9590700},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{8372743,
author={Chang, Yue Shan and Lin, Kuan-Ming and Tsai, Yi-Ting and Zeng, Yu-Ren and Hung, Cheng-Xiang},
booktitle={2018 27th Wireless and Optical Communication Conference (WOCC)},
title={Big data platform for air quality analysis and prediction},
year={2018},
volume={},
number={},
pages={1-3},
abstract={With the advance of industry, air quality (AQ) is increasingly becoming worse. There are increasingly AQ monitors device have been deployed around country for monitoring air-quality all year long. To estimate and predict AQ, such as PM (particulate matter) 2.5, become an important issue for government to improve people's quality of life. As we can know, there are many factors can affect the AQ, such as traffic, factory exhaust emissions, weather, incineration of garbage, and so on. In most well-developed countries, these pollution sources are monitored for future environmental policy making. In this paper, we will propose a semantic ETL (Extract-Transform-Load) framework on cloud platform for AQ prediction. In the platform, we exploit ontology to concretize the relationship of PM 2.5 from various data sources and to merge those data with the same concept but different naming into the unified database. We implement the ETL framework on the cloud platform, which includes computing nodes and storage nodes. The computing nodes are used to execute data mining algorithms for predicting, and storage modes are used to store retrieved, preprocessed, and analyzed data. We utilize restful web service as the front end API to retrieve analyzed data, and finally we exploit browser to show the visualized result to demonstrate the estimation and prediction. It shows that the big data access framework on the cloud platform can work well for air quality analysis.},
keywords={Semantics;Big Data;Air quality;Data mining;Urban areas;Monitoring;Government;Air Quality;Big Data;Prediction;Cloud Environment},
doi={10.1109/WOCC.2018.8372743},
ISSN={2379-1276},
month={April},}
@INPROCEEDINGS{8421858,
author={Jiang, Wei and Ning, Xiuli and Xu, Yingcheng},
booktitle={2018 5th IEEE International Conference on Cyber Security and Cloud Computing (CSCloud)/2018 4th IEEE International Conference on Edge Computing and Scalable Cloud (EdgeCom)},
title={Review on Big Data Fusion Methods of Quality Inspection for Consumer Goods},
year={2018},
volume={},
number={},
pages={95-102},
abstract={Quality big data comes from a wide range, the problem is how to eliminate the structure barriers between various types of data from different sources, to achieve the effective integration for isolated or fragmented data, and then to mine the information, knowledge and wisdom according to the actual needs. It is urgent for the quality inspection departments to solve the problem of making a decision the first time and take preventive measures. This paper introduced the research status of named entity recognition, solid unified detection, data conflict resolution, data fusion method from the characteristics of quality inspection data. The existence problems are analyzed and the research direction is looking forward of the research on data fusion in this paper.},
keywords={Big Data;Inspection;Data integration;Semantics;Hidden Markov models;Thesauri;Support vector machines;Big data of quality inspection;data fusion;named entity recognition;entity resolution;data conflict resolution},
doi={10.1109/CSCloud/EdgeCom.2018.00025},
ISSN={},
month={June},}
@INPROCEEDINGS{9368701,
author={Wang, Xin and Zhao, Xinbin and Yu, Liling},
booktitle={2020 IEEE 2nd International Conference on Civil Aviation Safety and Information Technology (ICCASIT},
title={Data Mining on the Flight Quality of an Airline based on QAR Big Data},
year={2020},
volume={},
number={},
pages={955-958},
abstract={At present, the airlines have made some achievements in event analysis and investigation by using their quick access record (QAR) data. But where each airline's flight quality is in the industry, and whether there is a problem in itself, the airline can't find. In order to help airlines discover the existing flight quality problems, this article uses the QAR big data of the flight operational quality assurance (FOQA) Station of CAAC, and compares the industry-wide QAR data with the QAR data of individual airlines, and founds that the take-off pitch angle of a certain aircraft of A321 models is too small, by using mathematical statistics t test to verify, found the airline's the take-off pitch angle and the industry's the take-off pitch angle exist significant difference. The correlative speed at rotation and the speed at liftoff are also analyzed, and the significant difference is found. The FOQA Station of CAAC feeds back the problem to the airline and the authority. After the investigation of the airline and the authority, there are problems with the airline. And the airline immediately starts to rectify it.},
keywords={Quality assurance;Atmospheric modeling;Big Data;Data models;Time measurement;Safety;Mathematical model;flight quality;pitch;QAR data;normal distribution;t test},
doi={10.1109/ICCASIT50869.2020.9368701},
ISSN={},
month={Oct},}
@INPROCEEDINGS{8996332,
author={Wang, Jinghan and Zhang, Jinnan and Yuan, XueGuang and Tang, Yu and Hao, Hongyu and Zuo, Yong and Tan, Zebin and Qiao, Min and Cao, Yang Hua and Ai, Lingmei and Wan, Yihang and Chen, Hao},
booktitle={2019 Chinese Automation Congress (CAC)},
title={Air quality data analysis and forecasting platform based on big data},
year={2019},
volume={},
number={},
pages={2042-2046},
abstract={Nowadays, with the continuous development of big data technology, various industries use big data technology to process and mine massive data, and realize the value of data efficiently. In terms of air quality data processing, big data technology can also play a certain advantage. The platform is based on big data technology to design an air quality data analysis and prediction platform including data layer, business layer, interaction layer and visualization platform. Data is cleaned, calibrated, and stored in the data layer to ensure data consistency, integrity, and security. The air quality data is analyzed and predicted at the business layer. The interaction layer includes the functions of algorithm management, data query, and the data visualization platform provides intuitive information display. This design is a significant application for fully exploiting environmental data information. It has powerful data processing functions and scalability, which is a reliable data analysis and prediction platform.},
keywords={Air quality;Big Data;Data mining;Photonics;Optical fiber communication;Telecommunications;Clustering algorithms;Air quality;Big data;Data mining;Data visualization},
doi={10.1109/CAC48633.2019.8996332},
ISSN={2688-0938},
month={Nov},}
@INPROCEEDINGS{9888215,
author={Han, Wei and Wu, Shenggang and Liu, Qiang and Cai, Jianzhen},
booktitle={2022 International Conference on Computation, Big-Data and Engineering (ICCBE)},
title={Research and Design of Construction Engineering Quality Management System Based on Big Data and BIM Technology},
year={2022},
volume={},
number={},
pages={172-176},
abstract={With the rapid development of information technologies such as big data, cloud computing, the Internet of Things, and 5G in recent years, the construction industry, as the traditional industry, urgently requests the transformation of its project management from extensive and low-efficiency mode to high-quality development mode. Under the background of the gradual development of newly-emerged technologies and concepts such as BIM technology, smart construction, and digital twin and based on the full investigation of the quality management requirements of construction enterprises, closed-loop management is focused on the whole process of construction engineering. From the perspective of construction engineering quality management, current theoretical tools such as the Internet of Things, big data, and BIM technology are combined. Taking Python as a basis, the mature and open-source WEB framework, database, and front-end and back-end technologies are used to design and construct a set of construction engineering quality management systems. We explore the digital potential of construction engineering quality management systems to provide a new way of thinking for the informatization, systematization, and system process-oriented development of construction engineering quality management.},
keywords={Cloud computing;5G mobile communication;Project management;Transforms;Big Data;Digital twins;Internet of Things;Big data;BIM;quality management},
doi={10.1109/ICCBE56101.2022.9888215},
ISSN={},
month={May},}
@INPROCEEDINGS{9524049,
author={Yachen, Wang},
booktitle={2020 International Conference on Robots & Intelligent System (ICRIS)},
title={Quality Assurance Scheme for Undergraduate Teaching Evaluation Based on Data Mining},
year={2020},
volume={},
number={},
pages={442-445},
abstract={To solve the problems existing in the process of teaching quality evaluation and to improve the accuracy of undergraduate teaching quality evaluation, a teaching quality evaluation model based on data mining algorithm is designed. Aiming at the factors such as the large amount of data to be mined and the quality of mining which is easily affected, the improved Apriori algorithm based on partition is applied in the model. Through the process of data collection and data preprocessing, the database suitable for association rule mining is established. Then we can find out which key factors can affect the teaching quality according to the analysis of association rules, so as to provide a strong basis for teaching decision-making and management. The results show that the data mining algorithm can describe the differences between the teaching quality grades of colleges, and acquire high-precision evaluation results of university teaching quality. Moreover, the error of teaching quality evaluation in colleges is far less than that of the current typical teaching quality evaluation methods.},
keywords={Quality assurance;Databases;Education;Decision making;Data preprocessing;Data collection;Data models;teaching evaluation;Apriori;ASP.net;data mining;WICA},
doi={10.1109/ICRIS52159.2020.00114},
ISSN={},
month={Nov},}
@INPROCEEDINGS{8730858,
author={Benabbas, Aboubakr and Nicklas, Daniela},
booktitle={2019 IEEE International Conference on Pervasive Computing and Communications Workshops (PerCom Workshops)},
title={Quality-Aware Sensor Data Stream Management in a Living Lab Environment},
year={2019},
volume={},
number={},
pages={445-446},
abstract={Sensor data is error-prone. Developers of pervasive applications must take the limitations of sensors into account when processing the data. To relieve the developers from the task of data cleaning and quality monitoring, we need a set of tools to model sensor data quality and to integrate the quality information into the stream data processing. In this dissertation, the goal is to provide a framework of tools to semi-automatically generate sensor models and stream processing queries for sensors with quality and context information for a quality-aware data stream processing.},
keywords={Data integrity;Computational modeling;Data models;Tools;Pervasive computing;Monitoring;Quality of service;data stream processing;data quality;sensors;context},
doi={10.1109/PERCOMW.2019.8730858},
ISSN={},
month={March},}
@INPROCEEDINGS{5070851,
author={Yan-fang, Yue and Rui-gang, Zhang and Guang, Yang and Guang-le, Ge},
booktitle={2009 Sixth International Conference on Information Technology: New Generations},
title={Application of COMERO Data Collecting in Quality Management System},
year={2009},
volume={},
number={},
pages={1567-1567},
abstract={Summary form only given. Exact and real-time collection of quality data from manufacturing process is the most important step in quality management system. It is the foundation of cumulating, analyzing and effectively controlling the processpsilas quality. As an advanced and exact metrical instrument, 3-degree coordinate measuring machining (CMM) has been widely used in machinery manufacturing industry. However, it could only use text file to record the measuring result, which can not be integrated with the quality management system. This paper introduces appropriative interface software, which can automatically read the measurement data from 3-degree CMMpsilas RTF file and can automatically transfer the certain data to database and then let quality management system directly transfer the measurement result in order to analyze the correlative accessorypsilas quality. This analysis is useful to understand the characteristic of random error system, the trend of error, the order of error distributing and the reasons for error and also form the quality report and early warning information for quality control. Meanwhile, informationpsilas automatically transmission can obviously improve working efficiency and effectively avoid the artificial errors. Heterogeneous datapsilas conversion is the key point of system integration. This paper comprehensively analysis the expression mode of every dimension and error in the measurement text files mentioned above, and then applies certain program. In order to understand the meaning of relevant data, key words are needed. Using character string in the measurement files, which match the regular expression and the method of exhaustion, we could get the key words. Then the data, which quality management system needs, is formed by transferring data types with homonymous matching and written into the database. In order to achieve the informationpsilas automatically transition, this paper also studies the data synchronization mechanism and ensure the measurement data can be synchronously updated in the quality management system by setting the trigger rule.},
keywords={Quality management;Error correction;Coordinate measuring machines;Software measurement;Databases;Real time systems;Manufacturing processes;Process control;Instruments;Machining;quality management;data collection;COMERO},
doi={10.1109/ITNG.2009.167},
ISSN={},
month={April},}
@INPROCEEDINGS{4548030,
author={Elphick, Sean and Gosbell, Vic},
booktitle={2007 Australasian Universities Power Engineering Conference},
title={The variation of power quality indices due to data analysis procedure},
year={2007},
volume={},
number={},
pages={1-7},
abstract={Power quality data is often reported using statistical confidence levels. This will exclude the most extreme data for a certain length of time depending on the interval over which the confidence level is applied. There is considerable conjecture as to the effect of applying statistical measures over different time intervals, e.g. several days, weeks or one year. If statistical confidence levels are applied over long intervals, the length of time not included in the statistical confidence interval is long. During such intervals disturbance levels may be continuously high and not be accounted for in the statistical parameter. This study investigates the effect different methods of aggregating data to a specific reporting period will have on the calculated index. Several data processing methods are trialled to evaluate the effect of using different aggregation intervals to produce an index to characterise disturbance levels for the whole year.},
keywords={Power quality;Data analysis;Monitoring;Power system harmonics;Voltage fluctuations;Power measurement;Aggregates;Instruments;Time measurement;Data processing;Power Quality;Power Quality Indices},
doi={10.1109/AUPEC.2007.4548030},
ISSN={},
month={Dec},}
@INPROCEEDINGS{9172875,
author={Zhang, Guobao},
booktitle={2020 IEEE Fifth International Conference on Data Science in Cyberspace (DSC)},
title={A data traceability method to improve data quality in a big data environment},
year={2020},
volume={},
number={},
pages={290-294},
abstract={In the actual project of data sharing and data governance, the problems of data heterogeneity and low data quality have not been solved. Data quality detection based on syntax rules can effectively find data quality problems, but it had not improved the data quality, especially in a variety of heterogeneous data source environments. This paper designs a data governance model based on data traceability, and we can get data feedback and revision through this model. The proposed method considers the different ownership of data and may form a closed-loop data service chain including effective data validation, data tracking, and data revision, data release. The analysis of the case shows that the method is effective to improve the data quality and meet the requirements of data security also.},
keywords={Data Governance;Data Credibility;Data Traceability},
doi={10.1109/DSC50466.2020.00051},
ISSN={},
month={July},}
@INPROCEEDINGS{7821627,
author={Wickramage, Narada},
booktitle={2016 Future Technologies Conference (FTC)},
title={Quality assurance for data science: Making data science more scientific through engaging scientific method},
year={2016},
volume={},
number={},
pages={307-309},
abstract={Credibility of science is fundamentally due to the strenuous efforts made to verify the general consistency among relevant facts, theories, applications, research methodologies, etc. and scientific method which emphasizes the significance of continuously building and testing hypotheses has withstood the test of time as a successful methodology of acquiring a body of knowledge, we can rely on, at least within a certain context. A paradigm based on composition of data rich services to gather data to replicate real world scenarios through complexity science based simulators, where quality of data as well as theories explaining them is primarily assured via building and testing of hypotheses, can improve our understanding of what we try to comprehend by engaging data science. While simulators would at least partially automate the implementation of scientific method, a credibility ranking mechanism, would not only help determining and disseminating rankings pertaining to the quality of data as well as theories explaining them but also receive & publish feedback regarding rankings. Including methods used in complex system analysis as part of simulators would enhance the scientific rigor of establishing the credibility of knowledge we have. Providing simulation as a service and making graphical hypothesis builders & testers available for external parties (sometimes even members of general public) would democratice the process of ascertaining the believability of data & associated theoretical models thereby further enhancing the Quality of Knowledge.},
keywords={Data science;Data models;Reservoirs;Complexity theory;Complex systems;Testing;Analytical models;Quality Assurance;Data Science;Composition of Services;Category Theory;Complex Systems;Formal Modelling;Simulation;Complexity Science;Scientific Method;Simulation-as-a-Service;GUI based hypotheses builders and testers;QA4DS;Quality of Knowledge;Crowd Data Science},
doi={10.1109/FTC.2016.7821627},
ISSN={},
month={Dec},}
@INPROCEEDINGS{1151615,
author={Perlroth, I. and Hamilton, D.},
booktitle={OCEANS 81},
title={Quality Control Procedures in Processing Oceanographic Data},
year={1981},
volume={},
number={},
pages={261-263},
abstract={The National Oceanographic Data Center (NODC) receives significant volumes of oceanographic environmental data for processing. These data vary from the conventional ocean station data and expendable bathythermograph data to a wide variety of biological and pollution data. The need for a more objective environmental quality control became evident as the volume of data continued to increase. At the same time, the resources-enriched computer capability and large climatological files of fully processed data-for developing such a system became available. The NODC procedures, prior to the development of environmental models, were based primarily on the initial quality control by data donors and subjective analysis by NODC oceanographers. This study describes the use of ocean models for quality controlling ocean serial and XBT data.},
keywords={Quality control;Oceanographic techniques;Data analysis;Ocean temperature;Sea measurements;Pollution measurement;Marine pollution;Biology computing;Temperature measurement;Temperature dependence},
doi={10.1109/OCEANS.1981.1151615},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{8695373,
author={Zuo, Yiming and Vernica, Rares and Lei, Yang and Barcelo, Steven and Rogacs, Anita},
booktitle={2019 IEEE Conference on Multimedia Information Processing and Retrieval (MIPR)},
title={A Big Data Platform for Surface Enhanced Raman Spectroscopy Data with an Application on Image-Based Sensor Quality Control},
year={2019},
volume={},
number={},
pages={463-466},
abstract={Surface-enhanced Raman spectroscopy (SERS) significantly enhances the Raman scattering by molecules, enabling detection and identification of small quantities of relevant bio-/chemical markers in a wide range of applications. In this paper, we present a big data platform with both a local client and cloud server built for acquiring, processing, visualizing and storing SERS sensor data. The local client controls the hardware (i.e., spectrometer and stage) to collect SERS spectra from HP designed sensors, and offers the options to analyze, visualize and save the spectra with meta-data records, including relevant experimental conditions. The cloud server contains remote databases and web interface for centralized data management to users from different locations. Here we describe how this platform was built and demonstrate its use for automated sensor quality control based on sensor images. Sensor quality control is a common practice, employed in sensor production to select high performing sensors. Image-based approach is a natural way to perform sensor quality control without destructing the sensors. Automating this process using the proposed platform can also reduce the time spent and achieve consistent result by avoiding human visual inspection.},
keywords={Quality control;Servers;Cloud computing;Big Data;Visualization;Inspection;Databases;Big data platform, Surface Enhanced Raman Spectroscopy, sensor quality control},
doi={10.1109/MIPR.2019.00093},
ISSN={},
month={March},}
@INPROCEEDINGS{9664881,
author={Setiadi, Yusuf and Hidayanto, Achmad Nizar and Rachmawati, Fitri and Yohannes, Adhi Yuniarto Laurentius},
booktitle={2021 IEEE 7th International Conference on Computing, Engineering and Design (ICCED)},
title={Data Quality Management Maturity Model : A Case Study in Higher Education’s Human Resource Department},
year={2021},
volume={},
number={},
pages={1-5},
abstract={Data has increasingly become more imperative in organization’s decision-making process. Low data quality can cause extensive organizational problems, such as inaccurate decision-making and dropped business possibilities. This is because low-quality data does not present a clear description of the actual situation. In Human Resource (HR) management, low data quality can cause recruitment, career development, remuneration, and retirement processes. Therefore, proper data quality management must be implemented to produce data that suits the organization's needs. To determine how far the implementation of data quality management in the organization, measurement of the maturity level in data quality management is conducted. This study presented an evaluation of data quality management maturity level in HR of higher education, applying the Loshin data quality management maturity framework. The results of this study indicate that the maturity level in the Data quality expectations area is 2.17, the maturity level in the Data quality dimensions area is 2.16, the maturity level in the Policies area is 1.22, the maturity level in the Data quality protocols area is 2, 11, the maturity level in the Data governance area is 1.77, the maturity level in the Data standards area is 1.67, the maturity level in the Technology area is 1.44, and the maturity level in the Performance management area is 1.67. The result shows that the Policies area is the lowest due to the lack of regulations and good documentation regarding data management. It can be a concern in conducting evaluations for improving data quality management.},
keywords={Data integrity;Standards organizations;Decision making;Organizations;Documentation;Regulation;Data models;data quality;data quality management;data quality maturity model;information system},
doi={10.1109/ICCED53389.2021.9664881},
ISSN={2767-7826},
month={Aug},}
@INPROCEEDINGS{5347884,
author={Zhang, Dahai and Bi, Yanqiu and Zhao, Jianguo},
booktitle={2009 International Conference on Sustainable Power Generation and Supply},
title={A new data compression algorithm for power quality online monitoring},
year={2009},
volume={},
number={},
pages={1-4},
abstract={The increasing application of power quality data acquisition devices produces large amount of data that should be compressed. The paper investigates the characteristics of power quality data and delta modulation technique, and it proposes a lossless power data compression algorithm based on high-order delta modulation. The compression algorithm carries on multiple differential operations on power quality data, so it could reduce the magnitude of data and requires fewer bits for coding. The proposed algorithm has high compression ratio and little computation requirements. Furthermore, it is suitable for dealing with the power quality data measured at high sampling frequency, and it can work well with traditional compression methods such as Huffman coding. Simulations for several kinds of power quality data confirm its effectiveness and advantages over traditional Huffman coding method.},
keywords={Data compression;Power quality;Monitoring;Delta modulation;Huffman coding;Data acquisition;Compression algorithms;Power measurement;Frequency measurement;Sampling methods;data compression;delta modulation;Huffman coding;power quality;power system},
doi={10.1109/SUPERGEN.2009.5347884},
ISSN={2156-969X},
month={April},}
@INPROCEEDINGS{4813519,
author={Shahriar, Md. Sumon and Anam, Sarawat},
booktitle={2008 Second International Conference on Future Generation Communication and Networking Symposia},
title={Quality Data for Data Mining and Data Mining for Quality Data: A Constraint Based Approach in XML},
year={2008},
volume={2},
number={},
pages={46-49},
abstract={As quality data is important for data mining, reversely data mining is necessary to measure the quality of data. Specifically, in XML, the issue of quality data for mining purposes and also using data mining techniques for quality measures is becoming more necessary as a massive amount of data is being stored and represented over the Web. We propose two important interrelated issues: how quality XML data is useful for data mining in XML and how data mining in XML is used to measure the quality data for XML. When we address both issues, we consider XML constraints because constraints in XML can be used for quality measurement in XML data and also for finding some important patterns and association rules in XML data mining. We note that XML constraints can play an important role for data quality and data mining in XML. We address the theoretical framework rather than solutions. Our research framework is towards the broader task of data mining and data quality for XML data integrations.},
keywords={Data mining;XML;Conferences;Australia;Data engineering;Association rules;Databases;Proposals;XML;DATA QUALITY;DATA MINING;CONSTRAINTS IN XML},
doi={10.1109/FGCNS.2008.74},
ISSN={},
month={Dec},}
@ARTICLE{7299603,
author={Immonen, Anne and Pääkkönen, Pekka and Ovaska, Eila},
journal={IEEE Access},
title={Evaluating the Quality of Social Media Data in Big Data Architecture},
year={2015},
volume={3},
number={},
pages={2028-2043},
abstract={The use of freely available online data is rapidly increasing, as companies have detected the possibilities and the value of these data in their businesses. In particular, data from social media are seen as interesting as they can, when properly treated, assist in achieving customer insight into business decision making. However, the unstructured and uncertain nature of this kind of big data presents a new kind of challenge: how to evaluate the quality of data and manage the value of data within a big data architecture? This paper contributes to addressing this challenge by introducing a new architectural solution to evaluate and manage the quality of social media data in each processing phase of the big data pipeline. The proposed solution improves business decision making by providing real-time, validated data for the user. The solution is validated with an industrial case example, in which the customer insight is extracted from social media data in order to determine the customer satisfaction regarding the quality of a product.},
keywords={Big data;Social network services;Computer architecture;Meta data;Online services;architecture;big data;metadata;quality attribute;quality of data;Architecture;big data;metadata;quality attribute;quality of data},
doi={10.1109/ACCESS.2015.2490723},
ISSN={2169-3536},
month={},}
@INPROCEEDINGS{9005614,
author={Patel, Jayesh},
booktitle={2019 IEEE International Conference on Big Data (Big Data)},
title={An Effective and Scalable Data Modeling for Enterprise Big Data Platform},
year={2019},
volume={},
number={},
pages={2691-2697},
abstract={The enormous growth of the internet, enterprise applications, social media, and IoT devices in the current time caused a huge spike in enterprise data growth. Big data platform provided scalable storage to manage enterprise data growth and served easier data access to decision-makers, stakeholders and business users. It is a well-known challenge to classify, organize and store all this data and process it to provide business insights. Due to nature, variety, velocity, volume and value of data make it difficult to effectively process big data. Enterprises face challenges to apply complex business rules, to generate insights and to support data-driven decisions in a timely fashion. As big data lake integrates streams of data from a bunch of business units, stakeholders usually analyze enterprise-wide data from various data models. Data models are a vital component of Big data platform. Users may do complex processing, run queries and perform big table joins to generate required metrics depending on the available data models. It is usually a time consuming and resource-intensive process to find the value from data. It is a no-brainer that big data platform in the enterprise needs high-quality data modeling methods to reach an optimal mix of cost, performance, and quality. This paper addresses these challenges by proposing an effective and scalable way to organize and store data in Big Data Lake. It presents some of the basic principles and methodology to build scalable data models in a distributed environment. It also describes how it overcomes common challenges and presents findings.},
keywords={Data models;Big Data;Business;Analytical models;Computational modeling;Lakes;Solid modeling;Big Data;Big Data Lake;Scalable Data Modeling;Hadoop;Spark;Business Intelligence;Big Data Analytics},
doi={10.1109/BigData47090.2019.9005614},
ISSN={},
month={Dec},}
@INPROCEEDINGS{7822785,
author={Gyu-tae Kim and Yongkang Kim and Min-Seok Kwon and Park, Taesung},
booktitle={2016 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)},
title={Quality control plot for high dimensional omics data},
year={2016},
volume={},
number={},
pages={1763-1766},
abstract={Quality control (QC) becomes more important in pre-processing analysis of high dimensional omics data. Several routine QC processes became a standard process in omics data analysis. The standard QC analysis includes calculating quality-related measures, checking the consistency among samples, detecting outlying observations and so forth. QC analysis tends to be more important in the era of high dimensional omics data. Although several QC analysis tools providing simple graphical display have been developed by many researchers, they usually require a subjective decision on QC. Here, we propose high-dimensional data quality control (HidQC) plot which is a simple and efficient QC tool for handling high dimensional omics data. HidQC plot primarily focuses on identifying samples of poor quality by conducting a contrast analysis for the between/within group distances and the summary distances. HidQC plot checks the quality by investigating the consistency of samples for each group. Unlike other QC plots, HidQC plot provides the p-value of each sample based on the permutation test, which can be used as a more objective criterion to determine whether to use the sample or not. We applied HidQC plot to MicroArray Quality Control (MAQC) project 1 data to demonstrate its usefulness.},
keywords={Quality control;Quality Control (QC);Microarray;Omics data;MicroArray Quality Control(MAQC) project;High-dimensional data quality control (HidQC) plot},
doi={10.1109/BIBM.2016.7822785},
ISSN={},
month={Dec},}
@INPROCEEDINGS{6758125,
author={Zhang Rui and Yao Hong-jiao and Zhang Chuan-guang},
booktitle={Proceedings of 2013 2nd International Conference on Measurement, Information and Control},
title={Compression method of power quality data based on wavelet transform},
year={2013},
volume={02},
number={},
pages={987-990},
abstract={The demand for data compression has increased considerably due to the huge amount of the power quality data detected. Aiming at the problem the current power quality data compression methods are short of considering the correlation of the three-phase power quality data, a novel approach of data compression for three-phase power quality based on wavelet transform is presented in the paper. The simulation results show that the proposed method can not only get high signal noise ratio relative to the existing methods of the power quality data compression under the same compression ratio, but also control the power quality compression performance according to the practical requirement flexibly.},
keywords={Power quality;Wavelet transforms;Data compression;Voltage fluctuations;Interrupters;Educational institutions;power power quality;dq0 transform;wavelet transform;data compression},
doi={10.1109/MIC.2013.6758125},
ISSN={},
month={Aug},}
@INPROCEEDINGS{6204987,
author={Sidi, Fatimah and Ramli, Abdullah and Jabar, Marzanah A. and Lilly Suriani Affendey and Mustapha, Aida and Ibrahim, Hamidah},
booktitle={2012 International Conference on Information Retrieval & Knowledge Management},
title={Data quality comparative model for data warehouse},
year={2012},
volume={},
number={},
pages={268-272},
abstract={The growth of daily data and complexity in data warehouse with enhanced the information technology has created new challenges for information user. The demand for quality data has increase an awareness of the quality, reliable and accuracy of information in making fast and reliable decision-making. Nowadays, many organizations are depending on their resources in data warehouse. As that matter of fact, the qualities of data warehouse are greatly concern. The poor and error data will cause more trouble in data warehouse as data accessed from the same resources by the user. Here we present the systematic review comparative model to determine the data quality model as further research in our studies.},
keywords={Data models;Data warehouses;Accuracy;Quality management;Organizations;Complexity theory;data quality;data warehouses;data quality management;data quality dimension},
doi={10.1109/InfRKM.2012.6204987},
ISSN={},
month={March},}
@INPROCEEDINGS{9510379,
author={Qu, Xiaoyu and Dong, Kun and Zhao, Jianfeng and Liu, Weicheng and Shi, Zhan and Yu, Yue},
booktitle={2021 IEEE 4th International Electrical and Energy Conference (CIEEC)},
title={A novel identification and location method for transient power quality disturbance sources},
year={2021},
volume={},
number={},
pages={1-6},
abstract={The application of power electronics and high penetration of new energy generation have brought great economic and social benefits. Meanwhile, new power quality phenomena and new issues have come into existence, such as three-phase unbalance, harmonic and low-frequency resonance, etc. In order to solve power quality problems fundamentally. The paper proposes a novel identifying and locating method utilizing transient power quality data. Based on the operational big data of power system, spatio-temporal data model of power quality is established. Then, based on big data, by adopting random forest algorithm, the data is analyzed to identify and locate transient power quality problem disturbance sources. This paper simulates in IEEE 30 bus system, and the results indicate the accuracy in identifying and locating short circuit disturbance sources.},
keywords={Fault diagnosis;Power quality;Forestry;Big Data;Power system harmonics;Harmonic analysis;Data models;transient power quality problem;power quality big data;random forest;data-driven},
doi={10.1109/CIEEC50170.2021.9510379},
ISSN={},
month={May},}
@INPROCEEDINGS{7944943,
author={Kang, Gaganjot and Gao, Jerry Zeyu and Xie, Gang},
booktitle={2017 IEEE Third International Conference on Big Data Computing Service and Applications (BigDataService)},
title={Data-Driven Water Quality Analysis and Prediction: A Survey},
year={2017},
volume={},
number={},
pages={224-232},
abstract={Water quality becomes one of the important quality factors for the quality life in smart cities. Recently, water quality has been degraded due to diverse forms of pollution caused by disposal of human wastes, industrial wastes, automobile wastes. The increasing pollution affects water quality and the quality of people's life. Hence, water quality evaluation, monitoring, and prediction become an important and hot research subject. In the past, many environmental researchers have dedicated their research efforts on this subject using conventional approaches. Recently, many researchers begin to use the big data analytics approach to studying, evaluating, and predicting water quality due to the advances of big data applications and the availability of environmental sensing networks and sensor data. This paper reviews the published research results relating to water quality evaluation and prediction. Moreover, the paper classifies and compares the applied big data analytics approaches and big data based prediction models for water quality assessment. Furthermore, the paper also discusses the future research needs and challenges.},
keywords={Water pollution;Water resources;Big Data;Analytical models;Data models;Water;Indexes;Water quality evaluation;big data analytics;data-driven water quality evaluation;and water quality prediction},
doi={10.1109/BigDataService.2017.40},
ISSN={},
month={April},}
@INPROCEEDINGS{8332626,
author={Xia, Wenze and Xu, Zhuoming and Mao, Chengwang},
booktitle={2017 14th Web Information Systems and Applications Conference (WISA)},
title={User-Driven Filtering and Ranking of Topical Datasets Based on Overall Data Quality},
year={2017},
volume={},
number={},
pages={257-262},
abstract={Finding relevant and high-quality data is the eternal needs for data consumers (i.e., users). Many open data portals have been providing users with simple ways of finding datasets on a particular topic (i.e., topical datasets), which are not a way of filtering and ranking topical datasets based on data quality. Despite the recent advances in the development and standardization of data quality models and vocabulary, there is a lack of systematic research on approaches and tools for user-driven data quality-based filtering and ranking of topical datasets. In this paper we address the problem of user-driven filtering and ranking of topical datasets based on the overall data quality of datasets by developing a generic software architecture and the corresponding approach, called ODQFiRD, for filtering and ranking topical datasets according to user-specified data quality assessment criteria. Additionally, we use our implemented prototype of ODQFiRD to conduct a case study experiment on the U.S. Government's open data portal. The prototype implementation and experimental results show that our proposed ODQFiRD is achievable and effective.},
keywords={Data integrity;Measurement;Filtering;Data models;Portals;Quality assessment;Tools;data quality-based filtering and ranking;topical datasets;overall data quality;data quality assessment;Data Quality Vocabulary (DQV);open data portal},
doi={10.1109/WISA.2017.24},
ISSN={},
month={Nov},}
@INPROCEEDINGS{9798931,
author={Mohammed, Mahmood and Talburt, John R. and Dagtas, Serhan and Hollingsworth, Melissa},
booktitle={2021 International Conference on Computational Science and Computational Intelligence (CSCI)},
title={A Zero Trust Model Based Framework For Data Quality Assessment},
year={2021},
volume={},
number={},
pages={305-307},
abstract={Zero trust security model has been picking up adoption in various organizations due to its various advantages. Data quality is still one of the fundamental challenges in data curation in many organizations where data consumers don’t trust data due to associated quality issues. As a result, there is a lack of confidence in making business decisions based on data. We design a model based on the zero trust security model to demonstrate how the trust of data consumers can be established. We present a sample application to distinguish the traditional approach from the zero trust based data quality framework.},
keywords={Industries;Costs;Scientific computing;Data integrity;Computational modeling;Organizations;Data models;Data quality assessment;Zero Trust;Data Quality;Data quality dimensions;Trusted Data},
doi={10.1109/CSCI54926.2021.00123},
ISSN={},
month={Dec},}
@INPROCEEDINGS{9322931,
author={Mandrakov, Egor S. and Vasiliev, Victor A. and Dudina, Diana A.},
booktitle={2020 International Conference Quality Management, Transport and Information Security, Information Technologies (IT&QM&IS)},
title={Non-conforming Products Management in a Digital Quality Management System},
year={2020},
volume={},
number={},
pages={266-268},
abstract={This article addresses the issue of changes in the quality management system, with the digitalization of the company. More specifically, changes regarding the process of managing non-conforming products. The article reflects how the use of digital tools and new technologies affects the process of detecting non-conformities and how the procedure for working with non-conforming products changes: how is the collection of data on the characteristics of the facility, what decisions need to be made regarding the inconsistencies, how the structure of work with non-conformances is changing and to what extent the use of information tools is beneficial.},
keywords={Quality management;Information technology;Neural networks;Tools;Information security;Big Data;Standards organizations;quality management system;digital transformation;digital environment;non-conforming products;big data;neural networks;internet of things},
doi={10.1109/ITQMIS51053.2020.9322931},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{8859426,
author={He, Tieke and Chen, Shenghao and Hao, Lian and Liu, Jia},
booktitle={2019 IEEE 19th International Conference on Software Quality, Reliability and Security Companion (QRS-C)},
title={Quality Driven Judicial Data Governance},
year={2019},
volume={},
number={},
pages={66-70},
abstract={With the development of Smart Court 3.0, the amount of judicial data that can be stored and processed by the computer is increasing rapidly. People gradually realize that judicial data contains tremendous social and business value. However, we need stronger ability to handle with and apply massive, multi-source and heterogeneous judicial data. A complete data governance system should be built in order to make full use of the value of data assets. In such a data governance system, data quality control is one of the key steps of data governance, and also the bottleneck of data service development, because data quality determines the upper limit of data application. This paper proposes a judicial data quality measurement framework by analyzing some judicial business data, followed by a data governance method driven by it.},
keywords={Data integrity;Big Data;Decision making;Organizations;Standards organizations;data quality;judicial data governance;quality measurement},
doi={10.1109/QRS-C.2019.00026},
ISSN={},
month={July},}
@INPROCEEDINGS{7502278,
author={Shanmugam, Srinivasan and Seshadri, Gokul},
booktitle={2016 IEEE 2nd International Conference on Big Data Security on Cloud (BigDataSecurity), IEEE International Conference on High Performance and Smart Computing (HPSC), and IEEE International Conference on Intelligent Data and Security (IDS)},
title={Aspects of Data Cataloguing for Enterprise Data Platforms},
year={2016},
volume={},
number={},
pages={134-139},
abstract={As the adoption of enterprise Big Data platforms mature, the necessity to maintain systematic catalogue of data being processed and managed by these platforms becomes imperative. Enterprise data catalogues serve as centralized repositories of storing such metadata about data being handled by such platforms, enabling better data governance, security and control. Standardized approaches, methodologies and tools for data catalogues are being discussed and evolved. This paper introduces some key variables, attributes and indexes that need to be handled in such cataloguing solutions -- such as data contexts, data-system relationships, data quality, reliability, sensitivity and accessibility. The paper also discusses specific approaches on how each of these aspects can be adopted and applied to different enterprise contexts effectively.},
keywords={Context;Business;Metadata;Big data;Electronic mail;Reliability;Indexes;Enterprise data;Data catalogue;Metadata of data;Data context;Data quality;Data governance;Data as a service},
doi={10.1109/BigDataSecurity-HPSC-IDS.2016.52},
ISSN={},
month={April},}
@INPROCEEDINGS{9150244,
author={Zhang, Lanlan and Zou, Du},
booktitle={2020 International Conference on Big Data and Informatization Education (ICBDIE)},
title={Product quality prediction of rolling mill in big data environment},
year={2020},
volume={},
number={},
pages={34-38},
abstract={With the wide use of rolling mill in iron and steel industry, the quality of rolling mill products has become the primary goal of people. However, due to design defects and manufacturing quality problems, the quality of steel products is seriously affected, and the surface roughness and thickness of steel plate are important quality indicators. In this paper, by analyzing a large number of monitoring data of rolling mill condition and using BP neural network model [1], the discrete system model between monitoring data and “surface roughness” and “thickness error” of rolling steel plate is further established.},
keywords={Data models;Neural networks;Analytical models;Predictive models;Rough surfaces;Surface roughness;Mathematical model;Product quality;BP neural network;data analysis;surface roughness;thickness error},
doi={10.1109/ICBDIE50010.2020.00015},
ISSN={},
month={April},}
@INPROCEEDINGS{9660957,
author={Yuan, Tangxiao and Adjallah, Kondo Hloindo and Sava, Alexandre and Wang, Huifen and Liu, Linyan},
booktitle={2021 11th IEEE International Conference on Intelligent Data Acquisition and Advanced Computing Systems: Technology and Applications (IDAACS)},
title={Issues of Intelligent Data Acquisition and Quality for Manufacturing Decision-Support in an Industry 4.0 Context},
year={2021},
volume={2},
number={},
pages={1200-1205},
abstract={Data quality plays an essential role in decision-making, as the latter may incorporate some risks in different application areas. In the context of industry 4.0, the amount, the versatility, and the speed of information flow for decision-making are important issues. The quality and, in particular, the dependability of data is paramount. This paper investigates the leading data quality characteristics in the industry 4.0 environment with the related issues due to various interactions. It proposes a taxonomy of data sources and flows, from acquisition to the information extraction level for decision-making. The authors highlight the specific issues of error and uncertainty propagation management as significant research challenges for designing intelligent data collection and acquisition systems for industrial manufacturing decision support within the framework of the new generation of industry development. They review data quality characteristics definition and assessment requirements, and suggest classifying these characteristics into four categories. Data quality assessment methods and models with relation to decision-making are also examined. They willfully left aside the investigation of data quality improvement processes for a future more detailed paper.},
keywords={Measurement;Uncertainty;Systematics;Data integrity;Soft sensors;Decision making;Data acquisition;data quality;data source;data acquisition;manufacturing;decision-making;review},
doi={10.1109/IDAACS53288.2021.9660957},
ISSN={2770-4254},
month={Sep.},}
@INPROCEEDINGS{8964872,
author={Song, Jinyu and Hao, Jiandong and Gang, Chen and Suojuan, Zhang and Yiping, Guo},
booktitle={2019 10th International Conference on Information Technology in Medicine and Education (ITME)},
title={Design and Implementation of a Universal Data Quality Management Software Based on Data Flow},
year={2019},
volume={},
number={},
pages={645-648},
abstract={Data quality problems are analyzed to get several typical problems, such as data missing, data duplication, data abnormality, data inconsistency and data logic error. In order to resolve these problems, a universal data quality management software is proposed. This software provides data cleaning method for each problem and evaluates the effect of these methods, and manages the plug-in components for use. The architecture and critical technologies are introduced in detail, and main steps are shown with a specific application. According to the feedback of users, this new software is powerful, adaptable, simple to use, easy to set processes, with high data cleaning efficiency and accurate effect evaluations.},
keywords={Cleaning;Data integrity;Filling;Microsoft Windows;Data mining;Instruction sets;data flow;data quality;data cleaning;cleaning evaluation},
doi={10.1109/ITME.2019.00149},
ISSN={2474-3828},
month={Aug},}
@INPROCEEDINGS{7374131,
author={Juddoo, Suraj},
booktitle={2015 International Conference on Computing, Communication and Security (ICCCS)},
title={Overview of data quality challenges in the context of Big Data},
year={2015},
volume={},
number={},
pages={1-9},
abstract={Data quality management systems are thoroughly researched topics and have resulted in many tools and techniques developed by both academia and industry. However, the advent of Big Data might pose some serious questions pertaining to the applicability of existing data quality concepts. There is a debate concerning the importance of data quality for Big Data; one school of thought argues that high data quality methods are essential for deriving higher level analytics while another school of thought argues that data quality level will not be so important as the volume of Big Data would be used to produce patterns and some amount of dirty data will not mask the analytic results which might be derived. This paper aims to investigate various components and activities forming part of data quality management such as dimensions, metrics, data quality rules, data profiling and data cleansing. The result list existing challenges and future research areas associated with Big Data for data quality management.},
keywords={Big data;Context;Quality management;Frequency measurement;Organizations;Big Data;Data quality;Data profiling;Data cleansing;data quality rules;dimensions;metrics},
doi={10.1109/CCCS.2015.7374131},
ISSN={},
month={Dec},}
@INPROCEEDINGS{8453066,
author={Scavuzzo, Marco and Di Nitto, Elisabetta and Ardagna, Danilo},
booktitle={2018 IEEE/ACM 40th International Conference on Software Engineering (ICSE)},
title={[Journal First] Experiences and Challenges in Building a Data Intensive System for Data Migration},
year={2018},
volume={},
number={},
pages={93-93},
abstract={Recent analyses[2, 4, 5] report that many sectors of our economy and society are more and more guided by data-driven decision processes (e.g., health care, public administrations, etc.). As such, Data Intensive (DI) applications are becoming more and more important and critical. They must be fault-tolerant, they should scale with the amount of data, and be able to elastically leverage additional resources as and when these last ones are provided [3]. Moreover, they should be able to avoid data drops introduced in case of sudden overloads and should offer some Quality of Service (QoS) guarantees. Ensuring all these properties is, per se, a challenge, but it becomes even more difficult for DI applications, given the large amount of data to be managed and the significant level of parallelism required for its components. Even if today some technological frameworks are available for the development of such applications (for instance, think of Spark, Storm, Flink), we still lack solid software engineering approaches to support their development and, in particular, to ensure that they offer the required properties in terms of availability, throughput, data loss, etc. In fact, at the time of writing, identifying the right solution can require several rounds of experiments and the adoption of many different technologies. This implies the need for highly skilled persons and the execution of experiments with large data sets and a large number of resources, and, consequently, a significant amount of time and budget. To experiment with currently available approaches, we performed an action research experiment focusing on developing- testing-reengineering a specific DI application, Hegira4Cloud, that migrates data between widely used NoSQL databases, including so-called Database as a Service (DaaS), as well as on-premise databases. This is a representative DI system because it has to handle large volumes of data with different structures and has to guarantee that some important characteristics, in terms of data types and transactional properties, are preserved. Also, it poses stringent requirements in terms of correctness, high performance, fault tolerance, and fast and effective recovery. In our action research, we discovered that the literature offered some high level design guidelines for DI applications, as well as some tools to support modelling and QoS analysis/simulation of complex architectures, however the available tools were not yet. suitable to support DI systems. Moreover, we realized that the available big data frameworks we could have used were not flexible enough to cope with all possible application-specific aspects of our system. Hence, to achieve the desired level of performance, fault tolerance and recovery, we had to adopt a time-consuming, experiment-based approach [1, 6], which, in our case, consisted of three iterations: (1) the design and implementation of a Mediation Data Model capable of managing data extracted from different databases, together with a first monholitical prototype of Hegira4Cloud; (2) the improvement of performance of our prototype when managing and transferring huge amounts of data; (3) the introduction of fault-tolerant data extraction and management mechanisms, which are independent from the targeted databases. Among the others, an important issue that has forced us to reiterate in the development of Hegira4Cloud concerned the DaaS we interfaced with. In particular these DaaS, which are well-known services with a large number of users: (1) were missing detailed information regarding the behaviour of their APIs; (2) did not offer a predictable service; (3) were suffering of random downtimes not correlated with the datasets we were experimenting with. In this journal first presentation, we describe our experience and the issues we encountered that led to some important decisions during the software design and engineering process. Also, we analyse the state of the art of software design and verification tools and approaches in the light of our experience, and identify weaknesses, alternative design approaches and open challenges that could generate new research in these areas. More details can be found in the journal publication.},
keywords={Big Data;Databases;Quality of service;Tools;Fault tolerance;Fault tolerant systems;Software;Data intensive applications;Experiment driven action research;Big data;Data migration},
doi={10.1145/3180155.3182534},
ISSN={1558-1225},
month={May},}
@ARTICLE{8935096,
author={Li, Mingda and Wang, Hongzhi and Li, Jianzhong},
journal={Big Data Mining and Analytics},
title={Mining conditional functional dependency rules on big data},
year={2020},
volume={3},
number={1},
pages={68-84},
abstract={Current Conditional Functional Dependency (CFD) discovery algorithms always need a well-prepared training dataset. This condition makes them difficult to apply on large and low-quality datasets. To handle the volume issue of big data, we develop the sampling algorithms to obtain a small representative training set. We design the fault-tolerant rule discovery and conflict-resolution algorithms to address the low-quality issue of big data. We also propose parameter selection strategy to ensure the effectiveness of CFD discovery algorithms. Experimental results demonstrate that our method can discover effective CFD rules on billion-tuple data within a reasonable period.},
keywords={Big Data;Training;Data mining;Cleaning;Sampling methods;Heuristic algorithms;Fault tolerance;data mining;conditional functional dependency;big data;data quality},
doi={10.26599/BDMA.2019.9020019},
ISSN={2096-0654},
month={March},}
@INPROCEEDINGS{7979931,
author={Wen, Hongsheng and Chen, Zhiqiang and Gu, Jianping and Zhu, Qiangqiang},
booktitle={2016 7th International Conference on Cloud Computing and Big Data (CCBD)},
title={Big Data Analysis on Radiographic Image Quality},
year={2016},
volume={},
number={},
pages={341-346},
abstract={Mass data generated from in-service radiographic product contain assignable information on Image Quality (IQ). Analyzing data from routine work might supplement the time-consuming Image Quality Assurance Test Procedure (IQATP) to evaluate IQ and to know product type performance on site, which can also locate risks and give manufacturer directions for the further actions as well. This article illustrates methodologies of extracting IQ information from mass data and visual quality track, analysis, control, and risk mitigation in Big Data environments.},
keywords={Detectors;Image edge detection;Radiography;Standards;Image quality;X-ray imaging;Indexes;image quality;in-service;radiographic product;routine data;quality control},
doi={10.1109/CCBD.2016.073},
ISSN={},
month={Nov},}
@INPROCEEDINGS{9106668,
author={Shevchenko, Peter and Faurot, Noah and Barentine, Christian and Ries, Anthony},
booktitle={2020 Systems and Information Engineering Design Symposium (SIEDS)},
title={Improving Data Quality from Remote Eye Tracking Systems Using Real Time Feedback},
year={2020},
volume={},
number={},
pages={1-3},
abstract={This study proposes a solution to improve data quality from remote desktop eye trackers. Poor data quality from these systems regularly occurs as a result of participants unknowingly moving outside of the functional data collection area, i.e. the eye tracking box. Researchers are often not aware of the low quality data until after it has been recorded. As a result potentially large amounts of data are unusable. To alleviate this concern, we propose a real-time feedback system that alerts participants when poor eye tracking data are detected, thus enabling them to adjust their position in front of the eye tracker as soon as they move out of the functional data collection area. This capability allows researchers to acquire a higher percentage of useful data over the course of an experiment. Our approach utilized a Raspberry Pi that collected and interpreted data quality from an eye tracker in real time. Data quality from each eye was mapped to a light emitting diode (LED) placed above the computer monitor. The color of LED reflected the current quality of eye tracking data with green and red indicating high and low quality respectively. To determine if the system was effective, we compared the data quality for participants who used the system relative to participants who did not while they performed a cognitive task. Results show increased data quality for those participants using the feedback system. Our results suggest that future studies using remote desktop eye trackers can increase data quality by providing real-time data quality feedback to the participants.},
keywords={Tracking;Image color analysis;Data integrity;Gaze tracking;Data collection;Light emitting diodes;Real-time systems;eye-tracking;feedback;data quality},
doi={10.1109/SIEDS49339.2020.9106668},
ISSN={},
month={April},}