@ARTICLE{8667300,
author={Lee, Doyoung},
journal={IEEE Access},
title={Big Data Quality Assurance Through Data Traceability: A Case Study of the National Standard Reference Data Program of Korea},
year={2019},
volume={7},
number={},
pages={36294-36299},
abstract={In the era of big data, the scientific and social demand for quality data is aggressive and urgent. This paper sheds light on the expanded role of metrology of verifying validated procedures of data production and developing adequate uncertainty evaluation methods to ensure the trustworthiness of data and information. In this regard, I explore the mechanism of the national standard reference data (SRD) program of Korea, which connects various scientific and social sectors to metrology by applying useful metrological concepts and methods to produce reliable data and convert such data into national standards. In particular, the changing interpretation of metrological key concepts, such as “measurement,” “traceability,” and “uncertainty,” will be explored and reconsidered from the perspective of data quality assurance. As a result, I suggest the concept of “data traceability” with “the matrix of data quality evaluation” according to the elements of a data production system and related evaluation criteria. To conclude, I suggest social and policy implications for the new role of metrology and standards for producing and disseminating reliable knowledge sources from big data.},
keywords={Standards;Uncertainty;Big Data;Metrology;Reliability;Measurement uncertainty;Biomedical measurement;Big data;data quality;data traceability;metrology;standard reference data;uncertainty},
doi={10.1109/ACCESS.2019.2904286},
ISSN={2169-3536},
month={},}
@ARTICLE{404034,
author={Wang, R.Y. and Storey, V.C. and Firth, C.P.},
journal={IEEE Transactions on Knowledge and Data Engineering},
title={A framework for analysis of data quality research},
year={1995},
volume={7},
number={4},
pages={623-640},
abstract={Organizational databases are pervaded with data of poor quality. However, there has not been an analysis of the data quality literature that provides an overall understanding of the state-of-art research in this area. Using an analogy between product manufacturing and data manufacturing, this paper develops a framework for analyzing data quality research, and uses it as the basis for organizing the data quality literature. This framework consists of seven elements: management responsibilities, operation and assurance costs, research and development, production, distribution, personnel management, and legal function. The analysis reveals that most research efforts focus on operation and assurance costs, research and development, and production of data products. Unexplored research topics and unresolved issues are identified and directions for future research provided.<>},
keywords={Data analysis;Pulp manufacturing;Research and development management;Research and development;Production;Databases;Organizing;Cost function;Personnel;Law},
doi={10.1109/69.404034},
ISSN={1558-2191},
month={Aug},}
@INPROCEEDINGS{8589081,
author={Münzberg, Alexander and Sauer, Janina and Hein, Andreas and Rösch, Norbert},
booktitle={2018 14th International Conference on Wireless and Mobile Computing, Networking and Communications (WiMob)},
title={The use of ETL and data profiling to integrate data and improve quality in food databases},
year={2018},
volume={},
number={},
pages={231-238},
abstract={This paper focuses on integrating food data sources into a central database using extract, transform and load processing and the subsequent data quality enhancement. The obtained data will be transmitted by a food data web service to certain health apps for further use. Furthermore, it is planned to identify inconsistent, incorrect, duplicate and incomplete data using methods of data profiling so that they can be corrected. In order to quantify the data quality purposefully and appropriately, certain quality metrics were used. These metrics were calculated and evaluated using random test data selected from the food data.},
keywords={Databases;Food products;Data integrity;Measurement;ISO Standards;Europe;Conferences;food data;food information service;ETL process;data warehousing;data profiling;data quality improvement;data quality metrics},
doi={10.1109/WiMOB.2018.8589081},
ISSN={2160-4886},
month={Oct},}
@ARTICLE{273780,
author={Dabbs, W.W. and Sabin, D.D. and Grebe, T.E. and Mehta, H.},
journal={IEEE Computer Applications in Power},
title={Probing power quality data},
year={1994},
volume={7},
number={2},
pages={8-14},
abstract={A power quality problem can best be described as any variation in the electric power service resulting in misoperation or failure of end-use equipment. As customers seek to increase utilisation and efficiency, utilities strive to better understand power quality events and their effects on these customers. Utilities are creating special programs and organisations to deal with customer power quality needs. A problem shared by both parties is the need for improved methods in the collection, analysis, and reporting of very large amounts of measured power quality data. This article presents one method currently being used to characterise power quality levels on distribution systems throughout the United States. The method utilises a number of software systems, one of which is the data management and analysis program described in the article.<>},
keywords={Power quality;Power system management;Data analysis;Monitoring;Instruments;Databases;Software systems;Energy management;Quality management;Steady-state},
doi={10.1109/67.273780},
ISSN={1558-4151},
month={April},}
@INPROCEEDINGS{9845662,
author={Olaniyan, Folashikemi Maryam Asani and Owoseni, Adebowale},
booktitle={2022 IST-Africa Conference (IST-Africa)},
title={Toward Improved Data Quality in Public Health: Analysis of Anomaly Detection Tools applied to HIV/AIDS Data in Africa},
year={2022},
volume={},
number={},
pages={1-9},
abstract={The study examined the data quality efficiency of the WHO Data Quality Review (DQR) toolkit and PyCaret anomaly detection algorithms. The tools were applied to the African HIV/AIDS data (2015-2021) extracted from a public data repository (data.pepfar.gov). The research outcome suggests that unsupervised anomaly detection algorithms could complement the efficiency of the WHO DQR toolkit and improve Data Quality Assessment (DQA). In particular, the study showed that anomaly detection algorithms through python programming provide a more straightforward and more reliable process for detecting data inconsistencies, incompleteness, and timeliness appears more accurate than the WHO tool. Consequently, the study contributed to ongoing debates on improving health data quality in low-income African countries.},
keywords={Analytical models;Data integrity;Africa;Programming;Data models;Reliability;Public healthcare;Data quality review;anomaly detection;data quality assessment;public health;low-income country},
doi={10.23919/IST-Africa56635.2022.9845662},
ISSN={2576-8581},
month={May},}
@INPROCEEDINGS{5403316,
author={Ruan, Hongyong and Yu, Dongjin and Cao, Yong},
booktitle={2009 Second International Workshop on Computer Science and Engineering},
title={Research and Implementation of the Platform for Analyzing Data Quality},
year={2009},
volume={1},
number={},
pages={274-278},
abstract={With more and more redundant and dirty data accumulating in information systems nowadays, the problem of data quality is getting increasingly urgent. People usually analyze the data quality through the tools provided by the database management systems, which however bring much inconvenience and inefficiency. This article introduces a novel integrated platform for the data quality analysis, which loads, compares, and verifies the business data through the predefined regular expressions and the metamodel. Moreover, it presents a detailed analysis indexes containing the rules for evaluating the data quality. The implementation conducted in the labor market information system proves that the platform is quite applicable for the data quality analysis.},
keywords={Data analysis;Information analysis;Management information systems;Database systems;Quality assessment;Visual databases;Computer science;Data engineering;Computer security;Data security;data quality;metamodel;analysis indexes;information system},
doi={10.1109/WCSE.2009.669},
ISSN={},
month={Oct},}
@INPROCEEDINGS{8463402,
author={Tamboli, Roopak R. and Reddy, M. Shanmukh and Kara, Peter A. and Martini, Maria G. and Channappayya, Sumohana S. and Jana, Soumya},
booktitle={2018 Tenth International Conference on Quality of Multimedia Experience (QoMEX)},
title={A High-angular-resolution Turntable Data-set for Experiments on Light Field Visualization Quality},
year={2018},
volume={},
number={},
pages={1-3},
abstract={In this paper, we present a high-angular-resolution data-set created using a turntable arrangement. Seven distinct objects, positioned on an automated turntable, were captured from three camera positions for every half degree of rotation, generating 720 images for each camera position. For each object, the camera positions were registered to the coordinate system of the middle camera. Intrinsic parameters of the camera were also estimated. A data-set of this kind is instrumental for research in a variety of areas, such as light field visualization, manifold learning, visual quality assessment, evaluation of preferred object orientation etc. Due to the availability of three-view stereo, this data-set could be useful for studying view interpolation techniques.},
keywords={Cameras;Three-dimensional displays;Image resolution;Calibration;Visualization;Quality assessment;Light field;turntable data-set;angular resolution;quality assessment},
doi={10.1109/QoMEX.2018.8463402},
ISSN={2472-7814},
month={May},}
@INPROCEEDINGS{9347363,
author={Xie, Hanyang and Hu, Xiaoqi and Peng, Zewu and Jiang, Jiang and Wen, Bojian and Yang, Qiuyong},
booktitle={2020 IEEE 4th Conference on Energy Internet and Energy System Integration (EI2)},
title={Energy System Time Series Data Quality Maintenance System Based on Data Mining Technology},
year={2020},
volume={},
number={},
pages={3096-3100},
abstract={With improvement of science and technology, energy system becomes more intelligent and data scale becomes larger, so an efficient and reliable quality data management has implications for energy system. This paper proposes a grid time series data quality maintenance system based on data mining technology. In view of different characteristics of data structure in different systems, decision tree algorithm and data outlier detection methods are combined to improve efficiency of data detection while quickly locating issue type, which is convenient for data repair and improvement.},
keywords={Data integrity;Time series analysis;System integration;Maintenance engineering;Internet;Reliability;Decision trees;energy system time series data;data mining;decision tree;outlier detection;data quality maintenance},
doi={10.1109/EI250167.2020.9347363},
ISSN={},
month={Oct},}
@INPROCEEDINGS{4746551,
author={Yan, Hao and Diao, Xing-chun and Li, Kai-qi},
booktitle={2008 International Seminar on Future Information Technology and Management Engineering},
title={Research on Information Quality Driven Data Cleaning Framework},
year={2008},
volume={},
number={},
pages={537-539},
abstract={Considering the limited extensibility and interactivity of the current data cleaning work, this paper proposes a new extensible and interactive data cleaning framework driven by information quality. The framework implements the formation of data quality analysis strategy, data transformation strategy and cleaning result assessment strategy. It also realizes the control of the cleaning process. The framework has the significant features of extensibility and interactivity.},
keywords={Cleaning;Data analysis;Information management;Quality management;Software tools;Seminars;Information technology;Technology management;Engineering management;Data engineering;data cleaning framework;information quality},
doi={10.1109/FITME.2008.126},
ISSN={},
month={Nov},}
@INPROCEEDINGS{5556606,
author={Lucas, Ana},
booktitle={5th Iberian Conference on Information Systems and Technologies},
title={Corporate data quality management: From theory to practice},
year={2010},
volume={},
number={},
pages={1-7},
abstract={It is now assumed that poor quality data is costing large amounts of money to corporations all over the world. Although research on methods and techniques for data quality assessment and improvement have begun in the early nineties of the past century and being currently abundant and innovative, it is noted that the academic and professional communities virtually have no dialogue, which turns out to be harmful to both of them. The challenge of promoting the relevance in information systems research, without compromising the necessary rigor, is still present in the various disciplines of information systems scientific area, including the data quality one. In this paper we present “data as a corporate asset” as a business philosophy, and a framework for the concepts related to that philosophy, derived from the academic and professional literature. According to this framework, we present, analyze and discuss a single explanatory case study, developed in a fixed and mobile telecommunications company, operating in one of the European Union Countries. The results show that, in the absence of data stewardship roles, data quality problems become more of an "IT problem" than typically is considered in the literature, owing to Requirements Analysis Teams of the IS Development Units, to become a “quality negotiator” between the various stakeholders. Other findings are their bottom-up approach to data quality management, their biggest focus on motivating employees through innovative forms of communication, which appears to be a critical success factor (CSF) for data quality management, as well as the importance of a data quality champion leadership.},
keywords={Quality management;Data models;Companies;Context;Process control;data quality management;framework;data quality initiative;case study},
doi={},
ISSN={2166-0735},
month={June},}
@INPROCEEDINGS{8701948,
author={Ida, Masaaki},
booktitle={2019 21st International Conference on Advanced Communication Technology (ICACT)},
title={Consideration on the variation of financial data of institutions for canonical correlation analysis},
year={2019},
volume={},
number={},
pages={569-572},
abstract={In these days, progress of e-government and spread of electrical data lead to the prevail of public open databases, and also lead to the large amount of data analysis applications applying to these official open data. With regard to data analysis method, Canonical Correlation Analysis, which is one of the basic data analysis method and also data visualization method, is becoming the requisite skill for data scientists in this Big Data era. This paper examines the open data of financial data of education institutions. Especially, we focus on the higher education institutions and their financial data. In addition, we examine the variation of data and its problem to the data analysis. We aim to apply this analysis method and the result of consideration for supporting the improvement of quality assurance of higher education institutions.},
keywords={Correlation;Education;Data analysis;Databases;Correlation coefficient;Data visualization;Quality assurance;data analysis;canonical correlation analysis;data variation;financial data;higher education institution},
doi={10.23919/ICACT.2019.8701948},
ISSN={1738-9445},
month={Feb},}
@INPROCEEDINGS{9669614,
author={Pan, Weidong and Xie, Jiadong and Zhao, Yufeng and Liu, Baoyan and Hu, Kongfa},
booktitle={2021 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)},
title={Research On the Data Quality Control Model of the Traditional Chinese Medicine Inpatient Medical Record Home Page Based on XGBoost},
year={2021},
volume={},
number={},
pages={3870-3874},
abstract={Objective: Designs a XGBoost-based data quality control model for the traditional chinese medicine (TCM) inpatient medical record home page. Exploring the method of data normalization on the TCM inpatient medical record home page. Methods: Taking the data on the TCM inpatient medical record home page of a hospital in Jiangsu Province as the original data. Using correlation analysis to filter out some data items that have a higher degree of correlation with the data items to be quality control. Establishing a XGBoost-based data quality control model for the TCM inpatient medical record home page. Using the hierarchical 10-fold cross-validation to evaluate the model. Results: The experimental results show that the accuracy rate of the model can reach 88.60%. Conclusion: The data quality control model on the TCM inpatient medical record home page is conducive to improving the quality of the data on the TCM inpatient medical record home page and provides data support for medical research.},
keywords={Correlation;Dictionaries;Hospitals;Data integrity;Quality control;Predictive models;Data models;Traditional Chinese Medicine Inpatient Medical Record Home Page;Data Quality Control;XGBoost;Correlation Analysis;Model Construction},
doi={10.1109/BIBM52615.2021.9669614},
ISSN={},
month={Dec},}
@INPROCEEDINGS{4154483,
author={Yunus, Bahisham and Li, Haiyu},
booktitle={2006 IEEE International Power and Energy Conference},
title={Analysis of Power Quality Waveform for Data Transmission Efficiency over IEC 61850 Communication Standard},
year={2006},
volume={},
number={},
pages={161-166},
abstract={In power utilities, grid operators monitor power-quality phenomena such as voltage sag at several substations to obtain an overview of the general network state and evolution of power quality. Collections of these huge online data from disturbance recorders involved long and tedious downloading process, usage of Internet protocols consumed bandwidth and causes traffic congestion and collisions with other users of the wide area network. This paper describes an integrated application of power quality monitoring system for a modeled substation over the IEC 61850 standard developed in Java programming language. The application used wavelet compression technique for transmission of voltage sag waveform over IEC 61850 standard and then Internet to a control centre. Analysis done shows that the correct selection of mother wavelet can achieve over 90% data compression ability while preserving all significant power quality features required for power quality waveform analysis required by utilities.},
keywords={Power quality;Data communication;IEC standards;Communication standards;Voltage fluctuations;Monitoring;Substations;IP networks;Protocols;Bandwidth;efficiency;IEC 61850 standards;power quality waveforms;voltage sags;wavelet transform},
doi={10.1109/PECON.2006.346639},
ISSN={},
month={Nov},}
@INPROCEEDINGS{5489992,
author={Gǎşpǎresc, Gabriel},
booktitle={2010 9th International Conference on Environment and Electrical Engineering},
title={Data compression of power quality disturbances using wavelet transform and spline interpolation method},
year={2010},
volume={},
number={},
pages={285-288},
abstract={In this paper is described a data compression technique based on wavelet decomposition and spline interpolation for power quality disturbance. The technique consist in signal decomposition, thresholding of wavelet transform coefficients, decimation of the last coefficient and signal reconstruction using spline interpolation for the last coefficient. The optimal order of Daubechies scaling function recommended in order to achieve the best compression ratio and comparative compression results using different types of power quality disturbances are also presented. This technique is suitable mainly for data acquired at high sample rates.},
keywords={Data compression;Power quality;Wavelet transforms;Spline;Interpolation;Frequency;Signal resolution;Signal reconstruction;Costs;White noise;data compression;power quality disturbance;Wavelet transform;interpolation},
doi={10.1109/EEEIC.2010.5489992},
ISSN={},
month={May},}
@ARTICLE{8782595,
author={Wang, Songyun and Yuan, Jiabin and Li, Xin and Qian, Zhuzhong and Arena, Fabio and You, Ilsun},
journal={IEEE Access},
title={Active Data Replica Recovery for Quality-Assurance Big Data Analysis in IC-IoT},
year={2019},
volume={7},
number={},
pages={106997-107005},
abstract={QoS-aware big data analysis is critical in Information-Centric Internet of Things (IC-IoT) system to support various applications like smart city, smart grid, smart health, intelligent transportation systems, and so on. The employment of non-volatile memory (NVM) in cloud or edge system provides good opportunity to improve quality of data analysis tasks. However, we have to face the data recovery problem led by NVM failure due to the limited write endurance. In this paper, we investigate the data recovery problem for QoS guarantee and system robustness, followed by proposing a rarity-aware data recovery algorithm. The core idea is to establish the rarity indicator to evaluate the replica distribution and service requirement comprehensively. With this idea, we give the lost replicas with distinguishing priority and eliminate the unnecessary replicas. Then, the data replicas are recovered stage by stage to guarantee QoS and provide system robustness. From our extensive experiments and simulations, it is shown that the proposed algorithm has significant performance improvement on QoS and robustness than the traditional direct data recovery method. Besides, the algorithm gives an acceptable data recovery time.},
keywords={Nonvolatile memory;Quality of service;Data analysis;Data centers;Bandwidth;Big Data;Robustness;Big data analysis;data recovery;IC-IoT;NVM;QoS improvement},
doi={10.1109/ACCESS.2019.2932259},
ISSN={2169-3536},
month={},}
@INPROCEEDINGS{5287757,
author={Hai-ying, Liu},
booktitle={2009 Second International Conference on Intelligent Computation Technology and Automation},
title={Research on Process Quality Data Collection and Control Based on Computer Technology},
year={2009},
volume={2},
number={},
pages={615-618},
abstract={Quality data collection processes and process parameters adjustment is the key element of Quality control processes. The content of process quality data collection and control include process monitoring and product quality testing. Process parameter monitoring can be achieved by measuring the characteristics of the related parameters. Process product quality testing can be achieved by direct measuring product quality characteristics in the process or processes interval. Process quality data collection and control system have been studied in this paper.},
keywords={Quality control;Testing;Production;Computer aided manufacturing;Statistical analysis;Error correction;Process control;Computerized monitoring;Application software;Sampling methods;Process quality;Data collection and control;Computer technology},
doi={10.1109/ICICTA.2009.383},
ISSN={},
month={Oct},}
@INPROCEEDINGS{8823437,
author={Zeng, Yu-Ren and Chang, Yue Shan and Fang, You Hao},
booktitle={2019 International Conference on System Science and Engineering (ICSSE)},
title={Data Visualization for Air Quality Analysis on Bigdata Platform},
year={2019},
volume={},
number={},
pages={313-317},
abstract={With the advances of industry, air pollution is increasingly becoming serious, and most of governments in the world has deployed many devices to monitor daily air quality. Monitoring and forecasting of air quality has also become an important issue to improve the quality of people's lives. As far as we know, bad air quality does not only affect the health of the respiratory tract, it may but also even cause mental illness. Many researchers have investigated different approaches to work on air quality forecast, and the visualization of forecasting becomes important. In this paper, we present an architecture for visualizing forecasted air quality on a big data platform. We implemented an ETL (Extract-Transform-Load) based framework in the platform, which includes computing nodes and storage nodes. Computational nodes are used for data collection and for air quality forecasting over the next 1 to 8 hours through machine learning and deep learning. Storage nodes are used to retrieve, analyze, and preprocess of collected data. We use the RESTful Web Service as an API, and finally we use the browser to get the data by predefined API and to present the forecasted and monitored results with Google Map API and D3 JavaScript library. It reveals that the visualization on big data framework can work well for air quality analysis.},
keywords={Data visualization;Air quality;Big Data;Forecasting;Monitoring;Web servers;Air Quality;Big Data;Forecasting;Cloud Environment;Data Visualization},
doi={10.1109/ICSSE.2019.8823437},
ISSN={2325-0925},
month={July},}
@INPROCEEDINGS{9391025,
author={Wang, Fengling and Wang, Han and Xue, Liang},
booktitle={2021 IEEE 5th Advanced Information Technology, Electronic and Automation Control Conference (IAEAC)},
title={Research on Data Security in Big Data Cloud Computing Environment},
year={2021},
volume={5},
number={},
pages={1446-1450},
abstract={In the big data cloud computing environment, data security issues have become a focus of attention. This paper delivers an overview of conceptions, characteristics and advanced technologies for big data cloud computing. Security issues of data quality and privacy control are elaborated pertaining to data access, data isolation, data integrity, data destruction, data transmission and data sharing. Eventually, a virtualization architecture and related strategies are proposed to against threats and enhance the data security in big data cloud environment.},
keywords={Cloud computing;Data security;Data integrity;Big Data;Maintenance engineering;Virtualization;Information technology;big data;cloud computing;data security;big data cloud computing;security policy},
doi={10.1109/IAEAC50856.2021.9391025},
ISSN={2689-6621},
month={March},}
@INPROCEEDINGS{8621924,
author={Guntupally, Kavya and Devarakonda, Ranjeet and Kehoe, Kenneth},
booktitle={2018 IEEE International Conference on Big Data (Big Data)},
title={Spring Boot based REST API to Improve Data Quality Report Generation for Big Scientific Data: ARM Data Center Example},
year={2018},
volume={},
number={},
pages={5328-5329},
abstract={Web application technologies are growing rapidly with continuous innovation and improvements. This paper focuses on the popular Spring Boot [1] java-based framework for building web and enterprise applications and how it provides the flexibility for service-oriented architecture (SOA). One challenge with any Spring-based applications is its level of complexity with configurations. Spring Boot makes it easy to create and deploy stand-alone, production-grade Spring applications with very little Spring configuration. Example, if we consider Spring Model-View-Controller (MVC) framework [2], we need to configure dispatcher servlet, web jars, a view resolver, and component scan among other things. To solve this, Spring Boot provides several Auto Configuration options to setup the application with any needed dependencies. Another challenge is to identify the framework dependencies and associated library versions required to develop a web application. Spring Boot offers simpler dependency management by using a comprehensive, but flexible, framework and the associated libraries in one single dependency, which provides all the Spring related technology that you need for starter projects as compared to CRUD web applications. This framework provides a range of additional features that are common across many projects such as embedded server, security, metrics, health checks, and externalized configuration. Web applications are generally packaged as war and deployed to a web server, but Spring Boot application can be packaged either as war or jar file, which allows to run the application without the need to install and/or configure on the application server. In this paper, we discuss how Atmospheric Radiation Measurement (ARM) Data Center (ADC) at Oak Ridge National Laboratory, is using Spring Boot to create a SOA based REST [4] service API, that bridges the gap between frontend user interfaces and backend database. Using this REST service API, ARM scientists are now able to submit reports via a user form or a command line interface, which captures the same data quality or other important information about ARM data.},
keywords={Springs;Databases;Service-oriented architecture;Data integrity;Tools;Servers;Big Data;auto configuration;CRUD;java framework;service-oriented architecture;REST;spring boot},
doi={10.1109/BigData.2018.8621924},
ISSN={},
month={Dec},}
@INPROCEEDINGS{5361869,
author={Chen, Bing and Wang, Beizhan and Zheng, Chengman and Hu, Xueqin},
booktitle={2009 Fourth International Conference on Cooperation and Promotion of Information Resources in Science and Technology},
title={Research and Implementation of Information Quality Improvement},
year={2009},
volume={},
number={},
pages={225-229},
abstract={Information quality is the premise for scientific decision making. Along with the development of various types of information-sharing project, problems of information quality are increasingly apparent. This paper firstly analyzed the criteria of information quality from the relationship between information quality and data quality, and then proposed an improved scheme based on metadata to resolve the issue of information quality from the source. Finally, combined with the business of China Construction Bank this scheme has been implemented successfully. The practice proven that this scheme can reflect the system design changes and data quality real-timely and moreover provided some research direction for the improvement of information quality.},
keywords={Information systems;Quality management;Decision making;Information analysis;Management information systems;Quality assessment;Information resources;Artificial intelligence;Software quality;Standardization;total data quality management;goal-question-metric approach;information quality;metadata;data quality},
doi={10.1109/COINFO.2009.17},
ISSN={},
month={Nov},}
@INPROCEEDINGS{8622590,
author={Rueda, Diego F. and Vergara, Dahyr and Reniz, David},
booktitle={2018 IEEE International Conference on Big Data (Big Data)},
title={Big Data Streaming Analytics for QoE Monitoring in Mobile Networks: A Practical Approach},
year={2018},
volume={},
number={},
pages={1992-1997},
abstract={Traditionally, Mobile Network Operators (MNOs) use a set of Key Performance Indicators (KPIs) to measure the quality offered to their customers. However, these KPIs do not reflect the quality perceived by the customers because they are high-level and network-based metrics. Instead, Quality of Experience (QoE) monitoring of the most common mobile applications can help MNOs to determine when and where customer experience is degraded. In this paper, a customized tool based on Big Data Streaming is proposed to solve the needs of customer experience monitoring in a real-life MNO and to overcome the challenges of processing a large amount of data collected in 3G and 4G mobile networks. Moreover, real-life case studies of value creation through Big Data Analytics for telecommunication industry are also defined. Results show that the streaming data processing enables new opportunities for the MNO to take actions focused on customer experience improvement in near real-time.},
keywords={Quality of experience;Big Data;Monitoring;Streaming media;Real-time systems;Tools;Big Data Analytics;customer experience management;mobile networks;quality of experience;streaming data processing},
doi={10.1109/BigData.2018.8622590},
ISSN={},
month={Dec},}
@ARTICLE{7994799,
author={Urrutia, Angelica and Chavez, Emma and Motz, Regina and Gajardo, Rosa},
journal={IEEE Latin America Transactions},
title={An Ontology to Assess Data Quality Domains. A Case Study Applied to a Health Care Entity.},
year={2017},
volume={15},
number={8},
pages={1506-1512},
abstract={Data integration can extend scenarios in applications designed to fulfill the requirements of decision support systems. However, a critical aspect in database development is to determine the quality of the data sources. Thus, it is important to have an automatic mechanism to be able to measure the quality of the data source, where the level of quality depends on the amount of data with technical anomalies that exist in the upgrade process. This article aims to define a flexible process to asses data quality, which does not depend of the application domain, and has a declarative specification based on the use of domain ontologies and data quality metrics. As a prove of concepts a description of the use of the ontology and the possible metrics to use are proposed in a case study that applies to the health care domain. For the case a Semantic Web Rule Language (SWRL) was used.},
keywords={Ontologies;Measurement;IEEE transactions;Medical services;Decision support systems;Databases;Semantic Web;Data quality;Health informatics;Ontologies;Quality metrics},
doi={10.1109/TLA.2017.7994799},
ISSN={1548-0992},
month={},}
@INPROCEEDINGS{7840729,
author={Xu, Yanan and Zhu, Yanmin},
booktitle={2016 IEEE International Conference on Big Data (Big Data)},
title={When remote sensing data meet ubiquitous urban data: Fine-grained air quality inference},
year={2016},
volume={},
number={},
pages={1252-1261},
abstract={With the growth of the economy, the air quality is becoming a serious issue, especially for those developing countries, such as China. Therefore, it is very important for the public and the government to access real-time air quality information. Unfortunately, the limited number of air quality monitoring stations is unable to provide fine-grained air quality information in a huge city, such as Beijing. One cost-effective approach for obtaining fine-grained air quality information is to infer air quality with those measured data at the monitoring stations. However, existing inference techniques have poor performance because of the extreme data sparsity problem (e.g., only 0.2% data are known). We observe that remote sensing has been a high-quality data source about urban dynamics. In this paper, we propose to integrate remote sensing data and ubiquitous urban data for air quality inference. There are two main challenges, i.e., data heterogeneity and incomplete remote sensing data. In response to the challenges, we propose a two-stage inference approach. In the first stage, we use the AOT remote sensing data and the meteorological data to infer the air quality values with an Artificial Neural Network (ANN). After this stage, we significantly reduce the percentage of empty cells in the tensor representing the spatio-temporal air quality values. In the second stage, we propose a tensor decomposition method to infer the complete set of air quality values. We use the spatial features (i.e., road features and POI features) and the temporal features (i.e., meteorological features) as the constraints in the tensor decomposition process. Experiments with real data sets show that our approach has profound performance advantage over the state-of-the-art methods, such as U-Air.},
keywords={Air quality;Remote sensing;Monitoring;Tensile stress;Sensors;Urban areas;Pollution measurement;Remote sensing;AOT;air quality;inference;neural network;tensor decomposition},
doi={10.1109/BigData.2016.7840729},
ISSN={},
month={Dec},}
@INPROCEEDINGS{9433875,
author={Tummala, Sudhakar and Focke, Niels K.},
booktitle={2021 IEEE 18th International Symposium on Biomedical Imaging (ISBI)},
title={Machine Learning Framework For Fully Automatic Quality Checking Of Rigid And Affine Registrations In Big Data Brain MRI},
year={2021},
volume={},
number={},
pages={1734-1737},
abstract={Rigid and affine registrations to a common template are the essential steps during pre-processing of brain structural magnetic resonance imaging (MRI) data. Manual quality control (QC) of these registrations is quite tedious if the data contains several thousands of images. Therefore, we propose a machine learning (ML) framework for fully automatic QC of these registrations via global and local computation of the similarity functions such as normalized cross-correlation, normalized mutual-information, and correlation ratio, and using these as features for training of different ML classifiers. To facilitate supervised learning, misaligned images are generated. A structural MRI dataset consisting of 215 subjects from autism brain imaging data exchange is used for 5-fold cross-validation and testing. ML models based on local costs performed better than the models with global costs. Local cost based random forest, and AdaBoost models reached testing F1-scores and balanced accuracies of 0.98 and 0.95 respectively for QC of both rigid and affine registrations.},
keywords={Training;Magnetic resonance imaging;Supervised learning;Quality control;Big Data;Brain modeling;Data models;structural MRI;big data;machine learning;quality control;image registration},
doi={10.1109/ISBI48211.2021.9433875},
ISSN={1945-8452},
month={April},}
@INPROCEEDINGS{6329792,
author={Madraky, Abbas and Othman, Zulaiha Ali and Hamdan, Abdul Razak},
booktitle={2012 4th Conference on Data Mining and Optimization (DMO)},
title={Hair data model: A new data model for Spatio-Temporal data mining},
year={2012},
volume={},
number={},
pages={18-22},
abstract={Spatio-Temporal data is related to many of the issues around us such as satellite images, weather maps, transportation systems and so on. Furthermore, this information is commonly not static and can change over the time. Therefore the nature of this kind of data are huge, analysing data is a complex task. This research aims to propose an intermediate data model that can represented suitable for Spatio-Temporal data and performing data mining task easily while facing problem in frequently changing the data. In order to propose suitable data model, this research also investigate the analytical parameters, the structure and its specifications for Spatio-Temporal data. The concept of proposed data model is inspired from the nature of hair which has specific properties and its growth over the time. In order to have better looking and quality, the data is needed to maintain over the time such as combing, cutting, colouring, covering, cleaning etc. The proposed data model is represented by using mathematical model and later developed the data model tools. The data model is developed based on the existing relational and object-oriented models. This paper deals with the problems of available Spatio-Temporal data models for utilizing data mining technology and defines a new model based on analytical attributes and functions.},
keywords={Data models;Hair;Object oriented modeling;Data mining;Security;Mathematical model;Analytical models;hair data model;spatio-temporal data models;data warehouse model},
doi={10.1109/DMO.2012.6329792},
ISSN={2155-6946},
month={Sep.},}
@INPROCEEDINGS{7872917,
author={Rahman, H. and Ahmed, N. and Hussain, Md. I.},
booktitle={2016 Cloudification of the Internet of Things (CIoT)},
title={A hybrid data aggregation scheme for provisioning Quality of Service (QoS) in Internet of Things (IoT)},
year={2016},
volume={},
number={},
pages={1-5},
abstract={Internet of Things (IoT) is a new paradigm which is enormously gaining ground in today's world. In order to maintain desirable service quality in the transmission of sensed data, data aggregation schemes are highly used. The main goal of data aggregation scheme is to collect and aggregate data packets in an efficient manner so as to reduce power consumption, traffic congestion, and to increase network lifetime, data accuracy, etc. In this paper, a hybrid Quality of service-Aware Data Aggregation (QADA) scheme is proposed. This scheme combines the features of the cluster and tree-based data aggregation schemes and addresses some of their important limitations. Simulation results show that QADA outperforms cluster and tree-based aggregation schemes in terms of power consumption, network lifetime and bearing higher traffic load.},
keywords={Data aggregation;Sensors;Protocols;Wireless sensor networks;Power demand;Data communication;Time division multiple access;Data Aggregation;Internet of Things;Quality of Service;Wireless Sensor Network},
doi={10.1109/CIOT.2016.7872917},
ISSN={},
month={Nov},}
@INPROCEEDINGS{9060361,
author={Woodbridge, Diane and Hua, Nina and Suarez, Victoria and Reilly, Rebecca and Trinh, Philip and Intrevado, Paul},
booktitle={2019 IEEE SmartWorld, Ubiquitous Intelligence & Computing, Advanced & Trusted Computing, Scalable Computing & Communications, Cloud & Big Data Computing, Internet of People and Smart City Innovation (SmartWorld/SCALCOM/UIC/ATC/CBDCom/IOP/SCI)},
title={The Impact of Bike-Sharing Ridership on Air Quality: A Scalable Data Science Framework},
year={2019},
volume={},
number={},
pages={1950-1957},
abstract={This research explores the relationship between daily air quality indicator (AQI) values and the daily intensity of bike-share ridership in New York City. The authors designed and deployed a distributed data science framework on which to process and run Elastic Net, Random Forest Regression, and Gradient Boosted Regression Trees. Nine gigabytes of CitiBike ridership data, along with one gigabyte of air quality indicator (AQI) data were employed. All machine learning algorithms identified bike-share ridership intensity as either the most important or the second most important feature in predicting future daily AQIs. The authors also empirically demonstrated that although a distributed platform was necessary to ingest and pre-process the raw 10 gigabytes of data, the actual execution time of all three machine learning algorithms on cleaned, joined, and aggregated data was far faster on a local, commodity computer than on its distributed counterpart.},
keywords={Air quality;Distributed databases;Servers;Urban areas;Cloud computing;Vegetation;Indexes;Distributed computing;Distributed information systems;Distributed databases;Machine learning;Air pollution;Air quality;Intelligent transportation systems},
doi={10.1109/SmartWorld-UIC-ATC-SCALCOM-IOP-SCI.2019.00341},
ISSN={},
month={Aug},}
@ARTICLE{9320548,
author={Luo, Xin and Chen, Minzhi and Wu, Hao and Liu, Zhigang and Yuan, Huaqiang and Zhou, MengChu},
journal={IEEE Transactions on Automation Science and Engineering},
title={Adjusting Learning Depth in Nonnegative Latent Factorization of Tensors for Accurately Modeling Temporal Patterns in Dynamic QoS Data},
year={2021},
volume={18},
number={4},
pages={2142-2155},
abstract={A nonnegative latent factorization of tensors (NLFT) model precisely represents the temporal patterns hidden in multichannel data emerging from various applications. It often adopts a single latent factor-dependent, nonnegative and multiplicative update on tensor (SLF-NMUT) algorithm. However, learning depth in this algorithm is not adjustable, resulting in frequent training fluctuation or poor model convergence caused by overshooting. To address this issue, this study carefully investigates the connections between the performance of an NLFT model and its learning depth via SLF-NMUT to present a joint learning-depth-adjusting scheme for it. Based on this scheme, a Depth-adjusted Multiplicative Update on tensor algorithm is innovatively proposed, thereby achieving a novel depth-adjusted nonnegative latent-factorization-of-tensors (DNL) model. Empirical studies on two industrial data sets demonstrate that compared with the state-of-the-art NLFT models, a DNL model achieves significant accuracy gain when performing missing data estimation on a high-dimensional and incomplete tensor with high efficiency. Note to Practitioners—Multichannel data are often encountered in various big-data-related applications. It is vital for a data analyzer to correctly capture the temporal patterns hidden in them for efficient knowledge acquisition and representation. This article focuses on analyzing temporal QoS data, which is a representative kind of multichannel data. To correctly extract their temporal patterns, an analyzer should correctly describe their nonnegativity. Such a purpose can be achieved by building a nonnegative latent factorization of tensors (NLFT) model relying on a single latent factor-dependent, nonnegative and multiplicative update on tensor (SLF-NMUT) algorithm. But its learning depth is not adjustable, making an NLFT model frequently suffer from severe fluctuations in its training error or even fail to converge. To address this issue, this study carefully investigates the learning rules for an NLFT model’s decision parameters using an SLF-NMUT and proposes a joint learning-depth-adjusting scheme. This scheme manipulates the multiplicative terms in SLF-NMUT-based learning rules linearly and exponentially, thereby making the learning depth adjustable. Based on it, this study builds a novel depth-adjusted nonnegative latent-factorization-of-tensors (DNL) model. Compared with the existing NLFT models, a DNL model better represents multichannel data. It meets industrial needs well and can be used to achieve high performance in data analysis tasks like temporal-aware missing data estimation},
keywords={Tensors;Big Data;Quality of service;Computational efficiency;Machine learning;Web services;Algorithm;big data;dynamics;high-dimensional and incomplete (HDI) data;machine learning;missing data estimation;multichannel data;nonnegative latent factorization of tensors (NLFT);temporal pattern;quality of service (QoS);web service},
doi={10.1109/TASE.2020.3040400},
ISSN={1558-3783},
month={Oct},}
@INPROCEEDINGS{7881388,
author={El Bekri, Nadia and Peinsipp-Byma, Elisabeth},
booktitle={2016 International Conference on Computational Science and Computational Intelligence (CSCI)},
title={Assuring Data Quality by Placing the User in the Loop},
year={2016},
volume={},
number={},
pages={468-471},
abstract={Advanced analytical techniques such as data mining, text mining or predictive analytics are concepts that are increasingly important in the area of discovering large data sets. Various business areas recognize that data in all formats and sizes can provide significant support for decision-making. Large amounts of data can contain explicit knowledge in form of patterns. Errors within the data can falsify extracted patterns. Data is useful if it is correct, organized and interpreted correctly. Data mining algorithms can help improve data quality. Algorithms can suggest hints on possible errors. Possible errors need a mechanism that decides whether the error is true or false. The solution this paper introduces is to integrate users in the quality assurance process for decision support systems. The user can assess whether an error is true or false.},
keywords={Classification algorithms;Quality assurance;Data mining;Training data;Decision support systems;Cleaning;Algorithm design and analysis;Data quality;User in the loop;Data Mining},
doi={10.1109/CSCI.2016.0095},
ISSN={},
month={Dec},}
@INPROCEEDINGS{8078801,
author={Song, Huaming and Cao, Zhexiu},
booktitle={2017 IEEE 2nd International Conference on Big Data Analysis (ICBDA)},
title={Research on product quality evaluation based on big data analysis},
year={2017},
volume={},
number={},
pages={173-177},
abstract={In order to evaluate product quality from nonnumerical data, we propose the product quality evaluation model based on big data analysis including data collecting, data preprocessing, quality feature extraction, vector quantization and quality classification. Quality feature word extension algorithm, reviews quantization algorithm and machine learning algorithm are applied. We finally obtain the qualified rate(88.94%) and 7 features that most concerned by consumers through the analysis of 184,967 effective product reviews of wooden toys. In the end, we compare the SVM machine learning algorithm with decision tree and naive bayes, and discuss the credibility of the results. Our research on product quality evaluation extends the application of big data analysis, and also presents a new method to evaluate product quality in the field of manufacture.},
keywords={Quality assessment;Product design;Feature extraction;Dictionaries;Toy manufacturing industry;Data mining;Sentiment analysis;quality evaluation;online reviews;big data analysis;machine learning},
doi={10.1109/ICBDA.2017.8078801},
ISSN={},
month={March},}
@INPROCEEDINGS{7483222,
author={Zellal, Nouha and Zaouia, Abdellah},
booktitle={2015 Third World Conference on Complex Systems (WCCS)},
title={An exploratory investigation of Factors Influencing Data Quality in Data Warehouse},
year={2015},
volume={},
number={},
pages={1-6},
abstract={Data quality is of paramount importance in any Business Intelligence (BI) project. In fact, the Decision Support System can be the reference decision unless the data is consistent, updated, completed and fit for use for the decision makers and the decision tasks. Otherwise, the non-quality data may lead policy makers to make bad decisions, to miss opportunities or even commit very serious business mistakes. Considering this issue, we propose to study in this paper the factors that influence the quality of intelligence data, or rather the factors moderating the influence of the quality of source data since this last one is inevitable. We conduct an exploratory research in order to gather information from the literature review that helps us to suggest hypothesis of our research model "Factors Influencing Data Quality in a Data Warehouse". The objective of identifying the critical ones throw a confirmatory research (that we don't expose in this paper) will be to enable stakeholders to better use their scare resources while implementing a BI project by focusing on these key areas that are most likely to have a greater impact.},
keywords={Data warehouses;Warehousing;Quality management;Context;Data models;Companies;Data Quality;Data Warehouse;Business Intelligence},
doi={10.1109/ICoCS.2015.7483222},
ISSN={},
month={Nov},}
@INPROCEEDINGS{8273343,
author={Mahecha Moyano, John Ferney and López Beltrán, Nicolás Estefan and Velandia Vega, John Alexander},
booktitle={2017 Congreso Internacional de Innovacion y Tendencias en Ingenieria (CONIITI)},
title={Assessing data quality in open data: A case study},
year={2017},
volume={},
number={},
pages={1-5},
abstract={This article focuses on measuring the data quality extracted from the API www.datos.gov.co in the area of contracts. This platform allows public entities obey law 1712 of 2014 [2] but this does not have a validation system of minimum data quality. Three metrics are taken as a reference: completeness, traceability and compliance. The measuring of the metrics is done with a software called RapidMiner. This software allows to do data mining, in this specific case the processes for the measuring of the data stored are done and thus determine if the platform www.datos.gov.co has problems. For the programming of the prototype spiral methodology [7] is used. Calculations is created in each phase. Within the data found, serious inconsistencies are found within the platform and within the data, recalling the law 1712 of 2014, it says that the public data must be complete, which shows a breach of it.},
keywords={Measurement;Prototypes;Data integrity;Data mining;Databases;Metadata;Software;Open data;Data Quality;Traceability;Completeness;Compliance;Software architecture and Business process model},
doi={10.1109/CONIITI.2017.8273343},
ISSN={},
month={Oct},}
@ARTICLE{9006814,
author={Chen, Wuhui and Liu, Baichuan and Paik, Incheon and Li, Zhenni and Zheng, Zibin},
journal={IEEE Transactions on Engineering Management},
title={QoS-Aware Data Placement for MapReduce Applications in Geo-Distributed Data Centers},
year={2021},
volume={68},
number={1},
pages={120-136},
abstract={With growing data volumes and the scaling of data center clusters, communication resources often become a bottleneck in service provisioning for many MapReduce applications (e.g., training machine learning models). Therefore, data placements that bring data blocks closer to data consumers (e.g., MapReduce applications) are seen as a promising solution. In this article, we propose an efficient data-placement technique that considers network traffic reduction as well as QoS guarantees for the data blocks to optimize the communication resources. We first formulate the joint optimization of the data-placement problem, propose a generic model for minimizing communication costs, and show that the joint data-placement problem is NP-hard. To solve this problem, we propose a heuristic algorithm considering traffic flows in the network topology of data centers by first seeking optimal QoS-aware data placement based on golden division on a Zipflike replica distribution, then transforming the joint data-placement problem into a block-dependence tree (BDT) construction problem, and finally reducing the BDT construction to a graph-partitioning problem. The experimental results demonstrate that our data-placement approach could effectively improve the performance of MapReduce jobs with lower communication costs and less job execution time for big-data processing.},
keywords={Data centers;Quality of service;Data transfer;Distributed databases;Data models;Optimization;Network topology;Big-data processing;data placement;geo-distributed data centers;QoS aware},
doi={10.1109/TEM.2020.2971717},
ISSN={1558-0040},
month={Feb},}
@INPROCEEDINGS{5567975,
author={Fang, Li and Yue, Jianwei and Yu, Zhuoyuan},
booktitle={2010 18th International Conference on Geoinformatics},
title={A spatial data checking system based on quality rules},
year={2010},
volume={},
number={},
pages={1-4},
abstract={This paper describes the design and implementation of a spatial data checking system based on quality rules. A relational expression of defining quality rules is presented and new quality rules can be extended by using this structure. Methods of storing and parsing these quality rules for data checking are discussed. Based on this, a spatial data checking system is developed, in which extended data checking types can be defined and data checking tasks are made to execute automatic checking for spatial databases. The system has been applied to the construction of spatial databases in Jilin city and proved to be effective.},
keywords={Spatial databases;Construction industry;Quality control;XML;Cities and towns;User interfaces;data quality;data checking;quality control;quality rules;spatial databases},
doi={10.1109/GEOINFORMATICS.2010.5567975},
ISSN={2161-0258},
month={June},}
@INPROCEEDINGS{9849309,
author={Bai, Jin and Gao, Jingnan and Ma, Wei and Dang, Yu and Lu, Chenni and Liu, Chang and Dong, Shuai and Li, Hai},
booktitle={2022 3rd International Conference on Geology, Mapping and Remote Sensing (ICGMRS)},
title={Research on Quality Evaluation of Large Scale Topographic Update Database},
year={2022},
volume={},
number={},
pages={150-153},
abstract={Basic surveying and mapping is a public welfare undertaking. It provides indispensable geographic information for economic development, social people’s livelihood, urban planning and national defense construction. It is a national strategic resource and an important basis for implementing development planning, macro management, safeguarding national security, and building ecological civilization. In order to ensure the quality of the updated results of basic surveying and mapping topographic data and the scientific planning and smooth implementation of national economic construction and social development, inspection, acceptance and quality evaluation have become important means to ensure the quality of results. Starting from the field sampling inspection of large-scale topographic data updating results participated by the author in the whole process, this paper leads out and discusses the problems, causes, and solutions that are easy to occur in the production process of large-scale topographic data updating results from the quality problems, and summarizes and considers them in combination with typical problems. It can provide a reference for the future sampling of large scale topographic data and the subsequent quality analysis and evaluation.},
keywords={Economics;Databases;Geology;Urban planning;Buildings;Production;Inspection;Topographic data;Quality evaluation;Field sampling inspection;Basic surveying and mapping;Factor statistics},
doi={10.1109/ICGMRS55602.2022.9849309},
ISSN={},
month={April},}
@ARTICLE{8843909,
author={Bors, Christian and Gschwandtner, Theresia and Miksch, Silvia},
journal={IEEE Computer Graphics and Applications},
title={Capturing and Visualizing Provenance From Data Wrangling},
year={2019},
volume={39},
number={6},
pages={61-75},
abstract={Data quality management and assessment play a vital role for ensuring the trust in the data and its fitness-of-use for subsequent analysis. The transformation history of a data wrangling system is often insufficient for determining the usability of a dataset, lacking information how changes affected the dataset. Capturing workflow provenance along the wrangling process and combining it with descriptive information as data provenance can enable users to comprehend how these changes affected the dataset, and if they benefited data quality. We present DQProv Explorer, a system that captures and visualizes provenance from data wrangling operations. It features three visualization components: allowing the user to explore the provenance graph of operations and the data stream, the development of quality over time for a sequence of wrangling operations applied to the dataset, and the distribution of issues across the entirety of the dataset to determine error patterns.},
keywords={Data integrity;Measurement;Data visualization;History;Data models;Data Wrangling;Data Cleansing;Data Quality;Quality Metrics;Data Provenance;Sensemaking},
doi={10.1109/MCG.2019.2941856},
ISSN={1558-1756},
month={Nov},}
@INPROCEEDINGS{9794846,
author={Chistyakova, Tamara B. and Makaruk, Roman V. and Tedtoev, Azamat Ch.},
booktitle={2022 XXV International Conference on Soft Computing and Measurements (SCM)},
title={Methods and Technologies of Application of Fuzzy Models for Processing Industrial Data and Quality Management of Polymer Materials},
year={2022},
volume={},
number={},
pages={34-37},
abstract={The paper presents methods and technologies for the use of fuzzy models for the analysis and quality control of polymer materials. In accordance with the approaches under consideration, a single reconfigurable computer system was developed that allows for a comprehensive analysis of the quality of products of multi-assortment polymer industries. The methods and technologies that allow using fuzzy models to evaluate the color characteristics of polymer materials are described, which makes it possible to eliminate the incompleteness of the input data about the object of study and improve the quality of the results of the system as a whole},
keywords={Analytical models;Image color analysis;Software packages;Computational modeling;Quality control;Predictive models;Data models;processing of large industrial data;methods and technologies;polymer film;analysis;film color characteristics;fuzzy models;quality indicators;expert system;quality control},
doi={10.1109/SCM55405.2022.9794846},
ISSN={},
month={May},}
@INPROCEEDINGS{9643782,
author={Wenz, Viola and Kesper, Arno and Taentzer, Gabriele},
booktitle={2021 ACM/IEEE International Conference on Model Driven Engineering Languages and Systems Companion (MODELS-C)},
title={Detecting Quality Problems in Data Models by Clustering Heterogeneous Data Values},
year={2021},
volume={},
number={},
pages={150-159},
abstract={Data is of high quality if it is fit for its intended use. The quality of data is influenced by the underlying data model and its quality. One major quality problem is the heterogeneity of data as quality aspects such as understandability and interoperability are impaired. This heterogeneity may be caused by quality problems in the data model. Data heterogeneity can occur in particular when the information given is not structured enough and just captured in data values, often due to missing or non-suitable structure in the underlying data model. We propose a bottom-up approach to detecting quality problems in data models that manifest in heterogeneous data values. It supports an explorative analysis of the existing data and can be configured by domain experts according to their domain knowledge. All values of a selected data field are clustered by syntactic similarity. Thereby an overview of the data values’ diversity in syntax is provided. It shall help domain experts to understand how the data model is used in practice and to derive potential quality problems of the data model. We outline a proof-of-concept implementation and evaluate our approach using cultural heritage data.},
keywords={Uncertainty;Quality assurance;Documentation;Syntactics;Data models;Model driven engineering;Encoding;Data model;Model quality;Clustering;Semi structured data},
doi={10.1109/MODELS-C53483.2021.00027},
ISSN={},
month={Oct},}
@INPROCEEDINGS{5523332,
author={He, Yihai and Chang, Wenbing},
booktitle={2010 International Conference on Intelligent Computation Technology and Automation},
title={Research on the Computer Integrated Quality System for Product Design Process},
year={2010},
volume={3},
number={},
pages={148-151},
abstract={Aiming at the fact that there is no computer integrated quality system for quality control in product design phase, a data management model of design quality based on quality characteristics (QCs) is proposed, and based on this model an integrated quality control system suitable for product design phase is developed. Firstly, the content of QCs is defined and its key function in the process of product design quality control is emphasized, and the concept of design quality data is declared and the mapping relationships of product QCs and design quality data are given. Secondly, the data management model of design quality based on QCs is proposed, which is consistent with the standard of product data management (PDM). Finally, a computer Integrated Quality System for Design Process (DP-CIQS) is developed, the validity and effectiveness of DP-CIQS is verified via a preliminary application.},
keywords={Product design;Quality control;Quality management;Process design;Design engineering;Technology management;Assembly;Reliability engineering;Manufacturing processes;Fixtures;product design;computer integrated quality system;quality characteristics;design quality data;quality data management model},
doi={10.1109/ICICTA.2010.283},
ISSN={},
month={May},}
@INPROCEEDINGS{9232648,
author={Hossen, Md Ismail and Goh, Michael and Hossen, Abid and Rahman, Md. Armanur},
booktitle={2020 11th IEEE Control and System Graduate Research Colloquium (ICSGRC)},
title={A Study on the Aspects of Quality of Big Data on Online Business and Recent Tools and Trends Towards Cleaning Dirty Data},
year={2020},
volume={},
number={},
pages={209-213},
abstract={The reliability, efficiency, and accuracy of e-business depend on the quality of data that is associated with a buyer, seller, brokers, e-business portals, admins, managers, decision-makers and so on. However, maintaining the quality of data in e-business is very challenging. It is because e-business data typically comes from different communication channels and sources. Integrating and managing the data quality of different sources is generally much troublesome than dealing with traditional business data. Even though there are several data cleaning methods and tools exist those methods and tools have some constraints. None of them directly working, particularly on e-business data that motivates to do research to highlight the aspects of big data quality related to e-business. Therefore, this research demonstrates the problems related to data quality related to online business, discusses the existing literature of data quality, the current tools and techniques that are being used for data quality and provides a research finding highlighting the weaknesses of current tools to address the problem of online business.},
keywords={Data integrity;Tools;Companies;Cleaning;Task analysis;Machine learning;Regulation;E-business;Big data;data quality;dirty data;machine learning},
doi={10.1109/ICSGRC49013.2020.9232648},
ISSN={},
month={Aug},}
@INPROCEEDINGS{6409222,
author={Chen, Mengjie and Song, Meina and Han, Jing and Haihong, E.},
booktitle={2012 World Congress on Information and Communication Technologies},
title={Survey on data quality},
year={2012},
volume={},
number={},
pages={1009-1013},
abstract={With high quality of data, enterprises will add value of business. How-ever, poor data has resulted in waste of resource, low service efficiency and high costs in every area. In this paper, we firstly focus on basic issues of data quality, like where the quality problems come from and how to describe it. Then we study some cases of data quality management from a holistic enterprise perspective to the details of perspective, that is, hierarchical management architecture, frameworks, approaches and algorithm. Quality assessment of data is also an important theme, which is used to show whether data is good enough and to help people master credibility of data quality. We study some assessment algorithm and models. When problem occur, what tools should be used in a project is also included in the paper. Finally, we provide some outstanding research topics and unresolved issues for future.},
keywords={Data models;Quality assessment;Data mining;Standards organizations;data quality;data quality management;quality assessment;quality tool},
doi={10.1109/WICT.2012.6409222},
ISSN={},
month={Oct},}
@INPROCEEDINGS{8760967,
author={Barbosa, Wesley L. and Alves-Souza, Solange N. and Correa-Pizzigatti, Pedro and DeSouza, Luiz S.},
booktitle={2019 14th Iberian Conference on Information Systems and Technologies (CISTI)},
title={Data Quality Problems Identified in the Bioclimatic Data Collection Process - A Survey},
year={2019},
volume={},
number={},
pages={1-7},
abstract={Bioclimatic data support several researches that aim to identify the influence of climatic factors on biodiversity on the planet. In order to make these studies possible, the quality of the data that supports the analyzes must be guaranteed from the beginning of the data life cycle so that the results and models generated reflect the real scenario of the investigated phenomena. However, the collection of climate and biodiversity data presents significant challenges. This work performs a survey of the main quality problems identified in the bioclimatic data collection process. The methodological procedure consisted in identifying the problems, assigning the data quality dimension affected, and suggestions for possible solutions to the problems. The results of this survey showed that ambiguous methodological procedures during the data gathering and human interference are important factors for data quality impairment and the information obtained from these data. Thus, the correction of the data should focus on the collection processes and procedures, not the raw data itself.},
keywords={Biological system modeling;Bioinformatics;Data integrity;Biodiversity;Meteorology;Planets;data quality;data issues;climatic;biodiversity;bioclimatic},
doi={10.23919/CISTI.2019.8760967},
ISSN={2166-0727},
month={June},}
@INPROCEEDINGS{9458779,
author={Foroni, Daniele and Lissandrini, Matteo and Velegrakis, Yannis},
booktitle={2021 IEEE 37th International Conference on Data Engineering (ICDE)},
title={The F4U System for Understanding the Effects of Data Quality},
year={2021},
volume={},
number={},
pages={2717-2720},
abstract={We demonstrate a system that enables a data-centric approach in understanding data quality. Instead of directly quantifying data quality as traditionally done, it disrupts the quality of the dataset and monitors the deviations in the output of an analytic task at hand. It computes the correlation factor between the disruption and the deviation and uses it as the quality metric. This allows users to understand not only the quality of their dataset but also the effect that present and future quality issues have to the intended analytic tasks. This is a novel data-centric approach aimed at complementing existing solutions. On top of the new information that it provides, and in contrast to existing techniques of data quality, it neither requires knowledge of the clean datasets, nor of the constraints on which the data should comply.},
keywords={Measurement;Correlation;Data integrity;Conferences;Data engineering;Task analysis;Monitoring;Data Quality;Data Profiling;Data Mining;Data Cleaning},
doi={10.1109/ICDE51399.2021.00312},
ISSN={2375-026X},
month={April},}
@INPROCEEDINGS{6092314,
author={Woodring, Jonathan and Mniszewski, Susan and Brislawn, Christopher and DeMarle, David and Ahrens, James},
booktitle={2011 IEEE Symposium on Large Data Analysis and Visualization},
title={Revisiting wavelet compression for large-scale climate data using JPEG 2000 and ensuring data precision},
year={2011},
volume={},
number={},
pages={31-38},
abstract={We revisit wavelet compression by using a standards-based method to reduce large-scale data sizes for production scientific computing. Many of the bottlenecks in visualization and analysis come from limited bandwidth in data movement, from storage to networks. The majority of the processing time for visualization and analysis is spent reading or writing large-scale data or moving data from a remote site in a distance scenario. Using wavelet compression in JPEG 2000, we provide a mechanism to vary data transfer time versus data quality, so that a domain expert can improve data transfer time while quantifying compression effects on their data. By using a standards-based method, we are able to provide scientists with the state-of-the-art wavelet compression from the signal processing and data compression community, suitable for use in a production computing environment. To quantify compression effects, we focus on measuring bit rate versus maximum error as a quality metric to provide precision guarantees for scientific analysis on remotely compressed POP (Parallel Ocean Program) data.},
keywords={Data visualization;Transform coding;Image coding;Bit rate;Quantization;Wavelet transforms},
doi={10.1109/LDAV.2011.6092314},
ISSN={},
month={Oct},}
@INPROCEEDINGS{8416208,
author={Blanquer, Ignacio and Meira, Wagner},
booktitle={2018 48th Annual IEEE/IFIP International Conference on Dependable Systems and Networks Workshops (DSN-W)},
title={EUBra-BIGSEA, A Cloud-Centric Big Data Scientific Research Platform},
year={2018},
volume={},
number={},
pages={47-48},
abstract={This paper describes the achievements of project EUBra-BIGSEA, which has delivered programming models and data analytics tools for the development of distributed Big Data applications. As framework components, multiple data models are supported (e.g. data streams, multidimensional data, etc.) and efficient mechanisms to ensure privacy and security, on top of a QoS-aware layer for the smart and rapid provisioning of resources in a cloud-based environment.},
keywords={Big Data;Data analysis;Data privacy;Quality of service;Data models;Security;Biological system modeling;Cloud-Computing,-Big-Data,-Quality-of-Service,-Data-Analytics},
doi={10.1109/DSN-W.2018.00023},
ISSN={2325-6664},
month={June},}
@ARTICLE{8861049,
author={Kim, Sunho and Castillo, Ricardo Pérez Del and Caballero, Ismael and Lee, Jimwoo and Lee, Changsoo and Lee, Downgwoo and Lee, Sangyub and Mate, Alejandro},
journal={IEEE Access},
title={Extending Data Quality Management for Smart Connected Product Operations},
year={2019},
volume={7},
number={},
pages={144663-144678},
abstract={Smart connected product (SCP) operation embodies the concept of the internet of things (IoT). To increase the probability of success of SCP operations for customers, the high quality of the IoT data across operations is imperative. IoT data go beyond sensor data, as integrate some other various type of data such as timestamps, device metadata, business data, and external data through SCP operation processes. Therefore, traditional data-centric approaches that analyze sensor data and correct their errors are not enough to preserve, in long-term basis, adequate levels of quality of IoT data. This research provides and alternative framework of data quality management as a process-centric approach to improve the quality of IoT data. The proposed framework extends the process reference model (PRM) for data quality management (DQM) defined in ISO 8000-61, and tailored to fully adapt to the special requirements of the IoT data management. These involve several adaptations: first, the scope of the SCP operations for data quality management is determined, and the processes required for SCP operations are defined following the process description format of ISO 8000-61. Second, the relationship between the processes and the structure of the processes in the technology stack of the SCP operations are described to cover the actual nature of the IoT data flows. Finally, a new IoT DQM-PRM is proposed by integrating the processes for the SCP operations with DQM-PRM. When these processes are executed in the organization, the quality of IoT data composed of data of various types can be continuously improved and the utilization rate of SCP operations is expected to increase.},
keywords={Internet of Things;Data integrity;Business;Metadata;ISO Standards;Wireless sensor networks;Data analysis;IoT;Internet of Things;SCP;smart connected product;data quality;data quality management;process reference model;ISO 8000-61;DQM;PRM},
doi={10.1109/ACCESS.2019.2945124},
ISSN={2169-3536},
month={},}
@INPROCEEDINGS{8607406,
author={Mukwakungu, S. C. and Bakama, E. M. and Lumbwe, A. K. and Bolipombo, M. M. and Niati, D. and Ibrahimu, K. and Kasongo, J. E. and Mbohwa, C.},
booktitle={2018 IEEE International Conference on Industrial Engineering and Engineering Management (IEEM)},
title={Assessment of Quality of Service at the Main Laboratory of the LAB Aimed at Satisfying Internal Customer Needs},
year={2018},
volume={},
number={},
pages={586-591},
abstract={This paper's objectives are to establish and document internal clients' perception about the quality of service received at the Main Laboratory of the LAB, a national laboratory dealing with transmittable diseases in South Africa. The study followed a quantitative design approach with cross functional examinations. Data collection tool was based on “SERVQUAL” model. Findings show that in terms of the quality dimensions, the LAB's centres performed variably in many aspects and to a varying degree in different quality dimensions measured. Each centre had its own unique set of challenges. The recommendations made in this study can be implemented as a solution to the problems faced by the LAB and other similar departments. This study viewed from a South African perspective, is first of its kind as it explores the effectiveness of the implementation of a Quality Management System at a biosafety level 4, the only one on the African continent.},
keywords={Medical services;Quality of service;Tools;Reliability;Organizations;Standards;Service quality;SERVQUAL;Quality Management System (QMS)},
doi={10.1109/IEEM.2018.8607406},
ISSN={2157-362X},
month={Dec},}
@INPROCEEDINGS{8806407,
author={Wang, Yongzhi},
booktitle={2019 International Conference on Robots & Intelligent System (ICRIS)},
title={Research on Evaluation Method of Tourism Quality of Characteristic Towns in Southwest Minority Areas Based on ORA Network Data Analysis},
year={2019},
volume={},
number={},
pages={306-310},
abstract={In order to better promote the development of tourism scenic spots in small towns with ethnic characteristics, this paper proposes a tourism quality evaluation method for small towns with ethnic characteristics in southwest China based on ORA network data analysis. Based on ORA network data analysis principle, this paper integrates and perfects the content of tourism scenic spot quality evaluation of small towns with ethnic characteristics, and sets up the tourism quality evaluation standard of small towns with ethnic characteristics. According to the standard, the integrated content is graded and data evaluation is carried out according to different grade type standards, thus effectively completing the evaluation of tourism quality of small towns with ethnic characteristics in southwest China. Finally, through experiments, it is proved that the tourism quality evaluation method based on ORA network data analysis has higher accuracy and practicability than the traditional method.},
keywords={Indexes;Data analysis;Cultural differences;Industries;Quality assessment;Analytic hierarchy process;Standards;ORA;Data analysis;Southwest ethnic region;Town tourism;Quality assessment},
doi={10.1109/ICRIS.2019.00085},
ISSN={},
month={June},}
@INPROCEEDINGS{9006446,
author={Homayouni, Hajar and Ghosh, Sudipto and Ray, Indrakshi and Kahn, Michael G},
booktitle={2019 IEEE International Conference on Big Data (Big Data)},
title={An Interactive Data Quality Test Approach for Constraint Discovery and Fault Detection},
year={2019},
volume={},
number={},
pages={200-205},
abstract={Data quality tests validate heterogeneous data to detect violations of syntactic and semantic constraints. The specification of these constraints can be incomplete because domain experts typically specify them in an ad hoc manner. Existing automated test approaches can generate false alarms and do not explain the constraint violations while reporting faulty data records. In previous work, we proposed ADQuaTe, which is an automated data quality test approach that uses an unsupervised deep learning techni que (1) to discover constraints from big datasets that may have been missed by experts, and (2) to label as suspicious those records that violate the constraints. These records are grouped and explanations for constraint violations are presented to domain experts who determine whether or not the groups are actually faulty. This paper presents ADQuaTe2, which extends ADQuaTe to use an interactive learning technique that incorporates expert feedback to retrain the learning model and improve the accuracy of constraint discovery and fault detection. We evaluate the effectiveness of the approach on real-world datasets from a health data warehouse and a plant diagnosis database. We also use datasets with known faults from the UCI repository to evaluate the improvement in the accuracy of the approach after incorporating ground truth knowledge.},
keywords={Fault detection;Data integrity;Data models;Semantics;Decision trees;Self-organizing feature maps;Inspection;Big Data;Data quality tests;Explainable learning;Interactive learning;Unsupervised learning},
doi={10.1109/BigData47090.2019.9006446},
ISSN={},
month={Dec},}
@INPROCEEDINGS{8601548,
author={Zhong, Qing and Yao, Wenlin and Lin, Linxue and Wang, Gang and Xu, Zhong},
booktitle={2018 International Conference on Power System Technology (POWERCON)},
title={Data Analysis and Applications of the Power Quality Monitoring},
year={2018},
volume={},
number={},
pages={4035-4039},
abstract={This paper presents three applications to transform the power quality (PQ) monitoring data into the useful information. With the increasing volume of the PQ monitoring data, mining the values of the data is very important for the power system operations. Three applications are introduced with the PQ monitoring data in Guangzhou grid, China. Firstly, the cumulative probability of PQ monitoring data is applied to certificate the PQ limits according to the national standards. Secondly, three types of voltage sags are counted by the PQ monitoring data to show the severity of voltage sags in local grid. Thirdly, the correlation analysis is applied to show the impact of PQ problem on the device malfunctions. The correlation coefficients between the PQ monitoring data and the device malfunction data can show the impacts of PQ problems on the devices directly. The malfunctions of capacitors/inductors are relevant to the voltage deviation and harmonic distortion obviously which is shown by the correlation coefficients. It is a good attempt to translate the PQ monitoring data into the useful information, which can help the operators decide.},
keywords={Monitoring;Power quality;Big Data;Standards;Correlation coefficient;Correlation;Big data;Power quality;Data analysis;Correlation coefficient},
doi={10.1109/POWERCON.2018.8601548},
ISSN={},
month={Nov},}
@INPROCEEDINGS{9617555,
author={Rahmani, Dita Aprillia and Kusumasari, Tien Fabrianti and Alam, Ekky Novriza},
booktitle={2021 International Conference on Data Science and Its Applications (ICoDSA)},
title={Addition of Process Decomposition in Open Source Tools-Based Cleansing Data Modules},
year={2021},
volume={},
number={},
pages={129-134},
abstract={In today's technological developments, data has an important role in supporting the achievement of goals for the company. The importance of data for companies is to fulfill the quality of supporting the company's business needs. The high quality that data has is of critical value to the company. However, there are many errors in the data that reduce the quality. Low-quality data, i.e., the data is inaccurate, incomplete, or out of date. There is a need for data quality management or Data Quality Management to manage data quality improvements to become consistent, accurate, complete, timely, and unique data. Regulating the quality of the data requires data cleansing. Data cleansing is a method to improve low-quality data by producing high-quality data. Therefore, this study will discuss the analysis and design of the decomposition of process packages for data cleansing in order to improve the quality of data that does not meet the company's needs. In the design carried out, the authors provide several solutions based on analysis to meet the needs of the company's data cleansing process flexibly with the decomposition of the process package that will be implemented using open-source tools, namely Pentaho Data Integration as the result of this research. There is a comparative evaluation using OpenRefine, which results in data cleansing using Pentaho Data Integration which is superior in the overall data cleaning process.},
keywords={Data integrity;Data integration;Companies;Tools;Data science;Cleaning;Open source software;data cleansing;pentaho data integration;openrefine},
doi={10.1109/ICoDSA53588.2021.9617555},
ISSN={},
month={Oct},}
@INPROCEEDINGS{8978560,
author={Mukwakungu, S.C. and Motapane, T.I. and Mbohwa, C.},
booktitle={2019 IEEE International Conference on Industrial Engineering and Engineering Management (IEEM)},
title={The Assessment of Internal Service Quality Perception of System Administrators - Case of Services Provided by Data Centre Hosting to Local Bank in South Africa},
year={2019},
volume={},
number={},
pages={400-404},
abstract={This study assesses the perception of service quality provided by data hosting companies to system administrators at one of South Africa's five major banks in Johannesburg in terms of the difference between the expected quality level and the actual service quality received to suggest recommendations aimed at resolving the issues that would be exposed. The SERVQUAL model was adopted in the survey design to classify key service quality dimensions. With a 100% response rate, an analysis of the gap score was conducted on the data collected. The findings showed that such an exercise has never been conducted at the company over the past five years, and it indicated that the overall quality of service is at its lowest as perceived by system administrators which leads to a constantly decreasing level of customer satisfaction. From the evaluation of all the quality dimensions, none of them met customers satisfaction criteria, with all the dimensions presenting negative gap scores. The study recommends that data hosting centers should implement data center service quality (DCSQ) to successfully meet the quality service expectations of system administrators.},
keywords={SERVQUAL;Quality Control;Data Hosting Centers;Service Level Objectives},
doi={10.1109/IEEM44572.2019.8978560},
ISSN={2157-362X},
month={Dec},}
@INPROCEEDINGS{9421436,
author={Wang, Xiaofeng and Jiang, Yong and Zhan, Gaofeng and Zhao, Tong},
booktitle={2020 5th International Conference on Mechanical, Control and Computer Engineering (ICMCCE)},
title={Quality Analysis and Evaluation Method for Multisource Aggregation Data based on Structural Equation Model},
year={2020},
volume={},
number={},
pages={1279-1282},
abstract={In the era of big data, how to evaluate the data quality of multi-source aggregation data is very important. The reason is that uneven data quality will directly lead to inaccurate or ambiguous data in the database, and indirectly lead to the deviation of subsequent data mining and decision-making. In this paper, structural equation model(SEM) is introduced to explore the effectiveness of various data quality evaluation indicators in data aggregation and finding out internal relationship between them. A new quality evaluation method of multi-source aggregation data is proposed, based on the regression's significance analysis and factor loads of each observation index in the SEM model. The case analysis shows that the proposed method is feasible and can be used to evaluate the quality of multi-source aggregation data adaptively for a long time.},
keywords={Analytical models;Adaptation models;Numerical analysis;Data integrity;Computational modeling;Urban areas;Data aggregation;Data Aggregation;Data Analysis;Quality Evaluation;Structural Equation Model},
doi={10.1109/ICMCCE51767.2020.00280},
ISSN={},
month={Dec},}
@INPROCEEDINGS{7761250,
author={Roarty, Hugh and Palamara, Laura and Kohut, Josh and Glenn, Scott},
booktitle={OCEANS 2016 MTS/IEEE Monterey},
title={Automated quality control of high frequency radar data II},
year={2016},
volume={},
number={},
pages={1-3},
abstract={A paper was presented at OCEANS'12 by the authors on a similar topic. The original paper was more of a formulation of ideas for automated quality control of HF radar data with only a listing of potential tests. This paper lays out the framework for the quality assurance methods and quality control tests for the entire data processing chain. The paper also synthesizes a number of papers that have been presented recently on this topic of HF radar data quality control.},
keywords={Radar;Quality control;Sea measurements;Sea surface;Real-time systems;Quality assurance;radar;remote sensing;quality control;MARACOOS},
doi={10.1109/OCEANS.2016.7761250},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{9491766,
author={Al Omari, Hania and Barham, Sawsan and Qusef, Abdallah},
booktitle={2021 International Conference on Information Technology (ICIT)},
title={Data Strategy and Its Impact on Open Government Data Quality},
year={2021},
volume={},
number={},
pages={648-653},
abstract={Technology is playing a major role in decision-making and innovation through its wide capabilities of data mining and analysis. Developed countries have recognized data as an asset that can transform their services and economy to a better level if used effectively. In this paper, the authors discuss how data strategy can unleash the power of data and how such strategy affects open government data quality positively and help countries to achieve better outcomes in terms of sharing and reusing data. This paper analyzes UK & US data strategies and concludes the main elements that data strategy should include. Also, Jordan’s model in open government data is discussed and the need to formulate a data strategy to enhance data sharing and quality of open data is explained.},
keywords={Technological innovation;Data integrity;Government;Decision making;Transforms;Data models;Information technology;Open Data;Open Government Data;Quality Management;Data Strategy},
doi={10.1109/ICIT52682.2021.9491766},
ISSN={},
month={July},}
@INPROCEEDINGS{7840595,
author={Haryadi, Adiska Fardani and Hulstijn, Joris and Wahyudi, Agung and van der Voort, Haiko and Janssen, Marijn},
booktitle={2016 IEEE International Conference on Big Data (Big Data)},
title={Antecedents of big data quality: An empirical examination in financial service organizations},
year={2016},
volume={},
number={},
pages={116-121},
abstract={Big data has been acknowledged for its enormous potential. In contrast to the potential, in a recent survey more than half of financial service organizations reported that big data has not delivered the expected value. One of the main reasons for this is related to data quality. The objective of this research is to identify the antecedents of big data quality in financial institutions. This will help to understand how data quality from big data analysis can be improved. For this, a literature review was performed and data was collected using three case studies, followed by content analysis. The overall findings indicate that there are no fundamentally new data quality issues in big data projects. Nevertheless, the complexity of the issues is higher, which makes it harder to assess and attain data quality in big data projects compared to the traditional projects. Ten antecedents of big data quality were identified encompassing data, technology, people, process and procedure, organization, and external aspects.},
keywords={Big data;Industries;Insurance;Companies;Finance;big data;data quality;big data quality;antecedents;finance},
doi={10.1109/BigData.2016.7840595},
ISSN={},
month={Dec},}
@INPROCEEDINGS{6118854,
author={Munawar and Salim, Naomie and Ibrahim, Roliana},
booktitle={2011 IEEE Ninth International Conference on Dependable, Autonomic and Secure Computing},
title={Towards Data Quality into the Data Warehouse Development},
year={2011},
volume={},
number={},
pages={1199-1206},
abstract={Commonly, DW development methodologies, paying little attention to the problem of data quality and completeness. One of the common mistakes made during the planning of a data warehousing project is to assume that data quality will be addressed during testing. In addition to the data warehouse development methodologies, we will introduce in this paper a new approach to data warehouse development. This proposal will be based on integration data quality into the whole data warehouse development phase, denoted by: integrated requirement analysis for designing data warehouse (IRADAH). This paper shows that data quality is not only an integrated part of data warehouse project, but will remain a sustained and ongoing activity.},
keywords={Data warehouses;Data models;Organizations;Adaptation models;Warehousing;Interviews;Data warehouse;data quality;data quality dimension;data quality integration},
doi={10.1109/DASC.2011.194},
ISSN={},
month={Dec},}
@INPROCEEDINGS{7870183,
author={Clarke, Roger},
booktitle={2016 European Intelligence and Security Informatics Conference (EISIC)},
title={Quality Assurance for Security Applications of Big Data},
year={2016},
volume={},
number={},
pages={1-8},
abstract={The quality of inferences drawn from data, big or small, is heavily dependent on the quality of the data and the quality of the processes applied to it. Big data analytics is emerging from laboratories and being applied to intelligence and security needs. To achieve confidence in the outcomes of these applications, a quality assurance framework is needed. This paper outlines the challenges, and draws attention to the consequences of misconceived and misapplied projects. It presents key aspects of the necessary risk assessment and risk management approaches, and suggests opportunities for research.},
keywords={Big data;Q-factor;Sociology;Statistics;Security;Reliability;Quality assurance;risk assessment;risk management;information quality;data semantics;data scrubbing;decision quality;transparency},
doi={10.1109/EISIC.2016.010},
ISSN={},
month={Aug},}
@INPROCEEDINGS{4341093,
author={Wang, Keqin and Tong, Shurong and Eynard, Benoit and Roucoules, Lionel and Matta, Nada},
booktitle={2007 International Conference on Wireless Communications, Networking and Mobile Computing},
title={Application of Data Mining in Manufacturing Quality Data},
year={2007},
volume={},
number={},
pages={5382-5385},
abstract={Knowledge on product quality is one of the most important knowledge sources throughout the product lifecycle for the efficiency and effectiveness of product design decisions. To provide quality related knowledge, this paper proposed one data mining based knowledge discovery approach. This approach can extract knowledge on product quality from large volume of quality related manufacturing data. The effectiveness of this approach is illustrated and validated by an example adapted from literature. Finally, some conclusions and future works are discussed.},
keywords={Data mining;Delta modulation;Product design;Manufacturing processes;Knowledge management;Data engineering;Quality management;Mechanical systems;Systems engineering and theory;Uncertainty},
doi={10.1109/WICOM.2007.1318},
ISSN={2161-9654},
month={Sep.},}
@INPROCEEDINGS{9458106,
author={Meli, Matthew and Gatt, Edward and Casha, Owen and Grech, Ivan and Micallef, Joseph},
booktitle={2020 International Conference on Computational Science and Computational Intelligence (CSCI)},
title={A Low Cost LoRa-based IoT Big Data Capture and Analysis System for Indoor Air Quality Monitoring},
year={2020},
volume={},
number={},
pages={376-381},
abstract={This paper presents a low cost LoRa-based IoT big data capture and analysis system for indoor air quality monitoring. This system is presented as an alternative solution to expensive and bulky indoor air quality monitors. It enables multiple low cost nodes to be distributed within a building such that extensive location-based indoor air quality data is generated. This data is captured by a gateway and forwarded to a cloud-based LoRaWAN network which in turn publishes the received data via MQTT. A cloud-based data forwarding server is used to capture, format and store this big data on a cloud-based document-oriented database. Cloud-based services are used for data visualization and analysis. Periodic indoor air quality graphs along with air quality index and thermal comfort index heat maps are generated.},
keywords={Temperature measurement;Temperature sensors;Cloud computing;Data visualization;Big Data;Logic gates;Particle measurements;IoT;LPWAN;Indoor Air Quality;Big Data;LoRa},
doi={10.1109/CSCI51800.2020.00070},
ISSN={},
month={Dec},}
@INPROCEEDINGS{7339050,
author={Huang, Min and Zhang, Tao and Wang, Jingyang and Zhu, Likun},
booktitle={2015 6th IEEE International Conference on Software Engineering and Service Science (ICSESS)},
title={A new air quality forecasting model using data mining and artificial neural network},
year={2015},
volume={},
number={},
pages={259-262},
abstract={In this paper, we have established a forecasting model of atmospheric pollution. The forecasting model which combines with data mining techniques and BP neural network algorithm is based on the monitoring data of air pollution obtained from Shijiazhuang air quality monitoring stations. Firstly this model uses the data mining technology to find the factors which affect air quality. Secondly it uses these factors data to train the neural network. Finally, the evaluation test of the forecasting model is evaluated. The results show that: The atmospheric quality forecasting model established in this paper can well meet the needs of practical application, because it has higher forecasting accuracy. The forecasting model improves the effectiveness and practicability, and can provide more reliable decision evidence for environmental protection departments.},
keywords={Atmospheric modeling;Predictive models;Pollution;Air quality;Forecasting;Meteorology;Neural networks;air quality forecasting;data mining;BP Neural Networks},
doi={10.1109/ICSESS.2015.7339050},
ISSN={2327-0594},
month={Sep.},}
@INPROCEEDINGS{9674175,
author={Yujun, Yang and Yimei, Yang and Wang, Zhou and Hongbo, Xiao and Liyun, Li},
booktitle={2021 18th International Computer Conference on Wavelet Active Media Technology and Information Processing (ICCWAMTIP)},
title={Research On The Construction of Agricultural Product Quality Maintenance And Quality Traceability System Based On Big Data},
year={2021},
volume={},
number={},
pages={655-659},
abstract={The quality and safety of agricultural products has been widely concerned by the whole society in recent years. Therefore, the traceability of agricultural products is a research hotspot of scholars. The quality and safety traceability system of agricultural products is an important method to monitor the quality and safety of agricultural products. The emergence and use of big data help to solve the problems of high cost, scattered information and incomplete industrial chain of quality and safety traceability of agricultural products and improve the efficiency and accuracy of the quality and safety traceability system of agricultural products. There are still some problems in the application of big data, such as weak pertinence. It is necessary to mine and use big data to realize the traceability of agricultural products.},
keywords={Costs;Information processing;Big Data;Media;Maintenance engineering;Wavelet analysis;Agricultural products;Quality traceability;agricultural product quality;big data;agricultural product traceability;data analysis},
doi={10.1109/ICCWAMTIP53232.2021.9674175},
ISSN={2576-8964},
month={Dec},}
@INPROCEEDINGS{5477583,
author={Zhao, Xiaofei and Huang, Zhiqiu},
booktitle={2010 2nd IEEE International Conference on Information Management and Engineering},
title={A quality evaluation approach for OLAP metadata of multidimensional OLAP data},
year={2010},
volume={},
number={},
pages={357-361},
abstract={The quality of metadata in OnLine Analytical Processing(OLAP) process has remarkable influence on the stability and reliability of OLAP tools. Model-driven metadata integration approach introduces the metadata management concept based on the object oriented paradigm for modeling and querying OLAP metadata of multidimensional data. In this model, the basic concepts of the object oriented model including object, class, and relationship between objects are applied to describe objects of multidimensional data and the OLAP operations. Thus the metadata of multidimensional OLAP data is modeled as a fact and a set of elements that are organized into class hierarchy. However, the quality evaluation of OLAP metadata which describes multidimensional data is difficult because of the structural essential of the metadata. In this paper, we propose a quality evaluation approach for model-driven OLAP metadata. This approach depends on a static formalization mechanism, various reasoning procedures supported by the formalization can be used for the quality evaluation tasks.},
keywords={Multidimensional systems;Object oriented modeling;Computer science;Data warehouses;Aggregates;Information science;Information analysis;Stability analysis;Quality management;Engineering management;OnLine Analytical Processing(OLAP);Metadata Management;Quality Evaluation;Model-driven Architecture;Formalization Mechanism},
doi={10.1109/ICIME.2010.5477583},
ISSN={},
month={April},}
@INPROCEEDINGS{9309805,
author={Ntanzi, K. and Lumbwe, A. K. and Mukwakungu, S. C. and Sukdeo, N.},
booktitle={2020 IEEE International Conference on Industrial Engineering and Engineering Management (IEEM)},
title={The Relationship Between the Implementation of Quality Management Practices and Service Quality in the South African Financial Service Industry},
year={2020},
volume={},
number={},
pages={1235-1240},
abstract={The purpose of this study is to evaluate the effect of quality management practices on customers, employees and service quality in the financial service industry. The sample of 30 customers and 30 employees were selected based on stratified and snowball sampling procedures respectively. This research used a mixed approach to collect data. The study was conducted by engaging with customers (business owners, students and the working class) in the form of interviews and questionnaires, and by using secondary data. The main quality principles was mainly based on employee satisfaction, customer focus and continual improvement to establish how customers choose the bank they bank depending on their different classes. The results reveal that employee satisfaction had a direct relationship with the level of quality the organization produced. In addition, business owners and the working class were more likely to consider the service quality of the organization before they decide to bank with them. This research shows the significance of the implementing quality practices in the financial service industry in order to gain or retain customers.},
keywords={Organizations;Quality management;Customer relationship management;Industries;Standards organizations;Financial services;Quality control;Quality management;service quality;financial service industry},
doi={10.1109/IEEM45057.2020.9309805},
ISSN={},
month={Dec},}
@INPROCEEDINGS{8258543,
author={Liu, Lixin and Chen, Jun},
booktitle={2017 IEEE International Conference on Big Data (Big Data)},
title={The influences of deep-sea vision data quality on observational analysis},
year={2017},
volume={},
number={},
pages={4789-4791},
abstract={Deep-sea study by optical observation method is an interdisciplinary subject and faces plenty of difficulties. To find out the influences of vision data quality, characteristic of vision data for deep-sea observation is analyzed, and a deep-sea landing experiment has been implemented. Data quality analyzing based on real deep-sea vision data that collected in the in-situ observation platform is actualized. It is expected that the research on influence mechanism of deep-sea vision quality is beneficial to the detection of region of interest, the judging of animal existence, the classification of species, and the trajectories labeling. Further analyzing on unsupervised deep-sea vision data quality control is necessary.},
keywords={Scattering;Optical sensors;Optical imaging;Fish;Backscatter;deep-sea observation;vision data quality;automatic judging},
doi={10.1109/BigData.2017.8258543},
ISSN={},
month={Dec},}
@ARTICLE{9268064,
author={Zhao, Yuxi and Gong, Xiaowen and Chen, Xu},
journal={IEEE Transactions on Mobile Computing},
title={Privacy-Preserving Incentive Mechanisms for Truthful Data Quality in Data Crowdsourcing},
year={2022},
volume={21},
number={7},
pages={2518-2532},
abstract={Data crowdsourcing is a promising paradigm that leverages the “wisdom” of a potentially large crowd of “workers” in many application domains. Quality-aware crowdsourcing is beneficial as it makes use of workers’ data quality to perform task allocation and data aggregation. However, a worker’s quality and data can be her private information that she may have incentive to misreport to the crowdsourcing requester. Moreover, a worker’s quality and data can depend on her sensitive information (e.g., location), which can be inferred from the outcomes of task allocation and data aggregation by an adversary. In this paper, we devise Privacy-preserving crowdsourcing mechanisms for truthful Data Quality Elicitation (PDQE). In these mechanisms, we design differentially private task allocation and data aggregation algorithms to prevent the inference of a worker’s quality and data from the outcomes of these algorithms. In the meantime, the mechanisms also incentivize workers to truthfully report their quality and data and make desired efforts. We first focus on the mechanisms for a single task (S-PDQE) and then extend it to the case of multiple tasks (M-PDQE). We further show that both the mechanisms achieve a bounded performance gap compared to the optimal strategy. We evaluate the proposed mechanisms using simulations based on real-world data, which corroborate their highly-desired properties on truthful data quality elicitation, data accuracy and privacy protection.},
keywords={Crowdsourcing;Task analysis;Data integrity;Data privacy;Resource management;Mobile computing;Inference algorithms;Data crowdsourcing;differentially privacy;incentive mechanism;data quality elicitation},
doi={10.1109/TMC.2020.3040138},
ISSN={1558-0660},
month={July},}
@INPROCEEDINGS{8725744,
author={Xiang, Zheng and Xiaofang, Liu and Weigang, Gao},
booktitle={2019 IEEE 4th International Conference on Cloud Computing and Big Data Analysis (ICCCBDA)},
title={Analysis of the Application of Military Big Data in Equipment Quality Information Management},
year={2019},
volume={},
number={},
pages={68-71},
abstract={This At present, big data has risen to the national strategy. Big data is fully integrated into the military field, becoming the driving force of military scientific research, the core element of construction management, and an important resource for war success. This paper mainly expounds the basic connotation of big data technology and military big data, and analyzes the application of military big data in equipment quality information management, and proposes information collection, storage, analysis, processing, exchange and feedback on equipment quality information management. The countermeasures provide methods and basis for military big data in equipment information management.},
keywords={Big Data;Military equipment;Information management;Distributed databases;Data analysis;Maintenance engineering;military big data;equipment quality information;management},
doi={10.1109/ICCCBDA.2019.8725744},
ISSN={},
month={April},}
@INPROCEEDINGS{8477366,
author={Bouhamidi, Moulay Hafid and Amar, Amine},
booktitle={2017 International Renewable and Sustainable Energy Conference (IRSEC)},
title={Data Quality Analysis for a Smart Solar Resource Assessment},
year={2017},
volume={},
number={},
pages={1-5},
abstract={The development of solar-based projects stimulates the demand's increase in the concern of reliable data, especially for the life cycle cost analysis of solar technologies. Reliable data requires several important aspects that should be taken into consideration. We can evoke for example, the instrument performance specifications, methods of installation, frequency of calibrations and the regularity of maintenance and data analysis. However, when analyzing solar radiation dataset, many questionable values are still identified. Thus, the crucial step is to develop more accurate automatic procedures for performing quality control (QC) of solar data, in addition to the maintenance and human monitoring. The objective of the automatic procedures is to identify aberrant observations and to reduce the manual workload of human validators, in order to guarantee reliability and allow comparison analysis between different observational datasets. In the same context, we present and we analyze an implemented procedure, using several tests, for 3 stations of Moroccan Agency for Solar Energy (masen). The analysis shows that the adopted procedure is very efficient for quality assessment. However, the procedure should be integrated in a global quality process, to ensure the procurement of bankable data.},
keywords={Sensors;Data integrity;Cleaning;Maintenance engineering;Solar radiation;Calibration;Wind speed;component;Data quality;Irradiations;Meterological data;Bankable data},
doi={10.1109/IRSEC.2017.8477366},
ISSN={2380-7393},
month={Dec},}
@INPROCEEDINGS{9477756,
author={Li, Yongquan and Wang, Yanqing and Song, Xiankun and Cheng, Xinyu},
booktitle={2021 IEEE 3rd International Conference on Computer Science and Educational Informatization (CSEI)},
title={Research on Course Quality Evaluation System Based on Artificial Intelligence},
year={2021},
volume={},
number={},
pages={163-167},
abstract={In view of the complexity and inefficiency of the course quality assessment process, this paper conducts an in depth study of behavior analysis technology and designs an automated and intelligent curriculum quality assessment system. The system uses various artificial intelligence algorithms to analyze the collected images and text data related to classroom quality and produce objective visual results. The course quality assessment system is divided into four functional modules: curriculum quality index management, data acquisition management, data analysis management and system of rights management, which can quickly generate visual course quality assessment results based on data analysis results. By comparison, the results of intelligent evaluation and existing artificial evaluation have high consistency. The system is fully functional and intelligent at the touch of a button, which can greatly improve the efficiency of curriculum quality assessment and reduce the complexity of work.},
keywords={Visualization;Data analysis;Conferences;Neural networks;Education;Data acquisition;Quality assessment;artificial intelligence;data analysis;course quality;system design},
doi={10.1109/CSEI51395.2021.9477756},
ISSN={},
month={June},}
@INPROCEEDINGS{5380432,
author={Liang, Siyuan and Wu, Kening},
booktitle={2009 Second International Conference on Computer and Electrical Engineering},
title={Design of Data Management System for Agricultural Land Quality Dynamic Monitoring},
year={2009},
volume={1},
number={},
pages={487-491},
abstract={The design of a dynamic monitoring data management system for agricultural land is discussed in this paper. Advances in computer technology have simplified the process for dynamic monitoring of agricultural land quality. The main points discussed are: analysis of the data management system's functional requirements for dynamic monitoring; analysis and design for management system data flow and data storage structure; introduction on software module design; discussion and additional explanation of the system.},
keywords={Quality management;Computerized monitoring;Multimedia databases;Technology management;Memory;Data analysis;Conference management;Engineering management;Geology;Agriculture;dynamic monitoring of the quality of agricultural land;data management;system design},
doi={10.1109/ICCEE.2009.186},
ISSN={},
month={Dec},}
@INPROCEEDINGS{8343025,
author={Cai, Hong-xia and Wei, Zhuang-yu},
booktitle={2017 8th IEEE International Conference on Software Engineering and Service Science (ICSESS)},
title={Analysis of civil aircraft quality data under the support of big data},
year={2017},
volume={},
number={},
pages={766-770},
abstract={In the production assembly manufacturing process, a large amount of quality data has been generated by civil aircraft equipment system. With the passage of time and the accumulation of data, these massive data cannot be dealt with effectively using traditional statistical analysis of discrete manufacturing industry. To solve this problem, the method of quality data analysis for unsupervised learning presented in this paper was developed, after evaluating the generating characteristics of the civil aircraft quality data and the problems associated with the processing of the traditional quality data analysis. On this basis, in this paper, according to the disorder association, complex structure and large amount of data of the civil aircraft quality data, the data mining association analysis Apriori algorithm and the big data Splunk platform are introduced to effectively reduce the complexity of the quality data analysis through the complementary advantages of both, and put the data in an orderly, coherent state. The results show that the developed method is effective with high efficiency value.},
keywords={Aircraft;Aircraft manufacture;Production;Data mining;Big Data;Itemsets;quality oriented;aircraft;data analysis;Apriori algorithm;big data Splunk platform},
doi={10.1109/ICSESS.2017.8343025},
ISSN={2327-0594},
month={Nov},}
@INPROCEEDINGS{4592824,
author={Yao, Yiyong and Dai, Gang and Zhao, Liping},
booktitle={2008 7th World Congress on Intelligent Control and Automation},
title={Embedded quality analysis platform based on HPI technology},
year={2008},
volume={},
number={},
pages={5847-5851},
abstract={The embedded quality data acquisition and analysis platform which meets the demand of field real-time quality control against more and more complicated quality control in inter-enterprise is presented in this paper. With HPI technology and regionalization of HPI memory mapping strategy first being adopted in this platform and the accomplishment of HPI driver under Windows CE.net, seamless connection between ARM and DSP is well implemented. Based on event-driven strategy for field quality data real-time acquisition and chain table priority strategy for multi-task scheduling, real-time mixture quality data acquisition and analysis on DSP and the friendly human-interface for real-time display and network data exchange on ARM are realized through HPI technology. System platform for field data dynamic processing and analysis is implemented and improved accordingly.},
keywords={Data acquisition;Digital signal processing;Real time systems;Quality control;Driver circuits;Manufacturing;Hardware;Field quality analysis;HPI driver;chain table based on priority;Event-driven},
doi={10.1109/WCICA.2008.4592824},
ISSN={},
month={June},}
@INPROCEEDINGS{8931867,
author={Bicevskis, Janis and Nikiforova, Anastasija and Bicevska, Zane and Oditis, Ivo and Karnitis, Girts},
booktitle={2019 Sixth International Conference on Social Networks Analysis, Management and Security (SNAMS)},
title={A Step Towards a Data Quality Theory},
year={2019},
volume={},
number={},
pages={303-308},
abstract={Data quality issues have been topical for many decades. However, a unified data quality theory has not been proposed yet, since many concepts associated with the term “data quality” are not straightforward enough. The paper proposes a user-oriented data quality theory based on clearly defined concepts. The concepts are defined by using three groups of domain-specific languages (DSLs): (1) the first group uses the concept of a data object to describe the data to be analysed, (2) the second group describes the data quality requirements, and (3) the third group describes the process of data quality evaluation. The proposed idea proved to be simple enough, but at the same time very effective in identifying data defects, despite the different structures of data sets and the complexity of data. Approbation of the approach demonstrated several advantages: (a) a graphical data quality model allows defining of data quality even by non-IT and non-data quality professionals, (b) data quality model is not related to the information system that has accumulated data, i.e., this approach lets users analyse the “third-party” data, and (c) data quality can be described at least at two levels of abstraction - informally, using natural language, or formally, including executable program routines or SQL statements.},
keywords={Data integrity;Data models;Data mining;Analytical models;Social networking (online);data quality;data object;domain-specific language;executable model},
doi={10.1109/SNAMS.2019.8931867},
ISSN={},
month={Oct},}
@INPROCEEDINGS{5331803,
author={Lu, Jinsuo and Huang, Tinglin},
booktitle={2009 Fifth International Joint Conference on INC, IMS and IDC},
title={Data Mining on Forecast Raw Water Quality from Online Monitoring Station Based on Decision-Making Tree},
year={2009},
volume={},
number={},
pages={706-709},
abstract={The excessive propagation of algae caused by eutrophication of aquatic environment in the urban source water supply is the main issues of concern to drinking water purification industry in recent years. The prediction of algae in raw water can offer time guarantee for operation of contingency caused by excessive propagation of algae which can ensure the safety of water supply. In the study, we collected 115 daily measured data about indirect monitoring of raw water quality of algae and solar irradiance data from online monitoring and direct-line artificial monitoring of chlorophyll content of raw water. We select decision-making tree which is very visible and easy realized as data mining tools, and set up decision-making tree model which is used to predict the level of chlorophyll in raw water in next day. To enable online monitoring data and artificial monitoring data with the same dimension, combined with the algal growth dynamics, we transform several on-line monitoring data of dissolved oxygen and solar irradiance data in one day into one data per day, that is mean calculating the average standard deviation and average. The former 100 sets of data are used to train and set up decision-making tree model which is to predict the level of chlorophyll in next day. The rest 15 sets of data are used to test data. The results of simulation show that the prediction accuracy can reach 80%.},
keywords={Data mining;Monitoring;Decision making;Algae;Predictive models;Water resources;Purification;Beverage industry;Safety;Testing;data mining;water quality;forcasting;decision-making tree},
doi={10.1109/NCM.2009.261},
ISSN={},
month={Aug},}
@INPROCEEDINGS{9732098,
author={Wrembel, Robert},
booktitle={2021 Eighth International Conference on Social Network Analysis, Management and Security (SNAMS)},
title={Still Open Problems in Data Warehouse and Data Lake Research: extended abstract},
year={2021},
volume={},
number={},
pages={01-03},
abstract={During recent years, we observe a widespread of new data sources, especially all types of social media and IoT devices, which produce huge data volumes, whose content ranges from fully structured to totally unstructured. All these types of data are commonly referred to as big data. They are typically described by the three most important characteristics, called 3V [1], namely: an extremely large volume, a variety of data models and structures (data representations), as well as a high velocity at which data are generated. We argue that out of these three Vs, the most challenging is variety [2]. Such data need to be integrated and transformed into a common representation, which is suitable for analysis, in a similar manner as traditional (mainly table-like) data.},
keywords={Social networking (online);Soft sensors;Transforms;Data warehouses;Big Data applications;Data models;Security;data integration;data warehouse;data lake;big data;extract transform load;data processing workflow;data processing pipeline;data quality;ETL optimization;data source evolution;metadata},
doi={10.1109/SNAMS53716.2021.9732098},
ISSN={},
month={Dec},}
@INPROCEEDINGS{9642916,
author={Kunakov, Egor P. and Gulov, Alexei E. and Lontsikh, Natalia P. and Rodionov, Nikita S. and Golovina, Elena Y.},
booktitle={2021 International Conference on Quality Management, Transport and Information Security, Information Technologies (IT&QM&IS)},
title={Applicability of Quality Metrics and New Approaches to the PDCA Cycle in Improving Quality Control and Management Systems Applied in Aircraft Manufacturing Processes},
year={2021},
volume={},
number={},
pages={305-308},
abstract={Digital control and information security technologies are used in modern technological processes. Integrated control tools designed to make correct decisions based on the electronic models should be used in the development of technological systems. The problems of interface quality assurance are due to complex metrics measurement and analysis processes. An increase in the number and duration of management time delays and mismanagement of large volumes of data increase the number of errors and test time and deteriorate the productivity of software tools used for determining the applicability of quality metrics. The article describes the process of improvement of technological parameters and characteristics by improving the Deming PDCA quality management cycle and software interfaces. It describes the role of risk accounting in the elimination of inconsistencies in the systems and processes.},
keywords={Measurement;Productivity;Digital control;Quality assurance;Manufacturing processes;Process control;Information security;Information technology;engineering technology;information systems;user interface development;software product;quality management system;risk;process approach;continuous improvement;Deming cycle},
doi={10.1109/ITQMIS53292.2021.9642916},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{8290181,
author={Xiu, Q. and Muro, K.},
booktitle={2017 IEEE International Conference on Industrial Engineering and Engineering Management (IEEM)},
title={Robust inference traceability technology for product quality enhancement},
year={2017},
volume={},
number={},
pages={1699-1703},
abstract={In manufacturing industry, it is important to analyze defect manufacturing data for product quality enhancement. Manufacturing data for defect analysis are retrieved by individual ID, which is unique identifier assigned to each individual product. However, in production lines existing machining processes such as sintering process and cutting process, individual ID can't be attached. Hence, there arises a problem that manufacturing data for defect analysis can't be prepared. Therefore, we developed Robust Inference Traceability (RIT) technology which can accurately estimate individual ID in various manufacturing conditions such as processing time variation, parallel process and reversal of production order. RIT manages it by absorbing lead time variation and estimating production time. The result of applying RIT to actual manufacturing data shows that the technology obtained an average inference accuracy of 92.9%. As a result, it can be estimated that product quality can be enhanced by defect analysis such as root cause analysis.},
keywords={Quality assessment;Product design;Manufacturing processes;Estimation;Correlation;data preparation;production time;quality control;traceability},
doi={10.1109/IEEM.2017.8290181},
ISSN={2157-362X},
month={Dec},}
@INPROCEEDINGS{9609770,
author={Munawar},
booktitle={2021 1st International Conference on Computer Science and Artificial Intelligence (ICCSAI)},
title={Extract Transform Loading (ETL) Based Data Quality for Data Warehouse Development},
year={2021},
volume={1},
number={},
pages={373-378},
abstract={Extract Transform Loading (ETL) plays a decisive role in data warehouse (DW) construction. It involves retrieval informations from multiple sources to improve information quality in DW for decision making process. A DW development relies on the development of ETL. Therefore, ETL conceptual model not only represents an overview of overall process, but also as a mapping amongst data sources, DW targets and required transformation to make sure that data quality (DQ) dimensions are incorporated in order to meet the requirements. In this paper, an ETL framework is proposed which incorporates data quality to improve information processes in data warehouse development through ‘the story’ of process whilst others framework more to technical approach. In order to be useful, the proposed framework compared with other framework in case of advantages and disadvantages for future improvement.},
keywords={Computer science;Data integrity;Loading;Decision making;Transforms;Data warehouses;Data models;data quality;data quality incorporation;ETL;style;data warehouse},
doi={10.1109/ICCSAI53272.2021.9609770},
ISSN={},
month={Oct},}
@ARTICLE{6815642,
author={Ye, Sijing and Zhu, Dehai and Yao, Xiaochuang and Zhang, Nan and Fang, Shuai and Li, Lin},
journal={IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing},
title={Development of a Highly Flexible Mobile GIS-Based System for Collecting Arable Land Quality Data},
year={2014},
volume={7},
number={11},
pages={4432-4441},
abstract={In recent years, well-designed terminal-based methods for collecting index data have gradually replaced traditional pen-and-paper methods and have been extensively used in numerous studies. These new approaches offer users increased accuracy, efficiency, consumption, and data compatibility compared to traditional methods. In general, we find that spatial data content and quality index systems vary widely across different arable land regions. Thus, a system for the investigation of arable land quality indices that has the flexibility to utilize various types of spatial data and quality indices without requiring program modification is needed. This paper presents the framework, the module partition, and the structure of the data exchange interface for a highly flexible mobile GIS-based system, which we call the “arable land quality index data collection system” (ALQIDCS). This system incorporates a series of self-adaptive methods, a data table-driven model and two types of formulas for flexible data collection and processing. We tested our prototype system by investigating arable land quality in the Da Xing District, Beijing and in the Te Da La Qi District, Inner Mongolia, China. The results indicate that the ALQIDCS can effectively adapt to variations in spatial data and quality index systems and meet different objectives. The limitations of the ALQIDCS and suggestions for future work are also presented.},
keywords={Indexes;Spatial databases;Data collection;Mobile communication;Data models;Vectors;Geographic information systems;Agricultural data collection;Android;arable land quality monitoring;mobile geographic information system (GIS);Agricultural data collection;Android;arable land quality monitoring;mobile geographic information system (GIS)},
doi={10.1109/JSTARS.2014.2320635},
ISSN={2151-1535},
month={Nov},}
@INPROCEEDINGS{9701096,
author={Deng, Siyang and Liu, Zhu and Huang, Lvchao and Wang, Yonggui and Chen, Kaiming},
booktitle={2021 IEEE International Conference on Energy Internet (ICEI)},
title={Resampled data splicing method based on continuous single-cycle used in power quality analysis},
year={2021},
volume={},
number={},
pages={52-55},
abstract={When analyzing the spectrum of sampling data received from analog-to-digital converter, it usually could not satisfy the constraint that the length of the signal should be an integer power of 2 for FFT calculation. The common approach is to increase the sampling time or to fill the zeros after the signal, but these methods would cause the spectrum leakage or increase the time consuming for analysis. Therefore, this paper proposes an improved power quality data processing algorithm based on signal interpolation resample for continuous single cycle, and compares it with the common methods. The simulation results show that the improved algorithm can accurately capture the real frequency characteristics of the signal without increasing the signal sampling load or the amount of calculation.},
keywords={Interpolation;Splicing;Simulation;Conferences;Power quality;Signal sampling;Data processing;continuous single cycle;data splicing;linear interpolation;power quality;resampling},
doi={10.1109/ICEI52466.2021.00015},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{6199204,
author={Chaogui Zhang and Zhiyong Zheng and Fuqiang Zhang and Jiangtao Ren},
booktitle={Proceedings 2011 International Conference on Transportation, Mechanical, and Electrical Engineering (TMEE)},
title={Multidimensional traffic GPS data quality analysis using data cube model},
year={2011},
volume={},
number={},
pages={307-310},
abstract={In recent years, transport agencies collect more and more GPS data of probe vehicle, data mining on these immense amounts of traffic GPS data is necessary. However, since the GPS data are non-uniform and discontinuous in the network, the quality of these collected GPS data is unreliable and will be worse when there exists lots of noisy data. What's more, if the researchers lack another type of traffic data such as loop sensors' data for verification, the result of data mining will become unreliable. Therefore, we present an approach for multidimensional traffic GPS data quality analysis using data cube model. We propose data valid density, data ideality and an overall indicator to describe the quality of GPS data. The experiment results show that our approach can describe the data quality status of the network and help evaluate the reliability of traffic parameter estimations.},
keywords={Global Positioning System;Roads;Reliability;Vehicles;Probes;Parameter estimation;Data mining;traffic GPS data;data quality;reliability;data cube;roll-up;OLAP},
doi={10.1109/TMEE.2011.6199204},
ISSN={},
month={Dec},}
@INPROCEEDINGS{1640149,
author={Paternostro, C. and Pruessner, A. and Semkiw, R.},
booktitle={Proceedings of OCEANS 2005 MTS/IEEE},
title={Designing a quality oceanographic data processing environment},
year={2005},
volume={},
number={},
pages={2527-2531 Vol. 3},
abstract={Oceanographic data are increasing by data types and volume making present methods of processing and determining quality a cumbersome task. The National Oceanic and Atmospheric Administration's (NOAA) Center for Operational Oceanographic Products and Services (CO-OPS) is developing an end-to-end, state-of-the-art data management system to ingest, quality control, analyze, and disseminate water velocity and related data. The benefits include streamlining preliminary analysis, thus allowing time and resources for more in-depth investigations of the physical phenomena, increasing consistency of results between users, and improving overall data quality. The design of the system architecture follows a planned structured methodology improving the quality of the software developed. Designing a Web-based modular system will allow flexibility so the system can accommodate new analyses, reports and plots as well as allow for future data types. Well-defined algorithms will be implemented determining the quality of both the data and the analyses. This data management system will provide oceanographers the means to study water velocity data using a wide suite of mathematical and graphical tools. This will allow users to focus on the analysis results rather than the process.},
keywords={Data processing;Data analysis;Data visualization;Software quality;Quality control;Software maintenance;Oceans;Control system analysis;Algorithm design and analysis;Collaborative software},
doi={10.1109/OCEANS.2005.1640149},
ISSN={0197-7385},
month={Sep.},}
@INPROCEEDINGS{8054906,
author={Diop, Mouhamed and Camara, Mamadou Samba and Fall, Ibrahima and Bah, Alassane},
booktitle={2017 Intelligent Systems and Computer Vision (ISCV)},
title={A methodology for prior management of temporal data quality in a data mining process},
year={2017},
volume={},
number={},
pages={1-8},
abstract={In Data Mining (DM) projects, more specifically in the Data Understanding and the Data Preparation phases, several techniques found in the literature are used to detect and handle data quality problems such as missing data, outliers, inconsistent data or time-variant data. However, the main limitation in the application of these techniques is the complexity caused by a lack of anticipation in the detection and resolution of data quality problems. Then, a DM process model designed for the prior management of data quality was recently proposed. It has the distinctive feature of having linked the DM process and the Software Engineering (SE) one by combining them in parallel. However, authors of that work [1] have just specified what should be done, not how it should be. The present research work is an improvement of that DM process model. It adds to it a methodology that indicates in a concrete way a guideline on how to combine the SE process and the DM one to anticipate and manage data quality problems that can be found during the mining process. This work will specifically address the case of temporal data. The main contribution of this methodology is the definition, in concrete terms, of how to anticipate and automate all activities necessary to remove temporal data quality problems in a mining process.},
keywords={Data mining;Data models;Software;Databases;Complexity theory;Software engineering;Data Mining;Data Quality;Temporal Data;CRISP-DM;Software Engineering;Data warehousing},
doi={10.1109/ISACV.2017.8054906},
ISSN={},
month={April},}
@INPROCEEDINGS{9488490,
author={Karakostas, Anastasios and Poler, Raul and Fraile, Francisco and Vrochidis, Stefanos},
booktitle={2021 IEEE International Workshop on Metrology for Industry 4.0 & IoT (MetroInd4.0&IoT)},
title={Industrial Data Services for Quality Control in Smart Manufacturing – the i4Q Framework},
year={2021},
volume={},
number={},
pages={454-457},
abstract={This paper presents a new innovative framework to support smart manufacturing quality assurance. More specifically, the i4Q framework provides an IoT-based Reliable Industrial Data Services (RIDS), a complete suite consisting of 22 innovative Solutions, able to manage the huge amount of industrial data coming from cheap cost-effective, smart, and small size interconnected factory devices for supporting manufacturing online monitoring and control. The i4Q Framework guarantees data reliability with functions grouped into five basic capabilities around the data cycle: sensing, communication, computing infrastructure, storage, and analysis-optimization. i4Q RIDS includes simulation and optimization tools for manufacturing line continuous process qualification, quality diagnosis, reconfiguration and certification for ensuring high manufacturing efficiency, leading to an integrated approach to zero-defect manufacturing. This paper presents the main principles of the i4Q framework and the relevant industrial case studies on which it will be evaluated.},
keywords={Quality assurance;Quality control;Tools;Reliability engineering;Production facilities;Telecommunication computing;Sensors;Product Quality;Process Quality;Data Quality;Data Reliability;Blockchain;Virtual Sensors;Digital Twins;Process Simulation;Process Optimization;Zero-defect Manufacturing},
doi={10.1109/MetroInd4.0IoT51437.2021.9488490},
ISSN={},
month={June},}
@INPROCEEDINGS{4054999,
author={Asheibi, Ali and Stirling, David and Robinson, Duane},
booktitle={2006 Canadian Conference on Electrical and Computer Engineering},
title={Identification of Load Power Quality Characteristics using Data Mining},
year={2006},
volume={},
number={},
pages={157-162},
abstract={The rapid increase in computer technology and the availability of large scale power quality monitoring data should now motivate distribution network service providers to attempt to extract information that may otherwise remain hidden within the recorded data. Such information may be critical for identification and diagnoses of power quality disturbance problems, prediction of system abnormalities or failure, and alarming of critical system situations. Data mining tools are an obvious candidate for assisting in such analysis of large scale power quality monitoring data. This paper describes a method of applying unsupervised and supervised learning strategies of data mining in power quality data analysis. Firstly underlying classes in harmonic data from medium and low voltage (MV/LV) distribution systems were identified using clustering. Secondly the link analysis is used to merge the obtained clusters into supergroups. The characteristics of these super-groups are discovered using various algorithms for classification techniques. Finally the a priori algorithm of association rules is used to find the correlation between the harmonic currents and voltages at different sites (substation, residential, commercial and industrial) for the interconnected supergroups},
keywords={Power quality;Data mining;Large-scale systems;Condition monitoring;Clustering algorithms;Computer networks;Distributed computing;Availability;Computerized monitoring;Supervised learning;power quality;harmonics;data mining},
doi={10.1109/CCECE.2006.277720},
ISSN={0840-7789},
month={May},}
@INPROCEEDINGS{8257913,
author={Benbernou, Salima and Ouziri, Mourad},
booktitle={2017 IEEE International Conference on Big Data (Big Data)},
title={Enhancing data quality by cleaning inconsistent big RDF data},
year={2017},
volume={},
number={},
pages={74-79},
abstract={We address the problem of dealing with inconsistencies in fusion of big data sources using Resource Description Framework (RDF) and ontologies. We propose a scalable approach ensuring data quality for query answering over big RDF data in a distributed way on a Spark ecosystem. In so doing, the cleaning inconsistent big RDF data approach is built on the following steps (1) modeling consistency rules to detect the inconsistency triples even if it is implicitly hidden including inference and inconsistent rules (2) detecting inconsistency through rule evaluation based on Apache Spark framework to discover the minimally sub-set of inconsistent triples (3) cleaning the inconsistency through finding the best repair for consistent query answering.},
keywords={Resource description framework;Ontologies;Sparks;Cleaning;Inference algorithms;Automobiles;Jacobian matrices;Big data;Data quality;Inconsistency;Logical inference;RDF data cleaning;Apache Spark ecosystems},
doi={10.1109/BigData.2017.8257913},
ISSN={},
month={Dec},}
@INPROCEEDINGS{8786289,
author={Fereira, Marcos and Silva, Leandro A.},
booktitle={2018 XLIV Latin American Computer Conference (CLEI)},
title={Data Quality Measurement Framework},
year={2018},
volume={},
number={},
pages={455-463},
abstract={Data Quality evaluation is a key fundamental in Knowledge Data Discovery projects. There are some project frameworks, like CRISP-DM and DAMA DMBOK, that recommend the preparation of the Data Quality Report, as a tool to describe the found problems during the data exploration phase and to describe an approach to fix those problems. However, those frameworks are very generic in their guidelines and neither tell what exactly should be measured nor how to associate any measure to the data quality. Data Profiling tools and some ETL(Extraction, Transformation and Loading) tools as well, implement some basic Statistical Description tooling, but they do not propose any general methodology to evaluate quantitatively the quality of a set of data, except, perhaps, in the IBM Watson Analytics tool. This article proposes a quantitative measure for data quality evaluation, based on Statistical Description tools.},
keywords={Tools;Data integrity;Loading;Guidelines;Indexes;Data mining;Software;Data Quality, Data Profiling, Dat Mining, Data Governance, preprocessing},
doi={10.1109/CLEI.2018.00061},
ISSN={},
month={Oct},}
@INPROCEEDINGS{5707184,
author={Comerio, Marco and Truong, Hong-Linh and Batini, Carlo and Dustdar, Schahram},
booktitle={2010 IEEE International Conference on Service-Oriented Computing and Applications (SOCA)},
title={Service-oriented data quality engineering and data publishing in the cloud},
year={2010},
volume={},
number={},
pages={1-6},
abstract={Traditional data quality engineering techniques, often used and deployed within a single enterprise environment, are inadequate to cope with the rapid change of data, with a multitude of quality degrees, to be used in contemporary business models. The emerging cloud computing paradigm could potentially offer high-quality, composable data and techniques, under the Software-as-a-Service (SaaS), Data-asa-Service (DaaS) and crowdsourcing models, for data quality engineering and data publishing. However, so far how to utilize the potential of cloud computing models for data quality engineering has not been discussed. In this paper, we analyze requirements of data quality engineering and quality-aware data publishing processes in the cloud and we provide a conceptual architecture utilizing and supporting the SaaS, DaaS and crowdsourcing models for the realization of such processes.},
keywords={Cloud computing;Publishing;Data models;Computational modeling;Computer architecture;Business;Databases;Data Quality Engineering;Cloud Computing;Software-as-a-Service;Data-as-a-Service;Crowdsourcing},
doi={10.1109/SOCA.2010.5707184},
ISSN={2163-2871},
month={Dec},}
@INPROCEEDINGS{8843451,
author={Doku, Ronald and Rawat, Danda B. and Liu, Chunmei},
booktitle={2019 IEEE 20th International Conference on Information Reuse and Integration for Data Science (IRI)},
title={Towards Federated Learning Approach to Determine Data Relevance in Big Data},
year={2019},
volume={},
number={},
pages={184-192},
abstract={In the past few years, data has proliferated to astronomical proportions; as a result, big data has become the driving force behind the growth of many machine learning innovations. However, the incessant generation of data in the information age poses a needle in the haystack problem, where it has become challenging to determine useful data from a heap of irrelevant ones. This has resulted in a quality over quantity issue in data science where a lot of data is being generated, but the majority of it is irrelevant. Furthermore, most of the data and the resources needed to effectively train machine learning models are owned by major tech companies, resulting in a centralization problem. As such, federated learning seeks to transform how machine learning models are trained by adopting a distributed machine learning approach. Another promising technology is the blockchain, whose immutable nature ensures data integrity. By combining the blockchain's trust mechanism and federated learning's ability to disrupt data centralization, we propose an approach that determines relevant data and stores the data in a decentralized manner.},
keywords={Data models;Blockchain;Machine learning;Mobile handsets;Data privacy;Cryptography;Big Data;Federated Learning Approach, Data Relevance, Big Data Analytics, Machine Learning},
doi={10.1109/IRI.2019.00039},
ISSN={},
month={July},}
@INPROCEEDINGS{9148143,
author={Lu, Xinghua and Liu, Peihao and Nie, Weidong and Zhang, Hao},
booktitle={2020 International Conference on Computer Information and Big Data Applications (CIBDA)},
title={Whole Process Tracing Model of Pigeon Quality in Block Chain Environment},
year={2020},
volume={},
number={},
pages={435-439},
abstract={In order to improve the whole process traceability of meat pigeon quality, a whole process traceability model of meat pigeon quality based on block chain data fusion is proposed. The method comprises the following steps of: constructing a statistical information distribution model for tracing the whole process of meat pigeon quality; reorganizing the structure of information sources for tracing the whole process of meat pigeon quality by adopting a data structure reorganization method; establishing an information source characteristic distribution model for tracing the whole process of meat pigeon quality; carrying out tracking identification and large data mining of meat pigeon quality information by adopting an association rule mining method under a block chain mode; constructing a meat pigeon quality whole process tracing model; and combining information extraction and optimal scheduling of meat pigeon quality. The quantitative feature distribution set of meat pigeon quality is extracted, and the statistical feature analysis of the whole process traceability of meat pigeon quality is realized by combining the information detection and feature positioning methods. The dynamic evaluation of meat pigeon quality information is realized by using the meat pigeon quality statistical large data analysis method. The optimization design of the whole process traceability model of meat pigeon quality is realized by combining the block chain data fusion and the knowledge map feature analysis method. The simulation results show that the method has good real-time performance, strong dynamic tracing ability and good information positioning ability for the quality of meat pigeons.},
keywords={Data models;Data mining;Production;Data integration;Feature extraction;Process control;block chain;Pigeons;Quality;Tracing the whole process},
doi={10.1109/CIBDA50819.2020.00104},
ISSN={},
month={April},}
@INPROCEEDINGS{8258218,
author={Colborne, Adrienne and Smit, Michael},
booktitle={2017 IEEE International Conference on Big Data (Big Data)},
title={Identifying and mitigating risks to the quality of open data in the post-truth era},
year={2017},
volume={},
number={},
pages={2588-2594},
abstract={Big Data analysis often relies on open data, integrating it with large private data sets, using it as ground truth information, or providing it as part of the input to large simulations. Data can be released openly by governments to achieve various objectives: transparency, informing citizen engagement, or supporting private enterprise, to name a few. To the latter objective, Big Data analytics algorithms rely on high-quality, timely access to various data sources, including open data. Examples include retail analytics drawing on open demographic data and weather forecast systems drawing on open weather and climate data. In this paper, we describe the rise of post-truth in society, and the risks this poses to the quality, integrity, and authenticity of open data. We also discuss approaches to identifying, assessing, and mitigating these risks, and suggest future steps to manage this data quality concern.},
keywords={Big Data;Meteorology;Portals;Voting;open data;post-truth;fake news;risk identification;risk mitigation;data quality assurance},
doi={10.1109/BigData.2017.8258218},
ISSN={},
month={Dec},}
@INPROCEEDINGS{5521667,
author={Masseroli, Marco and Ghisalberti, Giorgio and Tettamanti, Luca},
booktitle={2010 IEEE International Conference on BioInformatics and BioEngineering},
title={Detection of Errors and Inconsistencies in Biomolecular Databases through Integrative Approaches and Quality Controls},
year={2010},
volume={},
number={},
pages={294-295},
abstract={Most of the available biomolecular data are scattered in many databases, are computationally derived and include errors and inconsistencies. Here we show an integrative approach and a set of automatic procedures to test the quality of genomic and proteomic data from several different biomolecular databases integrated in our GFINDer data warehouse (http://www.bioinformatics.polimi.it/GFINDer/).},
keywords={Error correction;Databases;Quality control;Ontologies;Bioinformatics;Intrusion detection;Genomics;Data warehouses;Scattering;Automatic testing;Data quality;Biological data warehousing},
doi={10.1109/BIBE.2010.60},
ISSN={},
month={May},}
@INPROCEEDINGS{9274628,
author={Sulistyo, Haidar Alvinanda and Kusumasari, Tien Fabrianti and Alam, Ekky Novriza},
booktitle={2020 3rd International Conference on Computer and Informatics Engineering (IC2IE)},
title={Implementation of Data Cleansing Pattern Module for Data Quality Management Application using Open Source Tools},
year={2020},
volume={},
number={},
pages={7-12},
abstract={In today's digital world, maintaining the quality of the data is emerging as the keystone of business to obtain high quality data to make sure that the organizations create accurate and wise decision-making. Data were taken from various applications and mostly still raw and to transform the data into valuable and valid information, we need the data quality management in an organization is required to maintain its quality. The insight of the data is an essential tool for decision-makers. Good quality data exists when the data has been processed with data quality management. Good quality data has several beneficial impacts on organizations, and the ever-increasing amount of data in the rapidly expanding technological world of today makes analyzing it much more exciting. Therefore, this research will discuss one of the methods in data quality management that are data cleansing. We propose the design and implementation of pattern modules in data cleansing processing using open source tools to make the process of data cleansing of food and drug distribution permits in Indonesian government agencies more flexible and also create uniformity of the pattern data to obtain more insight to explore.},
keywords={Tools;Data integrity;Government;Drugs;Standards organizations;Open source software;Informatics;data cleansing;data quality;data quality management;open-source tools;pattern},
doi={10.1109/IC2IE50715.2020.9274628},
ISSN={},
month={Sep.},}
@ARTICLE{9565819,
author={Liu, Yongnan and Guan, Xin and Peng, Yu and Chen, Hongyang and Ohtsuki, Tomoaki and Han, Zhu},
journal={IEEE Journal on Selected Areas in Communications},
title={Blockchain-Based Task Offloading for Edge Computing on Low-Quality Data via Distributed Learning in the Internet of Energy},
year={2022},
volume={40},
number={2},
pages={657-676},
abstract={With the development of the Internet of energy, more and more participants share data by different types of edge devices. However, such multi-source heterogenous data typically contain low-quality data, e.g., missing values, which may result in potential risks. Besides, resource-constrained devices incur large latency in edge computing networks. To alleviate such latency, distributed task offloading schemes are designed to share the computation burden between edge nodes and nearby servers. However, there are three main drawbacks of such schemes. First, low-quality data are not carefully evaluated by constraints under scenarios, which may result in slow convergence in distributed computation. Second, multi-source data including sensitive information are computed and shared among edge nodes without privacy protection. Third, distributed tasks on low-quality data may result in low-quality results even with an optimal offloading scheme. To address the problems above, a task offloading framework for edge computing based on consortium blockchain and distributed reinforcement learning is proposed in this paper, which can provide high-quality task offloading policies with data privacy protected. This framework consists of three key components: data quality evaluation (DQ) with multiple data quality dimensions, data repairing (DR) with a repairing algorithm based on a novel repairing consensus mechanism and distributed reinforcement learning for task arrangement (DELTA) with a distributed reinforcement learning algorithm based on a novel low-quality data distributing strategy. Numeric results are presented to illustrate the effectiveness and efficiency of the proposed task offloading framework for edge computing on low-quality data in the IoE.},
keywords={Task analysis;Blockchains;Data integrity;Edge computing;Organizations;Data privacy;Reinforcement learning;Internet of energy;data quality;consortium blockchain;distributed learning;task offloading},
doi={10.1109/JSAC.2021.3118417},
ISSN={1558-0008},
month={Feb},}
@INPROCEEDINGS{9544855,
author={Philip, Stephin and Vashisth, Pawan and Chaturvedi, Anant and Gupta, Neha},
booktitle={2021 Third International Conference on Inventive Research in Computing Applications (ICIRCA)},
title={Imputation of Missing Values using Improved K-Means Clustering Algorithm to Attain Data Quality},
year={2021},
volume={},
number={},
pages={295-301},
abstract={A data warehouse aids in the management of large amounts of data that may be stored in order to handle user input during the computer process. The major issue with a data warehouse is to maintain the data that the user stores in good quality. Some traditional techniques can improve data quality while also increasing efficiency. Each unit of data has a unique feature that has been researched by many researchers and has an influence on data quality. This research article has enhanced the K-Means method by utilizing the Euclidean Distance metric to detect missing values from the gathered sources and replace them with closest values while maintaining the data's consistency, exactness, and quality. yThe improved data will assist developers in analysing data quality prior to data integration by allowing them to make informed decisions quickly in accordance with business requirements. Improved K-Means achieves better accuracy and requires less computational time for clustering data objects when compared to other related approaches.},
keywords={Measurement;Data integrity;Loading;Data integration;Clustering algorithms;Euclidean distance;Transforms;Data Warehouse;Data Quality;Extract Transform and Load;Data Purgation;Missing Data;Data Extraction;Data Transformation;Data Loading},
doi={10.1109/ICIRCA51532.2021.9544855},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{9458716,
author={Ding, Xiaoou and Wang, Hongzhi and Su, Jiaxuan and Wang, Muxian and Li, Jianzhong and Gao, Hong},
booktitle={2021 IEEE 37th International Conference on Data Engineering (ICDE)},
title={Leveraging Currency for Repairing Inconsistent and Incomplete Data (Extended Abstract)},
year={2021},
volume={},
number={},
pages={2315-2316},
abstract={With the growth of data from various sources, data quality is faced with multiple problems. In this paper, we study the multiple data cleaning on incompleteness and inconsistency with currency reasoning and determination. We introduce a 4-step method, named Imp3C, for error detection and repair in incomplete and inconsistent data without timestamps. We propose an integrated currency determining approach to compute currency order among tuples, thus, the dirty data can be repaired effectively considering the temporal impact. Experiments on three real-life datasets verify that Imp3C improves data repairing performance with multiple quality problems, especially in datasets with complex currency orders.},
keywords={Data integrity;Conferences;Maintenance engineering;Data engineering;Cognition;Cleaning;Currencies;Data cleaning;data quality management;currency determining;temporal data repairing},
doi={10.1109/ICDE51399.2021.00243},
ISSN={2375-026X},
month={April},}
@INPROCEEDINGS{9403739,
author={Wei, Li and Dawei, Wang and Lixia, Wang},
booktitle={2020 International Conference on Big Data & Artificial Intelligence & Software Engineering (ICBASE)},
title={Research on data Traceability Method Based on blockchain Technology},
year={2020},
volume={},
number={},
pages={45-49},
abstract={Energy Internet is a major innovation to deal with the environmental crisis and efficient energy management and use in the current society. The important condition to achieve this goal is to summarize, integrate, process and apply the data of various industries in the energy field, and then support the relevant management and decision-making. In this process, how to ensure the authenticity and credibility of data is one of the keys in the construction of energy Internet. Therefore, this paper will study the application scenarios of blockchain technology in data traceability. With the help of the natural characteristics of blockchain, such as decentralized, distributed storage, tamper proof, open and transparent, combined with relevant national standards and international theoretical models, based on the needs of energy Internet data integration and management, this paper will develop a data traceability method suitable for the energy industry, and build a covering energy Data life cycle model of Internet. Through the research of this paper, we can help all localities to establish data traceability mechanism in the energy Internet, to help users to accurately grasp where the data is created, what systems have been transferred, which users have carried out query and modification, and so on, so as to realize the monitoring and control of the whole process of data flow, which helps to improve the credibility of data, and also helps to ensure the safety and quality of data and promote the construction of energy Internet huge data application.},
keywords={Industries;Technological innovation;Distributed databases;Blockchain;Data models;Internet;Safety;blockchain;data traceability;data quality;data security;data governance;energy Internet;huge data},
doi={10.1109/ICBASE51474.2020.00017},
ISSN={},
month={Oct},}
@INPROCEEDINGS{8843473,
author={Homayouni, Hajar and Ghosh, Sudipto and Ray, Indrakshi},
booktitle={2019 IEEE 20th International Conference on Information Reuse and Integration for Data Science (IRI)},
title={ADQuaTe: An Automated Data Quality Test Approach for Constraint Discovery and Fault Detection},
year={2019},
volume={},
number={},
pages={61-68},
abstract={Data quality tests validate the data stored in databases and data warehouses to detect violations of syntactic and semantic constraints. Domain experts grapple with the issues related to the capturing of all the important constraints and checking that they are satisfied. Domain experts often define the constraints in an ad hoc manner based on their knowledge of the application domain and needs of the stakeholders. We propose ADQuaTe, which is an automated data quality test approach that uses an unsupervised machine learning technique to discover constraints that may have been missed by experts. ADQuaTe marks records that violate the constraints as suspicious and explains the violations. We evaluate ADQuaTe on real-world applications using a health data warehouse and a plant diagnosis database to demonstrate that the approach can uncover previously detected as well as new faults in the data.},
keywords={Drugs;Data integrity;Databases;Data warehouses;Data models;Semantics;Self-organizing feature maps;Data quality tests;Database;Data warehouse;Explainable learning;Machine learning;Unsupervised learning},
doi={10.1109/IRI.2019.00023},
ISSN={},
month={July},}
@ARTICLE{9640529,
author={Zhao, Yuxi and Gong, Xiaowen and Lin, Fuhong and Chen, Xu},
journal={IEEE Transactions on Mobile Computing},
title={Data Poisoning Attacks and Defenses in Dynamic Crowdsourcing with Online Data Quality Learning},
year={2021},
volume={},
number={},
pages={1-1},
abstract={Crowdsourcing has found a variety of applications. To improve data accuracy and cost-effectiveness, workers' data quality can be learned from their data in an online manner. However, crowdsourcing is vulnerable to data poisoning attacks, where the attacker reports malicious data to reduce aggregated data accuracy. In this paper, we study malicious data attacks on dynamic crowdsourcing where tasks are assigned and performed sequentially, and we explore online quality learning as a defense mechanism against the attack. We first focus on the asymptotic setting where workers' quality is accurately learned, based on which we turn to the non-asymptotic setting where the quality is estimated online with errors. For each setting, we characterize the conditions under which the attack strategy is effective. Our results show that the malicious noise variance needs to be within a certain range for the attack to be effective. Then we analyze the harm of effective attack. It reveals that the regret of the online quality learning algorithm can be substantially increased due to effective attacks. To further mitigate the attack, we also study median and MIE-based data aggregation as defense mechanisms. We evaluate the proposed attacks and defenses via simulation results based on real-world data.},
keywords={Crowdsourcing;Task analysis;Data integrity;Sensors;Data aggregation;Mobile computing;Estimation;Data crowdsourcing;data poisoning attack;online quality learning;defense},
doi={10.1109/TMC.2021.3133365},
ISSN={1558-0660},
month={},}
@INPROCEEDINGS{8407886,
author={Tong, Pengfei and Lu, Junguo and Yun, Kihyeon},
booktitle={2018 Chinese Control And Decision Conference (CCDC)},
title={Fault detection for semiconductor quality control based on Spark using data mining technology},
year={2018},
volume={},
number={},
pages={4372-4377},
abstract={The aim of this paper is to discuss how to apply data mining technology to semiconductor manufacturing process quality control. The significance of this paper is that it solves a practical engineering problem and is not limited to theoretical analysis. This paper proposes and completes a complete semiconductor quality control program, including the problem analysis, the field semiconductor data collection, the data preprocessing, the feature selection, the classification model selection, the model building, the model testing, the model contrast, and the model improvement. The data preprocessing includes data cleaning, data standardization, data formatting, and so on. And the paper uses the Fisher criterion algorithm to select useful features. In addition, this paper selects two data mining algorithms (SVM and Random Forest) based on the distributed computing platform (Spark) and establishes the corresponding models for analysis. After analyzing and comparing the models, it is found that Random Forest has stronger anti-overfitting ability and is more suitable for the semiconductor quality control.},
keywords={Data models;Data mining;Feature extraction;Analytical models;Quality control;Semiconductor device modeling;Manufacturing processes;Quality control;Data mining;Statistical learning;Feature extraction;Parallel computing;Classification},
doi={10.1109/CCDC.2018.8407886},
ISSN={1948-9447},
month={June},}