@INPROCEEDINGS{7761465,
author={Bushnell, Mark},
booktitle={OCEANS 2016 MTS/IEEE Monterey},
title={Quality Assurance / Quality Control of Real-Time Oceanographic Data},
year={2016},
volume={},
number={},
pages={1-4},
abstract={The U.S. Integrated Ocean Observing System (IOOSÂ®) Quality Assurance/Quality Control of Real Time Oceanographic Data (QARTOD) project approaches a five-year anniversary in 2017. The highly successful protocol already used to generate quality control manuals for nine specific oceanographic variables continues to serve the project well. It was recently used to create a high frequency radar QC manual, and is again being used to develop another manual with application to phytoplankton species and abundance. Tentative plans are to next address the QC of passive acoustic observations.},
keywords={Manuals;Real-time systems;Oceans;Quality control;Standards;Quality assurance;Testing;QARTOD;data quality control;real-time data},
doi={10.1109/OCEANS.2016.7761465},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{9282280,
author={Chen, Haihua and Chen, Jiangping and Ding, Junhua},
booktitle={2020 IEEE 20th International Conference on Software Quality, Reliability and Security (QRS)},
title={Data Evaluation and Enhancement for Quality Improvement of Machine Learning},
year={2020},
volume={},
number={},
pages={13-13},
abstract={The poor quality of a dataset may produce low quality machine learning system. Therefore, transfer learning as a demonstrated effective approach for data quality improvement has been widely used for improving the quality of machine learning. However, the "quality improvement" brought by transfer learning in some studies was not rigorously validated or was even misleading. In this paper, we first investigate the quality problem of the datasets that were used for building a machine learning system. The system was claimed to have achieved the best performance comparing to existing work on a machine learning task. However, the "best performance" was due to the poor quality of the datasets as well as the incorrect validation process. Then we described an experimental study to demonstrate the effectiveness of transfer learning for improving the quality of datasets. However, the experiment results also show the quality improvement of transfer learning is not guaranteed, and a set of requirements have to be meet before applying the approach. Based on the investigation and experiment results, we propose a group of data quality criteria and evaluation approaches for quality improvement of machine learning. We investigated the research problem and explained the results through studying a machine learning system for normalizing medical concepts in social media text with open datasets.},
keywords={Social networking (online);Data integrity;Machine learning;Software quality;Software reliability;Security;Task analysis},
doi={10.1109/QRS51102.2020.00014},
ISSN={},
month={Dec},}
@INPROCEEDINGS{8397554,
author={Zhang, Pengcheng and Xiong, Fang and Gao, Jerry and Wang, Jimin},
booktitle={2017 IEEE SmartWorld, Ubiquitous Intelligence & Computing, Advanced & Trusted Computed, Scalable Computing & Communications, Cloud & Big Data Computing, Internet of People and Smart City Innovation (SmartWorld/SCALCOM/UIC/ATC/CBDCom/IOP/SCI)},
title={Data quality in big data processing: Issues, solutions and open problems},
year={2017},
volume={},
number={},
pages={1-7},
abstract={With the rapid development of social networks, Internet of things, Cloud computing as well as other technologies, big data age is arriving. The increasing number of data has brought great value to the public and enterprises. Meanwhile how to manage and use big data better has become the focus of all walks of life. The 4V characteristics of big data have brought a lot of issues to the big data processing. The key to big data processing is to solve data quality issue, and to ensure data quality is a prerequisite for the successful application of big data technique. In this paper, we use recommendation systems and prediction systems as typical big data applications, and try to find out the data quality issues during data collection, data preprocessing, data storage and data analysis stages of big data processing. According to the elaboration and analysis of the proposed issues, the corresponding solutions are also put forward. Finally, some open problems to be solved in the future are also raised.},
keywords={Big Data;Data integrity;Data analysis;Social network services;Data preprocessing;Internet;Big Data;Big data processing;Data Quality;Recommendation system;Prediction system},
doi={10.1109/UIC-ATC.2017.8397554},
ISSN={},
month={Aug},}
@INPROCEEDINGS{9291525,
author={Li, Tao and Wang, Lei and Ren, Yongjun and Wang, Lingyun and Qian, Qi},
booktitle={2020 International Conferences on Internet of Things (iThings) and IEEE Green Computing and Communications (GreenCom) and IEEE Cyber, Physical and Social Computing (CPSCom) and IEEE Smart Data (SmartData) and IEEE Congress on Cybermatics (Cybermatics)},
title={Multi-source Meteorological Observation Data Quality Control Algorithm Based on Data Mining},
year={2020},
volume={},
number={},
pages={699-704},
abstract={Because of the development of the social economy, people's living standards are also constantly improving recent years. The effect of weather forecast on social economy is more and more important, which has a great influence on agricultural production and personal life. With the advancement of the observation automati-on business, small disturbances may cause systematic errors in observation data. The meteorological observation data's quality is an important factor that directly affects the accuracy of weather forecast and climate forecast. The traditional quality control algorithm uses the climatological limit value of historical data and the allowable value of elements to check, lacks sensitivity to elements fancy changes, and not suitable for demend of quality control. This paper introduces a quality control project of multi-source weather observation based on data mining. Starting from the correlation between the observations of the alike observation element which is not at the same time (time correlation), and the correlation between unlike observation elements at the same time (element correlation), combined with the relevant algorithms in data mining, the paper proposes Two different quality control methods for multi-source meteorological observation data, combined with the complementarity and correlation of the two methods, a synthetic quality control programme is established.},
keywords={Data integrity;Handheld computers;Correlation;Social computing;Internet of Things;Green computing;Conferences;Multi-source meteorological observation data;Data mining;Quality Control;Neural Networks},
doi={10.1109/iThings-GreenCom-CPSCom-SmartData-Cybermatics50389.2020.00121},
ISSN={},
month={Nov},}
@ARTICLE{8640879,
author={},
journal={IEEE P1159.3/D19, October 2018},
title={IEEE Approved Draft Recommended Practice for Power Quality Data Interchange Format (PQDIF)},
year={2019},
volume={},
number={},
pages={1-238},
abstract={A file format suitable for exchanging power quality related measurement and simulation data in a vendor independent manner is defined in this recommended practice. The format is designed to represent all power quality phenomena identified in IEEE Std 1159-2009, IEEE Recommended Practice on Monitoring Electric Power Quality, other power related measurement data, and is extensible to other data types as well. The recommended file format utilizes a highly compressed storage scheme to help reduce disk space and transmission times. The utilization of Globally Unique Identifiers (GUID) to represent each element in the file permits the format to be extensible without the need for a central registration authority.},
keywords={IEEE Standards;Power quality;Monitoring;File systems;Information exchange;data interchange;file format;IEEE 1159.3;measurement;monitoring;power quality;PQDIF},
doi={},
ISSN={},
month={Jan},}
@ARTICLE{8419804,
author={},
journal={IEEE P1159.3/D18, July 2018},
title={IEEE Draft Recommended Practice for Power Quality Data Interchange Format (PQDIF)},
year={2018},
volume={},
number={},
pages={1-232},
abstract={A file format suitable for exchanging power quality related measurement and simulation data in a vendor independent manner is defined in this recommended practice. The format is designed to represent all power quality phenomena identified in IEEE Std 1159-2009, IEEE Recommended Practice on Monitoring Electric Power Quality, other power related measurement data, and is extensible to other data types as well. The recommended file format utilizes a highly compressed storage scheme to help reduce disk space and transmission times. The utilization of Globally Unique Identifiers (GUID) to represent each element in the file permits the format to be extensible without the need for a central registration authority.},
keywords={IEEE Standards;Power distribution;Monitoring;Power measurement;Information exchange;Power quality;data interchange;file format;IEEE 1159.3;measurement;monitoring;power quality;PQDIF},
doi={},
ISSN={},
month={July},}
@INPROCEEDINGS{8686099,
author={Abdallah, Mohammad},
booktitle={2019 International Conference on Big Data and Computational Intelligence (ICBDCI)},
title={Big Data Quality Challenges},
year={2019},
volume={},
number={},
pages={1-3},
abstract={Big Data, is a growing technique these days. There are many uses of Big Data; Artificial Intelligence, Health Care, Business, and many more. For that reason, it becomes necessary to deal with this massive volume of data with caution and care in a term to make sure that the data used and produced is in high quality. Therefore, the Big Data quality is must, and its rules have to be satisfied. In this paper, the main Big Data Quality Factors, which need to be measured, is presented in the perspective of the data itself, the data management, data processing, and data users. This research highlights the quality factors that may be used later to create different Big Data quality models.},
keywords={Big Data;Quality Measurement;Quality Model;Quality Assurance},
doi={10.1109/ICBDCI.2019.8686099},
ISSN={},
month={Feb},}
@INPROCEEDINGS{7603869,
author={Li, Zhang and Chang-bao, Zheng and Shiqiang, Ma and Guoli, Li},
booktitle={2016 IEEE 11th Conference on Industrial Electronics and Applications (ICIEA)},
title={An improved method based on wavelet for power quality compression},
year={2016},
volume={},
number={},
pages={1750-1753},
abstract={The power quality monitoring system will produce large amounts of data, especially in long-time and high sampling. There are difficulty in transmission and storage of mass data. Data compression has become more and more important. This paper presents an enhanced method for compressing power quality data based on wavelet transformation. The power quality data which eliminated fundamental wave is processed with wavelet transform, and then, the threshold method is used to the wavelet coefficients. Several variety of power quality data were processed by Matlab. Experimental results show that the algorithm is practical and with good performance for power quality data compression.},
keywords={Power quality;Wavelet analysis;Data compression;Transient analysis;Wavelet coefficients;Power quality;Compression;Wavelet;Zero-crossing},
doi={10.1109/ICIEA.2016.7603869},
ISSN={2158-2297},
month={June},}
@INPROCEEDINGS{5369264,
author={Cai-yan, Liu and You-fa, Sun},
booktitle={2009 Third International Symposium on Intelligent Information Technology Application},
title={Application of Data Mining in Production Quality Management},
year={2009},
volume={2},
number={},
pages={284-287},
abstract={Application of data mining in manufacturing enterprises' quality management is introduced. Quality factor analysis is very important for quality control and production management. In this article, based on Rough Set theory an improved Apriori algorithm is put forward to mine quantitative relationship among different factors which influence product quality. The improved algorithm overcomes the traditional Apriori algorithm's limitation of qualitative analysis.},
keywords={Data mining;Production;Quality management;Set theory;Steel;Quality control;Association rules;Algorithm design and analysis;Technology management;Inspection;quality management;data mining;Apriori algorithm},
doi={10.1109/IITA.2009.81},
ISSN={},
month={Nov},}
@INPROCEEDINGS{9661000,
author={Scislo, Lukasz and Szczepanik-Scislo, Nina},
booktitle={2021 11th IEEE International Conference on Intelligent Data Acquisition and Advanced Computing Systems: Technology and Applications (IDAACS)},
title={Air Quality Sensor Data Collection and Analytics With IoT for an Apartment With Mechanical Ventilation},
year={2021},
volume={2},
number={},
pages={932-936},
abstract={The aim of the research was to develop a concept of a remote measurement system for air quality management using IoT for an apartment with a mechanical exhaust system. The constant monitoring system with the possibility of manual control by the occupants allows teaching the occupants how to chose optimal settings with the biggest impact on the air quality. Additionally, using cloud data analysis, the measurements can be compared with simulations performed before the building construction. This allows choosing the proper apartment depending on the foreseen occupancy schedule and the family size.},
keywords={Cloud computing;Schedules;Data analysis;Memory;Air quality;Ventilation;Mechanical variables measurement;air quality;variable air volume;VAV;cooling and ventilation;control strategy},
doi={10.1109/IDAACS53288.2021.9661000},
ISSN={2770-4254},
month={Sep.},}
@INPROCEEDINGS{8308252,
author={Ahmed, Hana Haj},
booktitle={2017 IEEE/ACS 14th International Conference on Computer Systems and Applications (AICCSA)},
title={Data Quality Assessment in the Integration Process of Linked Open Data (LOD)},
year={2017},
volume={},
number={},
pages={1-6},
abstract={Linked Open Data (LOD) entails a set of best practices for publishing and connecting structured data on the Web, which allows sharing and exchanging information in an inter-operable and reusable manner. The increasing adoption of these principles has lead to the creation of a globally distributed and huge informative space that covers various domains such as government, libraries, life sciences, and media. This offers a great opportunity to end-users to build semantic applications by exploring and consuming heterogeneous and dispersed possibly interlinked data. Thus, consuming linked data can be considered as a typical scenario of linked data integration in which a user requires to combine data residing in large and varying quality LOD datasets.In this paper, we examine the specifics of linked data integration and focus on three key challenges, namely data quality profiling and assessment, conflict resolution and quality improvement. We postulate that data quality assessment can act both as a deciding factor for conflict resolution and as an indicator of low quality data which need to be improved.},
keywords={Data integrity;Linked data;Measurement;Data integration;Semantics;Knowledge based systems;Tools;data quality;linked open data;assessment;data integration;improvement},
doi={10.1109/AICCSA.2017.178},
ISSN={2161-5330},
month={Oct},}
@INPROCEEDINGS{9516862,
author={Xu, Xijiao and He, Huanming and Song, Wei and Gong, Jiayu},
booktitle={2021 IEEE/ACIS 19th International Conference on Computer and Information Science (ICIS)},
title={Analysis on the Quality Model of Big Data Software},
year={2021},
volume={},
number={},
pages={78-81},
abstract={With the rapid development of the big data system, The big data system has the characteristics of large data scale, diverse data and high computational complexity. Its testing method has to be constantly improved. By analyzing the general software quality model, and combining the characteristics of the big data software, a set of quality model for the big data software is formed.},
keywords={Analytical models;Information science;Computational modeling;Software quality;Big Data;Data models;Computational complexity;Big Data;the Quality Requirements;Software Model},
doi={10.1109/ICIS51600.2021.9516862},
ISSN={},
month={June},}
@INPROCEEDINGS{8432046,
author={Chai, Haiyan and Zhang, Nan and Liu, Bojiang and Tang, Longli},
booktitle={2018 IEEE International Conference on Software Quality, Reliability and Security Companion (QRS-C)},
title={A Software Defect Management System Based on Knowledge Base},
year={2018},
volume={},
number={},
pages={652-653},
abstract={Software testing is an effective way to improving software quality, a software defect is identified before it goes live, a massive amount of bug-related data is accumulated during software testing, there is a point in studying how to improve the working efficiency of software testing through integration and use of such data. In this thesis, a software defect management system based on knowledge base is designed, where the three-tier knowledge base architecture is used to manage defects, data mining is performed with factual knowledge generated from the testing to derive rule knowledge that will be used for defect prediction, and the appropriate strategy knowledge is configured to manage bugs, so as to improve the working efficiency of software testing.},
keywords={Knowledge based systems;Data mining;Software testing;Computer bugs;Software quality;knowledge base, data mining, defect management, software testing},
doi={10.1109/QRS-C.2018.00118},
ISSN={},
month={July},}
@INPROCEEDINGS{4737862,
author={Keqin Wang and Shurong Tong and Roucoules, Lionel and Eynard, Benoit},
booktitle={2008 IEEE International Conference on Industrial Engineering and Engineering Management},
title={Analysis of consumersâ requirements for data/information quality by using HOQ},
year={2008},
volume={},
number={},
pages={213-217},
abstract={Data/information quality (DQ/IQ*) has great impact on data consumersâ decisions. In order to provide high quality data/information, data consumersâ requirements for DQ/IQ have to be analyzed and identified. Right requirement identification is fundamental to data quality control activities including DQ/IQ measurement, evaluation, improvement, etc. Quality function deployment (QFD) and the house of quality (HOQ) are effective tools to translate consumersâ requirements into specific DQ dimensions for DQ improvement. This work briefly introduces QFD, HOQ and their constitutive elements. Data consumers are also examined. A methodology of applying HOQ in DQ/IQ, which includes five major steps, is described in details. Then an example of product design information quality is presented. By using the methodology, the weak points of DQ in industrial firms can be identified in the form of DQ dimensions in order to take actions to improve data/information quality.},
keywords={Information analysis;Quality function deployment;Mechanical systems;Usability;Information systems;Databases;Automotive materials;Data analysis;Quality management;Technology management;Data quality;information quality;data consumer;QFD;HOQ},
doi={10.1109/IEEM.2008.4737862},
ISSN={2157-362X},
month={Dec},}
@INPROCEEDINGS{9760944,
author={Zheng, Yiyi},
booktitle={2022 International Conference on Sustainable Computing and Data Communication Systems (ICSCDS)},
title={Computer-Aided Realization of Innovative Clothing Design under the Background of Big Data: from the Perspective of Image Quality Evaluation},
year={2022},
volume={},
number={},
pages={555-558},
abstract={This paper studies the computer-aided realization of innovative clothing design under the background of big data from the perspective of image quality evaluation. Now it explains the application of computer technology in the production of renderings, analyzes the current situation of computer-aided design in clothing design, and proposes some application strategies. To promote the innovative development of the apparel design industry. Combining the background of big data, this article proposes an innovative clothing design strategy based on image quality evaluation. Starting from the various elements of clothing design, from the perspective of data mining, discover fashion elements to achieve the purpose of clothing design innovation, so as to meet the ever-changing social needs.},
keywords={Image quality;Industries;Technological innovation;Design automation;Clothing;Production;Big Data;Image Quality Evaluation;Big Data;Computer-Aided Realization;Innovative Clothing Design},
doi={10.1109/ICSCDS53736.2022.9760944},
ISSN={},
month={April},}
@ARTICLE{5710916,
author={Chen, Kuang and Chen, Harr and Conway, Neil and Hellerstein, Joseph M. and Parikh, Tapan S.},
journal={IEEE Transactions on Knowledge and Data Engineering},
title={Usher: Improving Data Quality with Dynamic Forms},
year={2011},
volume={23},
number={8},
pages={1138-1153},
abstract={Data quality is a critical problem in modern databases. data-entry forms present the first and arguably best opportunity for detecting and mitigating errors, but there has been little research into automatic methods for improving data quality at entry time. In this paper, we propose Usher, an end-to-end system for form design, entry, and data quality assurance. Using previous form submissions, Usher learns a probabilistic model over the questions of the form. Usher then applies this model at every step of the data-entry process to improve data quality. Before entry, it induces a form layout that captures the most important data values of a form instance as quickly as possible and reduces the complexity of error-prone questions. During entry, it dynamically adapts the form to the values being entered by providing real-time interface feedback, reasking questions with dubious responses, and simplifying questions by reformulating them. After entry, it revisits question responses that it deems likely to have been entered incorrectly by reasking the question or a reformulation thereof. We evaluate these components of Usher using two real-world data sets. Our results demonstrate that Usher can improve data quality considerably at a reduced cost when compared to current practice.},
keywords={Probabilistic logic;Data models;Adaptation model;Predictive models;Cleaning;Bayesian methods;Databases;Data quality;data entry;form design;adaptive form.},
doi={10.1109/TKDE.2011.31},
ISSN={1558-2191},
month={Aug},}
@INPROCEEDINGS{9743087,
author={Diah Sitawati, Haryani and Ruldeviyani, Yova and Nizar Hidayanto, Achmad and Septa Amanda, Rizaldy and Gagah Nugroho, Anggoro},
booktitle={2021 International Seminar on Machine Learning, Optimization, and Data Science (ISMODE)},
title={Data Quality Improvement: Case Study Financial Regulatory Authority Reporting},
year={2022},
volume={},
number={},
pages={272-277},
abstract={Financial Regulatory Authority implements Integrated Reporting to improve data quality which is very important because this report will be input for making monetary and macroprudential policies. Data assessment is needed to fulfill the needs of standardization of data quality. The objective of this research is to analyze the quality of critical data from the Integrated Reporting for the monthly period that is reported to the financial regulatory authorities based on the dimensions that are used to perform measurements. There are 4 dimensions of data quality used to define the data quality requirements in this Financial Regulatory Authority, which are completeness, accuracy, currency, and timeliness. To analyze the data, a specific framework for financial data was chosen which is Quality Assessment on Financial Data (QAFD) framework with slight modification. The result of the objective assessment from 48 variables for the completeness dimension is 100% for the mandatory variable, the dimensions of syntactic accuracy and accuracy that related to precision and validity show 100% results but the semantic accuracy for loan, time deposit, and demand deposit shows the percentage of 81.77%, 85.04% and 81.22%, currency dimension is 96.93% and timeliness dimension is 80.89%. Comparison between objective and subjective assessments from 6 variables as a sample shows that there is still a discrepancy for the currency and accuracy dimensions, while the completeness and timeliness dimensions are aligned between objective and subjective assessments. Based on the results of this study, recommendations were made to regulators to improve data quality.},
keywords={Seminars;Regulators;Data integrity;Semantics;Standardization;Machine learning;Syntactics;data quality;data quality assessment;financial data;QAFD;data quality dimension},
doi={10.1109/ISMODE53584.2022.9743087},
ISSN={},
month={Jan},}
@INPROCEEDINGS{4084862,
author={Wang, Jidong and Wang, Chengshan},
booktitle={TENCON 2005 - 2005 IEEE Region 10 Conference},
title={Compression of Power Quality Disturbance Data Based on Energy Threshold and Adaptive Arithmetic Encoding},
year={2005},
volume={},
number={},
pages={1-4},
abstract={Recently, power quality issues have captured more attention. It is necessary to monitor power quality in order to analyze and evaluate it. The monitors will record huge data during disturbance. It is inconvenient for data storage and transmission. Compression of power quality disturbance data will save storage space efficiently and accelerate transmission speed. This paper proposes energy threshold method based on wavelet transform, and then integrates adaptive arithmetic encoding to compress disturbance data. The compression ratio is improved and performance is better. Four typical power quality disturbances including voltage sag, swell, interruption and transient impulse are used to test the proposed method, the validity is verified by simulation results.},
keywords={Power quality;Arithmetic;Encoding;Discrete wavelet transforms;Continuous wavelet transforms;Wavelet transforms;Monitoring;Wavelet analysis;Data engineering;Power engineering and energy;power quality;wavelet transform;adaptive arithmetic encoding;data compression},
doi={10.1109/TENCON.2005.300848},
ISSN={2159-3450},
month={Nov},}
@INPROCEEDINGS{6394366,
author={Moossavizadeh, Seyed Mohammad Hossein and Mohsenzadeh, Mehran and Arshadi, Nasrin},
booktitle={2012 International Conference on Computer Science and Service System},
title={A New Algorithmic Approach to Detect the Good Point Access in the Precautionary Process for Data Quality},
year={2012},
volume={},
number={},
pages={487-490},
abstract={Data quality is a complex concept and has no structure. Hence, data quality field faces with a wide range of difficulties and uncertainties for measurement and evaluation. The lack of precise standards and algorithmic techniques are considered as two major problems for data quality assessment. To prevent causes of data quality problems is a new issue, which can remarkably reduce costs imposed by the quality loss in information systems. The present paper introduced a computational approach to detect success/failure of the precautionary process. Through studying different parts of the process and assessing its good point, a distinct algorithm was presented. Generally, there are various types of information systems due to the lack of a single organizational and information standard. Therefore, the proposed algorithms can be specialized for each given information system.},
keywords={Information systems;Data mining;Companies;Educational institutions;Quality assessment;Standards organizations;Data Quality;Precaution;Information System;Data Quality Assessment;Good point;Algorithm},
doi={10.1109/CSSS.2012.128},
ISSN={},
month={Aug},}
@INPROCEEDINGS{7053666,
author={Yue Shen and Hanwen Zhang and Guohai Liu and Hui Liu and Wei Xia and Hongxuan Wu},
booktitle={Proceeding of the 11th World Congress on Intelligent Control and Automation},
title={Power quality data compression based on sparse representation and compressed sensing},
year={2014},
volume={},
number={},
pages={5561-5566},
abstract={A power quality data compression method combining compressive sampling with adaptive matching pursuit reconstruction based on compressed sampling theorem is presented to solve the massive power quality data collection, compression and storage problems. First, the original power quality data was sampled and compressed simultaneously by random matrix projection method based on compressed sampling theorem. Then the proposed adaptive matching pursuit reconstruction algorithm was used to achieve accurate power quality data reconstruction. The proposed method breaks through the traditional framework of data compression by merging compression into sampling process and could reconstruct original power quality data from the small amount of sampling points from the compressed data. Simulation shows the proposed CS-based power quality data compression method can not only reduce hardware requirements, but also increase the efficiency of data compression.},
keywords={Power quality;Data compression;Compressed sensing;Educational institutions;Matching pursuit algorithms;Reconstruction algorithms;Intelligent control;power quality;data compression;compressed sensing;reconstruction algorithm},
doi={10.1109/WCICA.2014.7053666},
ISSN={},
month={June},}
@INPROCEEDINGS{9661060,
author={Scislo, Lukasz},
booktitle={2021 11th IEEE International Conference on Intelligent Data Acquisition and Advanced Computing Systems: Technology and Applications (IDAACS)},
title={Quality Assurance and Control of Steel Blade Production Using Full Non-Contact Frequency Response Analysis and 3D Laser Doppler Scanning Vibrometry System},
year={2021},
volume={1},
number={},
pages={419-423},
abstract={Quality management is one of the crucial aspects of the Industry 4.0 concept of industrial production. The key matter is to limit the time for the quality system processes in the total production time. This paper presents the use of non-destructive, non-contact experimental modal analysis using the single or multipoint approach for measurement over the surface of the product. The example of such quality assurance measurements, with the use of a 3D Laser Doppler Vibrometry System, is a proposal of advanced instrumentation and data acquisition systems which is the perfect fit for Industry 4.0 factory quality management systems.},
keywords={Three-dimensional displays;Quality assurance;Data acquisition;Measurement by laser beam;Process control;Laser excitation;Frequency response;quality assurance;quality control;LDV;Laser Doppler Vibrometer;modal analysis;EMA},
doi={10.1109/IDAACS53288.2021.9661060},
ISSN={2770-4254},
month={Sep.},}
@INPROCEEDINGS{7516521,
author={Prathibha, E. and Manjunatha, A. and Basavaraj, Sunil},
booktitle={2016 Biennial International Conference on Power and Energy Systems: Towards Sustainable Energy (PESTSE)},
title={Dual tree complex wavelet transform based approach for power quality monitoring and data compression},
year={2016},
volume={},
number={},
pages={1-5},
abstract={Power quality disturbance is a one the challenging issues, where many researchers are gaining the attention towards it, now a day it is very much necessary to monitor power quality disturbances for analyze and other purpose. The quantity of the data captured in present power quality monitoring system has been increasing drastically. It is difficult to store and transmission huge data. So compression technique is required to reduce the storage space for data. This paper proposes Dual tree complex wavelet transform (DTCWT) method for power quality monitoring and integrates run length encoding technique for compress disturbance data. Voltage sag, swell, transients and flickers are Power quality disturbances used to test the proposed method. And Matlab was used for generation of test signals and to implement the different algorithms for data compression.},
keywords={Voltage fluctuations;Wavelet transforms;Power quality;Data compression;Monitoring;Hafnium;Power Quality Disturbances;Dual Tree complex wavelet Transform;Feature extraction;Data compression},
doi={10.1109/PESTSE.2016.7516521},
ISSN={},
month={Jan},}
@INPROCEEDINGS{8367700,
author={Yu, Bin and Zhang, Chen and Tang, ZhouHua and Sun, JiangYan},
booktitle={2018 IEEE 3rd International Conference on Big Data Analysis (ICBDA)},
title={Verification method of data quality in science and technology cloud in Shaanxi province},
year={2018},
volume={},
number={},
pages={319-323},
abstract={This paper analyzes and summarizes the data quality problems in the Shaanxi Science and Technology Resource Coordination Center âScience and Technology Cloudâ project. These two major problems about scientific and technological information data quality are verified. One is data redundancy caused by organizations' name abbreviation and the other is partial scientific and technological information data missing. This paper designs and implements solutions to the problems in âScience and Technology Cloudâ project. This paper extracts 15643 data from scientific and technical talent pool and scientific literature library. The experimental results verify the effectiveness and feasibility of the solution of data redundancy and data missing.},
keywords={Data integrity;Redundancy;Databases;Dynamic programming;Education;Remuneration;Organizations;Science and Technology Cloud;data quality;data redundancy;missing value processing},
doi={10.1109/ICBDA.2018.8367700},
ISSN={},
month={March},}
@INPROCEEDINGS{5518231,
author={Liu, Yujia and Liu, Zhiming},
booktitle={2010 4th International Conference on Bioinformatics and Biomedical Engineering},
title={Research on the Evaluation Methods of Surface Water Quality Based on Spatial Data},
year={2010},
volume={},
number={},
pages={1-4},
abstract={Taking Second Songhua River as the study area, established the SQL Server-based spatial data warehouse. On this basis, using GIS, statistical data mining, visualized data mining and BP neural network to finish data preprocessing, build water quality assessment forecast model, and analyze spatial distributing of water quality. The results show that it is adapted to manage and analyze the spatial data if water resource was investigated by Spatial Data Mining(SDM), and it is easier to discover deep-seated information and rules from the mass of data; In the central and the southeast part of Second Songhua River, the water qualities are better than other places, most of them are class II or class III; The water qualities are poor in the northwest and the south part of Second Songhua River, they are class IV, class V or below class V; Every single evaluation index of water quality class shows that the changing trend from northwest to southeast are from high to low, and there are some special indexes have the trend of ascending in the southern part; According to comprehensive analyzing of spatial distribution character of Second Songhua River water quality, it has obvious area distribution, the mountainous area in the southern part of drainage basin has better water quality, however, it is poor in the northwest, both natural and human activities have influence on it.},
keywords={Rivers;Water resources;Data mining;Information analysis;Data warehouses;Geographic Information Systems;Data visualization;Neural networks;Data preprocessing;Quality assessment},
doi={10.1109/ICBBE.2010.5518231},
ISSN={2151-7622},
month={June},}
@INPROCEEDINGS{8750934,
author={Labouseur, Alan G. and Matheus, Carolyn C.},
booktitle={2019 IEEE 35th International Conference on Data Engineering Workshops (ICDEW)},
title={Dynamic Data Quality for Static Blockchains},
year={2019},
volume={},
number={},
pages={19-21},
abstract={Blockchain's popularity has changed the way people think about data access, storage, and retrieval. Because of this, many classic data management challenges are imbued with renewed significance. One such challenge is the issue of Dynamic Data Quality. As time passes, data changes in content and structure and thus becomes dynamic. Data quality, therefore, also becomes dynamic because it is an aggregate characteristic of the changing content and changing structure of data itself. But blockchain is a static structure. The friction between static blockchains and Dynamic Data Quality give rise to new research opportunities, which the authors address in this paper.},
keywords={Blockchain;Data integrity;Aggregates;Distributed databases;Software;Friction;Distributed ledger;Blockchain, Dynamic Data Quality, Graphs},
doi={10.1109/ICDEW.2019.00-41},
ISSN={2473-3490},
month={April},}
@INPROCEEDINGS{9263667,
author={Xiangwei, Kong},
booktitle={2020 13th International Congress on Image and Signal Processing, BioMedical Engineering and Informatics (CISP-BMEI)},
title={Evaluation of Flight Test Data Quality Based on Rough Set Theory},
year={2020},
volume={},
number={},
pages={1053-1057},
abstract={With the continuous development of flight test technology, the test system is filled with massive, multi-structured, and multi-dimensional data resources. The value of big data has been fully recognized by the society. How to tap the value of data has become an application in various research fields and industries. The most concerned issue of the field. Whether the data is rubbish or treasure, the most important question is whether the data to be analyzed and mined is of high quality. A low-quality data source will not only fail to reflect the value of the data, but may also run counter to the actual situation, which has side effects. In order to effectively evaluate the quality of flight test data, according to the characteristics of test data in flight test, a test data quality evaluation method based on rough set theory is proposed, standard test methods and test indicators for flight test data quality are proposed, and data quality evaluation is given. The method of rule extraction realizes the quality evaluation of flight test data.},
keywords={Data integrity;Rough sets;Feature extraction;Image color analysis;Shape;Packet loss;Data mining;Flight test;data quality;rough set;quality evaluation},
doi={10.1109/CISP-BMEI51763.2020.9263667},
ISSN={},
month={Oct},}
@INPROCEEDINGS{5359651,
author={Januzaj, Eshref and Januzaj, Visar},
booktitle={2009 Third International Conference on Advanced Engineering Computing and Applications in Sciences},
title={An Application of Data Mining to Identify Data Quality Problems},
year={2009},
volume={},
number={},
pages={17-22},
abstract={Modern information systems consist of many distributed computer and database systems. The integration of such distributed data into a single data warehouse system is confronted with the well known problem of low data quality. In this paper we present an approach that facilitates a dynamic identification of spurious and error-prone data stored in a large data warehouse. The identification of data quality problems is based on data mining techniques, such as clustering, subspace clustering and classification. Furthermore, we present via a case study the applicability of our approach on real data. The experimental results show that our approach efficiently identifies data quality problems.},
keywords={Data mining;Data analysis;Data warehouses;Database systems;Data engineering;Distributed computing;Application software;Internet;Companies;Computer applications;Data Quality;Data Mining;Clustering;Classification},
doi={10.1109/ADVCOMP.2009.11},
ISSN={},
month={Oct},}
@INPROCEEDINGS{8085517,
author={Lingfeng, Zhang and Feng, Feng and Heng, Huang},
booktitle={2017 12th International Conference on Computer Science and Education (ICCSE)},
title={Wine quality identification based on data mining research},
year={2017},
volume={},
number={},
pages={358-361},
abstract={For the quality of the wine big data identification technology, the introduction of data mining classification algorithm, effectively according to the content of several impact compounds in wine level identification;Are introduced including the Logistic regression and BP neural network and SVM classification algorithm, in view of the three algorithms identify the modeling analysis of wine quality. Data mining is closely related to big data, applying data mining to the wine in the quality detection of big data, can quickly to the quality of the wine.},
keywords={Logistics;Data models;Classification algorithms;Data mining;Algorithm design and analysis;Support vector machines;Analytical models;big data;data mining;quality identification},
doi={10.1109/ICCSE.2017.8085517},
ISSN={2473-9464},
month={Aug},}
@INPROCEEDINGS{7799649,
author={Wei, Jie and Xu, Zhuoming and Xia, Wenze},
booktitle={2016 13th Web Information Systems and Applications Conference (WISA)},
title={DQAF: Towards DQV-Based Dataset Quality Annotation Using the Web Annotation Data Model},
year={2016},
volume={},
number={},
pages={24-27},
abstract={The W3C Data on the Web Best Practices Working Group is standardizing the Data Quality Vocabulary (DQV) for expressing data quality of Web-published datasets. As proposed in the DQV specification, quality annotations on datasets, one kind of quality information described using DQV, are achieved through Web annotations. Meanwhile, the W3C Web Annotation Working Group is creating a standard Web Annotation Data Model on the basis of the Open Annotation (OA) Data Model. Despite the significant progress in standardization, there is a lack of systematic research on Web tools for DQV-based dataset quality annotation. This paper therefore proposes a Dataset Quality Annotation Framework (DQAF) that provides annotating users with an interactive visual user interface, through which DQV-based dataset quality annotation can be readily achieved using the OA data model to produce machine-readable quality annotation data. We have implemented a proof-of-concept prototype of DQAF and conducted case study experiments with the prototype. The results indicate that DQAF is feasible and implementable, and annotating users can obtain a better understanding of and more intuitive interaction with the quality annotation data by means of the user interface.},
keywords={Partitioning algorithms;Computational modeling;Topology;Data models;Indexing;Social network services;dataset quality annotation;quality metadata;Data Quality Vocabulary (DQV);Web Annotation Data Model;interactive visual user interface;usage-centered design},
doi={10.1109/WISA.2016.15},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{9300119,
author={Ali, Taghrid Z. and Abdelaziz, Tawfig M. and Maatuk, Abdelsalam M. and Elakeili, Salwa M.},
booktitle={2020 21st International Arab Conference on Information Technology (ACIT)},
title={A Framework for Improving Data Quality in Data Warehouse: A Case Study},
year={2020},
volume={},
number={},
pages={1-8},
abstract={Nowadays, the development of data warehouses shows the importance of data quality in business success. Data warehouse projects fail for many reasons, one of which is the low quality of data. High-quality data achievement in data warehouses is a persistent challenge. Data cleaning aims at finding, correcting data errors and inconsistencies. This paper presents a general framework for the implementation of data cleaning according to the scientific principles followed in the data warehouse field, where the framework offers guidelines that define and facilitate the implementation of the data cleaning process to the enterprises interested in the data warehouse field. The research methodology used in this study is qualitative research, in which the data are collected through system analyst interviews. The study concluded that the low level of data quality is an obstacle to any progress in the implementation of modern technological projects, where data quality is a prerequisite for the success of its business, including the data warehouse.},
keywords={Data integrity;Cleaning;Data warehouses;Information systems;Databases;Business;Task analysis;Data warehousing;data quality;data cleaning},
doi={10.1109/ACIT50332.2020.9300119},
ISSN={},
month={Nov},}
@INPROCEEDINGS{5625701,
author={Ali, Kamran and Mubeen Ahmed Warraich},
booktitle={2010 International Conference on Information and Emerging Technologies},
title={A framework to implement data cleaning in enterprise data warehouse for robust data quality},
year={2010},
volume={},
number={},
pages={1-6},
abstract={every day, every hour, every minute, every second trillion of bytes of data is being generated by enterprises especially in telecom sector. To achieve level best decisions for business profits, access to that data in a well-situated and interactive way is always a dream of business executives and managers. Data warehouse is the only viable solution that can bring that dream into a reality. The enhancement of future endeavors to make decisions depends on the availability of correct information that based on quality of data underlying. The quality data can only be produced by cleaning data prior to loading into data warehouse. So correctness of data is essential for well-informed and reliable decision making. The framework proposed in this paper implements robust data quality to ensure consistent and correct loading of data into data warehouse that necessary to disciplined, accurate and reliable data analysis, data mining and knowledge discovery.},
keywords={Data warehouses;Business;Databases;Cleaning;Data models;Data mining;Loading;Data cleaning;Data quality;Data warehousing;Data mining},
doi={10.1109/ICIET.2010.5625701},
ISSN={},
month={June},}
@INPROCEEDINGS{9155928,
author={Tavakoli, Mohammadreza and Elias, Mirette and KismihÃ³k, GÃ¡bor and Auer, SÃ¶ren},
booktitle={2020 IEEE 20th International Conference on Advanced Learning Technologies (ICALT)},
title={Quality Prediction of Open Educational Resources A Metadata-based Approach},
year={2020},
volume={},
number={},
pages={29-31},
abstract={In the recent decade, online learning environments have accumulated millions of Open Educational Resources (OERs). However, for learners, finding relevant and high quality OERs is a complicated and time-consuming activity. Furthermore, metadata play a key role in offering high quality services such as recommendation and search. Metadata can also be used for automatic OER quality control as, in the light of the continuously increasing number of OERs, manual quality control is getting more and more difficult. In this work, we collected the metadata of 8,887 OERs to perform an exploratory data analysis to observe the effect of quality control on metadata quality. Subsequently, we propose an OER metadata scoring model, and build a metadata-based prediction model to anticipate the quality of OERs. Based on our data and model, we were able to detect high-quality OERs with the F1 score of 94.6%.},
keywords={Metadata;Quality control;Predictive models;Open Educational Resources;Measurement;Data analysis;OER;open educational resources;metadata quality;OER quality;Big data;data analysis;quality prediction},
doi={10.1109/ICALT49669.2020.00007},
ISSN={2161-377X},
month={July},}
@INPROCEEDINGS{7474371,
author={Ayyalasomayajula, Haripriya and Gabriel, Edgar and Lindner, Peggy and Price, Daniel},
booktitle={2016 IEEE Second International Conference on Big Data Computing Service and Applications (BigDataService)},
title={Air Quality Simulations Using Big Data Programming Models},
year={2016},
volume={},
number={},
pages={182-184},
abstract={Forecasts of daily pollutant levels have become a standard part of weather predictions in television, on-line, and in newspapers. Research groups also need to analyze larger timeframes across more locations to correlate long term developments for different pollutants with multiple serious health effects such as asthma. This paper presents a comparison of the Hadoop MapReduce and Spark programing models for air quality simulations, guiding future code development for the research groups interested in these analyses. Two use cases have been used, namely (i) calculating the eight hour rolling average of pollutants in a restricted region, (ii) identifying clusters of sensors showing similar patterns in pollutant concentration over multiple years in the state of Texas. The data set used in this analysis is air pollution data collected over fifteen years at 179 monitor sites across the state of Texas for a variety of pollutants. Our results reveal 20-25% performance benefits for the Spark solutions over MapReduce. Furthermore, it documents performance benefits of the Spark MLlib machine learning library over the Mahout library which is based on the MapReduce programing model.},
keywords={Sparks;Atmospheric modeling;Air quality;Analytical models;Sensors;Computational modeling;Data models;Air Quality Simulations;MapReduce;Spark},
doi={10.1109/BigDataService.2016.26},
ISSN={},
month={March},}
@INPROCEEDINGS{9716355,
author={Vinisha, Feby A and Sujihelen, L.},
booktitle={2022 4th International Conference on Smart Systems and Inventive Technology (ICSSIT)},
title={Study on Missing Values and Outlier Detection in Concurrence with Data Quality Enhancement for Efficient Data Processing},
year={2022},
volume={},
number={},
pages={1600-1607},
abstract={Data analytics is the process of analyzing raw data to make predictions and derive conclusions. This process involves collecting and organizing data to discover hidden patterns and draw insight into the data. The methods and approaches of data analytics are automated using various algorithms and mathematical formulas. Data analytics provides real-time and actionable perceptions on data that enable more accurate and prompter decision-making. During the data acquisition phase, missing values and outliers are encountered that affect the model's reliability. Missing values are the data missing in a dataset, more common on large datasets that arise due to information loss, dropout, or non-response of participants. Missing values affects the accuracy of the result and also may lead to biased results. Outliers are the abnormal values that happen to have deviated from the normal distribution pattern of data distribution. Outliers are extreme, unrealistic, extremely big, or small values in a dataset that arise due to manual errors like participant response errors and data entry errors. Outliers also affect the accuracy of the results and lead to over or underestimated resultant values. As missing values and outliers degrade the performance of the analytical data models, various research works have focused on finding and handling such values. This paper reviews the various aspects of missing values and outliers in the preprocessing phase of data analytics to enhance the accuracy of the data model.},
keywords={Data analysis;Data integrity;Data preprocessing;Data acquisition;Prediction algorithms;Data models;Real-time systems;Missing Values;Outlier Detection;Data Analytics;Data Quality Improvement;Data Preprocessing},
doi={10.1109/ICSSIT53264.2022.9716355},
ISSN={},
month={Jan},}
@INPROCEEDINGS{7848697,
author={Tan, Julian SK and Ang, Ai Kiar and Lu, Liu and Gan, Sheena WQ and Corral, Marilyn G},
booktitle={2016 IEEE Region 10 Conference (TENCON)},
title={Quality Analytics in a Big Data supply chain: Commodity data analytics for quality engineering},
year={2016},
volume={},
number={},
pages={3455-3463},
abstract={While the world is experiencing a global shortage of natural resources, a new one in the form of Digital Data has emerged! The ability to harness this new resource has become a renewed basis for competitive advantage where leveraging Big Data effectively means winning in the marketplace. It is going to transform industries and professions around the world. However, traditional data management techniques and analytical methodologies that has taken us from the late 20th century and into the early 21st century are not sustainable in today's business environment where organizations are constantly being challenged to right size the work force, increase labor productivity, increase customer satisfaction and at the same time improving product quality and reliability.},
keywords={Supply chains;Big data;Manufacturing;Data visualization;Market research;Industries;Supply Chain;Analytics;Industrie 4.0;IoT;Big Data;Internet of Things;Predictive;Prescriptive;Cognitive;Descriptive;Data Management;Data Source;Systems of Engagement;Systems of Records;Quality;Commodity},
doi={10.1109/TENCON.2016.7848697},
ISSN={2159-3450},
month={Nov},}
@INPROCEEDINGS{7099579,
author={Nurprasetio, P. and Fassois, S. D.},
booktitle={1999 European Control Conference (ECC)},
title={Multivariate dimensional accuracy data analysis in automobile assembly: Modeling, reduction and quality control issues},
year={1999},
volume={},
number={},
pages={1813-1818},
abstract={The problem of dimensional accuracy data analysis in automobile assembly operations is considered. The data are measured via Optical Coordinate Measuring Machines (OCMM's), and issues of multivariate modeling, analysis, reduction (sensor number and location determination) and quality control are addressed. The study is based upon a novel multivariate time series analysis framework that accounts for both spatial cross correlation and serial autocorrelation. Its effectiveness is demonstrated via actual and simulated plant data.},
keywords={Analytical models;Data models;Quality control;Correlation;Autoregressive processes;Sea measurements;Dispersion;OCMM data analysis;multivariate time series;data reduction;statistical quality control;automobile assembly},
doi={10.23919/ECC.1999.7099579},
ISSN={},
month={Aug},}
@INPROCEEDINGS{7577697,
author={Yu, Changhui},
booktitle={2016 Fifth International Conference on Agro-Geoinformatics (Agro-Geoinformatics)},
title={Research of time series air quality data based on exploratory data analysis and representation},
year={2016},
volume={},
number={},
pages={1-5},
abstract={The environmental problem, especially the air quality such as the content of PM2.5, is an important hot spot in the international community. Many cities release of air environment quality data in real time to monitor the dynamic changes of environment and had accumulated lots of environment data. By exploring and analyzing these time series monitoring data, we can get a lot of interesting information. The paper explores the time series air quality monitoring data based on exploratory data analysis and visual representation. The analysis results can be used to study the time distribution of air environmental quality and its dynamic changes.},
keywords={Monitoring;Data analysis;Market research;Urban areas;Air pollution;environment problem;air quality data;NO2;exploratory data Analysis},
doi={10.1109/Agro-Geoinformatics.2016.7577697},
ISSN={},
month={July},}
@INPROCEEDINGS{4341153,
author={He, Shu-Guang and Li, Li and Qi, Er-Shi},
booktitle={2007 International Conference on Wireless Communications, Networking and Mobile Computing},
title={Study on the Continuous Quality Improvement Systems of LED Packaging Based on Data Mining},
year={2007},
volume={},
number={},
pages={5625-5628},
abstract={LED is one of the most widely used components in electric products. And the LED packaging is a very important process between semiconductor manufacturers and the electric product manufacturers. Based on the analysis of the characteristics of the LED packaging processes, a quality control model based on SPC (statistical process control) and data mining is put forward. The data mining is used as the quality data analysis tool and the quality diagnosis method. Then an infrastructure of the integrated continuous quality improvement systems of the LED packaging is put forward. In this infrastructure, there are three layers of the data collection layer, the data analysis layer and the result viewer layer. Furthermore, the data warehouse of LED packaging is designed with the snowflake schema. A 3 layer yield rate SPC is studied and the decision tree method is used as a quality diagnosis method based on the designed data warehouse. Finally, a prototype of the continuous quality improvement system is developed.},
keywords={Light emitting diodes;Data mining;Semiconductor device packaging;Manufacturing processes;Semiconductor device manufacture;Data analysis;Data warehouses;Quality control;Process control;Decision trees},
doi={10.1109/WICOM.2007.1378},
ISSN={2161-9654},
month={Sep.},}
@INPROCEEDINGS{8985803,
author={Rahmawati, Sinta Denovi and Ruldeviyani, Yova},
booktitle={2019 Fourth International Conference on Informatics and Computing (ICIC)},
title={Data Quality Management Strategy to Improve the Quality of Worker's Wage and Income Data: A Case Study in BPS-Statistics Indonesia, 2018},
year={2019},
volume={},
number={},
pages={1-6},
abstract={Data quality was a problem for professionals and academics in their research. The issues of poor data quality will harm the organization's business. Based on the quality target of social statistics in the strategic plan of BPS-Statistics Indonesia in 2015-2019 and worker's wage and income data from the Survey of Data Requirement (SKD) in 2017, there is 59 percent gap between expectations and realization of data quality satisfaction. Therefore, the assessment of the data quality management maturity model is important because it will be the basis for making recommendations to improve data quality. The framework used in this study is the data quality framework by David Loshin and DQM DMBOK. Overall, the average of data quality maturity level is still at level 3 (defined). From the results of the data quality maturity level, a list of characteristic gaps that have not been implemented is obtained. The results of the gap mapping get several DQM activities based on DMBOK. This activity is described as several strategy recommendations for improving the quality of worker's wage and income data.},
keywords={Data Quality Management;DQM;Loshin's framework;DMBOK},
doi={10.1109/ICIC47613.2019.8985803},
ISSN={},
month={Oct},}
@INPROCEEDINGS{7046920,
author={Fan, Lang and Ma, Hui},
booktitle={2014 International Conference on Management of e-Commerce and e-Government},
title={Comparative Study of Products Quality Control System of Countries in the Era of Big Data},
year={2014},
volume={},
number={},
pages={211-214},
abstract={Product quality control is an important outcome of the development of human society and production management of core areas, while its system is an important content of the quality and safety system. National regulatory system for product quality very seriously, however, product quality and safety are occurring. What is a quality management system? Regulatory information and what is the relationship between implementation of the system? Internet and "big data" but also can lead to changes in the regulatory process and innovation of social governance and strict regulatory regime covering the whole process? This series of quality control problems have been highlighted. Thus, drawing on the experience of other countries, along the historical context, reflections on both sorting and summarizing, and building more accurate inference and description are very urgent and necessary.},
keywords={Safety;Quality assessment;Product design;Standards;Control systems;Big data;Educational institutions;Product quality control;Big data;Regulatory regime},
doi={10.1109/ICMeCG.2014.51},
ISSN={},
month={Oct},}
@INPROCEEDINGS{9026025,
author={Xinrui, Yang and Lei, Wu and Ruiyi, Li},
booktitle={2019 International Conference on Meteorology Observations (ICMO)},
title={Data Quality Evaluation of Chinese Wind Profile Radar Network in 2018},
year={2019},
volume={},
number={},
pages={1-4},
abstract={In 2018, a total of 98 wind profile radars are stable and upload data to CMA. Based on the â Data quality control and evaluation system of wind profile radar network (V1.0)â independently designed and developed by Meteorological Observation Center of CMA, the quality of data is evaluated and analyzed with the following contents: detection capabilities and data accuracy. The main conclusions are as follows: (1) The effective detection height of over 80% of the wind profile radars in the whole network has met the design indicators. The main reasons causing non-compliance of the effective detection height are system failure and low system performance. (2) The data of wind profile radars is compared with the data of GRAPES forecasting field, more than 90% of the data participating in the evaluation is relatively accurate. After data quality control, the standard deviations of U and V components are both within 3.05 m/s, compared with the non-quality control, they are decreased by 19.3 % and 19.5 %,furthermore, they are decreased by 4.1 % and 2.9 % compared with 2017. The reasons leading to the poor accuracy of data mainly are system failure, system observation parameter setting errors and data non-compliance. Based on the above two assessments, 19 sites have poor data availability throughout the year, accounting for 19.4% of all the wind profile radars, and 18 sites have poor data availability during part time of the year. The factors that affect data quality have the following characteristics: the number of sites with low system performance has increased year by year; the system observation parameter setting errors and the system failure that can not be repaired for a long time remain; due to signal interference shutdown, the lack of observation data is serious. In the future, China will improve data quality and enhance the efficiency of data application by strengthening management of business operation at assessment sites and the equipment operation at non-assessment sites, upgrading equipments with low system performance, and improving data quality control algorithms and so on.},
keywords={Data integrity;Radar detection;Wind;Standards;Indexes;Meteorological radar;2018;Chinese wind profile radar network;Data quality;Evaluation},
doi={10.1109/ICMO49322.2019.9026025},
ISSN={},
month={Dec},}
@INPROCEEDINGS{7848472,
author={Yeo Chii, Yvonne and Xue, Feng and Low, Wen Wei and Yoon, Jung H. and Gold, Steve},
booktitle={2016 IEEE Region 10 Conference (TENCON)},
title={A big data approach for memory quality management},
year={2016},
volume={},
number={},
pages={2448-2452},
abstract={As memory technology scaling continues to advance to sub 20nm technology and memory capacity becomes higher, memory quality management becomes more challenging to achieve client's quality expectations especially in this new era of computing. Transformation of traditional quality management approaches becomes necessary to drive memory quality improvements. A big data analytics approach is presented in this paper to demonstrate its application on end to end quality management process to drive continuous memory quality improvements.},
keywords={Data analysis;Memory management;Manufacturing;Engines;Random access memory;Quality management;Data mining;Big data;Analytics;Memory Quality;Supplier Quality},
doi={10.1109/TENCON.2016.7848472},
ISSN={2159-3450},
month={Nov},}
@ARTICLE{9261414,
author={Duan, Gui-Jiang and Yan, Xin},
journal={IEEE Access},
title={A Real-Time Quality Control System Based on Manufacturing Process Data},
year={2020},
volume={8},
number={},
pages={208506-208517},
abstract={Quality prediction is one of the key links of quality control. Benefitting from the development of digital manufacturing, manufacturing process data have grown rapidly, which allows product quality predictions to be made based on a real-time manufacturing process. A real-time quality control system (RTQCS) based on manufacturing process data is presented in this paper. In this study, the relationship between the product real-time quality status and processing task process was established by analyzing the relationship between the product manufacturing resources and the quality status. The key quality characteristics of the product were identified by analyzing the similarity of the product quality characteristic variations in the manufacturing process based on the big data technology, and a quality-resource matrix was constructed. Based on the quality-resource matrix, the RTQCS was established by introducing an association-rule incremental-update algorithm. Finally, the RTQCS was applied in actual production, and the performance of RTQCS was verified by experiments. The experiments showed that the RTQCS can effectively guarantee the quality of product manufacturing and improve the manufacturing efficiency during production.},
keywords={Manufacturing processes;Production;Quality control;Product design;Real-time systems;Manufacturing;Quality assessment;Quality management;production control;prediction methods},
doi={10.1109/ACCESS.2020.3038394},
ISSN={2169-3536},
month={},}
@INPROCEEDINGS{6680518,
author={Rajan, Naresh Sundar and Gouripeddi, Ramkiran and Facelli, Julio C.},
booktitle={2013 IEEE International Conference on Healthcare Informatics},
title={A Service Oriented Framework to Assess the Quality of Electronic Health Data for Clinical Research},
year={2013},
volume={},
number={},
pages={482-482},
abstract={Retrospective/observational clinical research studies are dependent on the secondary use of electronic health record (EHR) data for obtaining important results about the effectiveness of different medical interventions. In contrast to traditional clinical trials these studies provide results from real-world clinical settings, but suffer from data quality issues. Therefore, it is important to take into account the nature and quality of data when designing these studies in order to differentiate between true and artifactual variations [1]. We are developing a service-oriented framework to assess the quality of EHR data.},
keywords={Quality assessment;Data models;Medical services;Terminology;Computer architecture;Engines;Conferences;Data Quality;Secondar use of EHR data;Service-Oriented Architecture;Comparative Rffectiveness Research;Data Analytics for Healthcare;Data mining},
doi={10.1109/ICHI.2013.70},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{8622388,
author={Norman, Ryan and Bolin, Jason and Powell, Edward T. and Amin, Sanket and Nacker, John},
booktitle={2018 IEEE International Conference on Big Data (Big Data)},
title={Using Big Data Analytics to Create a Predictive Model for Joint Strike Fighter},
year={2018},
volume={},
number={},
pages={3590-3596},
abstract={The amount of information needed to acquire knowledge on today's acquisition systems is growing exponentially due to more complex, higher resolution, software-intensive acquisition systems that need to operate in System-of-Systems (SoS), Family-of-Systems (FoS), Joint, and Coalition environments. Unfortunately, the tools and methods necessary to rapidly collect, aggregate, and analyze this information have not evolved as a whole in conjunction with this increased system complexity and, therefore, has made analysis and evaluation increasingly deficient and ineffective. The Test Resource Management Center's (TRMC's) vision is to build a DoD test and evaluation (T&E) knowledge management (KM) and analysis capability that leverages commercial big data analysis and cloud computing technologies to improve evaluation quality and reduce decision-making time. An evaluation revolution, starting with the Joint Strike Fighter (JSF) program, is underway to ensure the T&E community can support the demands of next-generation weapon systems.The true product of T&E is knowledge ascertained through the collection of information about a system or item under test. However, the T&E community's ability to provide this knowledge is hampered by more complex systems, more complex environments, and the need to be more agile in support of strategic initiatives, such as agile acquisition and the 3rd Offset Strategy. This increased complexity and need for speed cause delayed analysis and problems that go undetected during T&E. The primary reason for these shortfalls is antiquated tools and processes that make data hard to locate, aggregate, and convert into knowledge. In short, DoD has not evolved its evaluation infrastructure as its weapon systems have evolved.Conversely, commercial entities, such as medical observation and diagnosis, electric power distribution, retail, and industrial manufacturing, have embraced agility in their methodologies while modernizing analytics capabilities to keep up with the massive influx of data. Raw physical sensors could provide data, higher-quality image or video cameras, radio frequency identification (RFID) devices, faster data collectors, more detailed point-of-sale information or digitized records, and ultimately is providing more data to analysts in size and complexity than ever before. As more data has become available, an interrelated phenomenon is the desire of analysts to ask more detailed questions about their consumers and their business infrastructure. To drive the process of implementing big data analytics, businesses have begun establishing analytics centers which either take pre-defined business cases and apply methods to address them or implement existing knowledge within the data architecture to create a higher level of awareness to business groups or the company at-large. To meet these demands, data storage and computation architectures have become more sophisticated, dozens of technologies were developed for large-scale processing (such as Apache Hadoop or GreenPlum), and streaming architectures which allow data to be processed and actioned on in real-time as it is collected have become commonplace. The net result of these commercial best practices is a solid foundation for the DoD to transform how it uses data to achieve faster, better, and smarter decisions throughout the acquisition lifecycle.},
keywords={Big Data;Knowledge management;Tools;US Department of Defense;Cloud computing;Computer architecture;Data analysis;Big Data;Data Analytics;Knowledge Management;Data Management;Virtualization;Cloud Computing;Predictive Maintainance;Department of Defense;Test and Evaluation},
doi={10.1109/BigData.2018.8622388},
ISSN={},
month={Dec},}
@INPROCEEDINGS{8731462,
author={Schelter, Sebastian and Grafberger, Stefan and Schmidt, Philipp and Rukat, Tammo and Kiessling, Mario and Taptunov, Andrey and Biessmann, Felix and Lange, Dustin},
booktitle={2019 IEEE 35th International Conference on Data Engineering (ICDE)},
title={Differential Data Quality Verification on Partitioned Data},
year={2019},
volume={},
number={},
pages={1940-1945},
abstract={Modern companies and institutions rely on data to guide every single decision. Missing or incorrect information seriously compromises any decision process. In previous work, we presented Deequ, a Spark-based library for automating the verification of data quality at scale. Deequ provides a declarative API, which combines common quality constraints with user-defined validation code, and thereby enables "unit tests for data". However, we found that the previous computational model of Deequ is not flexible enough for many scenarios in modern data pipelines, which handle large, partitioned datasets. Such scenarios require the evaluation of dataset-level quality constraints after individual partition updates, without having to re-read already processed partitions. Additionally, such scenarios often require the verification of data quality on select combinations of partitions. We therefore present a differential generalization of the computational model of Deequ, based on algebraic states with monoid properties. We detail how to efficiently implement the corresponding operators and aggregation functions in Apache Spark. Furthermore, we show how to optimize the resulting workloads to minimize the required number of passes over the data, and empirically validate that our approach decreases the runtimes for updating data metrics under data changes and for different combinations of partitions.},
keywords={Computational modeling;Data integrity;Data models;Frequency estimation;Analytical models;Libraries;data quality;data validation;incremental data processing},
doi={10.1109/ICDE.2019.00210},
ISSN={2375-026X},
month={April},}
@INPROCEEDINGS{9660802,
author={Nikiforova, Anastasija and Kozmina, Natalija},
booktitle={2021 Second International Conference on Intelligent Data Science Technologies and Applications (IDSTA)},
title={Stakeholder-centred Identification of Data Quality Issues: Knowledge that Can Save Your Business},
year={2021},
volume={},
number={},
pages={66-73},
abstract={The paper presents a study aimed at identifying the most widely occurring data quality issues that affect usersâ experience with data and their reuse, and presence of which may not only disrupt the willingness to work with data but also cause losses for businesses. The list of defects is intended to be identified as a result of the following activities: first, the list of the most widely occurring data quality requirements and/or dimensions should be established by means of literature analysis. Second, given the diversity and quantity of different data quality requirements and dimensions, this list should be reduced by means of the brainstorming session and DELPHI analysis, which involves 12 experts. The validity of the resulting list should then be verified by applying these requirements to real-world data, more precisely, open government data, which are freely available to every stakeholder. This activity involves 30 users with advanced data quality knowledge. This allows us to define a list of key data quality issues. Both the data holder and the data user with higher degree of confidence can make use of it to make sure that the data are error-free and are a trusted source to be used without losses for business. These requirements serve as part of a specification for the web-based data quality analysis tool to be developed.},
keywords={Data integrity;Government;Data science;Stakeholders;Open data;Business;data quality;stakeholder;Delphi;brainstorming;open government data;open data},
doi={10.1109/IDSTA53674.2021.9660802},
ISSN={},
month={Nov},}
@INPROCEEDINGS{5591731,
author={Lihong, Dong and Yunbing, Hou},
booktitle={2010 International Conference on E-Business and E-Government},
title={Study on Data Quality Evaluation of Coal and Gas Outburst},
year={2010},
volume={},
number={},
pages={827-831},
abstract={Data quality evaluation is an important part of the process of data mining. This article has build the information quality evaluation index system and evaluation model, determines the quantitative index for each quality dimension, and also demonstrates the formulas to calculate them. The article takes the completeness as an example: Evaluate the information quality completeness dimension of key factors about coal and gas outburst, and, accordingly, finish the completeness evaluations of other dimensions, which can provide effective approaches and basis for DM data acquisition and pre-processing in predicting coal and gas outburst.},
keywords={Accuracy;Indexes;Information services;Data mining;Data models;Quality assessment;Computational modeling;coal and gas outburst;data quality;dimension;assessment metadata;data warehousing},
doi={10.1109/ICEE.2010.215},
ISSN={},
month={May},}
@INPROCEEDINGS{4284163,
author={Bhatia, MPS and Singh, Harender and Kumar, Naresh},
booktitle={2007 IFIP International Conference on Wireless and Optical Communications Networks},
title={A Proposal for the Management of Mobile Network's Quality of Service (QoS) using Data Mining Methods},
year={2007},
volume={},
number={},
pages={1-5},
abstract={Today, the challenge for the service operators is not only to attract and subscribe new users but to retain already subscribed users. To gain a competitive edge over other service operators, the operating personnel have to measure the services provided to their users and the network performance in terms of Quality of Service (QoS) at regular periods. By analyzing the information in these measurements, they can manage the quality of service, which helps to improve their service and network performance. But due to the heavy increase in the number of users in recent years, they find it difficult to elicit essential information from such a large and complex data to manage the QoS using the existing methods. It is here that the recently developed and more powerful data mining methods come in handy. In this paper we proposed how data mining methods can be used to manage the mobile network QoS. We describe three data mining methods: Rough Set Theory, Classification and Regression Tree (CART), and Self Organizing Map (SOM).},
keywords={Quality of service;Proposals;Quality management;Data mining;Performance gain;Personnel;Gain measurement;Information analysis;Performance analysis;Set theory;Data Mining;Rough Set Theory;CART;SOM;Quality of Service (QoS);Mobile Network},
doi={10.1109/WOCN.2007.4284163},
ISSN={2151-7703},
month={July},}
@INPROCEEDINGS{9599192,
author={Zhang, Zhenwei and Wu, Wenyan and Wu, Dongjie},
booktitle={2021 2nd International Symposium on Computer Engineering and Intelligent Communications (ISCEIC)},
title={A Multi-Mode Learning Behavior Real-time Data Acquisition Method Based on Data Quality},
year={2021},
volume={},
number={},
pages={64-69},
abstract={With the rapid development of new technologies such as artificial intelligence, big data, and the Internet of Things, many researchers have probed into the study of learning analysis, trying to solve the problems of teaching by analyzing the learning behavior data from learning process. And in many learning behavior research, the sensor network usually consists of a host of mutually independent data sources, which can be used to monitor measured objects from multiple dimensions thereby obtaining the multi-source multi-modal sensory data. However, there still exist false negative readings, false positive readings and environmental interference, etc. Therefore, we propose a multi-source multimode sensory data acquisition method based on Date Quality(DQ). We first define the data quality in terms of four aspects-accuracy, integrity, consistency and instantaneity. Then, by the modeling there aspects respectively, we propose metrics to estimate the comprehensive data quality method of multi-source multi-mode sensory data. Finally, a data acquisition method is presented based on data quality, which selects a part of data sources for data transmission according to the given precision. This method aims at reducing the consumption of the sensory network on the premise of the data quality guarantee. An extensive experimental evaluation demonstrates the efficiency and effectiveness of the algorithm.},
keywords={Measurement;Data integrity;Data acquisition;Learning (artificial intelligence);Interference;Data models;Real-time systems;multi-mode Data;Learning behavior;data quality;data acquisition},
doi={10.1109/ISCEIC53685.2021.00021},
ISSN={},
month={Aug},}
@INPROCEEDINGS{7527846,
author={Kathiravelu, Pradeeban},
booktitle={2016 IEEE International Conference on Cloud Engineering Workshop (IC2EW)},
title={Software-Defined Networking-Based Enhancements to Data Quality and QoS in Multi-tenanted Data Center Clouds},
year={2016},
volume={},
number={},
pages={201-203},
abstract={Tenants assume various roles in the enterprise data center networks, requiring a differentiated Quality of Service (QoS), data quality and isolation guarantees among them. Traditionally, data storage and processing are handled in either distributed, or centralized manner. While distributed execution offers a higher horizontal scalability, it often comes with a trade-off of lack of centralized control, and hence often with a decreased accuracy and management efficiency. Software-Defined Networking (SDN) offers a global view of the entire data center network to a logically centralized controller. Hence, it provides the best of both worlds with minimal compromises: (i) scalability of the large-scale distributed systems. (ii) unified management capabilities of the traditional centralized systems. By deploying an extended SDN controller architecture, we attempt to enhance the data quality of stored and processed data and increase the QoS of the multi-tenanted data center network clouds.},
keywords={Distributed databases;Quality of service;Cloud computing;Emulation;Conferences;Routing;Biomedical imaging;Software-Defined Networking (SDN);Quality of Service (QoS);Data Quality;Data Centers;Multi-Tenancy;Message-Oriented Middleware (MOM)},
doi={10.1109/IC2EW.2016.19},
ISSN={},
month={April},}
@INPROCEEDINGS{8898129,
author={Gupta, Maneesha and Malhotra, Vaibhav and Shah, Bankim and Prakash, Shilpa and Sharma, Anuja and Kartikeyan, B.},
booktitle={IGARSS 2019 - 2019 IEEE International Geoscience and Remote Sensing Symposium},
title={RISAT-1 SAR HRS Mode Data Quality Evaluation},
year={2019},
volume={},
number={},
pages={1041-1044},
abstract={This paper presents the quality evaluation of the data products from India's first SAR viz. Radar Imaging Satellite (RISAT-1) in High Resolution Spotlight (HRS) Mode having circular polarimetry from space. Sample scenes of Level-2 terrain corrected georeferenced products are acquired in different regions to observe geometric data quality in terms of Location accuracy and Internal Distortion. Further, parameters are identified to evaluate the image and radiometric data quality, such as Background to Peak Ratio (BPRatio), Integrated Side Lobe ratio (ISLR), Peak to Side Lobe ratio (PSLR), Radar Cross Section (RCS) of the Corner Reflector (CR), geometric resolution in the circular polarization mode (Level-1 Single Look Complex products). Results of the analysis are encouraging and meets the specifications. Location accuracy is better than 80m across track and 110m along-track, which is better than the SAR processor specifications. Further, geometric resolution achieved is within the range of 1.18 m for azimuth direction and 0.75 meter for range direction. RCS from image matched well with the CR's.},
keywords={Image resolution;Synthetic aperture radar;Azimuth;Data integrity;Radiometry;Orbits;Image quality},
doi={10.1109/IGARSS.2019.8898129},
ISSN={2153-7003},
month={July},}
@INPROCEEDINGS{9209633,
author={Byabazaire, John and OâHare, Gregory and Delaney, Declan},
booktitle={2020 29th International Conference on Computer Communications and Networks (ICCCN)},
title={Using Trust as a Measure to Derive Data Quality in Data Shared IoT Deployments},
year={2020},
volume={},
number={},
pages={1-9},
abstract={Recent developments in Internet of Things have heightened the need for data sharing across application domains to foster innovation. As most of these IoT deployments are based on heterogeneous sensor types, there is increased scope for sharing erroneous, inaccurate or inconsistent data. This in turn may lead to inaccurate models built from this data. It is important to evaluate this data as it is collected to establish its quality. This paper presents an analysis of data quality as it is represented in Internet of Things (IoT) systems and some of the limitations of this representation. The paper then introduces the use of trust as a heuristic to drive data quality measurements. Trust is a well-established metric that has been used to determine the validity of a piece or source of data in crowd sourced or other unreliable data collection techniques. The analysis extends to detail an appropriate framework for representing data quality within the big data model. To demonstrate the application of a trust backed framework, we used data collected from a IoT deployment of sensors to measure air quality in which a low cost sensor was co-located with a gold reference sensor. Using data streams modeled based on a dataset from an IoT deployment, our initial results show that the framework's trust score are consistent with the accuracy measure of the machine learning models.},
keywords={Data integrity;Data models;Big Data;Biological system modeling;Measurement;Standards;Internet of Things;Data Quality;Internet of Things (IoT);Trust;Big Data Model;Machine learning},
doi={10.1109/ICCCN49398.2020.9209633},
ISSN={2637-9430},
month={Aug},}
@INPROCEEDINGS{7981158,
author={Kattmann, Christoph and Tenbohlen, Stefan},
booktitle={2017 IEEE Manchester PowerTech},
title={Visualization of power quality data},
year={2017},
volume={},
number={},
pages={1-5},
abstract={The visualization of massive amounts of data is a challenge for the evaluation of power quality, where hundreds of data points per second and location can be generated. Based on the theoretic foundations of data visualization, common visualizations for the current state of power quality as well as aggregated values are reviewed and analyzed. For several examples, the mapping from data point properties to visual dimensions is shown and discussed, highlighting the importance of the definition of exact goals for a visualization and illustrating possible pitfalls. In particular, the challenge of visualizing norm compliance is discussed and a proposal for a suitable plot type is made.},
keywords={Data visualization;Power quality;Visualization;Power measurement;Voltage measurement;Time measurement;Image color analysis;Power quality;Data visualization;Power grids},
doi={10.1109/PTC.2017.7981158},
ISSN={},
month={June},}
@INPROCEEDINGS{7382218,
author={Tao Dai and Hongpu Hu and Yanli Wan and Quan Chen and Yan Wang},
booktitle={2015 12th International Conference on Fuzzy Systems and Knowledge Discovery (FSKD)},
title={A data quality management and control framework and model for health decision support},
year={2015},
volume={},
number={},
pages={1792-1796},
abstract={Health data quality issues are important influencing factors on the validity and scientific nature of health decisions. However, a series of health data quality issues have emerged, such as serious lack of key data terms and non-uniform data standards because of the complexity and diversity of health data. Based on the current state of data quality for health decision support, the paper proposes a framework and model for health data quality management and control. And a method of data quality inspection, processing and assessment applicable for the health data with complex types is designed based on the proposed model. On the one hand, it defines the inspection model of the semi-structured health data, designs the quick calling strategy of the inspection rules, and makes clear the data quality inspection method and problematic data processing method. On the other hand, it proposes a mathematic model for objective and quantitative evaluation of the health data. This paper provides future reference for addressing the data quality management of health decision support systems and to protect for the practical value thereof.},
keywords={Inspection;Data models;Quality control;Process control;Quality assessment;Complexity theory;health decision support;data quality control;quality inspection;quality assessment},
doi={10.1109/FSKD.2015.7382218},
ISSN={},
month={Aug},}
@INPROCEEDINGS{9681424,
author={Dakay, Irish Tejero and Canillo, Angie Ceniza and Ferolin, Rosana J.},
booktitle={2021 1st International Conference in Information and Computing Research (iCORE)},
title={Improving Integration of Databases and Data Sets Supporting Quality Management in a Higher Education Institution: A Project Post Mortem Analysis},
year={2021},
volume={},
number={},
pages={187-192},
abstract={Amidst various quality assurance agencies seeking evidence of compliance in different reporting formats, the university in this particular case, finds itself seeking for efficient ways to generate and make use of data and consequently bring the organisation beyond a mere compliance culture to a continuous improvement culture. Aware of the different operational contexts of its units and disciplines, a project was carried out not to centralise or make uniform its operations but to find a common ground to see how these units collectively contribute to the attainment of institutional goals. The project measured its success on the degree of data integration, efficiency of data visualisation and the availability of action plans generated out of the visualised data. From the level of success attained, a retrospection was made to determine the impediments encountered. The analysis revealed causes ranging from the nature of the data requirements, how the integration project was operationalised, and the remaining issues despite the deployed integration procedure, organisational factors, hardware requirements and system compatibility. It concluded with key points necessary to not only celebrate the project success but also for the next improvement cycle or related project initiatives. It also provided insights about the related operational processes and quality management system for the areas being covered.},
keywords={Quality assurance;Databases;Education;Data visualization;Data integration;Hardware;Distance measurement;data integration;quality management;higher education;project management},
doi={10.1109/iCORE54267.2021.00052},
ISSN={},
month={Dec},}
@INPROCEEDINGS{7840586,
author={Challa, Jagat Sesh and Goyal, Poonam and Nikhil, S. and Mangla, Aditya and Balasubramaniam, Sundar S. and Goyal, Navneet},
booktitle={2016 IEEE International Conference on Big Data (Big Data)},
title={DD-Rtree: A dynamic distributed data structure for efficient data distribution among cluster nodes for spatial data mining algorithms},
year={2016},
volume={},
number={},
pages={27-36},
abstract={Parallelizing data mining algorithms has become a necessity as we try to mine ever increasing volumes of data. Spatial data mining algorithms like Dbscan, Optics, Slink, etc. have been parallelized to exploit a cluster infrastructure. The efficiency achieved by existing algorithms can be attributed to spatial locality preservation using spatial indexing structures like k-d-tree, quad-tree, grid files, etc. for distributing data among cluster nodes. However, these indexing structures are static in nature, i.e., they need to scan the entire dataset to determine the partitioning coordinates. This results in high data distribution cost when the data size is large. In this paper, we propose a dynamic distributed data structure, DD-Rtree, which preserves spatial locality while distributing data across compute nodes in a shared nothing environment. Moreover, DD-Rtree is dynamic, i.e., it can be constructed incrementally making it useful for handling big data. We compare the quality of data distribution achieved by DD-Rtree with one of the recent distributed indexing structure, SD-Rtree. We also compare the efficiency of queries supported by these indexing structures along with the overall efficiency of DBSCAN algorithm. Our experimental results show that DD-Rtree achieves better data distribution and thereby resulting in improved overall efficiency.},
keywords={Data structures;Clustering algorithms;Data mining;Indexing;Distributed databases;Algorithm design and analysis;Data mining;data distribution;spatial locality;neighborhood queries;k-NN queries;density based clustering},
doi={10.1109/BigData.2016.7840586},
ISSN={},
month={Dec},}
@INPROCEEDINGS{8088200,
author={FernÃ¡ndez, Marshall and DÃ¡vila, Abraham and Angeleri, Paula},
booktitle={2017 IEEE Colombian Conference on Communications and Computing (COLCOM)},
title={Data quality applied to an academic business intelligence solution: Lesson learned},
year={2017},
volume={},
number={},
pages={1-6},
abstract={Business intelligence covers a set of technologies allowing the extraction, transformation and loading of data into a data warehouse, and presents the information in a way that allows managers of an organization to make decisions. However, stored data in a warehouse does not always have the expected quality required and this can lead managers to make wrong decisions. The objective of this work is the development of a quality model applicable to an academic business intelligence solution, based on the ISO/IEC 25000 series of standards. In this research, an analysis of the data quality requirements was done, as an input for the respective evaluation. The results obtained were presented in a radar chart, along with a textual analysis. It could be concluded that data quality models and standards could be used successfully for evaluating the quality of an academic BI system, and this information is important for identifying and mitigating risks inherent to data and risks that depend on an information system.},
keywords={IEC Standards;ISO Standards;Data models;Business intelligence;Data warehouses;IEC;Business intelligence;Data warehouse;SQuaRE;ISO/IES 25000;ISO/IEC 25012;ISO/IEC 25024;data quality model;data quality metrics},
doi={10.1109/ColComCon.2017.8088200},
ISSN={},
month={Aug},}
@INPROCEEDINGS{7207219,
author={Taleb, Ikbal and Dssouli, Rachida and Serhani, Mohamed Adel},
booktitle={2015 IEEE International Congress on Big Data},
title={Big Data Pre-processing: A Quality Framework},
year={2015},
volume={},
number={},
pages={191-198},
abstract={With the abundance of raw data generated from various sources, Big Data has become a preeminent approach in acquiring, processing, and analyzing large amounts of heterogeneous data to derive valuable evidences. The size, speed, and formats in which data is generated and processed affect the overall quality of information. Therefore, Quality of Big Data (QBD) has become an important factor to ensure that the quality of data is maintained at all Big data processing phases. This paper addresses the QBD at the pre-processing phase, which includes sub-processes like cleansing, integration, filtering, and normalization. We propose a QBD model incorporating processes to support Data quality profile selection and adaptation. In addition, it tracks and registers on a data provenance repository the effect of every data transformation happened in the pre-processing phase. We evaluate the data quality selection module using large EEG dataset. The obtained results illustrate the importance of addressing QBD at an early phase of Big Data processing lifecycle since it significantly save on costs and perform accurate data analysis.},
keywords={Big data;Data integration;Accuracy;Distributed databases;Data analysis;Business;Big Data;Data Quality;pre-processing},
doi={10.1109/BigDataCongress.2015.35},
ISSN={2379-7703},
month={June},}
@INPROCEEDINGS{7603605,
author={Lin, Shunfu and Xie, Chao and Tang, Bo and Liu, Ronghui and Pan, Aiqiang},
booktitle={2016 IEEE 11th Conference on Industrial Electronics and Applications (ICIEA)},
title={The data mining application in the power quality monitoring data analysis},
year={2016},
volume={},
number={},
pages={338-342},
abstract={It is a main issue to find valuable information from the power quality data because of its big volume, heterogeneity and low value density in the power quality monitoring system of the grid. An analysis system of the power quality analysis based on the data mining technologies is presented in this paper, consisting of the technologies of data cleaning, data fusion, cluster analysis, correlation analysis, and etc. The proposed analysis system is applied in the power quality data analysis of a certain city power quality monitoring system. The meaningful variation laws of the power quality indices are obtained, which can provide valuable reference to the grid planning, dispatch and operation.},
keywords={Power quality;Correlation;Monitoring;Indexes;Data mining;Voltage fluctuations;Data analysis;power quality;data mining;cluster analysis;correlation analysis},
doi={10.1109/ICIEA.2016.7603605},
ISSN={2158-2297},
month={June},}
@INPROCEEDINGS{9297009,
author={Juddoo, Suraj and George, Carlisle},
booktitle={2020 3rd International Conference on Emerging Trends in Electrical, Electronic and Communications Engineering (ELECOM)},
title={A Qualitative Assessment of Machine Learning Support for Detecting Data Completeness and Accuracy Issues to Improve Data Analytics in Big Data for the Healthcare Industry},
year={2020},
volume={},
number={},
pages={58-66},
abstract={Tackling Data Quality issues as part of Big Data can be challenging. For data cleansing activities, manual methods are not efficient due to the potentially very large amount of data.. This paper aims to qualitatively assess the possibilities for using machine learning in the process of detecting data incompleteness and inaccuracy, since these two data quality dimensions were found to be the most significant by a previous research study conducted by the authors. A review of existing literature concludes that there is no unique machine learning algorithm most suitable to deal with both incompleteness and inaccuracy of data. Various algorithms are selected from existing studies and applied against a representative big (healthcare) dataset. Following experiments, it was also discovered that the implementation of machine learning algorithms in this context encounters several challenges for Big Data quality activities. These challenges are related to the amount of data particualar machine learning algorithms can scale to and also to certain data type restrictions imposed by some machine learning algorithms. The study concludes that 1) data imputation works better with linear regression models, 2) clustering models are more efficient to detect outliers but fully automated systems may not be realistic in this context. Therefore, a certain level of human judgement is still needed.},
keywords={Industries;Machine learning algorithms;Data integrity;Clustering algorithms;Medical services;Big Data;Tools;Big Data;Data Quality;Data Inaccuracy;Data incompleteness;Machine Learning},
doi={10.1109/ELECOM49001.2020.9297009},
ISSN={},
month={Nov},}
@INPROCEEDINGS{4280171,
author={Shu-guang, He and Li Li and Er-shi, Qi},
booktitle={2007 International Conference on Service Systems and Service Management},
title={Study on the Continuous Quality Improvement of Telecommunication Call Centers Based on Data Mining},
year={2007},
volume={},
number={},
pages={1-5},
abstract={Based on the study of the processes of telecommunication call centers, the service quality metrics of the call centers are put forward. And the mode of the continuous service quality improvement of the call centers based on data warehouse and data mining is studied. Then the process of the IVR (Interactive Voice Response) is analyzed and a mode for the efficiency improvement of IVR is put forward based on the exchange of the orders of the service items in the IVR. Then a service quality metrics of the agents, the ratio of recall in one hour, is put forward. This metrics can be used in the performance analysis of the agents. Furthermore, the model of the performance analysis and control of the ASA (Average Speed of Answer) based on data mining and SPC (Statistical Process Control) is put forward. At last, a method for forecasting the call arriving in is put forward based the time series analysis using dynamic data mining. The result certified that the efficiency and service quality of the telecommunication call center can be improved obviously using the method in this paper.},
keywords={Data mining;Telephony;Data analysis;Performance analysis;Process control;Helium;Quality management;Engineering management;Data engineering;Educational institutions;Data mining;Telecommunication call center;Continuous quality improvement;Statistical process control},
doi={10.1109/ICSSSM.2007.4280171},
ISSN={2161-1904},
month={June},}
@INPROCEEDINGS{8757524,
author={OZPOLAT, Zeynep and KARABATAK, Murat},
booktitle={2019 7th International Symposium on Digital Forensics and Security (ISDFS)},
title={Temperature Estimation with Time Series Analysis from Air Quality Data Set},
year={2019},
volume={},
number={},
pages={1-5},
abstract={With the expansion of the data size, data mining techniques are gaining more and more importance. Data mining consists of methods such as classification, clustering, time series estimation and association rule. In this study, a time series analysis is carried out in order to make an estimation for the future in accordance with the structure of the data set. Time series are series in which the variables are recorded in chronological order. The data set was created by recording the gas concentrations in the air at a time interval. These data are used to estimate the changes in air quality. Three types of time series analysis training algorithm are used in the study. The results given by the algorithms are close to each other and high performance has been determined. As a result of experimental studies, it is observed that time series analysis is sufficient to estimate air quality.},
keywords={Time series analysis;Estimation;Artificial neural networks;Air quality;Data mining;Training;Mathematical model;Data Mining;Time Series Analysis;Air Quality},
doi={10.1109/ISDFS.2019.8757524},
ISSN={},
month={June},}
@INPROCEEDINGS{7404613,
author={Bushnell, Mark},
booktitle={OCEANS 2015 - MTS/IEEE Washington},
title={Quality Assurance/Quality Control of Real-Time Oceanographic Data},
year={2015},
volume={},
number={},
pages={1-4},
abstract={The Quality Assurance/Quality Control of Real-Time Oceanographic Data (QARTOD) project was formally adopted in 2012 as a part of the U.S. Integrated Ocean Observing System (IOOS) Data Management and Communication (DMAC) system. A well-established process has resulted in eight manuals that provide specific quality control tests for a variety of U.S. IOOS core variables of interest. Specifically, the manuals address quality control (QC) for observations of dissolved oxygen, currents, waves, water levels, winds, temperature and salinity, ocean optics, and dissolved nutrients. Regional Associations within the U.S. are now working toward implementing these test procedures, which are also being incorporated by international ocean-observing organizations, the private sector, and manufacturers developing sensor improvements. The manuals are initially drafted and reviewed by a committee of subject matter experts. The resulting draft is distributed to the U.S. Regional Associations for a second round of reviews, followed by a third review from the wider international community. As such, the manuals represent the best processes desired by those who will implement the tests. An important aspect of the tests is the selection of thresholds and other test criteria by local operators, who are in the best position to understand and determine such limits. The tests are neither overly prescriptive nor overly generic and are designed to support a wide range of sensors and operator capabilities. Tests are identified as required, strongly recommended, or suggested, with test results falling into one of three categories: pass, suspect/of high interest, or fail. Test examples are provided in the manuals, and the instructions are sufficiently explicit for a programmer to use them to write software code for automated QC processes. The manuals are living documents. They are updated as the QARTOD project evolves and as input is received from operators, data users, and manufacturers. The dissolved oxygen, waves, and currents manuals received updates in 2015.},
keywords={Manuals;Real-time systems;Quality control;Ocean temperature;Salinity (geophysical);Quality assurance;QARTOD;data quality control;real-time data},
doi={10.23919/OCEANS.2015.7404613},
ISSN={},
month={Oct},}
@INPROCEEDINGS{7724284,
author={Swapna, S. and Niranjan, P. and Srinivas, B. and Swapna, R.},
booktitle={2016 3rd International Conference on Computing for Sustainable Global Development (INDIACom)},
title={Data cleaning for data quality},
year={2016},
volume={},
number={},
pages={344-348},
abstract={Now a day's every second data is being generated rapidly through internet and hence for making proper decision has become a huge task. We are surrounded by data but starving for knowledge, to achieve more profits in business, knowledge plays a key role for decision makers. Data warehouse provides a feasible solution to manage data which should be elegant and accurate for proper analysis and decision making. The data which is collected from different sources may have dirty data, cleaning of data should be done before the data is loaded into warehouse in order to achieve quality data. Once the data have been cleaned it will produce precise results, when the data mining query is applied. Hence consistent data is essential and reliable for decision making. We proposed an algorithm which constructs a Decision for every attribute and Missing values are to be replaced with leaf Node values by which we can achieve more quality data on which mining techniques can be applied for a quality analysis.},
keywords={Cleaning;Data mining;Electronic mail;Data warehouses;Decision trees;Conferences;Databases;Data Warehouse (DW);Data Profiling;OLTP;Data Quality (DQ);ETL},
doi={},
ISSN={},
month={March},}
@INPROCEEDINGS{9671890,
author={Geronazzo, Angela and Ziegler, Markus},
booktitle={2021 IEEE International Conference on Big Data (Big Data)},
title={QMLEx: Data Driven Digital Transformation in Marketing Analytics},
year={2021},
volume={},
number={},
pages={5900-5902},
abstract={This paper presents a data driven approach to replace expert driven business processes. The QMLEx methodology combines NLP algorithms to improve data quality, ML techniques to perform the task and exploits external data sources to eliminate the need for the expert input. The methodology is applied to our internal process devoted to creating groups of products with similar features, one of the most relevant use case in marketing analytics.},
keywords={Itemsets;Soft sensors;Digital transformation;Data integrity;Conferences;Big Data;Feature extraction;Digital transformation;entity linking;topic extraction;word embedding;pattern search;frequent itemset mining;data quality},
doi={10.1109/BigData52589.2021.9671890},
ISSN={},
month={Dec},}
@INPROCEEDINGS{9314391,
author={Faroukhi, Abou Zakaria and El Alaoui, Imane and Gahi, Youssef and Amine, Aouatif},
booktitle={2020 IEEE 2nd International Conference on Electronics, Control, Optimization and Computer Science (ICECOCS)},
title={Big Data Value Chain: A Unified Approach for Integrated Data Quality and Security},
year={2020},
volume={},
number={},
pages={1-8},
abstract={Big Data has grown significantly in recent years. This growth has led organizations to adopt Big Data Value Chains (BDVC) as the appropriate framework for unlocking the value to make suitable decisions. Despite its promising opportunities, Big Data raises new concerns such as data quality and security that could radically impact the effectiveness of the BDVC. These two essential aspects have become an urgent need for any Big Data project to provide meaningful datasets and reliable insights. In this contribution, we highlight the importance of considering data quality and security requirements. Then, we propose a coherent, unified framework that extends BDVC with security and quality aspects. Through quality and security reports, the model can self-evaluate and arrange tasks according to orchestration and monitoring process, allowing the BDVC to evolve at the organization pace and to align strategically with its objectives as well as to federate a sustainable ecosystem.},
keywords={Big Data;Security;Data integrity;Data models;Organizations;Decision making;Reliability;Big Data Value Chain;Data Management;Data Quality;Data Security;Process Integration;Orchestration},
doi={10.1109/ICECOCS50124.2020.9314391},
ISSN={},
month={Dec},}
@INPROCEEDINGS{7013706,
author={Gao, Jie and Zheng, Changbao and Hu, Cungang and Ma, Xinchen},
booktitle={2014 17th International Conference on Electrical Machines and Systems (ICEMS)},
title={An enhanced method based on wavelet for power quality compression},
year={2014},
volume={},
number={},
pages={1468-1471},
abstract={The high sampling rate of the power quality detection will produce large amounts of data in a long time. It's inconvenient for data transmission and storage. The technology of power quality compression is increasingly important. An enhanced method which based on wavelet is applied to the compression of power quality detective data is proposed in this paper. The compression of fault data from different type is performed by Matlab. The algorithm is realizable and has a good performance.},
keywords={Power quality;Wavelet coefficients;Data compression;Transient analysis;Educational institutions;Power quality;Compression;Wavelet transform;Zero-crossing},
doi={10.1109/ICEMS.2014.7013706},
ISSN={},
month={Oct},}
@INPROCEEDINGS{6972274,
author={Pastorello, Gilberto and Agarwal, Deb and Papale, Dario and Samak, Taghrid and Trotta, Carlo and Ribeca, Alessio and Poindexter, Cristina and Faybishenko, Boris and Gunter, Dan and Hollowgrass, Rachel and Canfora, Eleonora},
booktitle={2014 IEEE 10th International Conference on e-Science},
title={Observational Data Patterns for Time Series Data Quality Assessment},
year={2014},
volume={1},
number={},
pages={271-278},
abstract={Observational data are fundamental for scientific research in almost any domain. Recent advances in sensor and data management technologies are enabling unprecedented amounts of observational data to be collected and analyzed. However, an essential part of using observational data is not currently as scalable as data collection and analysis methods: data quality assurance and control. While specialized tools for very narrow domains do exist, general methods are harder to create. This paper explores the identification of data issues that lead to the creation of data tests and tools to perform data quality control activities. Developing this identification step in a systematic manner allows for better and more general quality control tools. As our case study, we use carbon, water, and energy fluxes as well as micro-meteorological data collected at field sites that are part of FLUXNET, a network of over 400 ecosystem-level monitoring stations. In an effort toward the release of a new global data set of fluxes, we are doing data quality control for these data. The experience from this work led to the creation of a catalog of issues identified in the data. This paper presents this catalog and its generalization into a set of patterns of data quality issues that can be detected in observational data.},
keywords={Quality assessment;Soil;Wind speed;Calibration;Heating;Instruments;observational data patterns;data quality;time series data;FLUXNET},
doi={10.1109/eScience.2014.45},
ISSN={},
month={Oct},}
@INPROCEEDINGS{8525075,
author={Aleksandrova, Svetlana V. and Vasiliev, Victor A. and Letuchev, Gennady M.},
booktitle={2018 IEEE International Conference "Quality Management, Transport and Information Security, Information Technologies" (IT&QM&IS)},
title={Digital Technology and Quality Management},
year={2018},
volume={},
number={},
pages={18-21},
abstract={Development of science and technology requires the development of new methods of quality management. Along with already existing methods and quality management systems new approaches. Article showing some areas for improvement of the existing and creation of new tools, techniques and quality management systems in the light of the development of digital technologies. In particular, the combination of known methods of quality management (TQM, Lean Production and other) and information technology (methods of Product Lifecycle Management (PLM), CALS-technologies, ERP ( Enterprise Resource Planning), PDM-system (Product Data Management), MES (manufacturing execution system), LIMS (Laboratory Information Management System), EAM (Enterprise Asset Management systems) and others) allows you to create new principles for a modern quality management system.},
keywords={Quality management;Production;Maintenance engineering;Information technology;Process control;Software;quality;quality management system;integrated management system;model of the integrated management system;TQM;EAM;MES;ERP;PLM;CALS},
doi={10.1109/ITMQIS.2018.8525075},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{8538098,
author={Liu, Yang and Lingling, Xv and Jiang, Peng and Yutong, Li and Mingtai, Shi and Haosong, Li and Zhongping, Xv and Jin, Li and Shuai, Wang and Dongliang, Hu and Jia, Wu and Dan, Su},
booktitle={2018 International Symposium in Sensing and Instrumentation in IoT Era (ISSI)},
title={Study of Data Integration Architecture for WideArea Distributed Power Quality of Power Grid},
year={2018},
volume={},
number={},
pages={1-6},
abstract={With the increasing degree of interconnection between regional power grids and the diversification of power quality interference sources, the problem of power quality has become a complex problem across provinces and regions. It is necessary to provide an analysis method for solving complex power quality problems between regions by carrying out analysis technology research based on the monitoring data of whole network power quality and exploring the correlation of interregional power quality problems. A new wide-area distributed power quality data fusion architecture is proposed in this paper. It solves the data source problem of the big data analysis of the power quality, and realizes the sharing of the data and information of the whole network power quality, and lays the theoretical foundation for the depth application of the power quality data by researching and designing the architecture aiming at multi-source, heterogeneous and distributed data integration technology and wide area distributed data storage technology.},
keywords={Power quality;Data integration;Distributed databases;Monitoring;Companies;Web services;Real-time systems;Power Quality Wide-Area Distribution Data Integration},
doi={10.1109/ISSI.2018.8538098},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{9871919,
author={Pezoulas, Vasileios C. and Tachos, Nikolaos S. and Olivotto, Iacopo and Barlocco, Fausto and Fotiadis, Dimitrios I.},
booktitle={2022 44th Annual International Conference of the IEEE Engineering in Medicine & Biology Society (EMBC)},
title={A âsmartâ Imputation Approach for Effective Quality Control Across Complex Clinical Data Structures},
year={2022},
volume={},
number={},
pages={1049-1052},
abstract={The overwhelming need to improve the quality of complex data structures in healthcare is more important than ever. Although data quality has been the point of interest in many studies, none of them has focused on the development of quantitative and explainable methods for data imputation. In this work, we propose a âsmartâ imputation workflow to address missing data across complex data structures in the context of in silico clinical trials. AI algorithms were utilized to produce high-quality virtual patient profiles. A search algorithm was then developed to extract the best virtual patient profiles through the definition of a profile matching score (PMS). A case study was conducted, where the real dataset was randomly contaminated with multiple missing values (e.g., 10 to 50%). In total, 10000 virtual patient profiles with less than 0.02 Kullback-Leibler (KL) divergence were produced to estimate the PMS distribution. The best generator achieved the lowest average squared absolute difference (0.4) and average correlation difference (0.02) with the real dataset highlighting its increased effectiveness for data imputation across complex clinical data structures.},
keywords={Heating systems;Correlation;Data integrity;Sociology;Semantics;Quality control;Data structures;data imputation;virtual profiles;complex clinical data structures;in silico trials},
doi={10.1109/EMBC48229.2022.9871919},
ISSN={2694-0604},
month={July},}
@INPROCEEDINGS{4155473,
author={Cardoso, Jorge},
booktitle={2006 3rd International IEEE Conference Intelligent Systems},
title={Workflow Quality of Service Management using Data Mining Techniques},
year={2006},
volume={},
number={},
pages={479-482},
abstract={Organizations have been aware of the importance of quality of service (QoS) for competitiveness for some time. It has been widely recognized that workflow systems are a suitable solution for managing the QoS of processes and workflows. The correct management of the QoS of workflows allows for organizations to increase customer satisfaction, reduce internal costs, and increase added value services. In this paper we show a novel method, composed of several phases, describing how organizations can apply data mining algorithms to predict the QoS for their running workflow instances. Our method has been validated using experimentation by applying different data mining algorithms to predict the QoS of workflow},
keywords={Quality of service;Quality management;Data mining;Costs;Workflow management software;Customer satisfaction;Prediction algorithms;Business process re-engineering;Runtime;Monitoring;Quality of Service;Data Mining;Business Process;Workflow},
doi={10.1109/IS.2006.348466},
ISSN={1941-1294},
month={Sep.},}
@INPROCEEDINGS{8419262,
author={Fox, Frank and Aggarwal, Vishal R. and Whelton, Helen and Johnson, Owen},
booktitle={2018 IEEE International Conference on Healthcare Informatics (ICHI)},
title={A Data Quality Framework for Process Mining of Electronic Health Record Data},
year={2018},
volume={},
number={},
pages={12-21},
abstract={Reliable research demands data of known quality. This can be very challenging for electronic health record (EHR) based research where data quality issues can be complex and often unknown. Emerging technologies such as process mining can reveal insights into how to improve care pathways but only if technological advances are matched by strategies and methods to improve data quality. The aim of this work was to develop a care pathway data quality framework (CP-DQF) to identify, manage and mitigate EHR data quality in the context of process mining, using dental EHRs as an example. Objectives: To: 1) Design a framework implementable within our e-health record research environments; 2) Scale it to further dimensions and sources; 3) Run code to mark the data; 4) Mitigate issues and provide an audit trail. Methods: We reviewed the existing literature covering data quality frameworks for process mining and for data mining of EHRs and constructed a unified data quality framework that met the requirements of both. We applied the framework to a practical case study mining primary care dental pathways from an EHR covering 41 dental clinics and 231,760 patients in the Republic of Ireland. Results: Applying the framework helped identify many potential data quality issues and mark-up every data point affected. This enabled systematic assessment of the data quality issues relevant to mining care pathways. Conclusion: The complexity of data quality in an EHR-data research environment was addressed through a re-usable and comprehensible framework that met the needs of our case study. This structured approach saved time and brought rigor to the management and mitigation of data quality issues. The resulting metadata is being used within cohort selection, experiment and process mining software so that our research with this data is based on data of known quality. Our framework is a useful starting point for process mining researchers to address EHR data quality concerns.},
keywords={Data mining;Data integrity;Dentistry;Registers;Systematics;Data visualization;EHR, research data, process mining, data quality},
doi={10.1109/ICHI.2018.00009},
ISSN={2575-2634},
month={June},}
@INPROCEEDINGS{6204995,
author={Sidi, Fatimah and Shariat Panahy, Payam Hassany and Affendey, Lilly Suriani and Jabar, Marzanah A. and Ibrahim, Hamidah and Mustapha, Aida},
booktitle={2012 International Conference on Information Retrieval & Knowledge Management},
title={Data quality: A survey of data quality dimensions},
year={2012},
volume={},
number={},
pages={300-304},
abstract={Nowadays, activities and decisions making in an organization is based on data and information obtained from data analysis, which provides various services for constructing reliable and accurate process. As data are significant resources in all organizations the quality of data is critical for managers and operating processes to identify related performance issues. Moreover, high quality data can increase opportunity for achieving top services in an organization. However, identifying various aspects of data quality from definition, dimensions, types, strategies, techniques are essential to equip methods and processes for improving data. This paper focuses on systematic review of data quality dimensions in order to use at proposed framework which combining data mining and statistical techniques to measure dependencies among dimensions and illustrate how extracting knowledge can increase process quality.},
keywords={Organizations;Accuracy;Information systems;Process control;Reliability;Data mining;Data Quality;Data Quality Dimensions;Types of Data},
doi={10.1109/InfRKM.2012.6204995},
ISSN={},
month={March},}
@ARTICLE{9159907,
author={Wu, Di and Luo, Xin and Shang, Mingsheng and He, Yi and Wang, Guoyin and Wu, Xindong},
journal={IEEE Transactions on Knowledge and Data Engineering},
title={A Data-Characteristic-Aware Latent Factor Model for Web Services QoS Prediction},
year={2022},
volume={34},
number={6},
pages={2525-2538},
abstract={How to accurately predict unknown quality-of-service (QoS) data based on observed ones is a hot yet thorny issue in Web service-related applications. Recently, a latent factor (LF) model has shown its efficiency in addressing this issue owing to its high accuracy and scalability. An LF model can be improved by identifying user and service neighborhoods based on user and service geographical information. However, such information can be difficult to acquire in most applications with the considerations of information security, identity privacy, and commercial interests in a real system. Besides, the existing LF model-based QoS predictors mostly ignore the reliability of given QoS data where noises commonly exist to cause accuracy loss. To address the above issues, this paper proposes a data-characteristic-aware latent factor (DCALF) model to implement highly accurate QoS predictions, where âdata-characteristic-awareâ indicates that it can appropriately implement QoS prediction according to the characteristics of given QoS data. Its main idea is two-fold: a) it detects the neighborhoods and noises of users and services based on the dense LFs extracted from the original sparse QoS data, b) it incorporates a density peaks-based clustering method into its modeling process for achieving the simultaneous detections of both neighborhoods and noises of QoS data. With such designs, it precisely represents the given QoS data in spite of their sparsity, thereby achieving highly accurate predictions for unknown ones. Experimental results on two QoS datasets generated by real-world Web services demonstrate that the proposed DCALF model outperforms state-of-the-art QoS predictors, making it highly competitive in addressing the issue of Web service selection and recommendation.},
keywords={Quality of service;Predictive models;Web services;Clustering algorithms;Data models;Sparse matrices;Reliability;Web Service;quality-of-service;QoS;latent factor analysis;density peak;data-characteristic-aware;missing data;big data;topological neighborhood;noise data;service selection;data science},
doi={10.1109/TKDE.2020.3014302},
ISSN={1558-2191},
month={June},}
@INPROCEEDINGS{9239752,
author={Wang, Wenjing and Yang, Shengquan},
booktitle={2020 International Conference on Computer Network, Electronic and Automation (ICCNEA)},
title={Research on Air Quality Forecasting Based on Big Data and Neural Network},
year={2020},
volume={},
number={},
pages={180-184},
abstract={Aiming at the problem that existing air quality prediction models cannot efficiently and accurately predict air quality in a big data environment, an air quality prediction method based on a big data platform to implement a distributed neural network is proposed. Collect historical data of the six pollutant concentrations that affect the air quality index and use it as input to a neural network model; A distributed neural network model containing the AQI change rule in the distributed neural network structure is adopted to realize the short-term prediction of the AQI. Experimental results show that air quality prediction models based on big data and neural networks can reveal the development trend of air quality through self-learning characteristics. And has higher prediction accuracy, It can provide a scientific basis for the degree of urban air pollution and help people make appropriate measures for different AQI levels.},
keywords={Air quality;Predictive models;Atmospheric modeling;Data models;Big Data;Biological neural networks;AQI Prediction;Big Data;Neural Network},
doi={10.1109/ICCNEA50255.2020.00045},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{8787092,
author={Guan, Zhibin and Ji, Tongkai and Qian, Xu and Ma, Yan and Hong, Xuehai},
booktitle={2017 5th Intl Conf on Applied Computing and Information Technology/4th Intl Conf on Computational Science/Intelligence and Applied Informatics/2nd Intl Conf on Big Data, Cloud Computing, Data Science (ACIT-CSII-BCD)},
title={A Survey on Big Data Pre-processing},
year={2017},
volume={},
number={},
pages={241-247},
abstract={In this paper, we briefly introduce some basic concepts and characteristics of big data. We are surrounded by massive amount of data but starving for knowledge. In the era of Big Data, how to quickly obtain high-quality and valuable information from massive amounts of data has become an important research direction. Hence, we focus our attention to the data pre-processing which is a sub-content of the data processing workflow. In this paper, the four phases of data pre-processing, including data cleansing, data integration, data reduction, and data transformation, have been discussed. And different approaches for a variety of purposes have been presented, which show current methods and techniques need to be further modified in order to improve the quality of data before data analysis.},
keywords={Big Data;Data integration;Data analysis;Dimensionality reduction;Feature extraction;Data integrity;big data processing;data pre-processing;data cleansing;dimension reduction;data quality},
doi={10.1109/ACIT-CSII-BCD.2017.49},
ISSN={},
month={July},}
@INPROCEEDINGS{5170977,
author={Xiao, Yongkang and Ji, Cuiling},
booktitle={2009 WRI World Congress on Computer Science and Information Engineering},
title={Management of Air Quality Monitor Data with Data Warehouse and GIS},
year={2009},
volume={4},
number={},
pages={148-152},
abstract={Air quality status is an important problem focused by all people. This paper utilizes Oracle 10 g to design and implement a prototype system of air quality data warehouse with the monitor data of 86 main cities from the year of 2000 to 2007 in China. To query and analyze the data in the data warehouse conveniently and effectively, it extends the star model to manage the spatial data with ArcGIS 9.0, and implements a Web spatial OLAP system to improve the ability of spatial analysis and visualization of traditional OLAP systems synchronously. Our work will help to evaluate air quality status, analyze its spatio-temporal characteristic, forecast and provide decision-making support for improvement of air quality.},
keywords={Quality management;Monitoring;Data warehouses;Geographic Information Systems;Protection;Cities and towns;Prototypes;Data visualization;Air pollution;Decision making;air quality;data warehouse;web spatial OLAP system;GIS},
doi={10.1109/CSIE.2009.280},
ISSN={},
month={March},}
@INPROCEEDINGS{4352911,
author={Giansanti, D. and Morelli, S. and Macellari, V.},
booktitle={2007 29th Annual International Conference of the IEEE Engineering in Medicine and Biology Society},
title={Experience at Italian National Institute of Health in the quality control in telemedicine: tools for gathering data information and quality assessing},
year={2007},
volume={},
number={},
pages={2803-2806},
abstract={The authors proposed a set of tools and procedures to perform a Telemedicine Quality Control process (TM-QC) to be submitted to the telemedicine (TM) manufacturers. The proposed tools were: the Informative Questionnaire (InQu), the Classification Form (ClFo), the Technical File (TF), the Quality Assessment Checklist (QACL). The InQu served to acquire the information about the examined TM product/service; the ClFo allowed to classify a TM product/service as belonging to one application area of TM. The TF was intended as a technical dossier of product and forced the TM supplier to furnish the only requested documentation of its product, so to avoid redundant information. The QACL was a checklist of requirements, regarding all the essential aspects of the telemedical applications, that each TM products/services must be met. The final assessment of the TM product/service was carried out via the QACL, by computing the number of agreed requirements: on the basis of this computation, a Quality Level (QL) was assigned to the telemedical application. Seven levels were considered, ranging from the Basic Quality Level (QL1- B) to the Excellent Quality Level (QL7-E). The TM-QC process resulted a powerful tool to perform the quality control of the telemedical applications and should be a guidance to all the TM practitioners, from the manufacturers to the expert evaluators. The quality control process procedures proposed thus could be adopted in future as routine procedures and could be useful in the assessing the TM delivering into the National Health Service versus the traditional face to face healthcare services.},
keywords={Quality control;Telemedicine;Medical services;Quality assessment;Application software;Manufacturing processes;Performance evaluation;Documentation;Certification;Control systems},
doi={10.1109/IEMBS.2007.4352911},
ISSN={1558-4615},
month={Aug},}
@INPROCEEDINGS{8669638,
author={Li, Lianzhi},
booktitle={2019 International Conference on Intelligent Transportation, Big Data & Smart City (ICITBS)},
title={Evaluation Model of Education Service Quality Satisfaction in Colleges and Universities Dependent on Classification Attribute Big Data Feature Selection Algorithm},
year={2019},
volume={},
number={},
pages={645-649},
abstract={In view of the insufficiency in the education service quality in colleges and universities, a kind of evaluation model of the education service quality satisfaction in the colleges and universities that is dependent on the classification attribute big data feature selection algorithm is put forward in this paper based on the existing work. On the basis of detailed description of the model components, further study on the evaluation method of the proposed model for the education service quality satisfaction in the colleges and universities is carried out. Under the guidance of the evaluation model of the education service quality satisfaction in the colleges and universities, the method for the construction of the evaluation model of the education service quality satisfaction in the colleges and universities is studied with the orientation to the education service resources in the colleges and universities under the open big data environment. In addition, experimental verification is carried out on the basis of the evaluation data in the 360 Encyclopedia on the education service quality satisfaction in the colleges and universities. The experimental results show that the model and method put forward in this paper can effectively evaluate the quality of the education service in the colleges and universities.},
keywords={Education;Correlation;Data models;Encyclopedias;Big Data;Mutual information;Compounds;Classification Attribute Big Data Feature Selection Algorithm;Education Service Quality in Colleges and Universities;Education Service in Colleges and Universities;Satisfaction Evaluation},
doi={10.1109/ICITBS.2019.00160},
ISSN={},
month={Jan},}
@INPROCEEDINGS{8258267,
author={Catarci, Tiziana and Scannapieco, Monica and Console, Marco and Demetrescu, Camil},
booktitle={2017 IEEE International Conference on Big Data (Big Data)},
title={My (fair) big data},
year={2017},
volume={},
number={},
pages={2974-2979},
abstract={Policy making has the strict requirement to rely on quantitative and high quality information. This paper will address the data quality issue for policy making by showing how to deal with Big Data quality in the different steps of a processing pipeline, with a focus on the integration of Big Data sources with traditional sources. In this respect, a relevant role is played by metadata and in particular by ontologies. Integration systems relying on ontologies enable indeed a formal quality evaluation of inaccuracy, inconsistency and incompleteness of integrated data. The paper will finally describe data confidentiality as a Big Data quality dimension, showing the main issues to be faced for its assurance.},
keywords={Big Data;Pipelines;Ontologies;Metadata;Google;Quality-driven policies;Big Data pipeline;ontology-based quality checking;Big Data confidentiality},
doi={10.1109/BigData.2017.8258267},
ISSN={},
month={Dec},}
@ARTICLE{4586220,
author={Tamagawa, Katsunori and Kitsuregawa, Masaru and Ikoma, Eiji and Ohta, Tetsu and Williams, Steve and Koike, Toshio},
journal={IEEE Systems Journal},
title={An Advanced Quality Control System for the CEOP/CAMP In-Situ Data Management},
year={2008},
volume={2},
number={3},
pages={406-413},
abstract={The Coordinated Enhanced Observing Period (CEOP) was proposed in 1997 as an initial step for establishing an integrated observation system for the global water cycle. The Enhanced Observing Period was conducted from October 2002 to December 2004, with satellite data, in-situ data, and model output data collected and available for integrated analysis. Under the framework of CEOP, the CEOP Asia-Australia Monsoon Project (CAMP) was organized and provided the in-situ dataset in the Asian region. CAMP included 13 different reference sites in the Asian monsoon region during Phase 1 (October 2002 to December 2004). These reference sites were operated by individual researchers for their own research objectives. Therefore, the various sites' data had important differences in observational elements, data formats, recording intervals, etc. This usually requires substantial manual data processing to use these data for scientific research which consumes a great deal of researcher time and energy. To reduce the time and effort for data quality checking and format conversion, the CAMP Data Center (CDC) established a Web-based quality control (QC) system. This paper introduces this in-situ data management and quality control system for the Asian region data under the framework of CEOP.},
keywords={Quality control;Quality management;Satellites;Atmospheric modeling;Energy management;Water resources;Information systems;Predictive models;Data assimilation;CD recording;Data management;observers;quality control},
doi={10.1109/JSYST.2008.927710},
ISSN={1937-9234},
month={Sep.},}
@INPROCEEDINGS{5720691,
author={Qing, An and Hongtao, Zhang and Zhikun, Hu and Zhiwen, Chen},
booktitle={2011 Third International Conference on Measuring Technology and Mechatronics Automation},
title={A Compression Approach of Power Quality Monitoring Data Based on Two-dimension DCT},
year={2011},
volume={1},
number={},
pages={20-24},
abstract={A compression approach of power quality monitoring data based on two-dimension Discrete Cosine Transform (DCT) is presented to deal with huge data about power quality event detection. The monitoring data was truncated and recomposed in multiple cycles to transform the one-dimension data into the two-dimension data, which was a matrix in essence. The matrix was divided into some sub-blocks all of which were 8Ã8 matrices. These matrices were performed two-dimension DCT. The elements at the same location of all sub-matrices form a new matrix, and the elements were at the equivalent energy level. The energy levels of new matrices were measured by average energy, and quantitative matrix was obtained by a threshold of average energy. The new matrices and quantitative matrix were used to represent the monitoring data set.},
keywords={Power quality;Voltage fluctuations;Monitoring;Discrete cosine transforms;Data compression;Wavelet transforms;Discrete cosine transform;Power quality monitoring;Data compression},
doi={10.1109/ICMTMA.2011.12},
ISSN={2157-1481},
month={Jan},}
@INPROCEEDINGS{897710,
author={Domijan, A. and Song, Z. and Baptista, G.N. and Montenegro, A. and Wang, X. and Gurlaskie, G.T. and Mattern, K.E.},
booktitle={Ninth International Conference on Harmonics and Quality of Power. Proceedings (Cat. No.00EX441)},
title={Power quality monitoring and analysis in a customized power industrial area using the FRIENDS concept},
year={2000},
volume={2},
number={},
pages={379-384 vol.2},
abstract={As one of the steps towards development of a FRIENDS (flexible reliable and intelligent electric energy delivery system) network for improving power quality, the concept of a customized power industrial area (CPIA) is being undertaken. The first phase of this development involves power quality monitoring and analysis as presented in this paper. Data collected from power quality records installed at various points in a customized power industrial area are processed to identify the causes and effects of various disturbances on power quality and to identify the power quality needs of facilities with the customized power industrial area.},
keywords={Power quality;Monitoring;Power system reliability;Electrical equipment industry;Voltage;Communication system control;Power system harmonics;Control systems;Surges;Costs},
doi={10.1109/ICHQP.2000.897710},
ISSN={},
month={Oct},}
@INPROCEEDINGS{7821610,
author={Kush, Ashwani and Hwang, C. Jinshong and Dattana, Vishal},
booktitle={2016 Future Technologies Conference (FTC)},
title={Big data analytics on MANET routing standardization using quality assurance metrics},
year={2016},
volume={},
number={},
pages={192-198},
abstract={An ad hoc network is a collection of wireless mobile nodes dynamically forming a temporary network without the use of any existing network infrastructure or centralized administration. Routing in ad-hoc network is a challenging issue. Big data analytics have been suggested for proper evaluation and decision making in routing and placement of ad hoc network nodes. This Paper analyses the performance of AODV and DSR routing protocols for the quality assurance metrics. The performance differentials of AODV and DSR protocols are analyzed using NS-2 which is the main network simulator, NAM (Network Animator) and compared in terms of scales applied on Packet Delivery Ratio (PDR), in different environments specified by varying pause time, speed and number of nodes.},
keywords={Routing protocols;Routing;Big data;Mobile ad hoc networks;Quality assurance;AODV;Big Data;PDR;Metrics;Quality},
doi={10.1109/FTC.2016.7821610},
ISSN={},
month={Dec},}
@INPROCEEDINGS{9828956,
author={Cao, Jie and Zhang, Ju and Lin, Xiao Guang and Sun, An Long},
booktitle={2022 IEEE 2nd International Conference on Information Communication and Software Engineering (ICICSE)},
title={Design and Implementation of a Perioperative Medical Data Quality Management Platform},
year={2022},
volume={},
number={},
pages={60-65},
abstract={At present, there are more than 60 million hospitalized surgeries each year in China, and hundreds of millions of medical data records have been accumulated. The diversity, speed and other characteristics make it confounding for perioperative medical data to comply with consistent standards, resulting in widespread quality problems. Many issues escape simple inspections because the data generated for surgeries are from multiple data streams. Hence perioperative medical data quality management platform is designed in this paper to unite data from multiple sources and address issues discovered from cross-referencing. By representing cross-referencing data rules with temporal logic, it implements a comprehensive work platform for data quality inspection, data quality control and data annotation of perioperative medical data.},
keywords={Systematics;Data integrity;Soft sensors;Data preprocessing;Surgery;Machine learning;Quality control;data quality management;perioperative medical data;temporal logic},
doi={10.1109/ICICSE55337.2022.9828956},
ISSN={},
month={March},}
@INPROCEEDINGS{8365994,
author={Gschwandtner, Theresia and Erhart, Oliver},
booktitle={2018 IEEE Pacific Visualization Symposium (PacificVis)},
title={Know Your Enemy: Identifying Quality Problems of Time Series Data},
year={2018},
volume={},
number={},
pages={205-214},
abstract={Sensible data analysis requires data quality control. An essential part of this is data profiling, which is the identification and assessment of data quality problems as a prerequisite for adequately handling these problems. Differentiating between actual quality problems and unusual, but valid data values requires the "human-in-the-loop" through the use of visual analytics. Unfortunately, existing approaches for data profiling do not adequately support the special characteristics of time, which is imperative to identify quality problems in time series data - a data type prevalent in a multitude of disciplines. In this design study paper, we outline the design, implementation, and evaluation of "Know Your Enemy" (KYE) - a visual analytics approach to assess the quality of time series data. KYE supports the task of data profiling with (1) predefined data quality checks, (2) user-definable, customized quality checks, (3) interactive visualization to explore and reason about automatically detected problems, and (4) the visual identification of hidden quality problems.},
keywords={Data integrity;Time series analysis;Data visualization;Task analysis;Prototypes;Bars;Taxonomy;data quality;data profiling;time series;visual analytics},
doi={10.1109/PacificVis.2018.00034},
ISSN={2165-8773},
month={April},}
@INPROCEEDINGS{9212069,
author={Schmihing, Frederik and Schindler, Matthias and Jochem, Roland},
booktitle={2020 25th IEEE International Conference on Emerging Technologies and Factory Automation (ETFA)},
title={Paradigm change towards Visual Analytics for data-driven quality improvement in highly flexible production systems},
year={2020},
volume={1},
number={},
pages={573-580},
abstract={Especially in highly flexible production systems, quality management is confronted with immense challenges. Data Analytics is regarded as a technology for coping with increasing complexity. Currently a lack of value-added orientation impairs the effectiveness of such initiatives. This is because the workers do not have the capabilities to process Data Analytics. One approach is to train these workers to enable them for this new technology. These experts are highly busy with their core tasks, so that the other approach is to reduce the complexity for the user of Analytics by providing an interface with a high and interactive usability to have a minimum of training efforts. Today the usage of Visual Management within the lean production paradigms is used to provide Key Performance Indicators and other static information to the workers in an analogue or even digital way. The main problem is that there is no possibility for the worker to analyze why Key Performance Indicators changed and to find the root causes for the development so that they can improve their processes based on the analysis they have done. This paper shows a concept how Visual Analytics can be applied to enable production and quality experts to perform their analysis directly at the value-added process and thus continuously improve process and product quality.},
keywords={Training;Lean production;Data analysis;Visual analytics;Key performance indicator;Data visualization;Complexity theory;production;quality;visual analytics;visualization;self-service},
doi={10.1109/ETFA46521.2020.9212069},
ISSN={1946-0759},
month={Sep.},}
@INPROCEEDINGS{4424178,
author={Bilik, Petr and Koval, Ludvik and Hula, Jiri},
booktitle={2007 9th International Conference on Electrical Power Quality and Utilisation},
title={Modular system for distributed power quality monitoring},
year={2007},
volume={},
number={},
pages={1-5},
abstract={BK-ELCOM is a modular HW & SW platform for power quality monitoring and analysis based on virtual instrumentation technology. The analyzer of BK-ELCOM product line is based on PC hardware equipped by 16-bit NI data acquisition board running the instrument firmware fully written in LabVIEWtrade 8.2. Implemented algorithms follow the requirements of the latest power quality standards like IEC61000-4-30, IEC61000-4-15, IEC61000-4-7, EN 50160. Newly developed data storage format EDF (Extended Data Format) brings open platform for storing any type data for power quality analyzer. EDF brings also new possibilities for the postprocessing software package.},
keywords={Power quality;Monitoring;Instruments;Data acquisition;IEC standards;Power measurement;Voltage;Frequency;Hardware;Application software;Power Quality;Virtual Instrumentation;EN50160;IEC 61000-4-30;IEC 61000-4-7;IEC 61000-4-15;EDF},
doi={10.1109/EPQU.2007.4424178},
ISSN={2150-6655},
month={Oct},}
@INPROCEEDINGS{5538292,
author={Ying Su and Al-Hakim, Latif},
booktitle={2010 The 2nd International Conference on Industrial Mechatronics and Automation},
title={Intelligent control model for Checking data quality in hospital process management},
year={2010},
volume={2},
number={},
pages={376-379},
abstract={Hospital process management ought to deliver better value in terms of end-to-end services. Making managerial decisions based on electronic data generated by the Hospital Information System (HIS) to be more easily manipulated and destroyed than paper document. It is therefore important for auditors to assure that the information in HIS is well-controlled and high quality. This research aims to develop a control model, namely the Information Gap Checking Mechanism (IGAP Checking Mechanism), to automatically check the information gap between computerized process flows and workflow. This study also justifies the feasibility of IGAP-Checking Mechanism by providing a real case study. The result indicates that the IGAP-Checking Mechanism can assist the case hospital in resolving information quality problems that have occurred in its HIS and can also provide value service for the hospital.},
keywords={Intelligent control;Hospitals;Quality management;Automatic control;Control systems;Automation;Electronic mail;Information systems;Design for disassembly;Mathematical model;Information Gap;Intelligent Control;hospital process;Checking Mechanism;Information Quality;Data Quality},
doi={10.1109/ICINDMA.2010.5538292},
ISSN={},
month={May},}
@INPROCEEDINGS{867593,
author={Santoso, S. and Lamoree, J.D.},
booktitle={2000 Power Engineering Society Summer Meeting (Cat. No.00CH37134)},
title={Power quality data analysis: from raw data to knowledge using knowledge discovery approach},
year={2000},
volume={1},
number={},
pages={172-177 vol. 1},
abstract={Power quality instrumentation has advanced significantly and allows continuous monitoring and the capability of capturing various power quality measurements. As a result more and more data is being collected, however, there is no practical method to conveniently convert the collected raw data into specific knowledge desired by end-users. In this panel session, a method of converting raw data into knowledge using the so-called knowledge discovery approach is presented. The motivation and background to automating the data converting process along with a real-world example implemented in an industrial power monitoring system is presented.},
keywords={Power quality;Data analysis;Instruments;Extraterrestrial measurements;Power measurement;Voltage;Memory;Computerized monitoring;Data mining;Energy capture},
doi={10.1109/PESS.2000.867593},
ISSN={},
month={July},}
@INPROCEEDINGS{4099171,
author={Crout, Richard L. and Conlee, Don T.},
booktitle={OCEANS 2006},
title={Quality Control of Minerals Management Service - Oil Company ADCP Data at NDBC: A Successful Partnership Implementation},
year={2006},
volume={},
number={},
pages={1-5},
abstract={The Minerals Management Service (MMS) requires that deep water oil drilling and production platforms in the northern Gulf of Mexico collect and provide current profile data to the National Data Buoy Center (NDBC). NDBC processes and displays the resulting currents on the NDBC website. NDBC has recently implemented quality control algorithms agreed upon by industry and the government. The resulting imagery and data, including quality control flags, are available on the publicly available NDBC website. The quality control algorithms and flags are presented and comparisons of the resulting files are described. Oil companies must collect current profile data when drilling wells or operating production platforms in water greater than 400 meters deep. They are required to collect the data at 20 minute intervals and transmit the data via FTP to NDBC. The data are received, decoded, and quality controlled at NDBC. The current profiles are then formatted in TEmperature Salinity and Current (TESAC) messages and transmitted over the Global Telecommunications System (GTS). The data are also viewed over the NDBC website as columnar listings and current vector stick plots. In order to determine the quality control algorithms for the current profiles, a committee of oil company, industry, and government representatives determined an approach that includes both individual bin (depth level) and profile algorithms. The algorithms take advantage of the fact that the Teledyne RDI Acoustic Doppler Current Profiler (ADCP) collects error velocity, percent good statistics for 3 and 4 beams, and correlation matrices and echo amplitudes for each beam. The algorithms described in this presentation were then implemented and flags generated for each quality control test. A total of nine flags are assigned within the NDBC database. The flags indicate good data (3), suspect data (2), or bad (1) data. Only bad data are not reproduced or plotted on the NDBC real-time webpage. Results from the implementation are being reviewed, but a quick look indicates that the algorithms are returning accurate descriptions of the ADCP data. The stick plots of ocean current with depth are much "cleaner" following the quality control implementation. The implementation of the quality control algorithms was delayed by Hurricanes Katrina and Rita, which impacted both the NDBC and the oil industry in the Gulf of Mexico. NDBC is now resubmitting past data files through the quality control algorithms to insure that all data at NDCB have been quality controlled. The results of this effort (including the quality control algorithms) are being shared with Integrated Ocean Observing System (IOOS) partners in an effort to standardize quality control of oceanographic data},
keywords={Quality control;Minerals;Quality management;Petroleum;Production;Industrial control;Government;Acoustic beams;Oceans;Oil drilling},
doi={10.1109/OCEANS.2006.307072},
ISSN={0197-7385},
month={Sep.},}
@INPROCEEDINGS{9540154,
author={Buelvas P., Julio H. and Avila B., Fernando E. and Gaviria G., Natalia and Munera R., Danny A.},
booktitle={2021 2nd Sustainable Cities Latin America Conference (SCLA)},
title={Data Quality Estimation in a Smart City's Air Quality Monitoring IoT Application},
year={2021},
volume={},
number={},
pages={1-6},
abstract={With the upcoming growth of the Internet of Things (IoT), which is translated into millions of interconnected devices reporting a high volume of data coming from heterogeneous sources (sensors), it is necessary to assess the confidence of data in order to provide the system with trustable information that can be used to get real insights from the physical world and thus take proper decisions or actions over it. Having in mind that ensuring data quality is key to ease user engagement, acceptance of IoT services and large scale deployments [1], a new critical issue arises which is related to the quality of the data in IoT. Some applications might have a different definitions and indicators for data quality (DQ) and thus different threshold for acceptance of the data. In this work, we explore a smart city application in the field of environmental monitoring and identify the related DQ indicators that apply within this context. Our approach is evaluated over a real dataset retrieved from SIATA's citizen scientist low-cost sensor network, an air quality monitoring system that can be encompassed within the IoT paradigm and that is composed by more than 200 nodes deployed all over the Aburra Valley in Antioquia, Colombia. The results show that feasibility assessing data quality and importance data quality awareness for an IoT application, as a tool for it to take proper actions on the real world. Our approach is evaluated over a real dataset retrieved from SIATA's citizen scientist low-cost sensor network, an air quality monitoring system that can be encompassed within the IoT paradigm and that is composed by more than 200 nodes deployed all over the Aburra Valley in Antioquia, Colombia. The results show that feasibility assessing data quality and importance data quality awareness for an IoT application, as a tool for it to take proper actions on the real world.},
keywords={Smart cities;Data integrity;Estimation;Tools;Air quality;Sensor systems;Internet of Things;Smart City;Internet of Things;Air Quality;Environmental Monitoring;Low-cost Sensor;Data Quality},
doi={10.1109/SCLA53004.2021.9540154},
ISSN={},
month={Aug},}
@INPROCEEDINGS{7364064,
author={Becker, David and King, Trish Dunn and McMullen, Bill},
booktitle={2015 IEEE International Conference on Big Data (Big Data)},
title={Big data, big data quality problem},
year={2015},
volume={},
number={},
pages={2644-2653},
abstract={A USAF sponsored MITRE research team undertook four separate, domain-specific case studies about Big Data applications. Those case studies were initial investigations into the question of whether or not data quality issues encountered in Big Data collections are substantially different in cause, manifestation, or detection than those data quality issues encountered in more traditionally sized data collections. The study addresses several factors affecting Big Data Quality at multiple levels, including collection, processing, and storage. Though not unexpected, the key findings of this study reinforce that the primary factors affecting Big Data reside in the limitations and complexities involved with handling Big Data while maintaining its integrity. These concerns are of a higher magnitude than the provenance of the data, the processing, and the tools used to prepare, manipulate, and store the data. Data quality is extremely important for all data analytics problems. From the study's findings, the "truth about Big Data" is there are no fundamentally new DQ issues in Big Data analytics projects. Some DQ issues exhibit return-s-to-scale effects, and become more or less pronounced in Big Data analytics, though. Big Data Quality varies from one type of Big Data to another and from one Big Data technology to another.},
keywords={Big data;Biomedical imaging;Personnel;Complexity theory;Bioinformatics;Process control;Instruments;Big Data;Data Quality;Returns to Scale},
doi={10.1109/BigData.2015.7364064},
ISSN={},
month={Oct},}
@INPROCEEDINGS{8109143,
author={Chengzan, Li and Yanfei, Hou and Jianhui, Li and Lili, Zhang},
booktitle={2017 IEEE 13th International Conference on e-Science (e-Science)},
title={ScienceDB: A Public Multidisciplinary Research Data Repository for eScience},
year={2017},
volume={},
number={},
pages={248-255},
abstract={Research data repositories are necessary infrastructures that ensure the data generated for research are accessible, stable, reliable, and reusable. Based on years of accumulated data work experience, the Computer Network Information Center of the Chinese Academy of Sciences has built a multi-disciplinary data repository ScienceDB for research users and teams using its big data storage, analysis and computing environments. This paper firstly introduces the motivation to develop ScienceDB and gives a profile to it. Then the overall technical framework of ScienceDB is introduced, and the key technologies such as the support for multidiscipline extensibility, data collaboration and data recommendation are analyzed deeply. And then this paper presents the functions and features of ScienceDB's current version and discusses some issues such as its data policy, data quality assurance measures, and current application status. Finally, it summarizes and puts forward that it needs to carry out more in-depth research and practice of ScienceDB in order to meet the higher requirements of eScience in terms of thorough data association and fusion, data analysis and mining, data evaluation, and so on.},
keywords={Metadata;Business;Data visualization;Data mining;Distributed databases;Collaboration;Computer architecture;research data repository;technical framework;multidiscipline;data recommendation;data collaboration;open data},
doi={10.1109/eScience.2017.38},
ISSN={},
month={Oct},}
@INPROCEEDINGS{9700829,
author={Cheng, Wei and Liu, Kun and Wang, Shenliang and Li, Li},
booktitle={2022 International Seminar on Computer Science and Engineering Technology (SCSET)},
title={Design and Implementation of GIS Basic Data Quality Management Tools for Power Network},
year={2022},
volume={},
number={},
pages={347-350},
abstract={The GIS based data quality is the premise of all data processing and analysis, and it has great significance for carrying out GIS business applications and improving the level of practical system. However, due to the adjustment of the platform model, the new and old system data migration, wrong data source input and other reasons, the power grid GIS basic data quality issues are serious. In order to solve the problem of quality management of stock data, this paper designs and implements a GIS data quality management tool based on graph logic processing algorithm, which includes data source management, configuration management, query management and so on. Test results show that the use of the tool technology and optimization of the relevant management process can continuously improve the power grid GIS based data quality, and enhance the practical application of GIS large data.},
keywords={Seminars;Data integrity;Soft sensors;Decision making;Production;Power grids;Personnel;GIS;power grid;basic data},
doi={10.1109/SCSET55041.2022.00085},
ISSN={},
month={Jan},}
@INPROCEEDINGS{8290178,
author={Mukwakungu, S. C. and Mbohwa, C.},
booktitle={2017 IEEE International Conference on Industrial Engineering and Engineering Management (IEEM)},
title={The impact and effectiveness of participating in external quality assurance programmes in quality management and improvement at a local institute medical laboratory, south africa},
year={2017},
volume={},
number={},
pages={1680-1687},
abstract={This study was conducted in a Medical Laboratory in Johannesburg, South Africa, to evaluate the effectiveness and impact of participating in External Quality Assurance (EQA) programs towards improving the correctness of lab results and continuous quality improvement. The study followed a quantitative approach whereby survey questionnaires were emailed and handed out to laboratory personnel. The participant's responses were summarized and analysed using frequency tables and histograms. The data analysis results indicated that the EQA programs play a vital role in quality management and improvement. Most participants indicated that they understood the role the EQA programs play and felt that it is necessary for a medical laboratory to participate in such programs, coupled with other quality assurance and quality control procedures such as IQC, daily QC procedures, corrective action and continuous education. This research showed that EQA plays a vital role in the correct interpretation and reporting of the lab results.},
keywords={Medical diagnostic imaging;ISO Standards;Total quality management;Testing;Quality assessment;Quality;Total Quality Management;External Quality Assessment},
doi={10.1109/IEEM.2017.8290178},
ISSN={2157-362X},
month={Dec},}
@ARTICLE{8697192,
author={},
journal={IEEE Std 1159.3-2019 (Revision of IEEE Std 1159.3-2003)},
title={IEEE Recommended Practice for Power Quality Data Interchange Format (PQDIF)},
year={2019},
volume={},
number={},
pages={1-185},
abstract={A file format suitable for exchanging power quality related measurement and simulation data in a vendor independent manner is defined in this recommended practice. The format is designed to represent all power quality phenomena identified in IEEE Std 1159-2009, IEEE Recommended Practice on Monitoring Electric Power Quality, other power related measurement data, and is extensible to other data types as well. The recommended file format utilizes a highly compressed storage scheme to help reduce disk space and transmission times. The utilization of Globally Unique Identifiers (GUID) to represent each element in the file permits the format to be extensible without the need for a central registration authority.},
keywords={IEEE Standards;Power distribution;Power quality;Power measurement;File systems;Information exchange;data interchange;file format;IEEE 1159.3;measurement;monitoring;power quality;PQDIF},
doi={10.1109/IEEESTD.2019.8697192},
ISSN={},
month={May},}
@INPROCEEDINGS{9678209,
author={Yalaoui, Mehdi and Boukhedouma, Saida},
booktitle={2021 International Conference on Information Systems and Advanced Technologies (ICISAT)},
title={A survey on data quality: principles, taxonomies and comparison of approaches},
year={2021},
volume={},
number={},
pages={1-9},
abstract={Nowadays, data generation keeps increasing exponentially due to the emergence of the Internet of Things (IoT) and Big data technologies. The manipulation of such Big amount of data becomes more and more difficult because of its size and its variety. For better governance of organizations (decision making, data analysis, earnings increase â¦), data quality and data governance at present of Big data are two major pillars for the design of any system handling data within the organization. This explains the number of researches conducted as it constitutes a research subject with several gaps and opportunities. Many works were conducted to define and standardize Data Quality (DQ) and its dimensions, others were directed to design and propose data quality assessment and improvement models or frameworks. This work aims to recall the data quality principles starting by the needed background knowledge, then identify and compare the relevant taxonomies existing in the literature, next surveys and compares the available Data quality assessment and improvement approaches. After that, we propose a metamodel highlighting the main concepts of DQ assessment and we describe a generic process for DQ assessment and improvement. Finally, we evoke the main challenges in the field of DQ before and after the emergence of Big Data.},
keywords={Data integrity;Taxonomy;Standards organizations;Decision making;Organizations;Big Data;Data models;Data Quality;Big Data;Quality Dimensions;Quality Metrics;Metamodel;Assessment process;Improvement},
doi={10.1109/ICISAT54145.2021.9678209},
ISSN={},
month={Dec},}