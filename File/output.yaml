1.chave:
- 9849793
2.author:
- Chen, Bingquan
- Nie, Guojian
- Jiang, Shixin
- Hu, Ning
3.title:
- Research on the Big Data-based Product Quality Data Package Construction and Application
4.keywords:
- Information science
- Data integration
- Companies
- Reliability theory
- Big Data
- Product design
- Quality assessment
- quality data package
- big data
- intelligent manufacturing
- data fusion
5.abstract:
- In the new environment of intelligent manufacturing, enterprise quality data has
  increased exponentially. How to manage, utilize, mine and analyze quality data has
  become a key issue in modern quality management. This article expands the definition
  of the product quality data package in the intelligent manufacturing environment,
  and proposes a big data-based product quality data package construction and management
  solution, gives a quality data fusion method based on business decision, outlines
  the application of quality data package. Finally, a chip manufacturing company was
  used to verify the feasibility of the product quality data package construction
  and management plan.
6.year:
- 2022
7.type_publication:
- inproceedings
8.doi:
- 10.1109/CTISC54888.2022.9849793
1.chave:
- 9245455
2.author:
- Loetpipatwanich, Sakda
- Vichitthamaros, Preecha
3.title:
- 'Sakdas: A Python Package for Data Profiling and Data Quality Auditing'
4.keywords:
- Data integrity
- Pipelines
- Data visualization
- Big Data
- Syntactics
- Software
- Python
- Data Quality Management
- Data Profiling
- Data Quality Auditing
- Python Package
- Data Pipeline
5.abstract:
- "Data Profiling and data quality management become a more significant part of data\
  \ engineering, which an essential part of ensuring that the system delivers quality\
  \ information to users. In the last decade, data quality was considered to need\
  \ more managing. Especially in the big data era that the data comes from many sources,\
  \ many data types, and an enormous amount. Thus it makes the managing of data quality\
  \ is more difficult and complicated. The traditional system was unable to respond\
  \ as needed. The data quality managing software for big data was developed but often\
  \ found in a high-priced, difficult to customize as needed, and mostly provide as\
  \ GUI, which is challenging to integrate with other systems. From this problem,\
  \ we have developed an opensource package for data quality managing. By using Python\
  \ programming language, Which is a programming language that is widely used in the\
  \ scientific and engineering field today. Because it is a programming language that\
  \ is easy to read syntax, small, and has many additional packages to integrate.\
  \ The software developed here is called \u201CSakdas\u201D this package has been\
  \ divided into three parts. The first part deals with data profiling provide a set\
  \ of data analyses to generate a data profile, and this profile will help to define\
  \ the data quality rules. The second part deals with data quality auditing that\
  \ users can set their own data quality rules for data quality measurement. The final\
  \ part deals with data visualizing that provides data profiling and data auditing\
  \ report to improve the data quality. The results of the profiling and auditing\
  \ services, the user can specify both the form of a report for self-review. Or in\
  \ the form of JSON for use in post-process automation."
6.year:
- 2020
7.type_publication:
- inproceedings
8.doi:
- 10.1109/IBDAP50342.2020.9245455
1.chave:
- 8432043
2.author:
- Pan, Xing
- Zhang, Manli
- Chen, Xi
3.title:
- A Method of Quality Improvement Based on Big Quality Warranty Data Analysis
4.keywords:
- Warranties
- Data mining
- Product design
- Quality assessment
- Databases
- Big Data
- Reliability engineering
- quality warranty data
- big data analysis
- association rules
- quality improvement
- PDCA
5.abstract:
- Quality warranty data includes big data of product use and customer services, which
  is foundation of product quality and reliability improvement. This paper presents
  a method of quality warranty data analysis, which is based on the big data analysis
  technology. By means of the method of association rules mining, it distinguishes
  the association rules of failure modes while feeding back the information to the
  process of product design, production, and usage. To achieve product fault location
  and fault disposal, the key factors such as fault type and fault cause are analyzed.
  Meanwhile, this paper adopted the principles of PDCA circulation to propose a procedure
  of product quality improvement. The quality improvement procedure based on quality
  warranty data analysis provides a comprehensive and systematic quality improvement
  for different stages and different types of products. Finally, a case study of household
  appliances in China is given to illustrate the method.
6.year:
- 2018
7.type_publication:
- inproceedings
8.doi:
- 10.1109/QRS-C.2018.00115
1.chave:
- 8725668
2.author:
- Jin, Li
- Haosong, Li
- Zhongping, Xu
- Ting, Wang
- Shuai, Wang
- Yutong, Wei
- Dongliang, Hu
- Chunting, Kang
- Jia, Wu
- Dan, Su
3.title:
- Research on Wide-area Distributed Power Quality Data Fusion Technology of Power
  Grid
4.keywords:
- Power quality
- Data integration
- Distributed databases
- Monitoring
- Power grids
- Computer architecture
- Data models
- Power Quality
- Wide Area Distribution
- Data Integration
5.abstract:
- With the advancement of the "big operation" system construction, the online monitoring
  system for power quality has been integrated, and various power quality data have
  been incorporated into relevant organizations for unified management. Power quality
  management has a larger range of data, more types, and higher frequency. It needs
  to realize the unified storage management and efficient access of massive heterogeneous
  power quality data for the characteristics of data applications and the collection
  and aggregation of these effective data. This paper proposes a new type of grid
  wide-area distributed power quality data integration architecture, which is designed
  for multi-source, heterogeneous, distributed data integration technology and wide-area
  distributed data storage technology to solve the big data source problem and realize
  the sharing of power quality data information of the whole network.
6.year:
- 2019
7.type_publication:
- inproceedings
8.doi:
- 10.1109/ICCCBDA.2019.8725668
1.chave:
- 8605945
2.author:
- Taleb, Ikbal
- Serhani, Mohamed
- Dssouli, Rachida
3.title:
- Big Data Quality Assessment Model for Unstructured Data
4.keywords:
- Big Data
- Data integrity
- Data mining
- Feature extraction
- Data models
- Measurement
- Quality assessment
- Big Data
- Data Quality
- Unstructured Data
- Quality of Unstructured Big Data
5.abstract:
- Big Data has gained an enormous momentum the past few years because of the tremendous
  volume of generated and processed Data from diverse application domains. Nowadays,
  it is estimated that 80% of all the generated data is unstructured. Evaluating the
  quality of Big data has been identified to be essential to guarantee data quality
  dimensions including for example completeness, and accuracy. Current initiatives
  for unstructured data quality evaluation are still under investigations. In this
  paper, we propose a quality evaluation model to handle quality of Unstructured Big
  Data (UBD). The later captures and discover first key properties of unstructured
  big data and its characteristics, provides some comprehensive mechanisms to sample,
  profile the UBD dataset and extract features and characteristics from heterogeneous
  data types in different formats. A Data Quality repository manage relationships
  between Data quality dimensions, quality Metrics, features extraction methods, mining
  methodologies, data types and data domains. An analysis of the samples provides
  a data profile of UBD. This profile is extended to a quality profile that contains
  the quality mapping with selected features for quality assessment. We developed
  an UBD quality assessment model that handles all the processes from the UBD profiling
  exploration to the Quality report. The model provides an initial blueprint for quality
  estimation of unstructured Big data. It also, states a set of quality characteristics
  and indicators that can be used to outline an initial data quality schema of UBD.
6.year:
- 2018
7.type_publication:
- inproceedings
8.doi:
- 10.1109/INNOVATIONS.2018.8605945
1.chave:
- 8029366
2.author:
- Taleb, Ikbal
- Serhani, Mohamed
3.title:
- 'Big Data Pre-Processing: Closing the Data Quality Enforcement Loop'
4.keywords:
- Big Data
- Optimization
- Data models
- Quality assessment
- Big Data
- Data Quality Evaluation
- Data Quality Rules Discovery
- Big Data Pre-Processing
5.abstract:
- In the Big Data Era, data is the core for any governmental, institutional, and private
  organization. Efforts were geared towards extracting highly valuable insights that
  cannot happen if data is of poor quality. Therefore, data quality (DQ) is considered
  as a key element in Big data processing phase. In this stage, low quality data is
  not penetrated to the Big Data value chain. This paper, addresses the data quality
  rules discovery (DQR) after the evaluation of quality and prior to Big Data pre-processing.
  We propose a DQR discovery model to enhance and accurately target the pre-processing
  activities based on quality requirements. We defined, a set of pre-processing activities
  associated with data quality dimensions (DQD's) to automatize the DQR generation
  process. Rules optimization are applied on validated rules to avoid multi-passes
  pre-processing activities and eliminates duplicate rules. Conducted experiments
  showed an increased quality scores after applying the discovered and optimized DQR's
  on data.
6.year:
- 2017
7.type_publication:
- inproceedings
8.doi:
- 10.1109/BigDataCongress.2017.73
1.chave:
- 8386521
2.author:
- Hongxun, Tian
- Honggang, Wang
- Kun, Zhou
- Mingtai, Shi
- Haosong, Li
- Zhongping, Xu
- Taifeng, Kang
- Jin, Li
- Yaqi, Cai
3.title:
- Data quality assessment for on-line monitoring and measuring system of power quality
  based on big data and data provenance theory
4.keywords:
- Data integrity
- Power quality
- Monitoring
- Power measurement
- Redundancy
- Big Data
- Business
- power qualiy
- data quality
- big data
- data provenance
- data assessment
5.abstract:
- Currently, on-line monitoring and measuring system of power quality has accumulated
  a huge amount of data. In the age of big data, those data integrated from various
  systems will face big data application problems. This paper proposes a data quality
  assessment system method for on-line monitoring and measuring system of power quality
  based on big data and data provenance to assess integrity, redundancy, accuracy,
  timeliness, intelligence and consistency of data set and single data. Specific assessment
  rule which conforms to the situation of on-line monitoring and measuring system
  of power quality will be devised to found data quality problems. Thus it will provide
  strong data support for big data application of power quality.
6.year:
- 2018
7.type_publication:
- inproceedings
8.doi:
- 10.1109/ICCCBDA.2018.8386521
1.chave:
- 9742286
2.author:
- Wu, Xiangwei
- Yang, Hongqi
- Liu, Yuke
- Nie, Guojia
- Yang, Lihao
- Yang, Yun
3.title:
- An Intelligent Selection Method Based On Electronic Component Quality Data System
4.keywords:
- Switches
- Data systems
- Data models
- Information and communication technology
- component
- quality data
- data system
- intelligent selection
5.abstract:
- Aiming at the difficulty of electronic component quality data management and application,
  and the lack of data system and application methods required for data management
  in selection scenarios, this paper proposes an intelligent selection method based
  on electronic component quality data system, uses Bi-LSTM-ATT model for entity identification,
  and identifies data association based on entity relationship. By calculating the
  Tanimoto coefficient, the intelligent matching and push of similar products and
  substitute products are realized, and the intellectualization of component selection
  is fully supported. Finally, taking the scenario of fast switching diode selection
  as an example, the feasibility of the method proposed in this paper is verified,
  which provides a model for the intelligent application of quality data resources.
6.year:
- 2021
7.type_publication:
- inproceedings
8.doi:
- 10.1109/CECIT53797.2021.00045
1.chave:
- 8078781
2.author:
- Ning, Xiuli
- Xu, Yingcheng
- Gao, Xiaohong
- Li, Ying
3.title:
- Missing data of quality inspection imputation algorithm base on stacked denoising
  auto-encoder
4.keywords:
- Filling
- Algorithm design and analysis
- Noise reduction
- Training
- Clustering algorithms
- Inspection
- Big Data
- Big data of quality inspection
- Stacked denoising auto-encoder
- Filling algorithm
5.abstract:
- Analyzing and processing big data of quality inspection is the key factor in ensuring
  product quality and People's property security. Big data of quality inspection collected
  by social network and E-commerce is missing in most cases. And the incompleteness
  of data brings huge challenge for analyzing and processing. Therefore, the algorithm
  of data filling based on stacked denoising auto-encoder is proposed in this text.
  As the experiment shows that the algorithm proposed in this text is effective in
  dealing with big data of quality inspection.
6.year:
- 2017
7.type_publication:
- inproceedings
8.doi:
- 10.1109/ICBDA.2017.8078781
1.chave:
- 8332632
2.author:
- Liu, He
- Huang, Fupeng
- Li, Han
- Liu, Weiwei
- Wang, Tongxun
3.title:
- A Big Data Framework for Electric Power Data Quality Assessment
4.keywords:
- Big Data
- Data integrity
- Power grids
- History
- Real-time systems
- Sensors
- data quality
- electric power data
- data quality assessment
- big data
- framework
5.abstract:
- Since a low-quality data may influence the effectiveness and reliability of applications,
  data quality is required to be guaranteed. Data quality assessment is considered
  as the foundation of the promotion of data quality, so it is essential to access
  the data quality before any other data related activities. In the electric power
  industry, more and more electric power data is continuously accumulated, and many
  electric power applications have been developed based on these data. In China, the
  power grid has many special characteristic, traditional big data assessment frameworks
  cannot be directly applied. Therefore, a big data framework for electric power data
  quality assessment is proposed. Based on big data techniques, the framework can
  accumulate both the real-time data and the history data, provide an integrated computation
  environment for electric power big data assessment, and support the storage of different
  types of data.
6.year:
- 2017
7.type_publication:
- inproceedings
8.doi:
- 10.1109/WISA.2017.29
1.chave:
- 9148352
2.author:
- Peng, Zhibin
- Chen, Yuefeng
- Zhang, Zehong
- Qiu, Queling
- Han, Xiaoqiang
3.title:
- Implementation of Water Quality Management Platform for Aquaculture Based on Big
  Data
4.keywords:
- Data visualization
- Aquaculture
- Data mining
- Big Data
- Neural networks
- Data models
- Predictive models
- aquaculture
- big data
- water quality warning
- data visualization
5.abstract:
- In order to ensure the quality and quantity of aquaculture, aquaculture farmers
  need to grasp the water quality in time. However, most farmers have to collect water
  quality data manually at present, and cannot store and reuse that information rapidly.
  This paper aims to use SpringBoot framework and JPA framework to build a big data
  platform of acquisition automation and visualization, which realizes the data analysis
  and display of heterogeneous water quality and breeding information. The platform
  can make the water quality prediction and real-time warning. Meanwhile, it realizes
  the management of robots, users and breeding experts. The application of this platform
  will bring better social benefits to aquaculture farmers.
6.year:
- 2020
7.type_publication:
- inproceedings
8.doi:
- 10.1109/CIBDA50819.2020.00024
1.chave:
- 9378148
2.author:
- "O\u2019Shea, Enda"
- Khan, Rafflesia
- Breathnach, Ciara
- Margaria, Tiziana
3.title:
- Towards Automatic Data Cleansing and Classification of Valid Historical Data An
  Incremental Approach Based on MDD
4.keywords:
- Java
- Quality assurance
- Semantics
- Big Data
- Tools
- Syntactics
- Cleaning
- Data Collection
- Data Analytics
- Model-Driven Development
- Historical Data
- Data Parsing
- Data Cleaning
- Ethics
- Data Assurance
- Data Quality
5.abstract:
- 'The project Death and Burial Data: Ireland 1864-1922 (DBDIrl) examines the relationship
  between historical death registration data and burial data to explore the history
  of power in Ireland from 1864 to 1922. Its core Big Data arises from historical
  records from a variety of heterogeneous sources, some aspects are pre-digitized
  and machine readable. A huge data set (over 4 million records in each source) and
  its slow manual enrichment (ca 7,000 records processed so far) pose issues of quality,
  scalability, and creates the need for a quality assurance technology that is accessible
  to non-programmers. An important goal for the researcher community is to produce
  a reusable, high-level quality assurance tool for the ingested data that is domain
  specific (historic data), highly portable across data sources, thus independent
  of storage technology.This paper outlines the step-wise design of the finer granular
  digital format, aimed for storage and digital archiving, and the design and test
  of two generations of the techniques, used in the first two data ingestion and cleaning
  phases.The first small scale phase was exploratory, based on metadata enrichment
  transcription to Excel, and conducted in parallel with the design of the final digital
  format and the discovery of all the domain-specific rules and constraints for the
  syntax and semantic validity of individual entries. Excel embedded quality checks
  or database-specific techniques are not adequate due to the technology independence
  requirement. This first phase produced a Java parser with an embedded data cleaning
  and evaluation classifier, continuously improved and refined as insights grew. The
  next, larger scale phase uses a bespoke Historian Web Application that embeds the
  Java validator from the parser, as well as a new Boolean classifier for valid and
  complete data assurance built using a Model-Driven Development technique that we
  also describe. This solution enforces property constraints directly at data capture
  time, removing the need for additional parsing and cleaning stages. The new classifier
  is built in an easy to use graphical technology, and the ADD-Lib tool it uses is
  a modern low-code development environment that auto-generates code in a large number
  of programming languages. It thus meets the technology independence requirement
  and historians are now able to produce new classifiers themselves without being
  able to program. We aim to infuse the project with computational and archival thinking
  in order to produce a robust data set that is FAIR compliant (Free Accessible Inter-operable
  and Re-useable).'
6.year:
- 2020
7.type_publication:
- inproceedings
8.doi:
- 10.1109/BigData50022.2020.9378148
1.chave:
- 9332030
2.author:
- Sulistyo, Haidar
- Kusumasari, Tien
- Alam, Ekky
3.title:
- Implementation of Data Cleansing Null Method for Data Quality Management Dashboard
  using Pentaho Data Integration
4.keywords:
- Data integrity
- Government
- Decision making
- Data integration
- Information and communication technology
- Data cleansing
- Data quality
- Data quality management
- Null cleansing
- Pentaho data integration
5.abstract:
- Data is a collection of facts or information collected from various sources that
  are dirty and will affect the quality of decision-making in an organization. Data
  cleansing ensures that the data is correct, useable, and consistent. Data may be
  incomplete, inaccurate, or has the wrong format and needs to be corrected or deleted.
  Data cleansing processing can improve the quality of the data significantly. The
  data cleansing processing requires to create useful quality data that provides significant
  benefits for the recipient. The availability of data is crucial in an organization
  to develop competent, valid, and trustworthy decisions. The null or blank field
  in data is one of many problems to maintain data quality management in an organization,
  especially in Indonesian government agencies. The brand registration number permits
  contain many blank fields, including the complete data needed for the next step
  processing. Therefore, to solve the amount of blank data, this research will discuss
  the design and implementation of the data cleansing null method using Pentaho Data
  Integration (PDI). The result will be implemented to the data quality management
  (DQM) dashboard using the laravel framework and MySQL as a DBMS.
6.year:
- 2020
7.type_publication:
- inproceedings
8.doi:
- 10.1109/ICOIACT50329.2020.9332030
1.chave:
- 9322890
2.author:
- Vasiliev, Victor
- Aleksandrova, Svetlana
3.title:
- The Prospects for the Creation of a Digital Quality Management System DQMS
4.keywords:
- Quality management
- Process control
- Digital transformation
- Information technology
- Task analysis
- Information security
- Companies
- digital technologies
- quality
- quality management
- digital quality management system DQMS
- aerospace
- life cycle
- digital transformation
- Big data
- control of technological processes
- Internet of things
5.abstract:
- The development of digital technologies can give a new impetus to the development
  of quality management (QM). The development of new approaches based on the integration
  of quality management methods and digital technologies creates prerequisites for
  the digital transformation of the entire product lifecycle. The difficulty of creating
  an effective quality management system using digital technologies is not only in
  the absence of specialists in two areas of knowledge simultaneously, but also in
  the lack of integration of modern quality management methods with existing software
  products. In most ready-made solutions, quality management is limited to controlling
  process parameters and product quality. Automatic registration of process parameters
  with real-time data analysis should be additionally enabled in DQMS. This will allow
  you to organize monitoring and control of processes at each automated workplace.
  The accumulated analysis results will help you make decisions in difficult situations.
  A set of processes with digital control and analysis ensures quality assurance at
  all stages of the product lifecycle.
6.year:
- 2020
7.type_publication:
- inproceedings
8.doi:
- 10.1109/ITQMIS51053.2020.9322890
1.chave:
- 4424094
2.author:
- Vatra, Fanica
- Poida, Ana
- Stanescu, Carmen
3.title:
- Data system for the monitoring of power quality in the transmission substations
  supplying big consumers
4.keywords:
- Data systems
- Monitoring
- Power quality
- Substations
- Power measurement
- Electric variables measurement
- Data analysis
- Current measurement
- Frequency measurement
- Data acquisition
- power quality
- data acquisition
- monitoring
- data system
- big consumers
5.abstract:
- "During 2006, at CN Transelectrica - OMEPA Branch request, ISPE - Power Systems\
  \ Department has conceived the designing documentations (Feasibility Study and Tender\
  \ Documents) for \u201CPower Quality Analyzing System at the big consumers\u201D\
  . The present paper reports the purpose and technical endowment proposed by ISPE\
  \ for \u201CPower Quality Monitoring and Analyzing System\u201D that will be developed\
  \ at OMEPA."
6.year:
- 2007
7.type_publication:
- inproceedings
8.doi:
- 10.1109/EPQU.2007.4424094
1.chave:
- 9422032
2.author:
- Mao, Yifan
- Huang, Shasha
- Cui, Shuo
- Wang, HaiFeng
- Zhang, Junyan
- Ding, Wenhao
3.title:
- Multi dimensional data distribution monitoring based on OLAP
4.keywords:
- Measurement
- Analytical models
- Standards organizations
- Data integration
- Systems architecture
- Financial management
- Information age
- data link
- OLAP
- multidimensional data model
5.abstract:
- With the rapid development of the Internet, society is gradually entering the information
  age, and various data in enterprises have become the most important strategic core
  resources of all enterprises. The operation and decision-making of enterprises all
  require a large amount of data analysis. Nowadays, many companies do not pay enough
  attention to the monitoring of data asset distribution. In addition, various internal
  systems such as financial management and ERP systems are relatively independent.
  Each system has its own data organization standard, which makes it difficult to
  conduct a unified management of data. This also directly leads to the one-sided
  and subjective problem of enterprise managers' distribution of data assets. With
  the construction of the data center of each enterprise, the data of each system
  is aggregated to the center through data integration technology. Therefore, all
  enterprises need to build a multi-dimensional data distribution monitoring model
  around data links to comprehensively monitor the status of various data distributions
  across the company's entire network, and improve data service capabilities and sharing
  capabilities as well as the company's operational capabilities. This article uses
  OLAP technology to construct a multi-dimensional data distribution monitoring model
  for the data link in the process of power enterprise data integration. This article
  first selects the dimensions and metrics that need to be monitored in the multidimensional
  data, and then constructs the conceptual model, logical model and physical model
  of the multidimensional data using on line analytical processing technology. Finally,
  an example analysis of OLAP system architecture based on B/S structure is realized.
  The overall data distribution of the enterprise can be grasped by analyzing the
  various dimensions of the data link, such as System type, location distribution,
  and time.
6.year:
- 2020
7.type_publication:
- inproceedings
8.doi:
- 10.1109/ITCA52113.2020.00070
1.chave:
- 8109169
2.author:
- Pastorello, Gilberto
- Gunter, Dan
- Chu, Housen
- Christianson, Danielle
- Trotta, Carlo
- Canfora, Eleonora
- Faybishenko, Boris
- Cheah, You-Wei
- Beekwilder, Norm
- Chan, Stephen
- Dengel, Sigrid
- Keenan, Trevor
- O'Brien, Fianna
- Elbashandy, Abdelrahman
- Poindexter, Cristina
- Humphrey, Marty
- Papale, Dario
- Agarwal, Deb
3.title:
- 'Hunting Data Rogues at Scale: Data Quality Control for Observational Data in Research
  Infrastructures'
4.keywords:
- Quality control
- Automation
- Legged locomotion
- Carbon
- Soil
- Ecosystems
- data quality
- quality assurance
- quality control
- observational data
- research infrastructures
5.abstract:
- Data quality control is one of the most time consuming activities within Research
  Infrastructures (RIs), especially when involving observational data and multiple
  data providers. In this work we report on our ongoing development of data rogues,
  a scalable approach to manage data quality issues for observational data within
  RIs. The motivation for this work started with the creation of the FLUXNET2015 dataset,
  which includes carbon, water, and energy fluxes plus micrometeorological and ancillary
  data measured in over 200 sites around the world. To create an uniform dataset,
  including derived data products, extensive work on data quality control was needed.
  The unpredictable nature of observational data quality issues makes the automation
  of data quality control inherently difficult. Developed based on this experience,
  the data rogues methodology allows for increased automation of quality control activities
  by systematically identifying, cataloging, and documenting implementations of solutions
  to data issues. We believe this methodology can be extended and applied to others
  domains and types of data, making the automation of data quality control a more
  tractable problem.
6.year:
- 2017
7.type_publication:
- inproceedings
8.doi:
- 10.1109/eScience.2017.64
1.chave:
- 8674323
2.author:
- Sabtiana, Rela
- Yudhoatmojo, Satrio
- Hidayanto, Achmad
3.title:
- 'Data Quality Management Maturity Model: A Case Study in BPS-Statistics of Kaur
  Regency, Bengkulu Province, 2017'
4.keywords:
- Data integrity
- Data models
- Organizations
- Standards organizations
- Capability maturity model
- Protocols
- data quality
- data quality management
- maturity model
- data quality maturity model
5.abstract:
- Data are widely used in an organization not only for operation but also for strategic
  level use. Poor data quality can have negative impact for an organization such as
  poor decision making and planning. Therefore, data quality management becomes an
  issue growing today not only to the academic but also professional communities.
  Based on this issue, this paper presents and analyzes a case study developed in
  a governmental agency, BPS-Statistics of Kaur Regency. For analysis, a data quality
  maturity model is used to measure the implementation of data quality management
  in the organization. The results show that for the dimension of `Data quality expectations'
  is at a maturity of 4.25. `Data quality protocol' is at a maturity of 3.50. `Policies'
  reaches a maturity of 3.67. `Data quality protocol' and `Data standard' are at a
  maturity of 4.42. `Data governance' is at a maturity of 3.00. `Technology' is at
  a maturity 3.17. `Performance management' is at a maturity of 3.33. However, this
  also implies that implementing these particular dimensions will lead to a direct
  increase in overall maturity.
6.year:
- 2018
7.type_publication:
- inproceedings
8.doi:
- 10.1109/CITSM.2018.8674323
1.chave:
- 8078796
2.author:
- HongJu, Xiao
- Fei, Wang
- FenMei, Wang
- XiuZhen, Wang
3.title:
- Some key problems of data management in army data engineering based on big data
4.keywords:
- Data engineering
- Data analysis
- Data integration
- Big Data
- Distributed databases
- Data models
- Uncertainty
- army data engineering
- data management
- data integration
- data analysis
- representation of data analysis results
- data quality
5.abstract:
- This paper analyzed the challenges of data management in army data engineering,
  such as big data volume, data heterogeneous, high rate of data generation and update,
  high time requirement of data processing, and widely separated data sources. We
  discussed the disadvantages of traditional data management technologies to deal
  with these problems. We also highlighted the key problems of data management in
  army data engineering including data integration, data analysis, representation
  of data analysis results, and evaluation of data quality.
6.year:
- 2017
7.type_publication:
- inproceedings
8.doi:
- 10.1109/ICBDA.2017.8078796
1.chave:
- 9006358
2.author:
- Kumar, Jitendra
- Crow, Michael
- Devarakonda, Ranjeet
- Giansiracusa, Michael
- Guntupally, Kavya
- Olatt, Joseph
- Price, Zach
- Shanafield, Harold
- Singh, Alka
3.title:
- "Provenance\u2013aware workflow for data quality management and improvement for\
  \ large continuous scientific data streams"
4.keywords:
- Data integrity
- Instruments
- Atmospheric measurements
- Dictionaries
- Big Data
- Portals
- scientific data workflows
- data quality
- provenance
- atmospheric science
5.abstract:
- Data quality assessment, management and improvement is an integral part of any big
  data intensive scientific research to ensure accurate, reliable, and reproducible
  scientific discoveries. The task of maintaining the quality of data, however, is
  non-trivial and poses a challenge for a program like the Department of Energy's
  Atmospheric Radiation Measurement (ARM) that collects data from hundreds of instruments
  across the world, and distributes thousands of streaming data products that are
  continuously produced in near-real-time for an archive 1.7 Petabyte in size and
  growing. In this paper, we present a computational data processing workflow to address
  the data quality issues via an easy and intuitive web-based portal that allows reporting
  of any quality issues for any site, facility or instruments at a granularity down
  to individual variables in the data files. This portal allows instrument specialists
  and scientists to provide corrective actions in the form of symbolic equations.
  A parallel processing framework applies the data improvement to a large volume of
  data in an efficient, parallel environment, while optimizing data transfer and file
  I/O operations; corrected files are then systematically versioned and archived.
  A provenance tracking module tracks and records any change made to the data during
  its entire life cycle which are communicated transparently to the scientific users.
  Developed in Python using open source technologies, this software architecture enables
  fast and efficient management and improvement of data in an operational data center
  environment.
6.year:
- 2019
7.type_publication:
- inproceedings
8.doi:
- 10.1109/BigData47090.2019.9006358
1.chave:
- 5768635
2.author:
- Jihong, Shan
- Yalang, Mao
- Yi, Sun
3.title:
- Data model for product life cycle quality control mapping with product structure
  geometrical model
4.keywords:
- Quality control
- Data models
- Unified modeling language
- Solid modeling
- Product design
- Design automation
- quality control
- data model
- structure mapping model
5.abstract:
- This paper focus on the technique of learning the knowledge on the design quality
  and manufacture quality. The characteristic of quality control are illustrated,
  such as status, process, structure etc, and a lot of feature of the quality control
  information which consist of geometrical structure model, manufacture technique,
  detect, fault diagnose and data analysis were presented. Then an approach of the
  mapping between the product quality control information to the components geometrical
  model is put out, which can be implemented to optimal the product design, manufacture
  and assembly in quality control. Finally a prototype system was designed based on
  the data model.
6.year:
- 2011
7.type_publication:
- inproceedings
8.doi:
- 10.1109/CECNET.2011.5768635
1.chave:
- 7584971
2.author:
- Serhani, Mohamed
- El, Hadeel
- Taleb, Ikbal
- Nujum, Alramzana
3.title:
- An Hybrid Approach to Quality Evaluation across Big Data Value Chain
4.keywords:
- Big data
- Metadata
- Measurement
- Quality assessment
- Quality of service
- Unified modeling language
- Big Data
- Quality assessment
- Metadata
- Quality metrics
- quality Metadata
- Quality of process
- Hybrid quality assessment
5.abstract:
- 'While the potential benefits of Big Data adoption are significant, and some initial
  successes have already been realized, there remain many research and technical challenges
  that must be addressed to fully realize this potential. The Big Data processing,
  storage and analytics, of course, are major challenges that are most easily recognized.
  However, there are additional challenges related for instance to Big Data collection,
  integration, and quality enforcement. This paper proposes a hybrid approach to Big
  Data quality evaluation across the Big Data value chain. It consists of assessing
  first the quality of Big Data itself, which involve processes such as cleansing,
  filtering and approximation. Then, assessing the quality of process handling this
  Big Data, which involve for example processing and analytics process. We conduct
  a set of experiments to evaluate Quality of Data prior and after its pre-processing,
  and the Quality of the pre-processing and processing on a large dataset. Quality
  metrics have been measured to access three Big Data quality dimensions: accuracy,
  completeness, and consistency. The results proved that combination of data-driven
  and process-driven quality evaluation lead to improved quality enforcement across
  the Big Data value chain. Hence, we recorded high prediction accuracy and low processing
  time after we evaluate 6 well-known classification algorithms as part of processing
  and analytics phase of Big Data value chain.'
6.year:
- 2016
7.type_publication:
- inproceedings
8.doi:
- 10.1109/BigDataCongress.2016.65
1.chave:
- 8532518
2.author:
- Burkhardt, Andrew
- Berryman, Sheila
- Brio, Ashley
- Ferkau, Susan
- Hubner, Gloria
- Lynch, Kevin
- Mittman, Susan
- Sonderer, Kathy
3.title:
- Measuring Manufacturing Test Data Analysis Quality
4.keywords:
- Data integrity
- Manufacturing
- Measurement
- Decision making
- Production facilities
- Data models
- manufacturing test
- data quality
- test data quality
- cost of data quality
5.abstract:
- Manufacturing test data volumes are constantly increasing. While there has been
  extensive focus in the literature on big data processing, less focus has existed
  on data quality, and considerably less focus has been placed specifically on manufacturing
  test data quality. This paper presents a fully automated test data quality measurement
  developed by the authors to facilitate analysis of manufacturing test operations,
  resulting in a single number used to compare manufacturing test data quality across
  programs and factories, and focusing effort cost-effectively. The automation enables
  program and factory users to see, understand, and improve their test data quality
  directly. Immediate improvements in test data quality speed manufacturing test operation
  analysis, reducing elapsed time and overall spend in test operations. Data quality
  has significant financial impacts to businesses [1]. While manufacturing cost models
  are well understood, data quality cost models are less well understood (see Eppler
  & Helfert [2] who review manufacturing cost models and create a taxonomy for data
  quality costs). Kim & Choi [3] discuss measuring data quality costs, and a rudimentary
  data quality cost calculation is described in [4]. Haug et al. [5] describe a classification
  of costs for poor data quality, and while they do not provide a cost calculation,
  they do define optimality for data quality. Laranjeiro et al. [6] have a recent
  survey of poor data quality classification. Ge & Helfert [7] extend the work in
  [2], and provide an updated review of data quality costs. Test data is specifically
  addressed in the context of data processing in [8]. Big data quality efforts are
  reviewed in [9, 10]. Data quality metrics are discussed in [11], and requirements
  for data quality metrics are identified in [12]. Data inconsistencies are detailed
  in [13], while categorical data inconsistencies are explained in [14]. In the current
  work, manufacturing test data quality is directly correlated to the speed of manufacturing
  test operations analysis. A measurement for manufacturing test data quality indicates
  the speed at which analysis can be performed, and increases in the test data quality
  score have precipitated increases in the speed of analysis, described herein.
6.year:
- 2018
7.type_publication:
- inproceedings
8.doi:
- 10.1109/AUTEST.2018.8532518
1.chave:
- 7816918
2.author:
- Taleb, Ikbal
- Kassabi, Hadeel
- Serhani, Mohamed
- Dssouli, Rachida
- Bouhaddioui, Chafik
3.title:
- 'Big Data Quality: A Quality Dimensions Evaluation'
4.keywords:
- Big data
- Measurement
- Feature extraction
- Quality assessment
- Social network services
- Companies
- Context
- Big Data
- data quality dimensions
- data quality evaluation
- Big data sampling
5.abstract:
- Data is the most valuable asset companies are proud of. When its quality degrades,
  the consequences are unpredictable, can lead to complete wrong insights. In Big
  Data context, evaluating the data quality is challenging, must be done prior to
  any Big data analytics by providing some data quality confidence. Given the huge
  data size, its fast generation, it requires mechanisms, strategies to evaluate,
  assess data quality in a fast, efficient way. However, checking the Quality of Big
  Data is a very costly process if it is applied on the entire data. In this paper,
  we propose an efficient data quality evaluation scheme by applying sampling strategies
  on Big data sets. The Sampling will reduce the data size to a representative population
  samples for fast quality evaluation. The evaluation targeted some data quality dimensions
  like completeness, consistency. The experimentations have been conducted on Sleep
  disorder's data set by applying Big data bootstrap sampling techniques. The results
  showed that the mean quality score of samples is representative for the original
  data, illustrate the importance of sampling to reduce computing costs when Big data
  quality evaluation is concerned. We applied the Quality results generated as quality
  proposals on the original data to increase its quality.
6.year:
- 2016
7.type_publication:
- inproceedings
8.doi:
- 10.1109/UIC-ATC-ScalCom-CBDCom-IoP-SmartWorld.2016.0122
1.chave:
- 9323615
2.author:
- Han, Weiguo
- Jochum, Matthew
3.title:
- A Machine Learning Approach for Data Quality Control of Earth Observation Data Management
  System
4.keywords:
- Data integrity
- Machine learning
- Earth
- Satellites
- Big Data
- Process control
- Monitoring
- Big Data
- Machine Learning
- Earth Observation Data
- Data management
- Data Quality
- Random Forest
5.abstract:
- In the big data era, innovative technologies like cloud computing, artificial intelligence,
  and machine learning are increasingly utilized in the large-scale data management
  systems of many industry sectors to make them more scalable and intelligent. Applying
  them to automate and optimize earth observation data management is a hot topic.
  To improve data quality control mechanisms, a machine learning method in combination
  with built-in quality rules is presented in this paper to evolve processes around
  data quality and enhance management of earth observation data. The rules of quality
  check are set up to detect the common issues, including data completeness, data
  latency, bad data, and data duplication, and the machine learning model is trained,
  tested, and deployed to address these quality issues automatically and reduce manual
  efforts.
6.year:
- 2020
7.type_publication:
- inproceedings
8.doi:
- 10.1109/IGARSS39084.2020.9323615
1.chave:
- 9599117
2.author:
- West, Nikolai
- Gries, Jonas
- Brockmeier, Carina
- "G\xF6bel, Jens"
- Deuse, Jochen
3.title:
- 'Towards integrated Data Analysis Quality: Criteria for the application of Industrial
  Data Science'
4.keywords:
- Analytical models
- Phase measurement
- Data integrity
- Data acquisition
- Production
- Data science
- Data models
- Data quality
- data analysis quality
- industrial data science
- quality management
- quality assurance
- data quality criteria
5.abstract:
- The application of Industrial Data Science in context of connected Smart Products
  requires modeling and structuring data for its design, development and use. Especially
  for Smart Products, a comprehensive handling of data quality is mandatory, because
  of their interdisciplinary character and broad range of heterogeneous stakeholders
  covering the entire product lifecycle. The overall goal of data preparation is to
  provide high-quality data for application and evaluation by users. Established process
  models for industrial data analysis often treat the specification and assurance
  of data quality as a single-point activity with a defined conclusion. Providing
  end-to-end data quality has received little attention in the field of industrial
  data analytics. In this paper, we will (1) structure four distinct phases for ensuring
  end-to-end data quality along data analytics activities, (2) define a set of criteria
  and measures for meeting and quantifying data quality requirements based on established
  criteria, and (3) provide a step-by-step model for establishing and maintaining
  high Data Quality for Industrial Data Science applications. The quality criteria
  aim to identify pointwise and continuous actions during the data analysis process.
  Such criteria target a shared responsibility for maintaining data quality during
  analyses between analyst and user. The developed model provides an actionable approach
  for assessing and ensuring the requirements of Data Analysis Quality.
6.year:
- 2021
7.type_publication:
- inproceedings
8.doi:
- 10.1109/IRI51335.2021.00024
1.chave:
- 6842887
2.author:
- Stander, Tiaan
- Rens, Johan
3.title:
- Quality of supply data mining
4.keywords:
- Quality of service
- Data mining
- Data visualization
- Standards
- Visualization
- Voltage fluctuations
- Joining processes
- QoS
- PQ
- data mining
- SQL
- QoS reporting
- compliance to compatibility
- network risk management
- interactive data visualization
- contextualization
- data dashboards
5.abstract:
- Extracting useful network management information from a large volume of QoS data
  obtained all over a network can be simplified by innovative data mining techniques.
  The need for QoS expertise is reduced as interactive visualization by brushing and
  linking of datasets reveals interrelation of parameters. Data contextualization
  by annotated data can aid the assessment on the global level of compatibility between
  supply and use conditions. Data dashboards can further simplify the analysis of
  QoS data by recognizing the network connectivity of different sites, seasonal effects
  and direction of voltage waveform events.
6.year:
- 2014
7.type_publication:
- inproceedings
8.doi:
- 10.1109/ICHQP.2014.6842887
1.chave:
- 5557442
2.author:
- Man, Yuan
- Wei, Liu
- Gang, Huang
- Juntao, Gao
3.title:
- A Noval Data Quality Controlling and Assessing Model Based on Rules
4.keywords:
- Data models
- Databases
- Quality assessment
- Accuracy
- Dictionaries
- Quality control
- data
- data quality
- metadata
- rule base
5.abstract:
- "As a resource, data is the base for information construction and application. According\
  \ to the principle of \u201Cgarbage in and garbage out\u201D, it needs us to ensure\
  \ data reliability, no errors and accurately reflect the real situation to support\
  \ the right decisions. However, due to various reasons, it leads to poor quality\
  \ of dirty data in existing system business, while the dirty data is an important\
  \ factor which affects right decisions. For the above, in this paper, a metadata-based\
  \ data quality rule base is created for improving traditional quality control model,\
  \ a more practical application of the weighted assessment algorithm is proposed\
  \ and a three-tier data quality assessment system model is constructed based on\
  \ the study of definition and classification of quality, assessment algorithm, metadata\
  \ and the control theory. This model is confirmed to achieve comprehensive quality\
  \ of data management and control in oilfield practical applications."
6.year:
- 2010
7.type_publication:
- inproceedings
8.doi:
- 10.1109/ISECS.2010.15
1.chave:
- 9671672
2.author:
- Poon, Lex
- Farshidi, Siamak
- Li, Na
- Zhao, Zhiming
3.title:
- Unsupervised Anomaly Detection in Data Quality Control
4.keywords:
- Data integrity
- Decision making
- Process control
- Quality control
- Organizations
- Big Data
- Data models
- data quality control
- data quality assessment
- unsupervised learning
- anomaly detection
- automated data quality control
5.abstract:
- Data is one of the most valuable assets of an organization and has a tremendous
  impact on its long-term success and decision-making processes. Typically, organizational
  data error and outlier detection processes perform manually and reactively, making
  them time-consuming and prone to human errors. Additionally, rich data types, unlabeled
  data, and increased volume have made such data more complex. Accordingly, an automated
  anomaly detection approach is required to improve data management and quality control
  processes. This study introduces an unsupervised anomaly detection approach based
  on models comparison, consensus learning, and a combination of rules of thumb with
  iterative hyper-parameter tuning to increase data quality. Furthermore, a domain
  expert is considered a human in the loop to evaluate and check the data quality
  and to judge the output of the unsupervised model. An experiment has been conducted
  to assess the proposed approach in the context of a case study. The experiment results
  confirm that the proposed approach can improve the quality of organizational data
  and facilitate anomaly detection processes.
6.year:
- 2021
7.type_publication:
- inproceedings
8.doi:
- 10.1109/BigData52589.2021.9671672
1.chave:
- 6021598
2.author:
- Idris, Norizam
- Ahmad, Kamsuriah
3.title:
- Managing Data Source quality for data warehouse in manufacturing services
4.keywords:
- ISO standards
- Quality management
- Data warehouses
- Organizations
- Planning
- Data models
- Standards organizations
- Data Quality
- Data Source
- Quality Management System
- Data Warehouse
- ISO 9001:2008
5.abstract:
- Data quality and Data Source Management is one of the key success factors for data
  warehouse project. Many data warehouse projects fail due to poor quality of the
  data. It is believed that the problems can be fixed later and because of that, a
  lot of time will be spent to fix the error. If low quality data fed into the data
  warehouse system, the result will be not accurate if these data are used in the
  decision making. Many data warehouse and business intelligence projects failure
  are due to wrong or low quality data. Therefore this paper will underpins several
  aspects such as Total Data Quality Management (TDQM), ISO 9001:2008 and Quality
  Management System (QMS) in order to address data quality problems in the early stage
  and find out the best procedure to manage the data sources. To find a standard procedure
  in managing data source base on ISO 9001:2008 standard, process in managing data
  source is identified and a compared to the ISO 9001:2008 Quality Management System
  (QMS) requirements. As a result, this process is viewed as a kind of production
  process and relate to the concepts of quality management known from the manufacturing
  and service domain. More precisely, a high quality management system in managing
  data source is proposed. This system is based on ISO 9001:2008 standard and hopes
  it can help organizations in implementing and operating quality management system.
  By using ISO 9001:2008 framework to the process of managing data source, this approach
  will be similar to the manufacturing concept that has an added advantage when compared
  to traditional approaches in managing data source.
6.year:
- 2011
7.type_publication:
- inproceedings
8.doi:
- 10.1109/ICEEI.2011.6021598
1.chave:
- 5540836
2.author:
- Hongke, Han
- Linhai, Qi
3.title:
- Application and research of multidimensional data analysis in power quality
4.keywords:
- Multidimensional systems
- Data analysis
- Power quality
- Monitoring
- Voltage
- Power engineering computing
- Data mining
- Energy management
- Quality management
- Frequency
- Multidimensional Data Analysi
- Power Quality
- MS Analysis Services
5.abstract:
- To extract potentially useful information from a large number of power quality monitoring
  data, this article carries out the analysis of power quality data by the use of
  multi-dimensional data analysis from multiple perspectives. Considering the data
  mining in data pretreatment, disturbance recognition related applications, then
  analyzes and builds a power quality cube. Through the cube analysis, the manager
  can have a comprehensive and detailed understanding of the trends of power quality
  data in order to operate in the power to make scientific decision-making.
6.year:
- 2010
7.type_publication:
- inproceedings
8.doi:
- 10.1109/ICCDA.2010.5540836
1.chave:
- 8258221
2.author:
- Hee, Kim
3.title:
- 'Is data quality enough for a clinical decision?: Apply machine learning and avoid
  bias'
4.keywords:
- Data mining
- Medical diagnostic imaging
- Currencies
- Tools
- Medical services
- Measurement
- data quality
- decision quality
- healthcare
- clinical data
- EHRs
- data quality assurance
- stratified sampling
- bias
5.abstract:
- This paper provides a practical guideline for the assurance and (re-)usage of clinical
  data. It proposes a process which aims to provide a systematic data quality assurance
  even without involving a medical domain expert. Especially when (re-)using clinical
  data, data quality is an important topic because clinical data are not purposely
  collected. Therefore, data driven conclusions might be false, because a given dataset
  is not representative. These false data driven conclusions could even harm the life
  of patients. Thus, all researchers should adhere to some basic principles that can
  prevent false conclusions. Twelve empirical experiments were conducted in order
  to prove that my process is able to assure data quality with respect to the descriptive
  and predictive analysis. Descriptive results obtained by applying stratified sampling
  are conflicting in four out of nine population inputs. Sampling is carried based
  on the top ranked feature drawn by the Contextual Data Quality Assurance (CDQA).
  Between datasets these features are confirmed by the Mutual Data Quality Assurance
  (MDQA). Stratified sampled inputs improve predictive results compared to raw data.
  Both Area Under the Curve (AUC) scores and accuracy scores increase by three percent.
6.year:
- 2017
7.type_publication:
- inproceedings
8.doi:
- 10.1109/BigData.2017.8258221
1.chave:
- 8465410
2.author:
- Ogudo, Kingsley
- Nestor, Dahj
3.title:
- Modeling of an Efficient Low Cost, Tree Based Data Service Quality Management for
  Mobile Operators Using in-Memory Big Data Processing and Business Intelligence use
  Cases
4.keywords:
- Quality of service
- Big Data
- Business intelligence
- Structured Query Language
- Tools
- Sparks
- Service Quality Management
- In-Memory Big Data
- Business Intelligence
- Service Quality Index
- Over The Top Application (OTT)
- Data Traffic and ROI
5.abstract:
- Network Operators are shifting their business interest towards Data services in
  a geometric progression manner, as Data services is becoming the major source of
  Telco revenue. The wide use of Data platforms; such as WhatsApp, Skype, Hangout
  and other Over the Top (OTT) voice applications over the traditional voice services
  is a clear indication that Network Operators need to adjust their business model
  and needs. And couple with the adoption of Smartphones usage which grows continuously
  year by year, this means more subscribers to manage, large amount of transactions
  generated, more network resources to be added and evidently more human technical
  expertise required to ensure good service quality. That has led to high investment
  on Robust Service Quality Management (SQM) and Customer Experience Management (CEM)
  to stay competitive in the market. The high investment is justified by the integration
  of Big Data Solutions, Machine Learning capabilities and good visualization of insight
  data. However, the Return on Investment (ROI) of the expensive systems are not as
  conspicuous as the provided functionalities and business rules. Therefore, in this
  paper an efficient model for low cost SQM system is presented, exploring the advantages
  of In-Memory Big Data processing and low cost business Intelligence tools to showcase
  how a good Service Quality Management can be implemented with no big investment.
6.year:
- 2018
7.type_publication:
- inproceedings
8.doi:
- 10.1109/ICABCD.2018.8465410
1.chave:
- 7404600
2.author:
- Abeysirigunawardena, Dilumie
- Jeffries, Marlene
- Morley, Michael
- Bui, Alice
- Hoeberechts, Maia
3.title:
- Data quality control and quality assurance practices for Ocean Networks Canada observatories
4.keywords:
- Instruments
- Quality control
- Oceans
- Real-time systems
- Quality assurance
- Manuals
- Data models
- Data Quality Assurance
- Data Quality Control
- Automated data quality control
- Manual data quality control
- Cabled Ocean observatory
- data assessment annotations
- workflow
5.abstract:
- Cabled observatory installations permit the acquisition of large volumes of continuous,
  high-resolution data from in-situ instruments. This type of data acquisition presents
  new challenges and opportunities in the development of data quality assurance and
  quality control (QAQC) measures. Ocean Networks Canada (ONC) operates the world-leading
  NEPTUNE and VENUS cabled ocean observing systems in the NE Pacific, and a small
  seafloor observatory in the Canadian Arctic. ONC collects high-resolution, real-time
  data on physical, chemical, biological, and geological aspects of the ocean over
  long time periods, supporting research on complex earth and ocean processes with
  innovative methods. High quality research depends on high quality data, which in
  turn depends on robust data quality control practices. For the data to be useful
  to potential end users, they must be qualified under accepted international standards
  with additional metadata pertaining to methods of measurement, instrument calibrations,
  and subsequent data processing included. Ocean Network Canada's QAQC methodology
  presented here has been developed with the key objectives of maintaining consistency
  within a single data set and within a collection of data sets. The QAQC model also
  ensures that the end user has sufficient information on the quality and errors of
  the data to assess its suitability for their use. The data QAQC procedures and tools
  have the capability to associate distinctly different but related types of information
  with data to provide a systematic and timely examination of the measurements. Efforts
  have been taken to develop efficient and accurate data QAQC techniques and tools
  to ensure quality data delivery to the end users in a timely manner. The large volume
  of data coming from extremely complex, diverse, and unpredictable ocean environments
  has resulted in many challenges as well as opportunities to develop efficient and
  informative tools for data QAQC at ONC. This paper describes the current and future
  steps that ONC is undertaking to ensure that data delivered by the observatories
  are of high quality, easily accessible, and reliable.
6.year:
- 2015
7.type_publication:
- inproceedings
8.doi:
- 10.23919/OCEANS.2015.7404600
1.chave:
- 9821441
2.author:
- Kenett, Ron
- Shmueli, Galit
3.title:
- Quality of goal, data quality, and analysis quality
4.keywords:
- Data integrity
- Time measurement
- Data analysis
- Cleaning
- Task analysis
- Stakeholders
- Medical services
5.abstract:
- "This chapter examines quality in terms of these information quality (InfoQ) components:\
  \ quality of the analysis goal, data quality, analysis quality and quality of utility.\
  \ Although the quality of each of the individual components affects InfoQ, it is\
  \ the combination of the four that determines the level of InfoQ. The chapter aims\
  \ to help the reader understand the difference between the quality of a single component\
  \ and that of InfoQ. In a sense, we build here on the seminal work of Jeffreys,\
  \ de Finetti, Ramsey, Savage Luce, Raiffa, Shlaifer, Fishburn, and many others who\
  \ developed decision theory over 75 years ago, integrating data\u2010driven inference\
  \ and modeling with decision making. Our updated approach to InfoQ is predicated\
  \ on modern technologies that provide online access to structured and unstructured\
  \ data with advanced computational and visualization capabilities. In this context,\
  \ InfoQ needs to be viewed in a wide context where data is transformed into insights\
  \ that drive focused and adapted responses. This chapter provides the foundation\
  \ for an expanded approach to InfoQ."
6.year:
- 2017
7.type_publication:
- inbook
8.doi:
- 10.1002/9781118890622.ch2
1.chave:
- 7364060
2.author:
- Nobles, Alicia
- Vilankar, Ketki
- Wu, Hao
- Barnes, Laura
3.title:
- Evaluation of data quality of multisite electronic health record data for secondary
  analysis
4.keywords:
- Documentation
- Data warehouses
- Medical services
- Big data
- Databases
- Cleaning
- Medical diagnostic imaging
- electronic health records
- data quality
- big data
- multiple data vendors
- metrics
5.abstract:
- Currently, a large amount of data is amassed in electronic health records (EHRs).
  However, EHR systems are largely information silos, that is, uses of these systems
  are often confined to management of patient information and analytics specific to
  a clinician's practice. A growing trend in healthcare is combining multiple databases
  to support epidemiological research. The College Health Surveillance Network is
  the first national data warehouse containing EHR data from 31 different student
  health centers. Each member university contributes to the data warehouse by uploading
  select EHR data including patient demographics, diagnoses, and procedures to a common
  server on a monthly basis. In this paper, we focus on the data quality dimensions
  from a subsample of the data comprised of over 5.7 million patient visits for approximately
  980,000 patients with 4,465 unique diagnoses from 23 of those universities. We examine
  the data for measures of completeness, consistency, and availability for secondary
  use for epidemiological research. Additionally, clinical documentation practices
  and EHR vendor were evaluated as potential contributors to data quality. We found
  that overall about 70% of the data in the data warehouse is available for secondary
  use, and identified clinical documentation practices that are correlated to a reduction
  in data quality. This suggests that automated quality control and proactive clinical
  documentation support could reduce ad-hoc data cleaning needs resulting in greater
  data availability for secondary use.
6.year:
- 2015
7.type_publication:
- inproceedings
8.doi:
- 10.1109/BigData.2015.7364060
1.chave:
- 8457745
2.author:
- Taleb, Ikbal
- Serhani, Mohamed
- Dssouli, Rachida
3.title:
- 'Big Data Quality: A Survey'
4.keywords:
- Big Data
- Data integrity
- Quality management
- Data visualization
- Databases
- Sensors
- Social network services
- Big Data, Data Quality, Quality Management framework, Quality of Big Data
5.abstract:
- With the advances in communication technologies and the high amount of data generated,
  collected, and stored, it becomes crucial to manage the quality of this data deluge
  in an efficient and cost-effective way. The storage, processing, privacy and analytics
  are the main keys challenging aspects of Big Data that require quality evaluation
  and monitoring. Quality has been recognized by the Big Data community as an essential
  facet of its maturity. Yet, it is a crucial practice that should be implemented
  at the earlier stages of its lifecycle and progressively applied across the other
  key processes. The earlier we incorporate quality the full benefit we can get from
  insights. In this paper, we first identify the key challenges that necessitates
  quality evaluation. We then survey, classify and discuss the most recent work on
  Big Data management. Consequently, we propose an across-the-board quality management
  framework describing the key quality evaluation practices to be conducted through
  the different Big Data stages. The framework can be used to leverage the quality
  management and to provide a roadmap for Data scientists to better understand quality
  practices and highlight the importance of managing the quality. We finally, conclude
  the paper and point to some future research directions on quality of Big Data.
6.year:
- 2018
7.type_publication:
- inproceedings
8.doi:
- 10.1109/BigDataCongress.2018.00029
1.chave:
- 8276745
2.author:
- Micic, Natasha
- Neagu, Daniel
- Campean, Felician
- Zadeh, Esmaeil
3.title:
- Towards a Data Quality Framework for Heterogeneous Data
4.keywords:
- Data integrity
- Measurement
- Warranties
- Metadata
- Cleaning
- Big Data
- Heterogeneous Data Sets
- Data Quality
- Metadata
- Data Cleaning
- Data Quality Assessment
5.abstract:
- Every industry has significant data output as a product of their working process,
  and with the recent advent of big data mining and integrated data warehousing it
  is the case for a robust methodology for assessing the quality for sustainable and
  consistent processing. In this paper a review is conducted on Data Quality (DQ)
  in multiple domains in order to propose connections between their methodologies.
  This critical review suggests that within the process of DQ assessment of heterogeneous
  data sets, not often are they treated as separate types of data in need of an alternate
  data quality assessment framework. We discuss the need for such a directed DQ framework
  and the opportunities that are foreseen in this research area and propose to address
  it through degrees of heterogeneity.
6.year:
- 2017
7.type_publication:
- inproceedings
8.doi:
- 10.1109/iThings-GreenCom-CPSCom-SmartData.2017.28
1.chave:
- 5360632
2.author:
- Su, Ying
- Peng, Gongqian
- Jin, Zhanming
3.title:
- Assuring the Information Quality in Data Mining for a Finance Company
4.keywords:
- Data mining
- Finance
- Quality assurance
- Information analysis
- Resource management
- Quality management
- Project management
- Quality assessment
- Frequency
- Fuzzy systems
- Auto finance marketing
- data mining
- data warehouse
- quality assessment
- information quality
5.abstract:
- This paper describes a information quality assurance exercise undertaken for a finance
  company as part of a larger project in auto finance marketing. A methodology to
  estimate the effects of data accuracy, completeness and consistency on the data
  aggregate functions count, sum and average is presented. This methodology should
  be of specific interest to quality assurance practitioners for projects that harvest
  warehouse data for decision support to the management. The assessment comprised
  ten checks in three broad categories, to ensure the quality of information collected
  over 1103 attributes. The assessment discovered four critical gaps in the data that
  had to be corrected before the data could be transitioned to the analysis phase.
6.year:
- 2009
7.type_publication:
- inproceedings
8.doi:
- 10.1109/FSKD.2009.842
1.chave:
- 7077304
2.author:
- Panahy, Payam
- Sidi, Fatimah
- Affendey, Lilly
- Jabar, Marzanah
3.title:
- The impact of data quality dimensions on business process improvement
4.keywords:
- Information systems
- Organizations
- Accuracy
- Quality assessment
- Correlation
- Data mining
- Data Quality
- Data quality Dimension
- Business Process Improvement
- Information System
- Data Quality Problem
5.abstract:
- Guaranteeing high data quality level is an important issue to increase the efficiency
  of the business processes. In fact, poor data quality produces wrong information,
  which leads to the failure of the business process improvement. Identifying data
  quality problems has positive impact on overall effectiveness and efficiency of
  the process improvement. In fact, improving data quality often requires modifying
  business process enriching them with the most improvement activities. Such activities
  depend and change based on the data quality dimensions. In this paper, we focus
  on review of the impact of data quality dimensions on business process improvement
  in order to support managers to facilitate the implementation of process improvement.
  The evaluations of this research will use to refine and extend knowledge of relationship
  between data quality dimensions and business process improvement.
6.year:
- 2014
7.type_publication:
- inproceedings
8.doi:
- 10.1109/WICT.2014.7077304
1.chave:
- 5593829
2.author:
- Xiaoping, Zhao
- Zhenwang, Dong
- Chao, Luo
3.title:
- Integrative system structure of quality data acquisition, monitoring and management
  oriented to cotton-spinning enterprise
4.keywords:
- Cotton
- Monitoring
- Data acquisition
- Production
- Raw materials
- Quality management
- Spinning
- Cotton Spinning Corporation
- Quality Data Acquisition
- Monitoring and Management
- Integration
5.abstract:
- Traditionally, for a cotton spinning corporation, its quality data acquisition,
  analysis, and monitoring were separated. In order to improve the quality management
  of the enterprise, this paper completes and depicts the framework construction of
  integrative system of quality data acquisition, analysis, and monitoring oriented
  to the cotton spinning corporations. It also elaborates the technology of quality
  data acquisition, analysis, and monitoring in the process of production management
  of this enterprise. The executive results shows that this system has enhanced the
  quality awareness, significantly increased the products quality, and reinforced
  the core competitiveness of the enterprise.
6.year:
- 2010
7.type_publication:
- inproceedings
8.doi:
- 10.1109/ICCSE.2010.5593829
1.chave:
- 9026017
2.author:
- Wenxue, Chi
- Yong, Zhang
- Xiaochun, Zhang
- Junshan, Jing
- Peng, Yan
3.title:
- Analysis of Quality Control Effect of Reactive Gas Observation Data Based on Multiple
  Data Quality Control Methods
4.keywords:
- Quality control
- Inspection
- Process control
- Standards
- Instruments
- Atmosphere
- Data integrity
- Quality Control
- Reactive Gas
- ' Control Method'
- Quality Control, Reactive Gas
- Control Method
5.abstract:
- 'The quality control of observation data is an important link in the entire meteorological
  observation service system chain. The reactive gas observation is different from
  the observation of other meteorological elements in general, especially the special
  operations such as periodic zero / span inspection and multi-point calibration involved
  in instrument observation, and the quality control corresponding to its data is
  also special character. The main purpose of this article is to find the most effective
  quality control method for quality control of reactive gas observation data. Six
  types of quality control methods are used to perform quality control on reactive
  gas observation data, including extreme value check, triple standard deviation method,
  zero-span identification method, internal consistency check, time change check,
  and comprehensive identification check. By comprehensively identifying the data
  quality control result identification code given by each method, the quality control
  status of the data is finally determined. The quality control data used are the
  observation data of Beijing Shangdianzi atmospheric background station in 2018.
  Conclusion: A comparison of longsequence, multi-site analysis of the data after
  quality control using 6 methods and the data processed by a single method can draw
  conclusions. The multi-method comprehensive quality control results are superior
  to a single traditional meteorological observation data quality control method,
  which will cause fewer false judgments on the data and ensure the best quality control
  results of the observation data.'
6.year:
- 2019
7.type_publication:
- inproceedings
8.doi:
- 10.1109/ICMO49322.2019.9026017
1.chave:
- 9642876
2.author:
- Efimova, Olga
- Igolnikov, Boris
- Isakov, Michail
- Dmitrieva, Elizaveta
3.title:
- Data Quality and Standardization for Effective Use of Digital Platforms
4.keywords:
- Measurement
- Data integrity
- Biological system modeling
- Ecosystems
- Standards organizations
- Process control
- Companies
- data quality
- digital platforms
- standards
- metrics
5.abstract:
- Functioning of a company is largely based on digital technologies that form digital
  platforms - a set of products that implements innovative business models based on
  modern technologies, new ways of interaction with clients, contractors and partners.
  The purpose of the paper is to form metrics of data quality when using digital platforms.
  Studies of data quality metrics in various types of business and digital ecosystems
  made it possible to form a data quality scale capable of assessing the effectiveness
  of the role model, competency profiles of participants in digital ecosystems, and
  the sufficiency of resources. It is also necessary to evaluate the simplicity, efficiency
  and discipline of data quality control and management processes in order to eliminate
  the loss of digital trust of users. The paper focuses on the organizational model
  of data quality management based on the quality metrics system.
6.year:
- 2021
7.type_publication:
- inproceedings
8.doi:
- 10.1109/ITQMIS53292.2021.9642876
1.chave:
- 9260067
2.author:
- Wong, Ka
- Wong, Raymond
3.title:
- 'Big Data Quality Prediction on Banking Applications: Extended Abstract'
4.keywords:
- Big Data
- Data models
- Banking
- Neural networks
- Industries
- Data integrity
- Computational modeling
- Big Data
- Data Noise
- Data Quality
- Prediction and Machine Learning
5.abstract:
- Big data has been transformed into knowledge by information systems to add value
  in businesses. Enterprises relying on it benefit from risk management to a certain
  extent. The value, however, depends on the quality of data. The quality needs to
  be verified before any use of the data. Specifically, measuring the quality by simulating
  the real life situation and even forecast it accurately turns into a hot topic.
  In recent years, there have been numerous researches on the measurement and assessment
  of data quality. These are yet to utilize a scientific computational method for
  the measurement and prediction. Current methods either fail to make an accurate
  prediction or do not consider the correlation and time sequence factors of the data.
  To address this, we design a model to extend machine learning technique to business
  applications predicting this. Firstly, we implement the model to detect data noises
  from a risk dataset according to an international data quality standard from banking
  industry and then estimate their impacts with Gaussian and Bayesian methods. Secondly,
  we direct sequential learning in multiple deep neural networks for the prediction
  with an attention mechanism. The model is experimented with various network methodologies
  to show the predictive power of machine learning technique and is evaluated by validation
  data to confirm the model effectiveness. The model is scalable to apply to any industries
  utilizing big data other than the banking industry.
6.year:
- 2020
7.type_publication:
- inproceedings
8.doi:
- 10.1109/DSAA49011.2020.00119
1.chave:
- 7840906
2.author:
- Angryk, Rafal
- Galarus, Douglas
3.title:
- The SMART approach to comprehensive quality assessment of site-based spatial-temporal
  data
4.keywords:
- Sensors
- Quality control
- Metadata
- Meteorology
- Interpolation
- Quality assessment
- Time series analysis
- data quality
- data stream processing
- spatial-temporal data
- quality control
- interpolation
5.abstract:
- There is a need for comprehensive solutions to address the challenges of spatio-temporal
  data quality assessment. Emphasis is often placed on the quality assessment of individual
  observations from sensors but not on the sensors themselves nor upon site metadata
  such as location and timestamps. The focus of this paper is on the development and
  evaluation of a representative and comprehensive, interpolation-based methodology
  for assessment of spatio-temporal data quality. We call our method the SMART method,
  short for Simple Mappings for the Approximation and Regression of Time series. When
  applied to a real-world, meteorological data set, we show that our method outperforms
  standard interpolators and we identify numerous problematic sites that otherwise
  would not have been flagged as bad. We further identify sites for which metadata
  is incorrect. We believe that there are many problems with real data sets like these
  and, in the absence of an approach like ours, these problems have largely gone unidentified.
  Our results bring into question the validity of provider-based quality control indicators.
  In addition to providing a comprehensive solution, our approach is novel for the
  simple but effective way that it accounts for spatial and temporal variation.
6.year:
- 2016
7.type_publication:
- inproceedings
8.doi:
- 10.1109/BigData.2016.7840906
1.chave:
- 7364065
2.author:
- Rao, Dhana
- Gudivada, Venkat
- Raghavan, Vijay
3.title:
- Data quality issues in big data
4.keywords:
- Databases
- Big data
- Measurement
- Context
- Cleaning
- Biology
- Computers
- Data quality
- big data
- biological data
- information quality
5.abstract:
- Though the issues of data quality trace back their origin to the early days of computing,
  the recent emergence of Big Data has added more dimensions. Furthermore, given the
  range of Big Data applications, potential consequences of bad data quality can be
  for more disastrous and widespread. This paper provides a perspective on data quality
  issues in the Big Data context. it also discusses data integration issues that arise
  in biological databases and attendant data quality issues.
6.year:
- 2015
7.type_publication:
- inproceedings
8.doi:
- 10.1109/BigData.2015.7364065
1.chave:
- 8085513
2.author:
- Li, Yuqian
- Li, Peng
- Zhu, Feng
- Wang, Ruchuan
3.title:
- Design of higher education quality monitoring and evaluation platform based on big
  data
4.keywords:
- Education
- Monitoring
- Big Data
- Data mining
- Servers
- Memory
- Indexes
- big data
- monitoring and evaluation
- system design
5.abstract:
- Through the continuous collection and in-depth analysis of the quality monitoring
  data of colleges and universities, we combine the efficiency processing of big data
  and data evaluation, monitor the status of higher education normally, and construct
  a higher education quality monitoring and evaluation platform based on Spark. This
  platform is teaching centered with schools as its basis, including subsystems of
  data acquisition, data analysis, machine learning, data storage, data analysis and
  other areas. Through the application of the higher education quality monitoring
  platform, we can understand the current situation of the development of higher education
  scientifically, and provide the basis for the macro-decision of education administration
  department.
6.year:
- 2017
7.type_publication:
- inproceedings
8.doi:
- 10.1109/ICCSE.2017.8085513
1.chave:
- 8862267
2.author:
- Juneja, Ashish
- Das, Nripendra
3.title:
- 'Big Data Quality Framework: Pre-Processing Data in Weather Monitoring Application'
4.keywords:
- Big Data
- Data integrity
- Meteorology
- Monitoring
- Data mining
- Organizations
- Big Data
- Big Data Quality
- Data Quality
- preprocessing
- pre-processing
5.abstract:
- Big Data has become an imminent part of all industries and business sectors today.
  All organizations in any sector like energy, banking, retail, hardware, networking,
  etc all generate huge quantum of heterogenous data which if mined, processed and
  analyzed accurately can reveal immensely useful patterns for business heads to apply
  to generate and grow their businesses. Big Data helps in acquiring, processing and
  analyzing large amounts of heterogeneous data to derive valuable results. Quality
  of information is affected by size, speed and format in which data is generated.
  Hence, Quality of Big Data is of great relevance and importance. We propose addressing
  various aspects of the raw data to improve its quality in the pre-processing stage,
  as the raw data may not usable as-is. We are exploring process like Cleansing to
  fix as much data as feasible, Noise filters to remove bad data, as well sub-processes
  for Integration and Filtering along with Data Transformation/Normalization. We evaluate
  and profile the Big Data during acquisition stage, which is adapted to expectations
  to avoid cost overheads later while also improving and leading to accurate data
  analysis. Hence, it is imperative to improve Data quality even it is absorbed and
  utilized in an industry's Big Data system. In this paper, we propose a Pre-Processing
  Framework to address quality of data in a weather monitoring and forecasting application
  that also takes into account global warming parameters and raises alerts/notifications
  to warn users and scientists in advance.
6.year:
- 2019
7.type_publication:
- inproceedings
8.doi:
- 10.1109/COMITCon.2019.8862267
1.chave:
- 8825534
2.author:
- Hu, Chuanmin
- Barnes, Brian
- Feng, Lian
- Wang, Menghua
- Jiang, Lide
3.title:
- 'On the Interplay Between Ocean Color Data Quality and Data Quantity: Impacts of
  Quality Control Flags'
4.keywords:
- Oceans
- Image color analysis
- Data integrity
- Quality control
- Sun
- Sea measurements
- NASA
- Calibration
- data quality
- data quantity
- level-2 flags stray light
- ocean color
- quality control
- remote sensing
- sun glint
- uncertainty
- validation
5.abstract:
- "Nearly all calibration/validation activities for the satellite ocean color missions\
  \ have focused on data quality to produce data products of the highest quality (i.e.,\
  \ science quality) for climate-related research. Little attention, however, has\
  \ been paid to data quantity, particularly on how data quality control during data\
  \ processing impacts downstream data quality and data quantity. In this letter,\
  \ we attempt to fill this knowledge gap using measurements from the Visible Infrared\
  \ Imaging Radiometer Suite (VIIRS) onboard the Suomi National Polar-orbiting Partnership\
  \ (SNPP). For this sensor, the same level-1B data are processed independently using\
  \ different quality control methods by NASA and NOAA, respectively, allowing for\
  \ an in-depth evaluation of the interplay between data quantity and quality. The\
  \ results indicate that the methods to identify stray light and sun glint are the\
  \ two primary quality control procedures affecting data quantity, where the criteria\
  \ for flagging pixels \u201Ccontaminated\u201D by stray light and sun glint may\
  \ be relaxed in the NASA ocean color data processing to increase data quantity without\
  \ compromising data quality."
6.year:
- 2020
7.type_publication:
- article
8.doi:
- 10.1109/LGRS.2019.2936220
1.chave:
- 8465129
2.author:
- Juddoo, Suraj
- George, Carlisle
3.title:
- Discovering Most Important Data Quality Dimensions Using Latent Semantic Analysis
4.keywords:
- Data integrity
- Big Data
- Semantics
- Industries
- Bibliographies
- Libraries
- Production
- Big Data
- data quality
- health data
- data quality dimensions
- latent semantic analysis
5.abstract:
- Big Data quality is a field which is emerging. Many authors nowadays agree that
  data quality is still very relevant, even for Big Data uses. However, there is a
  lack of frameworks or guidelines about how to carry out those big data quality initiatives.
  The starting point of any data quality work is to determine the properties of data
  quality, termed as data quality dimensions (DQDs). Even those dimensions lack precise
  rigour in terms of definition from existing literature. This current research aims
  to contribute towards identifying the most important DQDs for big data in the health
  industry. It is a continuation of a previous work, which already identified five
  most important DQDs, using a human judgement based technique known as inner hermeneutic
  cycle. To remove potential bias coming from the human judgement aspect, this research
  uses the same set of literature but applies a statistical technique known to extract
  knowledge from a set of documents known as latent semantic analysis. The results
  confirm only 2 similar most important DQDs, namely accuracy and completeness.
6.year:
- 2018
7.type_publication:
- inproceedings
8.doi:
- 10.1109/ICABCD.2018.8465129
1.chave:
- 9006294
2.author:
- Arruda, Darlan
- Madhavji, Nazim
3.title:
- 'QualiBD: A Tool for Modelling Quality Requirements for Big Data Applications'
4.keywords:
- Tools
- Data models
- Containers
- Software
- Real-time systems
- Big Data applications
- Big Data Applications
- Quality Requirements
- Big Data Goal-oriented Requirements Language
- Requirements Modelling Tool
5.abstract:
- The development of Big Data applications is not well-explored, to our knowledge.
  Embracing Big Data in system building, questions arise as to how to elicit, specify,
  analyse, model, and document Big Data quality requirements. In our ongoing research,
  we explore a requirements modelling language for Big Data software applications.
  In this paper, we introduce QualiBD, a modelling tool that implements the proposed
  goal-oriented requirements language that facilitates the modelling of Big Data quality
  requirements.
6.year:
- 2019
7.type_publication:
- inproceedings
8.doi:
- 10.1109/BigData47090.2019.9006294
1.chave:
- 8509662
2.author:
- none
3.title:
- IEEE Draft Recommended Practice for Power Quality Data Interchange Format (PQDIF)
4.keywords:
- IEEE Standards
- Power quality
- Protocols
- Data integration
- data interchange
- file format
- IEEE 1159.3
- measurement
- monitoring
- power quality
- PQDIF
5.abstract:
- A file format suitable for exchanging power quality related measurement and simulation
  data in a vendor independent manner is defined in this recommended practice. The
  format is designed to represent all power quality phenomena identified in IEEE Std
  1159-2009, IEEE Recommended Practice on Monitoring Electric Power Quality, other
  power related measurement data, and is extensible to other data types as well. The
  recommended file format utilizes a highly compressed storage scheme to help reduce
  disk space and transmission times. The utilization of Globally Unique Identifiers
  (GUID) to represent each element in the file permits the format to be extensible
  without the need for a central registration authority.
6.year:
- 2018
7.type_publication:
- article
8.doi:
- ''
1.chave:
- 7799648
2.author:
- Xia, Wenze
- Xu, Zhuoming
- Wei, Jie
- Tian, Haimei
3.title:
- 'DQFIRD: Towards Data Quality-Based Filtering and Ranking of Datasets for Data Portals'
4.keywords:
- Visualization
- User interfaces
- Data models
- Resource description framework
- W3C
- Prototypes
- Context
- data quality-based filtering and ranking
- datasets
- faceted search
- Data Quality Vocabulary (DQV)
- quality metadata
- data portal
5.abstract:
- The Data on the Web Best Practices Working Group, as part of W3C Data Activity,
  is standardizing the Data Quality Vocabulary (DQV) for expressing data quality of
  datasets published on the Web. By exploiting such DQV-based quality metadata associated
  to the datasets in a data portal, data consumers can achieve data quality-based
  filtering and ranking of datasets on the portal's conventional search results to
  obtain desired datasets with high data-quality. Despite the significant progress
  in standardization, there is a lack of systematic research on approaches and tools
  for data quality-based filtering and ranking of Web published datasets. This paper
  therefore proposes a generic software framework for Data Quality-based Filtering
  and Ranking of Datasets (DQFIRD) in data portals. DQFIRD adopts faceted search (or
  faceted exploration) techniques to filter the search results of a data portal based
  on quality metadata about the resulting datasets, and then ranks the filtered datasets
  according to numeric values of quality measurements in the metadata. We designed
  the main algorithms of DQFIRD and implemented a prototype of DQFIRD using Java and
  Jena API. Furthermore, we used the prototype to conduct case study experiments and
  time efficiency test on the Faceted Taxonomy Materialization (FTM) algorithm, the
  most time-consuming online operation algorithm in DQFIRD. The results indicate that
  the proposed DQFIRD approach is implementable and effective, and it has low time
  complexity because the run-time of the FTM algorithm exhibits approximately a linear
  growth rate as the size of the relevant dataset quality metadata increases.
6.year:
- 2016
7.type_publication:
- inproceedings
8.doi:
- 10.1109/WISA.2016.14
1.chave:
- 9148148
2.author:
- Bai, Zhongxian
- Zhuo, Rongqing
3.title:
- Quality Management of Crowd Sensing Data Based on Machine Learning
4.keywords:
- Time series analysis
- Temperature sensors
- Mobile handsets
- Temperature distribution
- Databases
- Data models
- Machine Learning
- Crowd Sensing
- Big Data Analysis
- Abnormal Data Detection Management
- Clustering Method
5.abstract:
- Recently, research on crowd sensing data quality management is a new subject area
  developed based on wireless sensor network related concepts. Crowd sensing data
  has also experienced many links in the process of network propagation, so it is
  inevitable that there are abnormal data in the database. Therefore, how to filter
  these unreliable data to get more real data is particularly important. This paper
  takes somatosensory temperature as an example, solves the problem of calculating
  the similarity of unequal long-term sequences by using DTW technology, and then
  clusters and compares the data in the database to find out the abnormal data. Thereby,
  the accuracy of the somatosensory temperature database is improved, and relevant
  users can obtain more accurate information. The experimental results show that when
  the minimum DTW value exceeds the threshold t = 0.7, the more the number of simulation
  sequences, the more stable the accuracy rate, and the faster the growth rate of
  the running time.
6.year:
- 2020
7.type_publication:
- inproceedings
8.doi:
- 10.1109/CIBDA50819.2020.00049
1.chave:
- 9006187
2.author:
- Shrivastava, Shrey
- Patel, Dhaval
- Bhamidipaty, Anuradha
- Gifford, Wesley
- Siegel, Stuart
- Ganapavarapu, Venkata
- Kalagnanam, Jayant
3.title:
- 'DQA: Scalable, Automated and Interactive Data Quality Advisor'
4.keywords:
- Data integrity
- Machine learning
- Pipelines
- Cleaning
- Libraries
- Buildings
- Data quality
- machine learning
- data cleaning
- scalability
- automation
- data science
5.abstract:
- Fueled with growth in the fields of Internet of Things (IoT) and Big Data, data
  has become one of the most valuable assets in today's world. While we are leveraging
  this data for analyzing complex systems using machine learning and deep learning,
  a considerable amount of time and effort is spent on addressing data quality issues.
  If undetected, data quality issues can cause large deviations in the analysis, misleading
  data scientists. To ease the effort of identifying and addressing data quality challenges,
  we introduce DQA, a scalable, automated and interactive data quality advisor. In
  this paper, we describe the DQA framework, provide detailed description of its components
  and the benefits of integrating it in a data science process. We propose a programmatic
  approach for implementing the data quality framework which automatically generates
  dynamic executable graphs for performing data validations fine-tuned for a given
  dataset. We discuss the use of DQA to build a library of validation checks common
  to many applications. We provide insight into how DQA addresses many persistence
  and usability issues which currently make data cleaning a laborious task for data
  scientists. Finally, we provide a case study of how DQA is implemented in a realworld
  system and describe the benefits realized.
6.year:
- 2019
7.type_publication:
- inproceedings
8.doi:
- 10.1109/BigData47090.2019.9006187
1.chave:
- 7100100
2.author:
- Estler, Manfred
- Hilpert, Ralf
- Kiupel, Niels
- Soravia, Sergio
3.title:
- Using data mining methods for improvement of product and process quality
4.keywords:
- Data analysis
- Databases
- Statistical analysis
- Data mining
- Quality assessment
- Product design
- Safety
- Exploratory data analysis
- data mining
- statistical methods
- independence analysis
- data preprocessing
5.abstract:
- Recently, data analysis methods have been successfully employed in various areas
  of industry. With their help analysts gain valuable information and knowledge about
  business and technical processes. In addition to established and proven statistical
  methods, new data analysis techniques hiding behind catchwords such as Data Mining,
  Knowledge Discovery in Databases, and Computational Intelligence are increasingly
  used. These new approaches are primarily characterized by their ability to find
  conspicuous patterns in databases almost on their own. In a particular way, they
  support the general goal to detect existing and possibly still hidden information
  in databases. In chemical industry the information obtained is mainly meant to support
  the design of products and production processes, as well as the development of intelligent
  process management strategies.
6.year:
- 1999
7.type_publication:
- inproceedings
8.doi:
- 10.23919/ECC.1999.7100100
1.chave:
- 9073586
2.author:
- Khaleel, Majida
- Hamad, Murtadha
3.title:
- Data Quality Management for Big Data Applications
4.keywords:
- Distributed databases
- Metadata
- Big Data
- Standards
- File systems
- Data integrity
- Big Data
- data quality
- unstructured Data Distributed data file system
- and statistical model.
5.abstract:
- 'Currently, as a result of the continuous increase of data, one of the key issues
  is the development of systems and applications to deal with storage, management
  and processing of big numbers of data. These data are found in unstructured ways.
  Data management with traditional approaches is inappropriate because of the large
  and complex data sizes. Hadoop is a suitable solution for the continuous increase
  in data sizes. The important characteristics of the Hadoop are distributed processing,
  high storage space, and easy administration. Hadoop is better known for distributed
  file systems. In this paper, we have proposed techniques and algorithms that deal
  with big data including data collecting, data preprocessing, algorithms for data
  cleaning, A Technique for Converting Unstructured Data to Structured Data using
  metadata, distributed data file system (fragmentation algorithm) and Quality assurance
  algorithms by using the model is the statistical model to evaluate the highest educational
  institutions. We concluded that Metadata accelerates query response required and
  facilitates query execution, metadata will be content for reports, fields and descriptions.
  Total time access for three complex queries in distributed processing it is 00:
  03: 00 per second while in nondistributed processing it is at 00: 15: 77 per second,
  average is approximately five minutes per second. Quality assurance note values
  (T-test) is 0.239 and values (T-dis) is 1.96, as a result of dealing with scientific
  sets and humanities sets. In the comparison law, it can be deduced that if the t-test
  is smaller than the t-dis; so there is no difference between the mean of the scientific
  and humanities samples, the values of C.V for both scientific is (8.585) and humanities
  sets is (7.427), using the law of homogeneity know whether any sets are more homogeneous
  whenever the value of a small C.V was more homogeneous however the humanity set
  is more homogeneity.'
6.year:
- 2019
7.type_publication:
- inproceedings
8.doi:
- 10.1109/DeSE.2019.00072
1.chave:
- 8258380
2.author:
- Fu, Qian
- Easton, John
3.title:
- 'Understanding data quality: Ensuring data quality by design in the rail industry'
4.keywords:
- Industries
- Rails
- Data models
- Rail transportation
- Systematics
- Decision making
- data quality
- rail
- quality by design
- data quality schema
5.abstract:
- The railways worldwide are increasingly looking to the integration of their data
  resources coupled with advanced analytics to enhance traffic management, to provide
  new insights on the health of infrastructure assets, to provide soft linkages to
  other transport modes, and ultimately to enable them to better serve their customers.
  As in many industrial sectors, over the past decade the rail industry has been investing
  heavily in sensing technologies that record every aspect of the operation of the
  railway network. However, as any data scientist knows, it does not matter how good
  an algorithm is, if you put rubbish in, you get rubbish out; and as the traditional
  industry model of working with data only within the system that it was collected
  by becomes increasingly fragile, the industry is discovering that it knows less
  than it thought about the data it is gathering. When coupled with legacy data resources
  of unknown accuracy, such as design diagrams for assets that in many cases are decades
  old, the rail industry now faces a crisis in which its data may become essentially
  worthless due to a poor understanding of the quality of its data. This paper reports
  the findings of the first phase of a three-phase systematic review of literature
  about how data quality can be managed and evaluated in the rail domain. It begins
  by discussing why data quality matters in a rail context, before going on to define
  the quality, introduce and expand the concept of a data quality schema.
6.year:
- 2017
7.type_publication:
- inproceedings
8.doi:
- 10.1109/BigData.2017.8258380
1.chave:
- 9754682
2.author:
- Peng, Sha
- Tian, Zhuxiao
- Siqin, Zhuoya
- Xu, Xiaomin
3.title:
- Construction of Data Quality Evaluation Index for Manufacturing Multi-value Chain
  Collaborative Data Space Based on the Whole Life Cycle of Data
4.keywords:
- Manufacturing industries
- Data integrity
- Computational modeling
- Collaboration
- Organizations
- Aerospace electronics
- Data models
- manufacturing industry
- whole life cycle
- data space
- data quality
5.abstract:
- As a new data management model, data space can effectively manage a large amount
  of multi-heterogeneous dynamic data, but the construction of data space often needs
  to be based on accurate and scientific original data and to obtain valuable information
  in data, which poses a challenge to the data quality control of the whole life cycle
  of data, so it is especially important to evaluate the data quality. By analyzing
  the synergistic effect of multi-value chain in manufacturing industry and combining
  the dynamic system of the whole life cycle of data, the data quality evaluation
  index system is proposed from three aspects of data provider, data space construction
  and data user, combining four levels of data itself, technology, data flow layer
  and data management. Through the construction of AHP-TOPSIS data quality evaluation
  model, AHP is used to determine the index weight, TOPSIS is used to calculate the
  ideal solution and relative closeness degree, and the evaluation results are obtained.
  Through the application analysis of examples, quantitative evaluation of data quality,
  the construction, access and mining of multi-value chain collaborative data space
  can provide practical experience.
6.year:
- 2021
7.type_publication:
- inproceedings
8.doi:
- 10.1109/CCIS53392.2021.9754682
1.chave:
- 7823519
2.author:
- Li, Tao
- He, Yihai
- Zhu, Chunling
3.title:
- Big Data Oriented Macro-Quality Index Based on Customer Satisfaction Index and PLS-SEM
  for Manufacturing Industry
4.keywords:
- Indexes
- Mathematical model
- Data models
- Manufacturing industries
- Computational modeling
- Big data
- Customer satisfaction
- macro-quality evaluation
- big data
- macro-quality index
- customer satisfaction index
- PLS-SEM
5.abstract:
- The aim of this paper was to develop a novel macro-quality index driven by big quality
  data regarding the macro-quality management demands of manufacturing enterprises
  in Industry 4.0. Firstly, the connotation of big data of macro-quality management
  in manufacturing industry is expounded, which is the collection of the quality data
  in product lifecycle including the product quality data, process quality data and
  organizational ability data. Secondly, referring to the customer satisfaction index
  theory, a new big data oriented macro-quality index computation model based on the
  partial least square-structural equation modeling(PLS-SEM) theory is proposed, and
  the partial least square(PLS) is adopted to estimate the path parameters. Finally,
  a case study of macro-quality situation evaluation for the manufacturing industry
  of a city in China is presented. The final result shows that the proposed macro-quality
  index model is applicable and predictable.
6.year:
- 2016
7.type_publication:
- inproceedings
8.doi:
- 10.1109/ICIICII.2016.0052
1.chave:
- 5228165
2.author:
- Chen, Bing
- Weng, Xuchu
- Wang, Beizhan
- Hu, Xueqin
3.title:
- Analysis and solution of data quality in data warehouse of Chinese materia medica
4.keywords:
- Data warehouses
- Data mining
- Computer science
- Computer science education
- Data analysis
- Medical services
- Laboratories
- Data conversion
- Artificial intelligence
- Software quality
- data warehouse
- data quality
- metadata
5.abstract:
- "The data quality problem often be ignored in the process of data warehouse construction\
  \ and utilization. In order to avoid the phenomenon of \u201Cgarbage in, garbage\
  \ out\u201D which will influence the decision-making, the problem of data quality\
  \ must be paid great attention. In this paper, we took the Chinese materia medica\
  \ (Cmm) data warehouse in China Academy of Chinese Medical Sciences (CACMS) as example.\
  \ We analyzed the data quality problems in the process of its construction, cited\
  \ the reasons for bad data quality, and gave the key elements and their relations\
  \ for data quality analysis and assessment. At the end of article, we proposed a\
  \ series of assessment standards and solution scheme to improve the data quality\
  \ in Cmm data warehouse, and carried out in practice to prove it."
6.year:
- 2009
7.type_publication:
- inproceedings
8.doi:
- 10.1109/ICCSE.2009.5228165
1.chave:
- 7923744
2.author:
- Song, Yuhang
- Xu, Mai
- Li, Shengxi
3.title:
- 'Watching Videos with Certain and Constant Quality: PID-Based Quality Control Method'
4.keywords:
- Quality control
- Video coding
- Videos
- Data compression
- Encoding
- Video sequences
- video coding
- quality of experience
- PID-based quality control
5.abstract:
- 'In video coding, compressed videos with certain and constant quality can ensure
  quality of experience (QoE). To this end, we propose in this paper a novel PID-based
  quality control (PQC) method for video coding. Specifically, a formulation is modelled
  to control quality of video coding with two objectives: minimizing control error
  and quality fluctuation. Then, we apply the Laplace domain analysis to model the
  relationship between quantization parameter (QP) and control error in this formulation.
  Given the relationship between QP and control error, we propose a solution to the
  PQC formulation, such that videos can be compressed at certain and constant quality.
  Finally, experimental results show that our PQC method is effective in both control
  accuracy and quality fluctuation.'
6.year:
- 2017
7.type_publication:
- inproceedings
8.doi:
- 10.1109/DCC.2017.57
1.chave:
- 7761054
2.author:
- Thomas, Julie
3.title:
- Wave data analysis and Quality Control challenges
4.keywords:
- Quality control
- Standards
- Sea measurements
- Real-time systems
- Data analysis
- Oceanography
- Oceans
5.abstract:
- Since its inception in 1975, quality control has been at the forefront of the Coastal
  Data Information Program (CDIP). During the early days, with few pressure sensor
  stations deployed (Imperial Beach and Ocean Beach, CA), timely Quality Assurance/Quality
  Control (QA/QC) checks were done manually. However, as the program grew, it became
  evident that in order to maintain high quality standards commensurate with real
  time data dissemination, it would be necessary to develop automated quality control
  checks designed to accept qualified data and reject sets that fail to meet minimum
  standards. Both categories were archived and are available, in the public domain,
  for further manual or machine processing and inspection.
6.year:
- 2016
7.type_publication:
- inproceedings
8.doi:
- 10.1109/OCEANS.2016.7761054
1.chave:
- 9590700
2.author:
- Du, Jinming
3.title:
- Research on the Construction of Educational Data Quality Model Based on Multiple
  Constraints Model
4.keywords:
- Training
- Analytical models
- Systematics
- Databases
- Data integrity
- Biological system modeling
- Education
- Educational data
- Data quality
- Statistics
5.abstract:
- With the development of Internet and information technology, data has become an
  important asset related to the development prospects of society and all walks of
  life. At present, there are many quality problems in the use of educational data,
  which has brought great obstacles to exerting the value of educational data. Only
  by using scientific statistical methods, obtaining real, objective, comprehensive,
  scientific and effective basic data, and carrying out systematic and comprehensive
  analysis on the obtained data, can we give full play to its command and decision-making
  role. The development of big data and artificial intelligence technology provides
  new ideas for the analysis and evaluation of educational data quality, and is committed
  to restoring the overall picture of the education system and promoting the change
  of regional educational ecology. In this paper, an educational data quality analysis
  model based on multiple constraint model is proposed, which classifies the data
  in the database, divides the information in the database into several different
  categories according to the data characteristics, and establishes a quality management
  system for educational data in universities, so as to effectively improve the quality
  of educational data.
6.year:
- 2021
7.type_publication:
- inproceedings
8.doi:
- 10.1109/ICISCAE52414.2021.9590700
1.chave:
- 8372743
2.author:
- Chang, Yue
- Lin, Kuan-Ming
- Tsai, Yi-Ting
- Zeng, Yu-Ren
- Hung, Cheng-Xiang
3.title:
- Big data platform for air quality analysis and prediction
4.keywords:
- Semantics
- Big Data
- Air quality
- Data mining
- Urban areas
- Monitoring
- Government
- Air Quality
- Big Data
- Prediction
- Cloud Environment
5.abstract:
- With the advance of industry, air quality (AQ) is increasingly becoming worse. There
  are increasingly AQ monitors device have been deployed around country for monitoring
  air-quality all year long. To estimate and predict AQ, such as PM (particulate matter)
  2.5, become an important issue for government to improve people's quality of life.
  As we can know, there are many factors can affect the AQ, such as traffic, factory
  exhaust emissions, weather, incineration of garbage, and so on. In most well-developed
  countries, these pollution sources are monitored for future environmental policy
  making. In this paper, we will propose a semantic ETL (Extract-Transform-Load) framework
  on cloud platform for AQ prediction. In the platform, we exploit ontology to concretize
  the relationship of PM 2.5 from various data sources and to merge those data with
  the same concept but different naming into the unified database. We implement the
  ETL framework on the cloud platform, which includes computing nodes and storage
  nodes. The computing nodes are used to execute data mining algorithms for predicting,
  and storage modes are used to store retrieved, preprocessed, and analyzed data.
  We utilize restful web service as the front end API to retrieve analyzed data, and
  finally we exploit browser to show the visualized result to demonstrate the estimation
  and prediction. It shows that the big data access framework on the cloud platform
  can work well for air quality analysis.
6.year:
- 2018
7.type_publication:
- inproceedings
8.doi:
- 10.1109/WOCC.2018.8372743
1.chave:
- 8421858
2.author:
- Jiang, Wei
- Ning, Xiuli
- Xu, Yingcheng
3.title:
- Review on Big Data Fusion Methods of Quality Inspection for Consumer Goods
4.keywords:
- Big Data
- Inspection
- Data integration
- Semantics
- Hidden Markov models
- Thesauri
- Support vector machines
- Big data of quality inspection
- data fusion
- named entity recognition
- entity resolution
- data conflict resolution
5.abstract:
- Quality big data comes from a wide range, the problem is how to eliminate the structure
  barriers between various types of data from different sources, to achieve the effective
  integration for isolated or fragmented data, and then to mine the information, knowledge
  and wisdom according to the actual needs. It is urgent for the quality inspection
  departments to solve the problem of making a decision the first time and take preventive
  measures. This paper introduced the research status of named entity recognition,
  solid unified detection, data conflict resolution, data fusion method from the characteristics
  of quality inspection data. The existence problems are analyzed and the research
  direction is looking forward of the research on data fusion in this paper.
6.year:
- 2018
7.type_publication:
- inproceedings
8.doi:
- 10.1109/CSCloud/EdgeCom.2018.00025
1.chave:
- 9368701
2.author:
- Wang, Xin
- Zhao, Xinbin
- Yu, Liling
3.title:
- Data Mining on the Flight Quality of an Airline based on QAR Big Data
4.keywords:
- Quality assurance
- Atmospheric modeling
- Big Data
- Data models
- Time measurement
- Safety
- Mathematical model
- flight quality
- pitch
- QAR data
- normal distribution
- t test
5.abstract:
- At present, the airlines have made some achievements in event analysis and investigation
  by using their quick access record (QAR) data. But where each airline's flight quality
  is in the industry, and whether there is a problem in itself, the airline can't
  find. In order to help airlines discover the existing flight quality problems, this
  article uses the QAR big data of the flight operational quality assurance (FOQA)
  Station of CAAC, and compares the industry-wide QAR data with the QAR data of individual
  airlines, and founds that the take-off pitch angle of a certain aircraft of A321
  models is too small, by using mathematical statistics t test to verify, found the
  airline's the take-off pitch angle and the industry's the take-off pitch angle exist
  significant difference. The correlative speed at rotation and the speed at liftoff
  are also analyzed, and the significant difference is found. The FOQA Station of
  CAAC feeds back the problem to the airline and the authority. After the investigation
  of the airline and the authority, there are problems with the airline. And the airline
  immediately starts to rectify it.
6.year:
- 2020
7.type_publication:
- inproceedings
8.doi:
- 10.1109/ICCASIT50869.2020.9368701
1.chave:
- 8996332
2.author:
- Wang, Jinghan
- Zhang, Jinnan
- Yuan, XueGuang
- Tang, Yu
- Hao, Hongyu
- Zuo, Yong
- Tan, Zebin
- Qiao, Min
- Cao, Yang
- Ai, Lingmei
- Wan, Yihang
- Chen, Hao
3.title:
- Air quality data analysis and forecasting platform based on big data
4.keywords:
- Air quality
- Big Data
- Data mining
- Photonics
- Optical fiber communication
- Telecommunications
- Clustering algorithms
- Air quality
- Big data
- Data mining
- Data visualization
5.abstract:
- Nowadays, with the continuous development of big data technology, various industries
  use big data technology to process and mine massive data, and realize the value
  of data efficiently. In terms of air quality data processing, big data technology
  can also play a certain advantage. The platform is based on big data technology
  to design an air quality data analysis and prediction platform including data layer,
  business layer, interaction layer and visualization platform. Data is cleaned, calibrated,
  and stored in the data layer to ensure data consistency, integrity, and security.
  The air quality data is analyzed and predicted at the business layer. The interaction
  layer includes the functions of algorithm management, data query, and the data visualization
  platform provides intuitive information display. This design is a significant application
  for fully exploiting environmental data information. It has powerful data processing
  functions and scalability, which is a reliable data analysis and prediction platform.
6.year:
- 2019
7.type_publication:
- inproceedings
8.doi:
- 10.1109/CAC48633.2019.8996332
1.chave:
- 9888215
2.author:
- Han, Wei
- Wu, Shenggang
- Liu, Qiang
- Cai, Jianzhen
3.title:
- Research and Design of Construction Engineering Quality Management System Based
  on Big Data and BIM Technology
4.keywords:
- Cloud computing
- 5G mobile communication
- Project management
- Transforms
- Big Data
- Digital twins
- Internet of Things
- Big data
- BIM
- quality management
5.abstract:
- With the rapid development of information technologies such as big data, cloud computing,
  the Internet of Things, and 5G in recent years, the construction industry, as the
  traditional industry, urgently requests the transformation of its project management
  from extensive and low-efficiency mode to high-quality development mode. Under the
  background of the gradual development of newly-emerged technologies and concepts
  such as BIM technology, smart construction, and digital twin and based on the full
  investigation of the quality management requirements of construction enterprises,
  closed-loop management is focused on the whole process of construction engineering.
  From the perspective of construction engineering quality management, current theoretical
  tools such as the Internet of Things, big data, and BIM technology are combined.
  Taking Python as a basis, the mature and open-source WEB framework, database, and
  front-end and back-end technologies are used to design and construct a set of construction
  engineering quality management systems. We explore the digital potential of construction
  engineering quality management systems to provide a new way of thinking for the
  informatization, systematization, and system process-oriented development of construction
  engineering quality management.
6.year:
- 2022
7.type_publication:
- inproceedings
8.doi:
- 10.1109/ICCBE56101.2022.9888215
1.chave:
- 9524049
2.author:
- Yachen, Wang
3.title:
- Quality Assurance Scheme for Undergraduate Teaching Evaluation Based on Data Mining
4.keywords:
- Quality assurance
- Databases
- Education
- Decision making
- Data preprocessing
- Data collection
- Data models
- teaching evaluation
- Apriori
- ASP.net
- data mining
- WICA
5.abstract:
- To solve the problems existing in the process of teaching quality evaluation and
  to improve the accuracy of undergraduate teaching quality evaluation, a teaching
  quality evaluation model based on data mining algorithm is designed. Aiming at the
  factors such as the large amount of data to be mined and the quality of mining which
  is easily affected, the improved Apriori algorithm based on partition is applied
  in the model. Through the process of data collection and data preprocessing, the
  database suitable for association rule mining is established. Then we can find out
  which key factors can affect the teaching quality according to the analysis of association
  rules, so as to provide a strong basis for teaching decision-making and management.
  The results show that the data mining algorithm can describe the differences between
  the teaching quality grades of colleges, and acquire high-precision evaluation results
  of university teaching quality. Moreover, the error of teaching quality evaluation
  in colleges is far less than that of the current typical teaching quality evaluation
  methods.
6.year:
- 2020
7.type_publication:
- inproceedings
8.doi:
- 10.1109/ICRIS52159.2020.00114
1.chave:
- 8730858
2.author:
- Benabbas, Aboubakr
- Nicklas, Daniela
3.title:
- Quality-Aware Sensor Data Stream Management in a Living Lab Environment
4.keywords:
- Data integrity
- Computational modeling
- Data models
- Tools
- Pervasive computing
- Monitoring
- Quality of service
- data stream processing
- data quality
- sensors
- context
5.abstract:
- Sensor data is error-prone. Developers of pervasive applications must take the limitations
  of sensors into account when processing the data. To relieve the developers from
  the task of data cleaning and quality monitoring, we need a set of tools to model
  sensor data quality and to integrate the quality information into the stream data
  processing. In this dissertation, the goal is to provide a framework of tools to
  semi-automatically generate sensor models and stream processing queries for sensors
  with quality and context information for a quality-aware data stream processing.
6.year:
- 2019
7.type_publication:
- inproceedings
8.doi:
- 10.1109/PERCOMW.2019.8730858
1.chave:
- 5070851
2.author:
- Yan-fang, Yue
- Rui-gang, Zhang
- Guang, Yang
- Guang-le, Ge
3.title:
- Application of COMERO Data Collecting in Quality Management System
4.keywords:
- Quality management
- Error correction
- Coordinate measuring machines
- Software measurement
- Databases
- Real time systems
- Manufacturing processes
- Process control
- Instruments
- Machining
- quality management
- data collection
- COMERO
5.abstract:
- Summary form only given. Exact and real-time collection of quality data from manufacturing
  process is the most important step in quality management system. It is the foundation
  of cumulating, analyzing and effectively controlling the processpsilas quality.
  As an advanced and exact metrical instrument, 3-degree coordinate measuring machining
  (CMM) has been widely used in machinery manufacturing industry. However, it could
  only use text file to record the measuring result, which can not be integrated with
  the quality management system. This paper introduces appropriative interface software,
  which can automatically read the measurement data from 3-degree CMMpsilas RTF file
  and can automatically transfer the certain data to database and then let quality
  management system directly transfer the measurement result in order to analyze the
  correlative accessorypsilas quality. This analysis is useful to understand the characteristic
  of random error system, the trend of error, the order of error distributing and
  the reasons for error and also form the quality report and early warning information
  for quality control. Meanwhile, informationpsilas automatically transmission can
  obviously improve working efficiency and effectively avoid the artificial errors.
  Heterogeneous datapsilas conversion is the key point of system integration. This
  paper comprehensively analysis the expression mode of every dimension and error
  in the measurement text files mentioned above, and then applies certain program.
  In order to understand the meaning of relevant data, key words are needed. Using
  character string in the measurement files, which match the regular expression and
  the method of exhaustion, we could get the key words. Then the data, which quality
  management system needs, is formed by transferring data types with homonymous matching
  and written into the database. In order to achieve the informationpsilas automatically
  transition, this paper also studies the data synchronization mechanism and ensure
  the measurement data can be synchronously updated in the quality management system
  by setting the trigger rule.
6.year:
- 2009
7.type_publication:
- inproceedings
8.doi:
- 10.1109/ITNG.2009.167
1.chave:
- 4548030
2.author:
- Elphick, Sean
- Gosbell, Vic
3.title:
- The variation of power quality indices due to data analysis procedure
4.keywords:
- Power quality
- Data analysis
- Monitoring
- Power system harmonics
- Voltage fluctuations
- Power measurement
- Aggregates
- Instruments
- Time measurement
- Data processing
- Power Quality
- Power Quality Indices
5.abstract:
- Power quality data is often reported using statistical confidence levels. This will
  exclude the most extreme data for a certain length of time depending on the interval
  over which the confidence level is applied. There is considerable conjecture as
  to the effect of applying statistical measures over different time intervals, e.g.
  several days, weeks or one year. If statistical confidence levels are applied over
  long intervals, the length of time not included in the statistical confidence interval
  is long. During such intervals disturbance levels may be continuously high and not
  be accounted for in the statistical parameter. This study investigates the effect
  different methods of aggregating data to a specific reporting period will have on
  the calculated index. Several data processing methods are trialled to evaluate the
  effect of using different aggregation intervals to produce an index to characterise
  disturbance levels for the whole year.
6.year:
- 2007
7.type_publication:
- inproceedings
8.doi:
- 10.1109/AUPEC.2007.4548030
1.chave:
- 9172875
2.author:
- Zhang, Guobao
3.title:
- A data traceability method to improve data quality in a big data environment
4.keywords:
- Data Governance
- Data Credibility
- Data Traceability
5.abstract:
- In the actual project of data sharing and data governance, the problems of data
  heterogeneity and low data quality have not been solved. Data quality detection
  based on syntax rules can effectively find data quality problems, but it had not
  improved the data quality, especially in a variety of heterogeneous data source
  environments. This paper designs a data governance model based on data traceability,
  and we can get data feedback and revision through this model. The proposed method
  considers the different ownership of data and may form a closed-loop data service
  chain including effective data validation, data tracking, and data revision, data
  release. The analysis of the case shows that the method is effective to improve
  the data quality and meet the requirements of data security also.
6.year:
- 2020
7.type_publication:
- inproceedings
8.doi:
- 10.1109/DSC50466.2020.00051
1.chave:
- 7821627
2.author:
- Wickramage, Narada
3.title:
- 'Quality assurance for data science: Making data science more scientific through
  engaging scientific method'
4.keywords:
- Data science
- Data models
- Reservoirs
- Complexity theory
- Complex systems
- Testing
- Analytical models
- Quality Assurance
- Data Science
- Composition of Services
- Category Theory
- Complex Systems
- Formal Modelling
- Simulation
- Complexity Science
- Scientific Method
- Simulation-as-a-Service
- GUI based hypotheses builders and testers
- QA4DS
- Quality of Knowledge
- Crowd Data Science
5.abstract:
- Credibility of science is fundamentally due to the strenuous efforts made to verify
  the general consistency among relevant facts, theories, applications, research methodologies,
  etc. and scientific method which emphasizes the significance of continuously building
  and testing hypotheses has withstood the test of time as a successful methodology
  of acquiring a body of knowledge, we can rely on, at least within a certain context.
  A paradigm based on composition of data rich services to gather data to replicate
  real world scenarios through complexity science based simulators, where quality
  of data as well as theories explaining them is primarily assured via building and
  testing of hypotheses, can improve our understanding of what we try to comprehend
  by engaging data science. While simulators would at least partially automate the
  implementation of scientific method, a credibility ranking mechanism, would not
  only help determining and disseminating rankings pertaining to the quality of data
  as well as theories explaining them but also receive & publish feedback regarding
  rankings. Including methods used in complex system analysis as part of simulators
  would enhance the scientific rigor of establishing the credibility of knowledge
  we have. Providing simulation as a service and making graphical hypothesis builders
  & testers available for external parties (sometimes even members of general public)
  would democratice the process of ascertaining the believability of data & associated
  theoretical models thereby further enhancing the Quality of Knowledge.
6.year:
- 2016
7.type_publication:
- inproceedings
8.doi:
- 10.1109/FTC.2016.7821627
1.chave:
- 1151615
2.author:
- Perlroth, I.
- Hamilton, D.
3.title:
- Quality Control Procedures in Processing Oceanographic Data
4.keywords:
- Quality control
- Oceanographic techniques
- Data analysis
- Ocean temperature
- Sea measurements
- Pollution measurement
- Marine pollution
- Biology computing
- Temperature measurement
- Temperature dependence
5.abstract:
- The National Oceanographic Data Center (NODC) receives significant volumes of oceanographic
  environmental data for processing. These data vary from the conventional ocean station
  data and expendable bathythermograph data to a wide variety of biological and pollution
  data. The need for a more objective environmental quality control became evident
  as the volume of data continued to increase. At the same time, the resources-enriched
  computer capability and large climatological files of fully processed data-for developing
  such a system became available. The NODC procedures, prior to the development of
  environmental models, were based primarily on the initial quality control by data
  donors and subjective analysis by NODC oceanographers. This study describes the
  use of ocean models for quality controlling ocean serial and XBT data.
6.year:
- 1981
7.type_publication:
- inproceedings
8.doi:
- 10.1109/OCEANS.1981.1151615
1.chave:
- 8695373
2.author:
- Zuo, Yiming
- Vernica, Rares
- Lei, Yang
- Barcelo, Steven
- Rogacs, Anita
3.title:
- A Big Data Platform for Surface Enhanced Raman Spectroscopy Data with an Application
  on Image-Based Sensor Quality Control
4.keywords:
- Quality control
- Servers
- Cloud computing
- Big Data
- Visualization
- Inspection
- Databases
- Big data platform, Surface Enhanced Raman Spectroscopy, sensor quality control
5.abstract:
- Surface-enhanced Raman spectroscopy (SERS) significantly enhances the Raman scattering
  by molecules, enabling detection and identification of small quantities of relevant
  bio-/chemical markers in a wide range of applications. In this paper, we present
  a big data platform with both a local client and cloud server built for acquiring,
  processing, visualizing and storing SERS sensor data. The local client controls
  the hardware (i.e., spectrometer and stage) to collect SERS spectra from HP designed
  sensors, and offers the options to analyze, visualize and save the spectra with
  meta-data records, including relevant experimental conditions. The cloud server
  contains remote databases and web interface for centralized data management to users
  from different locations. Here we describe how this platform was built and demonstrate
  its use for automated sensor quality control based on sensor images. Sensor quality
  control is a common practice, employed in sensor production to select high performing
  sensors. Image-based approach is a natural way to perform sensor quality control
  without destructing the sensors. Automating this process using the proposed platform
  can also reduce the time spent and achieve consistent result by avoiding human visual
  inspection.
6.year:
- 2019
7.type_publication:
- inproceedings
8.doi:
- 10.1109/MIPR.2019.00093
1.chave:
- 9664881
2.author:
- Setiadi, Yusuf
- Hidayanto, Achmad
- Rachmawati, Fitri
- Yohannes, Adhi
3.title:
- "Data Quality Management Maturity Model : A Case Study in Higher Education\u2019\
  s Human Resource Department"
4.keywords:
- Data integrity
- Standards organizations
- Decision making
- Organizations
- Documentation
- Regulation
- Data models
- data quality
- data quality management
- data quality maturity model
- information system
5.abstract:
- "Data has increasingly become more imperative in organization\u2019s decision-making\
  \ process. Low data quality can cause extensive organizational problems, such as\
  \ inaccurate decision-making and dropped business possibilities. This is because\
  \ low-quality data does not present a clear description of the actual situation.\
  \ In Human Resource (HR) management, low data quality can cause recruitment, career\
  \ development, remuneration, and retirement processes. Therefore, proper data quality\
  \ management must be implemented to produce data that suits the organization's needs.\
  \ To determine how far the implementation of data quality management in the organization,\
  \ measurement of the maturity level in data quality management is conducted. This\
  \ study presented an evaluation of data quality management maturity level in HR\
  \ of higher education, applying the Loshin data quality management maturity framework.\
  \ The results of this study indicate that the maturity level in the Data quality\
  \ expectations area is 2.17, the maturity level in the Data quality dimensions area\
  \ is 2.16, the maturity level in the Policies area is 1.22, the maturity level in\
  \ the Data quality protocols area is 2, 11, the maturity level in the Data governance\
  \ area is 1.77, the maturity level in the Data standards area is 1.67, the maturity\
  \ level in the Technology area is 1.44, and the maturity level in the Performance\
  \ management area is 1.67. The result shows that the Policies area is the lowest\
  \ due to the lack of regulations and good documentation regarding data management.\
  \ It can be a concern in conducting evaluations for improving data quality management."
6.year:
- 2021
7.type_publication:
- inproceedings
8.doi:
- 10.1109/ICCED53389.2021.9664881
1.chave:
- 5347884
2.author:
- Zhang, Dahai
- Bi, Yanqiu
- Zhao, Jianguo
3.title:
- A new data compression algorithm for power quality online monitoring
4.keywords:
- Data compression
- Power quality
- Monitoring
- Delta modulation
- Huffman coding
- Data acquisition
- Compression algorithms
- Power measurement
- Frequency measurement
- Sampling methods
- data compression
- delta modulation
- Huffman coding
- power quality
- power system
5.abstract:
- The increasing application of power quality data acquisition devices produces large
  amount of data that should be compressed. The paper investigates the characteristics
  of power quality data and delta modulation technique, and it proposes a lossless
  power data compression algorithm based on high-order delta modulation. The compression
  algorithm carries on multiple differential operations on power quality data, so
  it could reduce the magnitude of data and requires fewer bits for coding. The proposed
  algorithm has high compression ratio and little computation requirements. Furthermore,
  it is suitable for dealing with the power quality data measured at high sampling
  frequency, and it can work well with traditional compression methods such as Huffman
  coding. Simulations for several kinds of power quality data confirm its effectiveness
  and advantages over traditional Huffman coding method.
6.year:
- 2009
7.type_publication:
- inproceedings
8.doi:
- 10.1109/SUPERGEN.2009.5347884
1.chave:
- 4813519
2.author:
- Shahriar, Md.
- Anam, Sarawat
3.title:
- 'Quality Data for Data Mining and Data Mining for Quality Data: A Constraint Based
  Approach in XML'
4.keywords:
- Data mining
- XML
- Conferences
- Australia
- Data engineering
- Association rules
- Databases
- Proposals
- XML
- DATA QUALITY
- DATA MINING
- CONSTRAINTS IN XML
5.abstract:
- 'As quality data is important for data mining, reversely data mining is necessary
  to measure the quality of data. Specifically, in XML, the issue of quality data
  for mining purposes and also using data mining techniques for quality measures is
  becoming more necessary as a massive amount of data is being stored and represented
  over the Web. We propose two important interrelated issues: how quality XML data
  is useful for data mining in XML and how data mining in XML is used to measure the
  quality data for XML. When we address both issues, we consider XML constraints because
  constraints in XML can be used for quality measurement in XML data and also for
  finding some important patterns and association rules in XML data mining. We note
  that XML constraints can play an important role for data quality and data mining
  in XML. We address the theoretical framework rather than solutions. Our research
  framework is towards the broader task of data mining and data quality for XML data
  integrations.'
6.year:
- 2008
7.type_publication:
- inproceedings
8.doi:
- 10.1109/FGCNS.2008.74
1.chave:
- 7299603
2.author:
- Immonen, Anne
- "P\xE4\xE4kk\xF6nen, Pekka"
- Ovaska, Eila
3.title:
- Evaluating the Quality of Social Media Data in Big Data Architecture
4.keywords:
- Big data
- Social network services
- Computer architecture
- Meta data
- Online services
- architecture
- big data
- metadata
- quality attribute
- quality of data
- Architecture
- big data
- metadata
- quality attribute
- quality of data
5.abstract:
- 'The use of freely available online data is rapidly increasing, as companies have
  detected the possibilities and the value of these data in their businesses. In particular,
  data from social media are seen as interesting as they can, when properly treated,
  assist in achieving customer insight into business decision making. However, the
  unstructured and uncertain nature of this kind of big data presents a new kind of
  challenge: how to evaluate the quality of data and manage the value of data within
  a big data architecture? This paper contributes to addressing this challenge by
  introducing a new architectural solution to evaluate and manage the quality of social
  media data in each processing phase of the big data pipeline. The proposed solution
  improves business decision making by providing real-time, validated data for the
  user. The solution is validated with an industrial case example, in which the customer
  insight is extracted from social media data in order to determine the customer satisfaction
  regarding the quality of a product.'
6.year:
- 2015
7.type_publication:
- article
8.doi:
- 10.1109/ACCESS.2015.2490723
1.chave:
- 9005614
2.author:
- Patel, Jayesh
3.title:
- An Effective and Scalable Data Modeling for Enterprise Big Data Platform
4.keywords:
- Data models
- Big Data
- Business
- Analytical models
- Computational modeling
- Lakes
- Solid modeling
- Big Data
- Big Data Lake
- Scalable Data Modeling
- Hadoop
- Spark
- Business Intelligence
- Big Data Analytics
5.abstract:
- The enormous growth of the internet, enterprise applications, social media, and
  IoT devices in the current time caused a huge spike in enterprise data growth. Big
  data platform provided scalable storage to manage enterprise data growth and served
  easier data access to decision-makers, stakeholders and business users. It is a
  well-known challenge to classify, organize and store all this data and process it
  to provide business insights. Due to nature, variety, velocity, volume and value
  of data make it difficult to effectively process big data. Enterprises face challenges
  to apply complex business rules, to generate insights and to support data-driven
  decisions in a timely fashion. As big data lake integrates streams of data from
  a bunch of business units, stakeholders usually analyze enterprise-wide data from
  various data models. Data models are a vital component of Big data platform. Users
  may do complex processing, run queries and perform big table joins to generate required
  metrics depending on the available data models. It is usually a time consuming and
  resource-intensive process to find the value from data. It is a no-brainer that
  big data platform in the enterprise needs high-quality data modeling methods to
  reach an optimal mix of cost, performance, and quality. This paper addresses these
  challenges by proposing an effective and scalable way to organize and store data
  in Big Data Lake. It presents some of the basic principles and methodology to build
  scalable data models in a distributed environment. It also describes how it overcomes
  common challenges and presents findings.
6.year:
- 2019
7.type_publication:
- inproceedings
8.doi:
- 10.1109/BigData47090.2019.9005614
1.chave:
- 7822785
2.author:
- Kim, Gyu-tae
- Kim, Yongkang
- Kwon, Min-Seok
- Park, Taesung
3.title:
- Quality control plot for high dimensional omics data
4.keywords:
- Quality control
- Quality Control (QC)
- Microarray
- Omics data
- MicroArray Quality Control(MAQC) project
- High-dimensional data quality control (HidQC) plot
5.abstract:
- Quality control (QC) becomes more important in pre-processing analysis of high dimensional
  omics data. Several routine QC processes became a standard process in omics data
  analysis. The standard QC analysis includes calculating quality-related measures,
  checking the consistency among samples, detecting outlying observations and so forth.
  QC analysis tends to be more important in the era of high dimensional omics data.
  Although several QC analysis tools providing simple graphical display have been
  developed by many researchers, they usually require a subjective decision on QC.
  Here, we propose high-dimensional data quality control (HidQC) plot which is a simple
  and efficient QC tool for handling high dimensional omics data. HidQC plot primarily
  focuses on identifying samples of poor quality by conducting a contrast analysis
  for the between/within group distances and the summary distances. HidQC plot checks
  the quality by investigating the consistency of samples for each group. Unlike other
  QC plots, HidQC plot provides the p-value of each sample based on the permutation
  test, which can be used as a more objective criterion to determine whether to use
  the sample or not. We applied HidQC plot to MicroArray Quality Control (MAQC) project
  1 data to demonstrate its usefulness.
6.year:
- 2016
7.type_publication:
- inproceedings
8.doi:
- 10.1109/BIBM.2016.7822785
1.chave:
- 6758125
2.author:
- Rui, Zhang
- Hong-jiao, Yao
- Chuan-guang, Zhang
3.title:
- Compression method of power quality data based on wavelet transform
4.keywords:
- Power quality
- Wavelet transforms
- Data compression
- Voltage fluctuations
- Interrupters
- Educational institutions
- power power quality
- dq0 transform
- wavelet transform
- data compression
5.abstract:
- The demand for data compression has increased considerably due to the huge amount
  of the power quality data detected. Aiming at the problem the current power quality
  data compression methods are short of considering the correlation of the three-phase
  power quality data, a novel approach of data compression for three-phase power quality
  based on wavelet transform is presented in the paper. The simulation results show
  that the proposed method can not only get high signal noise ratio relative to the
  existing methods of the power quality data compression under the same compression
  ratio, but also control the power quality compression performance according to the
  practical requirement flexibly.
6.year:
- 2013
7.type_publication:
- inproceedings
8.doi:
- 10.1109/MIC.2013.6758125
1.chave:
- 6204987
2.author:
- Sidi, Fatimah
- Ramli, Abdullah
- Jabar, Marzanah
- Affendey, Lilly
- Mustapha, Aida
- Ibrahim, Hamidah
3.title:
- Data quality comparative model for data warehouse
4.keywords:
- Data models
- Data warehouses
- Accuracy
- Quality management
- Organizations
- Complexity theory
- data quality
- data warehouses
- data quality management
- data quality dimension
5.abstract:
- The growth of daily data and complexity in data warehouse with enhanced the information
  technology has created new challenges for information user. The demand for quality
  data has increase an awareness of the quality, reliable and accuracy of information
  in making fast and reliable decision-making. Nowadays, many organizations are depending
  on their resources in data warehouse. As that matter of fact, the qualities of data
  warehouse are greatly concern. The poor and error data will cause more trouble in
  data warehouse as data accessed from the same resources by the user. Here we present
  the systematic review comparative model to determine the data quality model as further
  research in our studies.
6.year:
- 2012
7.type_publication:
- inproceedings
8.doi:
- 10.1109/InfRKM.2012.6204987
1.chave:
- 9510379
2.author:
- Qu, Xiaoyu
- Dong, Kun
- Zhao, Jianfeng
- Liu, Weicheng
- Shi, Zhan
- Yu, Yue
3.title:
- A novel identification and location method for transient power quality disturbance
  sources
4.keywords:
- Fault diagnosis
- Power quality
- Forestry
- Big Data
- Power system harmonics
- Harmonic analysis
- Data models
- transient power quality problem
- power quality big data
- random forest
- data-driven
5.abstract:
- The application of power electronics and high penetration of new energy generation
  have brought great economic and social benefits. Meanwhile, new power quality phenomena
  and new issues have come into existence, such as three-phase unbalance, harmonic
  and low-frequency resonance, etc. In order to solve power quality problems fundamentally.
  The paper proposes a novel identifying and locating method utilizing transient power
  quality data. Based on the operational big data of power system, spatio-temporal
  data model of power quality is established. Then, based on big data, by adopting
  random forest algorithm, the data is analyzed to identify and locate transient power
  quality problem disturbance sources. This paper simulates in IEEE 30 bus system,
  and the results indicate the accuracy in identifying and locating short circuit
  disturbance sources.
6.year:
- 2021
7.type_publication:
- inproceedings
8.doi:
- 10.1109/CIEEC50170.2021.9510379
1.chave:
- 7944943
2.author:
- Kang, Gaganjot
- Gao, Jerry
- Xie, Gang
3.title:
- 'Data-Driven Water Quality Analysis and Prediction: A Survey'
4.keywords:
- Water pollution
- Water resources
- Big Data
- Analytical models
- Data models
- Water
- Indexes
- Water quality evaluation
- big data analytics
- data-driven water quality evaluation
- and water quality prediction
5.abstract:
- Water quality becomes one of the important quality factors for the quality life
  in smart cities. Recently, water quality has been degraded due to diverse forms
  of pollution caused by disposal of human wastes, industrial wastes, automobile wastes.
  The increasing pollution affects water quality and the quality of people's life.
  Hence, water quality evaluation, monitoring, and prediction become an important
  and hot research subject. In the past, many environmental researchers have dedicated
  their research efforts on this subject using conventional approaches. Recently,
  many researchers begin to use the big data analytics approach to studying, evaluating,
  and predicting water quality due to the advances of big data applications and the
  availability of environmental sensing networks and sensor data. This paper reviews
  the published research results relating to water quality evaluation and prediction.
  Moreover, the paper classifies and compares the applied big data analytics approaches
  and big data based prediction models for water quality assessment. Furthermore,
  the paper also discusses the future research needs and challenges.
6.year:
- 2017
7.type_publication:
- inproceedings
8.doi:
- 10.1109/BigDataService.2017.40
1.chave:
- 8332626
2.author:
- Xia, Wenze
- Xu, Zhuoming
- Mao, Chengwang
3.title:
- User-Driven Filtering and Ranking of Topical Datasets Based on Overall Data Quality
4.keywords:
- Data integrity
- Measurement
- Filtering
- Data models
- Portals
- Quality assessment
- Tools
- data quality-based filtering and ranking
- topical datasets
- overall data quality
- data quality assessment
- Data Quality Vocabulary (DQV)
- open data portal
5.abstract:
- Finding relevant and high-quality data is the eternal needs for data consumers (i.e.,
  users). Many open data portals have been providing users with simple ways of finding
  datasets on a particular topic (i.e., topical datasets), which are not a way of
  filtering and ranking topical datasets based on data quality. Despite the recent
  advances in the development and standardization of data quality models and vocabulary,
  there is a lack of systematic research on approaches and tools for user-driven data
  quality-based filtering and ranking of topical datasets. In this paper we address
  the problem of user-driven filtering and ranking of topical datasets based on the
  overall data quality of datasets by developing a generic software architecture and
  the corresponding approach, called ODQFiRD, for filtering and ranking topical datasets
  according to user-specified data quality assessment criteria. Additionally, we use
  our implemented prototype of ODQFiRD to conduct a case study experiment on the U.S.
  Government's open data portal. The prototype implementation and experimental results
  show that our proposed ODQFiRD is achievable and effective.
6.year:
- 2017
7.type_publication:
- inproceedings
8.doi:
- 10.1109/WISA.2017.24
1.chave:
- 9798931
2.author:
- Mohammed, Mahmood
- Talburt, John
- Dagtas, Serhan
- Hollingsworth, Melissa
3.title:
- A Zero Trust Model Based Framework For Data Quality Assessment
4.keywords:
- Industries
- Costs
- Scientific computing
- Data integrity
- Computational modeling
- Organizations
- Data models
- Data quality assessment
- Zero Trust
- Data Quality
- Data quality dimensions
- Trusted Data
5.abstract:
- "Zero trust security model has been picking up adoption in various organizations\
  \ due to its various advantages. Data quality is still one of the fundamental challenges\
  \ in data curation in many organizations where data consumers don\u2019t trust data\
  \ due to associated quality issues. As a result, there is a lack of confidence in\
  \ making business decisions based on data. We design a model based on the zero trust\
  \ security model to demonstrate how the trust of data consumers can be established.\
  \ We present a sample application to distinguish the traditional approach from the\
  \ zero trust based data quality framework."
6.year:
- 2021
7.type_publication:
- inproceedings
8.doi:
- 10.1109/CSCI54926.2021.00123
1.chave:
- 9322931
2.author:
- Mandrakov, Egor
- Vasiliev, Victor
- Dudina, Diana
3.title:
- Non-conforming Products Management in a Digital Quality Management System
4.keywords:
- Quality management
- Information technology
- Neural networks
- Tools
- Information security
- Big Data
- Standards organizations
- quality management system
- digital transformation
- digital environment
- non-conforming products
- big data
- neural networks
- internet of things
5.abstract:
- 'This article addresses the issue of changes in the quality management system, with
  the digitalization of the company. More specifically, changes regarding the process
  of managing non-conforming products. The article reflects how the use of digital
  tools and new technologies affects the process of detecting non-conformities and
  how the procedure for working with non-conforming products changes: how is the collection
  of data on the characteristics of the facility, what decisions need to be made regarding
  the inconsistencies, how the structure of work with non-conformances is changing
  and to what extent the use of information tools is beneficial.'
6.year:
- 2020
7.type_publication:
- inproceedings
8.doi:
- 10.1109/ITQMIS51053.2020.9322931
1.chave:
- 8859426
2.author:
- He, Tieke
- Chen, Shenghao
- Hao, Lian
- Liu, Jia
3.title:
- Quality Driven Judicial Data Governance
4.keywords:
- Data integrity
- Big Data
- Decision making
- Organizations
- Standards organizations
- data quality
- judicial data governance
- quality measurement
5.abstract:
- With the development of Smart Court 3.0, the amount of judicial data that can be
  stored and processed by the computer is increasing rapidly. People gradually realize
  that judicial data contains tremendous social and business value. However, we need
  stronger ability to handle with and apply massive, multi-source and heterogeneous
  judicial data. A complete data governance system should be built in order to make
  full use of the value of data assets. In such a data governance system, data quality
  control is one of the key steps of data governance, and also the bottleneck of data
  service development, because data quality determines the upper limit of data application.
  This paper proposes a judicial data quality measurement framework by analyzing some
  judicial business data, followed by a data governance method driven by it.
6.year:
- 2019
7.type_publication:
- inproceedings
8.doi:
- 10.1109/QRS-C.2019.00026
1.chave:
- 7502278
2.author:
- Shanmugam, Srinivasan
- Seshadri, Gokul
3.title:
- Aspects of Data Cataloguing for Enterprise Data Platforms
4.keywords:
- Context
- Business
- Metadata
- Big data
- Electronic mail
- Reliability
- Indexes
- Enterprise data
- Data catalogue
- Metadata of data
- Data context
- Data quality
- Data governance
- Data as a service
5.abstract:
- As the adoption of enterprise Big Data platforms mature, the necessity to maintain
  systematic catalogue of data being processed and managed by these platforms becomes
  imperative. Enterprise data catalogues serve as centralized repositories of storing
  such metadata about data being handled by such platforms, enabling better data governance,
  security and control. Standardized approaches, methodologies and tools for data
  catalogues are being discussed and evolved. This paper introduces some key variables,
  attributes and indexes that need to be handled in such cataloguing solutions --
  such as data contexts, data-system relationships, data quality, reliability, sensitivity
  and accessibility. The paper also discusses specific approaches on how each of these
  aspects can be adopted and applied to different enterprise contexts effectively.
6.year:
- 2016
7.type_publication:
- inproceedings
8.doi:
- 10.1109/BigDataSecurity-HPSC-IDS.2016.52
1.chave:
- 9150244
2.author:
- Zhang, Lanlan
- Zou, Du
3.title:
- Product quality prediction of rolling mill in big data environment
4.keywords:
- Data models
- Neural networks
- Analytical models
- Predictive models
- Rough surfaces
- Surface roughness
- Mathematical model
- Product quality
- BP neural network
- data analysis
- surface roughness
- thickness error
5.abstract:
- "With the wide use of rolling mill in iron and steel industry, the quality of rolling\
  \ mill products has become the primary goal of people. However, due to design defects\
  \ and manufacturing quality problems, the quality of steel products is seriously\
  \ affected, and the surface roughness and thickness of steel plate are important\
  \ quality indicators. In this paper, by analyzing a large number of monitoring data\
  \ of rolling mill condition and using BP neural network model [1], the discrete\
  \ system model between monitoring data and \u201Csurface roughness\u201D and \u201C\
  thickness error\u201D of rolling steel plate is further established."
6.year:
- 2020
7.type_publication:
- inproceedings
8.doi:
- 10.1109/ICBDIE50010.2020.00015
1.chave:
- 9660957
2.author:
- Yuan, Tangxiao
- Adjallah, Kondo
- Sava, Alexandre
- Wang, Huifen
- Liu, Linyan
3.title:
- Issues of Intelligent Data Acquisition and Quality for Manufacturing Decision-Support
  in an Industry 4.0 Context
4.keywords:
- Measurement
- Uncertainty
- Systematics
- Data integrity
- Soft sensors
- Decision making
- Data acquisition
- data quality
- data source
- data acquisition
- manufacturing
- decision-making
- review
5.abstract:
- Data quality plays an essential role in decision-making, as the latter may incorporate
  some risks in different application areas. In the context of industry 4.0, the amount,
  the versatility, and the speed of information flow for decision-making are important
  issues. The quality and, in particular, the dependability of data is paramount.
  This paper investigates the leading data quality characteristics in the industry
  4.0 environment with the related issues due to various interactions. It proposes
  a taxonomy of data sources and flows, from acquisition to the information extraction
  level for decision-making. The authors highlight the specific issues of error and
  uncertainty propagation management as significant research challenges for designing
  intelligent data collection and acquisition systems for industrial manufacturing
  decision support within the framework of the new generation of industry development.
  They review data quality characteristics definition and assessment requirements,
  and suggest classifying these characteristics into four categories. Data quality
  assessment methods and models with relation to decision-making are also examined.
  They willfully left aside the investigation of data quality improvement processes
  for a future more detailed paper.
6.year:
- 2021
7.type_publication:
- inproceedings
8.doi:
- 10.1109/IDAACS53288.2021.9660957
1.chave:
- 8964872
2.author:
- Song, Jinyu
- Hao, Jiandong
- Gang, Chen
- Suojuan, Zhang
- Yiping, Guo
3.title:
- Design and Implementation of a Universal Data Quality Management Software Based
  on Data Flow
4.keywords:
- Cleaning
- Data integrity
- Filling
- Microsoft Windows
- Data mining
- Instruction sets
- data flow
- data quality
- data cleaning
- cleaning evaluation
5.abstract:
- Data quality problems are analyzed to get several typical problems, such as data
  missing, data duplication, data abnormality, data inconsistency and data logic error.
  In order to resolve these problems, a universal data quality management software
  is proposed. This software provides data cleaning method for each problem and evaluates
  the effect of these methods, and manages the plug-in components for use. The architecture
  and critical technologies are introduced in detail, and main steps are shown with
  a specific application. According to the feedback of users, this new software is
  powerful, adaptable, simple to use, easy to set processes, with high data cleaning
  efficiency and accurate effect evaluations.
6.year:
- 2019
7.type_publication:
- inproceedings
8.doi:
- 10.1109/ITME.2019.00149
1.chave:
- 7374131
2.author:
- Juddoo, Suraj
3.title:
- Overview of data quality challenges in the context of Big Data
4.keywords:
- Big data
- Context
- Quality management
- Frequency measurement
- Organizations
- Big Data
- Data quality
- Data profiling
- Data cleansing
- data quality rules
- dimensions
- metrics
5.abstract:
- Data quality management systems are thoroughly researched topics and have resulted
  in many tools and techniques developed by both academia and industry. However, the
  advent of Big Data might pose some serious questions pertaining to the applicability
  of existing data quality concepts. There is a debate concerning the importance of
  data quality for Big Data; one school of thought argues that high data quality methods
  are essential for deriving higher level analytics while another school of thought
  argues that data quality level will not be so important as the volume of Big Data
  would be used to produce patterns and some amount of dirty data will not mask the
  analytic results which might be derived. This paper aims to investigate various
  components and activities forming part of data quality management such as dimensions,
  metrics, data quality rules, data profiling and data cleansing. The result list
  existing challenges and future research areas associated with Big Data for data
  quality management.
6.year:
- 2015
7.type_publication:
- inproceedings
8.doi:
- 10.1109/CCCS.2015.7374131
1.chave:
- 8453066
2.author:
- Scavuzzo, Marco
- Di, Elisabetta
- Ardagna, Danilo
3.title:
- '[Journal First] Experiences and Challenges in Building a Data Intensive System
  for Data Migration'
4.keywords:
- Big Data
- Databases
- Quality of service
- Tools
- Fault tolerance
- Fault tolerant systems
- Software
- Data intensive applications
- Experiment driven action research
- Big data
- Data migration
5.abstract:
- 'Recent analyses[2, 4, 5] report that many sectors of our economy and society are
  more and more guided by data-driven decision processes (e.g., health care, public
  administrations, etc.). As such, Data Intensive (DI) applications are becoming more
  and more important and critical. They must be fault-tolerant, they should scale
  with the amount of data, and be able to elastically leverage additional resources
  as and when these last ones are provided [3]. Moreover, they should be able to avoid
  data drops introduced in case of sudden overloads and should offer some Quality
  of Service (QoS) guarantees. Ensuring all these properties is, per se, a challenge,
  but it becomes even more difficult for DI applications, given the large amount of
  data to be managed and the significant level of parallelism required for its components.
  Even if today some technological frameworks are available for the development of
  such applications (for instance, think of Spark, Storm, Flink), we still lack solid
  software engineering approaches to support their development and, in particular,
  to ensure that they offer the required properties in terms of availability, throughput,
  data loss, etc. In fact, at the time of writing, identifying the right solution
  can require several rounds of experiments and the adoption of many different technologies.
  This implies the need for highly skilled persons and the execution of experiments
  with large data sets and a large number of resources, and, consequently, a significant
  amount of time and budget. To experiment with currently available approaches, we
  performed an action research experiment focusing on developing- testing-reengineering
  a specific DI application, Hegira4Cloud, that migrates data between widely used
  NoSQL databases, including so-called Database as a Service (DaaS), as well as on-premise
  databases. This is a representative DI system because it has to handle large volumes
  of data with different structures and has to guarantee that some important characteristics,
  in terms of data types and transactional properties, are preserved. Also, it poses
  stringent requirements in terms of correctness, high performance, fault tolerance,
  and fast and effective recovery. In our action research, we discovered that the
  literature offered some high level design guidelines for DI applications, as well
  as some tools to support modelling and QoS analysis/simulation of complex architectures,
  however the available tools were not yet. suitable to support DI systems. Moreover,
  we realized that the available big data frameworks we could have used were not flexible
  enough to cope with all possible application-specific aspects of our system. Hence,
  to achieve the desired level of performance, fault tolerance and recovery, we had
  to adopt a time-consuming, experiment-based approach [1, 6], which, in our case,
  consisted of three iterations: (1) the design and implementation of a Mediation
  Data Model capable of managing data extracted from different databases, together
  with a first monholitical prototype of Hegira4Cloud; (2) the improvement of performance
  of our prototype when managing and transferring huge amounts of data; (3) the introduction
  of fault-tolerant data extraction and management mechanisms, which are independent
  from the targeted databases. Among the others, an important issue that has forced
  us to reiterate in the development of Hegira4Cloud concerned the DaaS we interfaced
  with. In particular these DaaS, which are well-known services with a large number
  of users: (1) were missing detailed information regarding the behaviour of their
  APIs; (2) did not offer a predictable service; (3) were suffering of random downtimes
  not correlated with the datasets we were experimenting with. In this journal first
  presentation, we describe our experience and the issues we encountered that led
  to some important decisions during the software design and engineering process.
  Also, we analyse the state of the art of software design and verification tools
  and approaches in the light of our experience, and identify weaknesses, alternative
  design approaches and open challenges that could generate new research in these
  areas. More details can be found in the journal publication.'
6.year:
- 2018
7.type_publication:
- inproceedings
8.doi:
- 10.1145/3180155.3182534
1.chave:
- 8935096
2.author:
- Li, Mingda
- Wang, Hongzhi
- Li, Jianzhong
3.title:
- Mining conditional functional dependency rules on big data
4.keywords:
- Big Data
- Training
- Data mining
- Cleaning
- Sampling methods
- Heuristic algorithms
- Fault tolerance
- data mining
- conditional functional dependency
- big data
- data quality
5.abstract:
- Current Conditional Functional Dependency (CFD) discovery algorithms always need
  a well-prepared training dataset. This condition makes them difficult to apply on
  large and low-quality datasets. To handle the volume issue of big data, we develop
  the sampling algorithms to obtain a small representative training set. We design
  the fault-tolerant rule discovery and conflict-resolution algorithms to address
  the low-quality issue of big data. We also propose parameter selection strategy
  to ensure the effectiveness of CFD discovery algorithms. Experimental results demonstrate
  that our method can discover effective CFD rules on billion-tuple data within a
  reasonable period.
6.year:
- 2020
7.type_publication:
- article
8.doi:
- 10.26599/BDMA.2019.9020019
1.chave:
- 7979931
2.author:
- Wen, Hongsheng
- Chen, Zhiqiang
- Gu, Jianping
- Zhu, Qiangqiang
3.title:
- Big Data Analysis on Radiographic Image Quality
4.keywords:
- Detectors
- Image edge detection
- Radiography
- Standards
- Image quality
- X-ray imaging
- Indexes
- image quality
- in-service
- radiographic product
- routine data
- quality control
5.abstract:
- Mass data generated from in-service radiographic product contain assignable information
  on Image Quality (IQ). Analyzing data from routine work might supplement the time-consuming
  Image Quality Assurance Test Procedure (IQATP) to evaluate IQ and to know product
  type performance on site, which can also locate risks and give manufacturer directions
  for the further actions as well. This article illustrates methodologies of extracting
  IQ information from mass data and visual quality track, analysis, control, and risk
  mitigation in Big Data environments.
6.year:
- 2016
7.type_publication:
- inproceedings
8.doi:
- 10.1109/CCBD.2016.073
1.chave:
- 9106668
2.author:
- Shevchenko, Peter
- Faurot, Noah
- Barentine, Christian
- Ries, Anthony
3.title:
- Improving Data Quality from Remote Eye Tracking Systems Using Real Time Feedback
4.keywords:
- Tracking
- Image color analysis
- Data integrity
- Gaze tracking
- Data collection
- Light emitting diodes
- Real-time systems
- eye-tracking
- feedback
- data quality
5.abstract:
- This study proposes a solution to improve data quality from remote desktop eye trackers.
  Poor data quality from these systems regularly occurs as a result of participants
  unknowingly moving outside of the functional data collection area, i.e. the eye
  tracking box. Researchers are often not aware of the low quality data until after
  it has been recorded. As a result potentially large amounts of data are unusable.
  To alleviate this concern, we propose a real-time feedback system that alerts participants
  when poor eye tracking data are detected, thus enabling them to adjust their position
  in front of the eye tracker as soon as they move out of the functional data collection
  area. This capability allows researchers to acquire a higher percentage of useful
  data over the course of an experiment. Our approach utilized a Raspberry Pi that
  collected and interpreted data quality from an eye tracker in real time. Data quality
  from each eye was mapped to a light emitting diode (LED) placed above the computer
  monitor. The color of LED reflected the current quality of eye tracking data with
  green and red indicating high and low quality respectively. To determine if the
  system was effective, we compared the data quality for participants who used the
  system relative to participants who did not while they performed a cognitive task.
  Results show increased data quality for those participants using the feedback system.
  Our results suggest that future studies using remote desktop eye trackers can increase
  data quality by providing real-time data quality feedback to the participants.
6.year:
- 2020
7.type_publication:
- inproceedings
8.doi:
- 10.1109/SIEDS49339.2020.9106668
1.chave:
- 7761465
2.author:
- Bushnell, Mark
3.title:
- Quality Assurance / Quality Control of Real-Time Oceanographic Data
4.keywords:
- Manuals
- Real-time systems
- Oceans
- Quality control
- Standards
- Quality assurance
- Testing
- QARTOD
- data quality control
- real-time data
5.abstract:
- "The U.S. Integrated Ocean Observing System (IOOS\xAE) Quality Assurance/Quality\
  \ Control of Real Time Oceanographic Data (QARTOD) project approaches a five-year\
  \ anniversary in 2017. The highly successful protocol already used to generate quality\
  \ control manuals for nine specific oceanographic variables continues to serve the\
  \ project well. It was recently used to create a high frequency radar QC manual,\
  \ and is again being used to develop another manual with application to phytoplankton\
  \ species and abundance. Tentative plans are to next address the QC of passive acoustic\
  \ observations."
6.year:
- 2016
7.type_publication:
- inproceedings
8.doi:
- 10.1109/OCEANS.2016.7761465
1.chave:
- 9282280
2.author:
- Chen, Haihua
- Chen, Jiangping
- Ding, Junhua
3.title:
- Data Evaluation and Enhancement for Quality Improvement of Machine Learning
4.keywords:
- Social networking (online)
- Data integrity
- Machine learning
- Software quality
- Software reliability
- Security
- Task analysis
5.abstract:
- The poor quality of a dataset may produce low quality machine learning system. Therefore,
  transfer learning as a demonstrated effective approach for data quality improvement
  has been widely used for improving the quality of machine learning. However, the
  "quality improvement" brought by transfer learning in some studies was not rigorously
  validated or was even misleading. In this paper, we first investigate the quality
  problem of the datasets that were used for building a machine learning system. The
  system was claimed to have achieved the best performance comparing to existing work
  on a machine learning task. However, the "best performance" was due to the poor
  quality of the datasets as well as the incorrect validation process. Then we described
  an experimental study to demonstrate the effectiveness of transfer learning for
  improving the quality of datasets. However, the experiment results also show the
  quality improvement of transfer learning is not guaranteed, and a set of requirements
  have to be meet before applying the approach. Based on the investigation and experiment
  results, we propose a group of data quality criteria and evaluation approaches for
  quality improvement of machine learning. We investigated the research problem and
  explained the results through studying a machine learning system for normalizing
  medical concepts in social media text with open datasets.
6.year:
- 2020
7.type_publication:
- inproceedings
8.doi:
- 10.1109/QRS51102.2020.00014
1.chave:
- 8397554
2.author:
- Zhang, Pengcheng
- Xiong, Fang
- Gao, Jerry
- Wang, Jimin
3.title:
- 'Data quality in big data processing: Issues, solutions and open problems'
4.keywords:
- Big Data
- Data integrity
- Data analysis
- Social network services
- Data preprocessing
- Internet
- Big Data
- Big data processing
- Data Quality
- Recommendation system
- Prediction system
5.abstract:
- With the rapid development of social networks, Internet of things, Cloud computing
  as well as other technologies, big data age is arriving. The increasing number of
  data has brought great value to the public and enterprises. Meanwhile how to manage
  and use big data better has become the focus of all walks of life. The 4V characteristics
  of big data have brought a lot of issues to the big data processing. The key to
  big data processing is to solve data quality issue, and to ensure data quality is
  a prerequisite for the successful application of big data technique. In this paper,
  we use recommendation systems and prediction systems as typical big data applications,
  and try to find out the data quality issues during data collection, data preprocessing,
  data storage and data analysis stages of big data processing. According to the elaboration
  and analysis of the proposed issues, the corresponding solutions are also put forward.
  Finally, some open problems to be solved in the future are also raised.
6.year:
- 2017
7.type_publication:
- inproceedings
8.doi:
- 10.1109/UIC-ATC.2017.8397554
1.chave:
- 9291525
2.author:
- Li, Tao
- Wang, Lei
- Ren, Yongjun
- Wang, Lingyun
- Qian, Qi
3.title:
- Multi-source Meteorological Observation Data Quality Control Algorithm Based on
  Data Mining
4.keywords:
- Data integrity
- Handheld computers
- Correlation
- Social computing
- Internet of Things
- Green computing
- Conferences
- Multi-source meteorological observation data
- Data mining
- Quality Control
- Neural Networks
5.abstract:
- Because of the development of the social economy, people's living standards are
  also constantly improving recent years. The effect of weather forecast on social
  economy is more and more important, which has a great influence on agricultural
  production and personal life. With the advancement of the observation automati-on
  business, small disturbances may cause systematic errors in observation data. The
  meteorological observation data's quality is an important factor that directly affects
  the accuracy of weather forecast and climate forecast. The traditional quality control
  algorithm uses the climatological limit value of historical data and the allowable
  value of elements to check, lacks sensitivity to elements fancy changes, and not
  suitable for demend of quality control. This paper introduces a quality control
  project of multi-source weather observation based on data mining. Starting from
  the correlation between the observations of the alike observation element which
  is not at the same time (time correlation), and the correlation between unlike observation
  elements at the same time (element correlation), combined with the relevant algorithms
  in data mining, the paper proposes Two different quality control methods for multi-source
  meteorological observation data, combined with the complementarity and correlation
  of the two methods, a synthetic quality control programme is established.
6.year:
- 2020
7.type_publication:
- inproceedings
8.doi:
- 10.1109/iThings-GreenCom-CPSCom-SmartData-Cybermatics50389.2020.00121
1.chave:
- 8640879
2.author:
- none
3.title:
- IEEE Approved Draft Recommended Practice for Power Quality Data Interchange Format
  (PQDIF)
4.keywords:
- IEEE Standards
- Power quality
- Monitoring
- File systems
- Information exchange
- data interchange
- file format
- IEEE 1159.3
- measurement
- monitoring
- power quality
- PQDIF
5.abstract:
- A file format suitable for exchanging power quality related measurement and simulation
  data in a vendor independent manner is defined in this recommended practice. The
  format is designed to represent all power quality phenomena identified in IEEE Std
  1159-2009, IEEE Recommended Practice on Monitoring Electric Power Quality, other
  power related measurement data, and is extensible to other data types as well. The
  recommended file format utilizes a highly compressed storage scheme to help reduce
  disk space and transmission times. The utilization of Globally Unique Identifiers
  (GUID) to represent each element in the file permits the format to be extensible
  without the need for a central registration authority.
6.year:
- 2019
7.type_publication:
- article
8.doi:
- ''
1.chave:
- 8419804
2.author:
- none
3.title:
- IEEE Draft Recommended Practice for Power Quality Data Interchange Format (PQDIF)
4.keywords:
- IEEE Standards
- Power distribution
- Monitoring
- Power measurement
- Information exchange
- Power quality
- data interchange
- file format
- IEEE 1159.3
- measurement
- monitoring
- power quality
- PQDIF
5.abstract:
- A file format suitable for exchanging power quality related measurement and simulation
  data in a vendor independent manner is defined in this recommended practice. The
  format is designed to represent all power quality phenomena identified in IEEE Std
  1159-2009, IEEE Recommended Practice on Monitoring Electric Power Quality, other
  power related measurement data, and is extensible to other data types as well. The
  recommended file format utilizes a highly compressed storage scheme to help reduce
  disk space and transmission times. The utilization of Globally Unique Identifiers
  (GUID) to represent each element in the file permits the format to be extensible
  without the need for a central registration authority.
6.year:
- 2018
7.type_publication:
- article
8.doi:
- ''
1.chave:
- 8686099
2.author:
- Abdallah, Mohammad
3.title:
- Big Data Quality Challenges
4.keywords:
- Big Data
- Quality Measurement
- Quality Model
- Quality Assurance
5.abstract:
- Big Data, is a growing technique these days. There are many uses of Big Data; Artificial
  Intelligence, Health Care, Business, and many more. For that reason, it becomes
  necessary to deal with this massive volume of data with caution and care in a term
  to make sure that the data used and produced is in high quality. Therefore, the
  Big Data quality is must, and its rules have to be satisfied. In this paper, the
  main Big Data Quality Factors, which need to be measured, is presented in the perspective
  of the data itself, the data management, data processing, and data users. This research
  highlights the quality factors that may be used later to create different Big Data
  quality models.
6.year:
- 2019
7.type_publication:
- inproceedings
8.doi:
- 10.1109/ICBDCI.2019.8686099
1.chave:
- 7603869
2.author:
- Li, Zhang
- Chang-bao, Zheng
- Shiqiang, Ma
- Guoli, Li
3.title:
- An improved method based on wavelet for power quality compression
4.keywords:
- Power quality
- Wavelet analysis
- Data compression
- Transient analysis
- Wavelet coefficients
- Power quality
- Compression
- Wavelet
- Zero-crossing
5.abstract:
- The power quality monitoring system will produce large amounts of data, especially
  in long-time and high sampling. There are difficulty in transmission and storage
  of mass data. Data compression has become more and more important. This paper presents
  an enhanced method for compressing power quality data based on wavelet transformation.
  The power quality data which eliminated fundamental wave is processed with wavelet
  transform, and then, the threshold method is used to the wavelet coefficients. Several
  variety of power quality data were processed by Matlab. Experimental results show
  that the algorithm is practical and with good performance for power quality data
  compression.
6.year:
- 2016
7.type_publication:
- inproceedings
8.doi:
- 10.1109/ICIEA.2016.7603869
1.chave:
- 5369264
2.author:
- Cai-yan, Liu
- You-fa, Sun
3.title:
- Application of Data Mining in Production Quality Management
4.keywords:
- Data mining
- Production
- Quality management
- Set theory
- Steel
- Quality control
- Association rules
- Algorithm design and analysis
- Technology management
- Inspection
- quality management
- data mining
- Apriori algorithm
5.abstract:
- Application of data mining in manufacturing enterprises' quality management is introduced.
  Quality factor analysis is very important for quality control and production management.
  In this article, based on Rough Set theory an improved Apriori algorithm is put
  forward to mine quantitative relationship among different factors which influence
  product quality. The improved algorithm overcomes the traditional Apriori algorithm's
  limitation of qualitative analysis.
6.year:
- 2009
7.type_publication:
- inproceedings
8.doi:
- 10.1109/IITA.2009.81
1.chave:
- 9661000
2.author:
- Scislo, Lukasz
- Szczepanik-Scislo, Nina
3.title:
- Air Quality Sensor Data Collection and Analytics With IoT for an Apartment With
  Mechanical Ventilation
4.keywords:
- Cloud computing
- Schedules
- Data analysis
- Memory
- Air quality
- Ventilation
- Mechanical variables measurement
- air quality
- variable air volume
- VAV
- cooling and ventilation
- control strategy
5.abstract:
- The aim of the research was to develop a concept of a remote measurement system
  for air quality management using IoT for an apartment with a mechanical exhaust
  system. The constant monitoring system with the possibility of manual control by
  the occupants allows teaching the occupants how to chose optimal settings with the
  biggest impact on the air quality. Additionally, using cloud data analysis, the
  measurements can be compared with simulations performed before the building construction.
  This allows choosing the proper apartment depending on the foreseen occupancy schedule
  and the family size.
6.year:
- 2021
7.type_publication:
- inproceedings
8.doi:
- 10.1109/IDAACS53288.2021.9661000
1.chave:
- 8308252
2.author:
- Ahmed, Hana
3.title:
- Data Quality Assessment in the Integration Process of Linked Open Data (LOD)
4.keywords:
- Data integrity
- Linked data
- Measurement
- Data integration
- Semantics
- Knowledge based systems
- Tools
- data quality
- linked open data
- assessment
- data integration
- improvement
5.abstract:
- Linked Open Data (LOD) entails a set of best practices for publishing and connecting
  structured data on the Web, which allows sharing and exchanging information in an
  inter-operable and reusable manner. The increasing adoption of these principles
  has lead to the creation of a globally distributed and huge informative space that
  covers various domains such as government, libraries, life sciences, and media.
  This offers a great opportunity to end-users to build semantic applications by exploring
  and consuming heterogeneous and dispersed possibly interlinked data. Thus, consuming
  linked data can be considered as a typical scenario of linked data integration in
  which a user requires to combine data residing in large and varying quality LOD
  datasets.In this paper, we examine the specifics of linked data integration and
  focus on three key challenges, namely data quality profiling and assessment, conflict
  resolution and quality improvement. We postulate that data quality assessment can
  act both as a deciding factor for conflict resolution and as an indicator of low
  quality data which need to be improved.
6.year:
- 2017
7.type_publication:
- inproceedings
8.doi:
- 10.1109/AICCSA.2017.178
1.chave:
- 9516862
2.author:
- Xu, Xijiao
- He, Huanming
- Song, Wei
- Gong, Jiayu
3.title:
- Analysis on the Quality Model of Big Data Software
4.keywords:
- Analytical models
- Information science
- Computational modeling
- Software quality
- Big Data
- Data models
- Computational complexity
- Big Data
- the Quality Requirements
- Software Model
5.abstract:
- With the rapid development of the big data system, The big data system has the characteristics
  of large data scale, diverse data and high computational complexity. Its testing
  method has to be constantly improved. By analyzing the general software quality
  model, and combining the characteristics of the big data software, a set of quality
  model for the big data software is formed.
6.year:
- 2021
7.type_publication:
- inproceedings
8.doi:
- 10.1109/ICIS51600.2021.9516862
1.chave:
- 8432046
2.author:
- Chai, Haiyan
- Zhang, Nan
- Liu, Bojiang
- Tang, Longli
3.title:
- A Software Defect Management System Based on Knowledge Base
4.keywords:
- Knowledge based systems
- Data mining
- Software testing
- Computer bugs
- Software quality
- knowledge base, data mining, defect management, software testing
5.abstract:
- Software testing is an effective way to improving software quality, a software defect
  is identified before it goes live, a massive amount of bug-related data is accumulated
  during software testing, there is a point in studying how to improve the working
  efficiency of software testing through integration and use of such data. In this
  thesis, a software defect management system based on knowledge base is designed,
  where the three-tier knowledge base architecture is used to manage defects, data
  mining is performed with factual knowledge generated from the testing to derive
  rule knowledge that will be used for defect prediction, and the appropriate strategy
  knowledge is configured to manage bugs, so as to improve the working efficiency
  of software testing.
6.year:
- 2018
7.type_publication:
- inproceedings
8.doi:
- 10.1109/QRS-C.2018.00118
1.chave:
- 4737862
2.author:
- Wang, Keqin
- Tong, Shurong
- Roucoules, Lionel
- Eynard, Benoit
3.title:
- "Analysis of consumers\u2019 requirements for data/information quality by using\
  \ HOQ"
4.keywords:
- Information analysis
- Quality function deployment
- Mechanical systems
- Usability
- Information systems
- Databases
- Automotive materials
- Data analysis
- Quality management
- Technology management
- Data quality
- information quality
- data consumer
- QFD
- HOQ
5.abstract:
- "Data/information quality (DQ/IQ*) has great impact on data consumers\u2019 decisions.\
  \ In order to provide high quality data/information, data consumers\u2019 requirements\
  \ for DQ/IQ have to be analyzed and identified. Right requirement identification\
  \ is fundamental to data quality control activities including DQ/IQ measurement,\
  \ evaluation, improvement, etc. Quality function deployment (QFD) and the house\
  \ of quality (HOQ) are effective tools to translate consumers\u2019 requirements\
  \ into specific DQ dimensions for DQ improvement. This work briefly introduces QFD,\
  \ HOQ and their constitutive elements. Data consumers are also examined. A methodology\
  \ of applying HOQ in DQ/IQ, which includes five major steps, is described in details.\
  \ Then an example of product design information quality is presented. By using the\
  \ methodology, the weak points of DQ in industrial firms can be identified in the\
  \ form of DQ dimensions in order to take actions to improve data/information quality."
6.year:
- 2008
7.type_publication:
- inproceedings
8.doi:
- 10.1109/IEEM.2008.4737862
1.chave:
- 9760944
2.author:
- Zheng, Yiyi
3.title:
- 'Computer-Aided Realization of Innovative Clothing Design under the Background of
  Big Data: from the Perspective of Image Quality Evaluation'
4.keywords:
- Image quality
- Industries
- Technological innovation
- Design automation
- Clothing
- Production
- Big Data
- Image Quality Evaluation
- Big Data
- Computer-Aided Realization
- Innovative Clothing Design
5.abstract:
- This paper studies the computer-aided realization of innovative clothing design
  under the background of big data from the perspective of image quality evaluation.
  Now it explains the application of computer technology in the production of renderings,
  analyzes the current situation of computer-aided design in clothing design, and
  proposes some application strategies. To promote the innovative development of the
  apparel design industry. Combining the background of big data, this article proposes
  an innovative clothing design strategy based on image quality evaluation. Starting
  from the various elements of clothing design, from the perspective of data mining,
  discover fashion elements to achieve the purpose of clothing design innovation,
  so as to meet the ever-changing social needs.
6.year:
- 2022
7.type_publication:
- inproceedings
8.doi:
- 10.1109/ICSCDS53736.2022.9760944
1.chave:
- 5710916
2.author:
- Chen, Kuang
- Chen, Harr
- Conway, Neil
- Hellerstein, Joseph
- Parikh, Tapan
3.title:
- 'Usher: Improving Data Quality with Dynamic Forms'
4.keywords:
- Probabilistic logic
- Data models
- Adaptation model
- Predictive models
- Cleaning
- Bayesian methods
- Databases
- Data quality
- data entry
- form design
- adaptive form.
5.abstract:
- Data quality is a critical problem in modern databases. data-entry forms present
  the first and arguably best opportunity for detecting and mitigating errors, but
  there has been little research into automatic methods for improving data quality
  at entry time. In this paper, we propose Usher, an end-to-end system for form design,
  entry, and data quality assurance. Using previous form submissions, Usher learns
  a probabilistic model over the questions of the form. Usher then applies this model
  at every step of the data-entry process to improve data quality. Before entry, it
  induces a form layout that captures the most important data values of a form instance
  as quickly as possible and reduces the complexity of error-prone questions. During
  entry, it dynamically adapts the form to the values being entered by providing real-time
  interface feedback, reasking questions with dubious responses, and simplifying questions
  by reformulating them. After entry, it revisits question responses that it deems
  likely to have been entered incorrectly by reasking the question or a reformulation
  thereof. We evaluate these components of Usher using two real-world data sets. Our
  results demonstrate that Usher can improve data quality considerably at a reduced
  cost when compared to current practice.
6.year:
- 2011
7.type_publication:
- article
8.doi:
- 10.1109/TKDE.2011.31
1.chave:
- 9743087
2.author:
- Diah, Haryani
- Ruldeviyani, Yova
- Nizar, Achmad
- Septa, Rizaldy
- Gagah, Anggoro
3.title:
- 'Data Quality Improvement: Case Study Financial Regulatory Authority Reporting'
4.keywords:
- Seminars
- Regulators
- Data integrity
- Semantics
- Standardization
- Machine learning
- Syntactics
- data quality
- data quality assessment
- financial data
- QAFD
- data quality dimension
5.abstract:
- Financial Regulatory Authority implements Integrated Reporting to improve data quality
  which is very important because this report will be input for making monetary and
  macroprudential policies. Data assessment is needed to fulfill the needs of standardization
  of data quality. The objective of this research is to analyze the quality of critical
  data from the Integrated Reporting for the monthly period that is reported to the
  financial regulatory authorities based on the dimensions that are used to perform
  measurements. There are 4 dimensions of data quality used to define the data quality
  requirements in this Financial Regulatory Authority, which are completeness, accuracy,
  currency, and timeliness. To analyze the data, a specific framework for financial
  data was chosen which is Quality Assessment on Financial Data (QAFD) framework with
  slight modification. The result of the objective assessment from 48 variables for
  the completeness dimension is 100% for the mandatory variable, the dimensions of
  syntactic accuracy and accuracy that related to precision and validity show 100%
  results but the semantic accuracy for loan, time deposit, and demand deposit shows
  the percentage of 81.77%, 85.04% and 81.22%, currency dimension is 96.93% and timeliness
  dimension is 80.89%. Comparison between objective and subjective assessments from
  6 variables as a sample shows that there is still a discrepancy for the currency
  and accuracy dimensions, while the completeness and timeliness dimensions are aligned
  between objective and subjective assessments. Based on the results of this study,
  recommendations were made to regulators to improve data quality.
6.year:
- 2022
7.type_publication:
- inproceedings
8.doi:
- 10.1109/ISMODE53584.2022.9743087
1.chave:
- 4084862
2.author:
- Wang, Jidong
- Wang, Chengshan
3.title:
- Compression of Power Quality Disturbance Data Based on Energy Threshold and Adaptive
  Arithmetic Encoding
4.keywords:
- Power quality
- Arithmetic
- Encoding
- Discrete wavelet transforms
- Continuous wavelet transforms
- Wavelet transforms
- Monitoring
- Wavelet analysis
- Data engineering
- Power engineering and energy
- power quality
- wavelet transform
- adaptive arithmetic encoding
- data compression
5.abstract:
- Recently, power quality issues have captured more attention. It is necessary to
  monitor power quality in order to analyze and evaluate it. The monitors will record
  huge data during disturbance. It is inconvenient for data storage and transmission.
  Compression of power quality disturbance data will save storage space efficiently
  and accelerate transmission speed. This paper proposes energy threshold method based
  on wavelet transform, and then integrates adaptive arithmetic encoding to compress
  disturbance data. The compression ratio is improved and performance is better. Four
  typical power quality disturbances including voltage sag, swell, interruption and
  transient impulse are used to test the proposed method, the validity is verified
  by simulation results.
6.year:
- 2005
7.type_publication:
- inproceedings
8.doi:
- 10.1109/TENCON.2005.300848
1.chave:
- 6394366
2.author:
- Moossavizadeh, Seyed
- Mohsenzadeh, Mehran
- Arshadi, Nasrin
3.title:
- A New Algorithmic Approach to Detect the Good Point Access in the Precautionary
  Process for Data Quality
4.keywords:
- Information systems
- Data mining
- Companies
- Educational institutions
- Quality assessment
- Standards organizations
- Data Quality
- Precaution
- Information System
- Data Quality Assessment
- Good point
- Algorithm
5.abstract:
- Data quality is a complex concept and has no structure. Hence, data quality field
  faces with a wide range of difficulties and uncertainties for measurement and evaluation.
  The lack of precise standards and algorithmic techniques are considered as two major
  problems for data quality assessment. To prevent causes of data quality problems
  is a new issue, which can remarkably reduce costs imposed by the quality loss in
  information systems. The present paper introduced a computational approach to detect
  success/failure of the precautionary process. Through studying different parts of
  the process and assessing its good point, a distinct algorithm was presented. Generally,
  there are various types of information systems due to the lack of a single organizational
  and information standard. Therefore, the proposed algorithms can be specialized
  for each given information system.
6.year:
- 2012
7.type_publication:
- inproceedings
8.doi:
- 10.1109/CSSS.2012.128
1.chave:
- 7053666
2.author:
- Shen, Yue
- Zhang, Hanwen
- Liu, Guohai
- Liu, Hui
- Xia, Wei
- Wu, Hongxuan
3.title:
- Power quality data compression based on sparse representation and compressed sensing
4.keywords:
- Power quality
- Data compression
- Compressed sensing
- Educational institutions
- Matching pursuit algorithms
- Reconstruction algorithms
- Intelligent control
- power quality
- data compression
- compressed sensing
- reconstruction algorithm
5.abstract:
- A power quality data compression method combining compressive sampling with adaptive
  matching pursuit reconstruction based on compressed sampling theorem is presented
  to solve the massive power quality data collection, compression and storage problems.
  First, the original power quality data was sampled and compressed simultaneously
  by random matrix projection method based on compressed sampling theorem. Then the
  proposed adaptive matching pursuit reconstruction algorithm was used to achieve
  accurate power quality data reconstruction. The proposed method breaks through the
  traditional framework of data compression by merging compression into sampling process
  and could reconstruct original power quality data from the small amount of sampling
  points from the compressed data. Simulation shows the proposed CS-based power quality
  data compression method can not only reduce hardware requirements, but also increase
  the efficiency of data compression.
6.year:
- 2014
7.type_publication:
- inproceedings
8.doi:
- 10.1109/WCICA.2014.7053666
1.chave:
- 9661060
2.author:
- Scislo, Lukasz
3.title:
- Quality Assurance and Control of Steel Blade Production Using Full Non-Contact Frequency
  Response Analysis and 3D Laser Doppler Scanning Vibrometry System
4.keywords:
- Three-dimensional displays
- Quality assurance
- Data acquisition
- Measurement by laser beam
- Process control
- Laser excitation
- Frequency response
- quality assurance
- quality control
- LDV
- Laser Doppler Vibrometer
- modal analysis
- EMA
5.abstract:
- Quality management is one of the crucial aspects of the Industry 4.0 concept of
  industrial production. The key matter is to limit the time for the quality system
  processes in the total production time. This paper presents the use of non-destructive,
  non-contact experimental modal analysis using the single or multipoint approach
  for measurement over the surface of the product. The example of such quality assurance
  measurements, with the use of a 3D Laser Doppler Vibrometry System, is a proposal
  of advanced instrumentation and data acquisition systems which is the perfect fit
  for Industry 4.0 factory quality management systems.
6.year:
- 2021
7.type_publication:
- inproceedings
8.doi:
- 10.1109/IDAACS53288.2021.9661060
1.chave:
- 7516521
2.author:
- Prathibha, E.
- Manjunatha, A.
- Basavaraj, Sunil
3.title:
- Dual tree complex wavelet transform based approach for power quality monitoring
  and data compression
4.keywords:
- Voltage fluctuations
- Wavelet transforms
- Power quality
- Data compression
- Monitoring
- Hafnium
- Power Quality Disturbances
- Dual Tree complex wavelet Transform
- Feature extraction
- Data compression
5.abstract:
- Power quality disturbance is a one the challenging issues, where many researchers
  are gaining the attention towards it, now a day it is very much necessary to monitor
  power quality disturbances for analyze and other purpose. The quantity of the data
  captured in present power quality monitoring system has been increasing drastically.
  It is difficult to store and transmission huge data. So compression technique is
  required to reduce the storage space for data. This paper proposes Dual tree complex
  wavelet transform (DTCWT) method for power quality monitoring and integrates run
  length encoding technique for compress disturbance data. Voltage sag, swell, transients
  and flickers are Power quality disturbances used to test the proposed method. And
  Matlab was used for generation of test signals and to implement the different algorithms
  for data compression.
6.year:
- 2016
7.type_publication:
- inproceedings
8.doi:
- 10.1109/PESTSE.2016.7516521
1.chave:
- 8367700
2.author:
- Yu, Bin
- Zhang, Chen
- Tang, ZhouHua
- Sun, JiangYan
3.title:
- Verification method of data quality in science and technology cloud in Shaanxi province
4.keywords:
- Data integrity
- Redundancy
- Databases
- Dynamic programming
- Education
- Remuneration
- Organizations
- Science and Technology Cloud
- data quality
- data redundancy
- missing value processing
5.abstract:
- "This paper analyzes and summarizes the data quality problems in the Shaanxi Science\
  \ and Technology Resource Coordination Center \u201CScience and Technology Cloud\u201D\
  \ project. These two major problems about scientific and technological information\
  \ data quality are verified. One is data redundancy caused by organizations' name\
  \ abbreviation and the other is partial scientific and technological information\
  \ data missing. This paper designs and implements solutions to the problems in \u201C\
  Science and Technology Cloud\u201D project. This paper extracts 15643 data from\
  \ scientific and technical talent pool and scientific literature library. The experimental\
  \ results verify the effectiveness and feasibility of the solution of data redundancy\
  \ and data missing."
6.year:
- 2018
7.type_publication:
- inproceedings
8.doi:
- 10.1109/ICBDA.2018.8367700
1.chave:
- 5518231
2.author:
- Liu, Yujia
- Liu, Zhiming
3.title:
- Research on the Evaluation Methods of Surface Water Quality Based on Spatial Data
4.keywords:
- Rivers
- Water resources
- Data mining
- Information analysis
- Data warehouses
- Geographic Information Systems
- Data visualization
- Neural networks
- Data preprocessing
- Quality assessment
5.abstract:
- Taking Second Songhua River as the study area, established the SQL Server-based
  spatial data warehouse. On this basis, using GIS, statistical data mining, visualized
  data mining and BP neural network to finish data preprocessing, build water quality
  assessment forecast model, and analyze spatial distributing of water quality. The
  results show that it is adapted to manage and analyze the spatial data if water
  resource was investigated by Spatial Data Mining(SDM), and it is easier to discover
  deep-seated information and rules from the mass of data; In the central and the
  southeast part of Second Songhua River, the water qualities are better than other
  places, most of them are class II or class III; The water qualities are poor in
  the northwest and the south part of Second Songhua River, they are class IV, class
  V or below class V; Every single evaluation index of water quality class shows that
  the changing trend from northwest to southeast are from high to low, and there are
  some special indexes have the trend of ascending in the southern part; According
  to comprehensive analyzing of spatial distribution character of Second Songhua River
  water quality, it has obvious area distribution, the mountainous area in the southern
  part of drainage basin has better water quality, however, it is poor in the northwest,
  both natural and human activities have influence on it.
6.year:
- 2010
7.type_publication:
- inproceedings
8.doi:
- 10.1109/ICBBE.2010.5518231
1.chave:
- 8750934
2.author:
- Labouseur, Alan
- Matheus, Carolyn
3.title:
- Dynamic Data Quality for Static Blockchains
4.keywords:
- Blockchain
- Data integrity
- Aggregates
- Distributed databases
- Software
- Friction
- Distributed ledger
- Blockchain, Dynamic Data Quality, Graphs
5.abstract:
- Blockchain's popularity has changed the way people think about data access, storage,
  and retrieval. Because of this, many classic data management challenges are imbued
  with renewed significance. One such challenge is the issue of Dynamic Data Quality.
  As time passes, data changes in content and structure and thus becomes dynamic.
  Data quality, therefore, also becomes dynamic because it is an aggregate characteristic
  of the changing content and changing structure of data itself. But blockchain is
  a static structure. The friction between static blockchains and Dynamic Data Quality
  give rise to new research opportunities, which the authors address in this paper.
6.year:
- 2019
7.type_publication:
- inproceedings
8.doi:
- 10.1109/ICDEW.2019.00-41
1.chave:
- 9263667
2.author:
- Xiangwei, Kong
3.title:
- Evaluation of Flight Test Data Quality Based on Rough Set Theory
4.keywords:
- Data integrity
- Rough sets
- Feature extraction
- Image color analysis
- Shape
- Packet loss
- Data mining
- Flight test
- data quality
- rough set
- quality evaluation
5.abstract:
- With the continuous development of flight test technology, the test system is filled
  with massive, multi-structured, and multi-dimensional data resources. The value
  of big data has been fully recognized by the society. How to tap the value of data
  has become an application in various research fields and industries. The most concerned
  issue of the field. Whether the data is rubbish or treasure, the most important
  question is whether the data to be analyzed and mined is of high quality. A low-quality
  data source will not only fail to reflect the value of the data, but may also run
  counter to the actual situation, which has side effects. In order to effectively
  evaluate the quality of flight test data, according to the characteristics of test
  data in flight test, a test data quality evaluation method based on rough set theory
  is proposed, standard test methods and test indicators for flight test data quality
  are proposed, and data quality evaluation is given. The method of rule extraction
  realizes the quality evaluation of flight test data.
6.year:
- 2020
7.type_publication:
- inproceedings
8.doi:
- 10.1109/CISP-BMEI51763.2020.9263667
1.chave:
- 5359651
2.author:
- Januzaj, Eshref
- Januzaj, Visar
3.title:
- An Application of Data Mining to Identify Data Quality Problems
4.keywords:
- Data mining
- Data analysis
- Data warehouses
- Database systems
- Data engineering
- Distributed computing
- Application software
- Internet
- Companies
- Computer applications
- Data Quality
- Data Mining
- Clustering
- Classification
5.abstract:
- Modern information systems consist of many distributed computer and database systems.
  The integration of such distributed data into a single data warehouse system is
  confronted with the well known problem of low data quality. In this paper we present
  an approach that facilitates a dynamic identification of spurious and error-prone
  data stored in a large data warehouse. The identification of data quality problems
  is based on data mining techniques, such as clustering, subspace clustering and
  classification. Furthermore, we present via a case study the applicability of our
  approach on real data. The experimental results show that our approach efficiently
  identifies data quality problems.
6.year:
- 2009
7.type_publication:
- inproceedings
8.doi:
- 10.1109/ADVCOMP.2009.11
1.chave:
- 8085517
2.author:
- Lingfeng, Zhang
- Feng, Feng
- Heng, Huang
3.title:
- Wine quality identification based on data mining research
4.keywords:
- Logistics
- Data models
- Classification algorithms
- Data mining
- Algorithm design and analysis
- Support vector machines
- Analytical models
- big data
- data mining
- quality identification
5.abstract:
- For the quality of the wine big data identification technology, the introduction
  of data mining classification algorithm, effectively according to the content of
  several impact compounds in wine level identification;Are introduced including the
  Logistic regression and BP neural network and SVM classification algorithm, in view
  of the three algorithms identify the modeling analysis of wine quality. Data mining
  is closely related to big data, applying data mining to the wine in the quality
  detection of big data, can quickly to the quality of the wine.
6.year:
- 2017
7.type_publication:
- inproceedings
8.doi:
- 10.1109/ICCSE.2017.8085517
1.chave:
- 7799649
2.author:
- Wei, Jie
- Xu, Zhuoming
- Xia, Wenze
3.title:
- 'DQAF: Towards DQV-Based Dataset Quality Annotation Using the Web Annotation Data
  Model'
4.keywords:
- Partitioning algorithms
- Computational modeling
- Topology
- Data models
- Indexing
- Social network services
- dataset quality annotation
- quality metadata
- Data Quality Vocabulary (DQV)
- Web Annotation Data Model
- interactive visual user interface
- usage-centered design
5.abstract:
- The W3C Data on the Web Best Practices Working Group is standardizing the Data Quality
  Vocabulary (DQV) for expressing data quality of Web-published datasets. As proposed
  in the DQV specification, quality annotations on datasets, one kind of quality information
  described using DQV, are achieved through Web annotations. Meanwhile, the W3C Web
  Annotation Working Group is creating a standard Web Annotation Data Model on the
  basis of the Open Annotation (OA) Data Model. Despite the significant progress in
  standardization, there is a lack of systematic research on Web tools for DQV-based
  dataset quality annotation. This paper therefore proposes a Dataset Quality Annotation
  Framework (DQAF) that provides annotating users with an interactive visual user
  interface, through which DQV-based dataset quality annotation can be readily achieved
  using the OA data model to produce machine-readable quality annotation data. We
  have implemented a proof-of-concept prototype of DQAF and conducted case study experiments
  with the prototype. The results indicate that DQAF is feasible and implementable,
  and annotating users can obtain a better understanding of and more intuitive interaction
  with the quality annotation data by means of the user interface.
6.year:
- 2016
7.type_publication:
- inproceedings
8.doi:
- 10.1109/WISA.2016.15
1.chave:
- 9300119
2.author:
- Ali, Taghrid
- Abdelaziz, Tawfig
- Maatuk, Abdelsalam
- Elakeili, Salwa
3.title:
- 'A Framework for Improving Data Quality in Data Warehouse: A Case Study'
4.keywords:
- Data integrity
- Cleaning
- Data warehouses
- Information systems
- Databases
- Business
- Task analysis
- Data warehousing
- data quality
- data cleaning
5.abstract:
- Nowadays, the development of data warehouses shows the importance of data quality
  in business success. Data warehouse projects fail for many reasons, one of which
  is the low quality of data. High-quality data achievement in data warehouses is
  a persistent challenge. Data cleaning aims at finding, correcting data errors and
  inconsistencies. This paper presents a general framework for the implementation
  of data cleaning according to the scientific principles followed in the data warehouse
  field, where the framework offers guidelines that define and facilitate the implementation
  of the data cleaning process to the enterprises interested in the data warehouse
  field. The research methodology used in this study is qualitative research, in which
  the data are collected through system analyst interviews. The study concluded that
  the low level of data quality is an obstacle to any progress in the implementation
  of modern technological projects, where data quality is a prerequisite for the success
  of its business, including the data warehouse.
6.year:
- 2020
7.type_publication:
- inproceedings
8.doi:
- 10.1109/ACIT50332.2020.9300119
1.chave:
- 5625701
2.author:
- Ali, Kamran
- Warraich, Mubeen
3.title:
- A framework to implement data cleaning in enterprise data warehouse for robust data
  quality
4.keywords:
- Data warehouses
- Business
- Databases
- Cleaning
- Data models
- Data mining
- Loading
- Data cleaning
- Data quality
- Data warehousing
- Data mining
5.abstract:
- every day, every hour, every minute, every second trillion of bytes of data is being
  generated by enterprises especially in telecom sector. To achieve level best decisions
  for business profits, access to that data in a well-situated and interactive way
  is always a dream of business executives and managers. Data warehouse is the only
  viable solution that can bring that dream into a reality. The enhancement of future
  endeavors to make decisions depends on the availability of correct information that
  based on quality of data underlying. The quality data can only be produced by cleaning
  data prior to loading into data warehouse. So correctness of data is essential for
  well-informed and reliable decision making. The framework proposed in this paper
  implements robust data quality to ensure consistent and correct loading of data
  into data warehouse that necessary to disciplined, accurate and reliable data analysis,
  data mining and knowledge discovery.
6.year:
- 2010
7.type_publication:
- inproceedings
8.doi:
- 10.1109/ICIET.2010.5625701
1.chave:
- 9155928
2.author:
- Tavakoli, Mohammadreza
- Elias, Mirette
- "Kismih\xF3k, G\xE1bor"
- "Auer, S\xF6ren"
3.title:
- Quality Prediction of Open Educational Resources A Metadata-based Approach
4.keywords:
- Metadata
- Quality control
- Predictive models
- Open Educational Resources
- Measurement
- Data analysis
- OER
- open educational resources
- metadata quality
- OER quality
- Big data
- data analysis
- quality prediction
5.abstract:
- In the recent decade, online learning environments have accumulated millions of
  Open Educational Resources (OERs). However, for learners, finding relevant and high
  quality OERs is a complicated and time-consuming activity. Furthermore, metadata
  play a key role in offering high quality services such as recommendation and search.
  Metadata can also be used for automatic OER quality control as, in the light of
  the continuously increasing number of OERs, manual quality control is getting more
  and more difficult. In this work, we collected the metadata of 8,887 OERs to perform
  an exploratory data analysis to observe the effect of quality control on metadata
  quality. Subsequently, we propose an OER metadata scoring model, and build a metadata-based
  prediction model to anticipate the quality of OERs. Based on our data and model,
  we were able to detect high-quality OERs with the F1 score of 94.6%.
6.year:
- 2020
7.type_publication:
- inproceedings
8.doi:
- 10.1109/ICALT49669.2020.00007
1.chave:
- 7474371
2.author:
- Ayyalasomayajula, Haripriya
- Gabriel, Edgar
- Lindner, Peggy
- Price, Daniel
3.title:
- Air Quality Simulations Using Big Data Programming Models
4.keywords:
- Sparks
- Atmospheric modeling
- Air quality
- Analytical models
- Sensors
- Computational modeling
- Data models
- Air Quality Simulations
- MapReduce
- Spark
5.abstract:
- Forecasts of daily pollutant levels have become a standard part of weather predictions
  in television, on-line, and in newspapers. Research groups also need to analyze
  larger timeframes across more locations to correlate long term developments for
  different pollutants with multiple serious health effects such as asthma. This paper
  presents a comparison of the Hadoop MapReduce and Spark programing models for air
  quality simulations, guiding future code development for the research groups interested
  in these analyses. Two use cases have been used, namely (i) calculating the eight
  hour rolling average of pollutants in a restricted region, (ii) identifying clusters
  of sensors showing similar patterns in pollutant concentration over multiple years
  in the state of Texas. The data set used in this analysis is air pollution data
  collected over fifteen years at 179 monitor sites across the state of Texas for
  a variety of pollutants. Our results reveal 20-25% performance benefits for the
  Spark solutions over MapReduce. Furthermore, it documents performance benefits of
  the Spark MLlib machine learning library over the Mahout library which is based
  on the MapReduce programing model.
6.year:
- 2016
7.type_publication:
- inproceedings
8.doi:
- 10.1109/BigDataService.2016.26
1.chave:
- 9716355
2.author:
- Vinisha, Feby
- Sujihelen, L.
3.title:
- Study on Missing Values and Outlier Detection in Concurrence with Data Quality Enhancement
  for Efficient Data Processing
4.keywords:
- Data analysis
- Data integrity
- Data preprocessing
- Data acquisition
- Prediction algorithms
- Data models
- Real-time systems
- Missing Values
- Outlier Detection
- Data Analytics
- Data Quality Improvement
- Data Preprocessing
5.abstract:
- Data analytics is the process of analyzing raw data to make predictions and derive
  conclusions. This process involves collecting and organizing data to discover hidden
  patterns and draw insight into the data. The methods and approaches of data analytics
  are automated using various algorithms and mathematical formulas. Data analytics
  provides real-time and actionable perceptions on data that enable more accurate
  and prompter decision-making. During the data acquisition phase, missing values
  and outliers are encountered that affect the model's reliability. Missing values
  are the data missing in a dataset, more common on large datasets that arise due
  to information loss, dropout, or non-response of participants. Missing values affects
  the accuracy of the result and also may lead to biased results. Outliers are the
  abnormal values that happen to have deviated from the normal distribution pattern
  of data distribution. Outliers are extreme, unrealistic, extremely big, or small
  values in a dataset that arise due to manual errors like participant response errors
  and data entry errors. Outliers also affect the accuracy of the results and lead
  to over or underestimated resultant values. As missing values and outliers degrade
  the performance of the analytical data models, various research works have focused
  on finding and handling such values. This paper reviews the various aspects of missing
  values and outliers in the preprocessing phase of data analytics to enhance the
  accuracy of the data model.
6.year:
- 2022
7.type_publication:
- inproceedings
8.doi:
- 10.1109/ICSSIT53264.2022.9716355
1.chave:
- 7848697
2.author:
- Tan, Julian
- Ang, Ai
- Lu, Liu
- Gan, Sheena
- Corral, Marilyn
3.title:
- 'Quality Analytics in a Big Data supply chain: Commodity data analytics for quality
  engineering'
4.keywords:
- Supply chains
- Big data
- Manufacturing
- Data visualization
- Market research
- Industries
- Supply Chain
- Analytics
- Industrie 4.0
- IoT
- Big Data
- Internet of Things
- Predictive
- Prescriptive
- Cognitive
- Descriptive
- Data Management
- Data Source
- Systems of Engagement
- Systems of Records
- Quality
- Commodity
5.abstract:
- While the world is experiencing a global shortage of natural resources, a new one
  in the form of Digital Data has emerged! The ability to harness this new resource
  has become a renewed basis for competitive advantage where leveraging Big Data effectively
  means winning in the marketplace. It is going to transform industries and professions
  around the world. However, traditional data management techniques and analytical
  methodologies that has taken us from the late 20th century and into the early 21st
  century are not sustainable in today's business environment where organizations
  are constantly being challenged to right size the work force, increase labor productivity,
  increase customer satisfaction and at the same time improving product quality and
  reliability.
6.year:
- 2016
7.type_publication:
- inproceedings
8.doi:
- 10.1109/TENCON.2016.7848697
1.chave:
- 7099579
2.author:
- Nurprasetio, P.
- Fassois, S.
3.title:
- 'Multivariate dimensional accuracy data analysis in automobile assembly: Modeling,
  reduction and quality control issues'
4.keywords:
- Analytical models
- Data models
- Quality control
- Correlation
- Autoregressive processes
- Sea measurements
- Dispersion
- OCMM data analysis
- multivariate time series
- data reduction
- statistical quality control
- automobile assembly
5.abstract:
- The problem of dimensional accuracy data analysis in automobile assembly operations
  is considered. The data are measured via Optical Coordinate Measuring Machines (OCMM's),
  and issues of multivariate modeling, analysis, reduction (sensor number and location
  determination) and quality control are addressed. The study is based upon a novel
  multivariate time series analysis framework that accounts for both spatial cross
  correlation and serial autocorrelation. Its effectiveness is demonstrated via actual
  and simulated plant data.
6.year:
- 1999
7.type_publication:
- inproceedings
8.doi:
- 10.23919/ECC.1999.7099579
1.chave:
- 7577697
2.author:
- Yu, Changhui
3.title:
- Research of time series air quality data based on exploratory data analysis and
  representation
4.keywords:
- Monitoring
- Data analysis
- Market research
- Urban areas
- Air pollution
- environment problem
- air quality data
- NO2
- exploratory data Analysis
5.abstract:
- The environmental problem, especially the air quality such as the content of PM2.5,
  is an important hot spot in the international community. Many cities release of
  air environment quality data in real time to monitor the dynamic changes of environment
  and had accumulated lots of environment data. By exploring and analyzing these time
  series monitoring data, we can get a lot of interesting information. The paper explores
  the time series air quality monitoring data based on exploratory data analysis and
  visual representation. The analysis results can be used to study the time distribution
  of air environmental quality and its dynamic changes.
6.year:
- 2016
7.type_publication:
- inproceedings
8.doi:
- 10.1109/Agro-Geoinformatics.2016.7577697
1.chave:
- 4341153
2.author:
- He, Shu-Guang
- Li, Li
- Qi, Er-Shi
3.title:
- Study on the Continuous Quality Improvement Systems of LED Packaging Based on Data
  Mining
4.keywords:
- Light emitting diodes
- Data mining
- Semiconductor device packaging
- Manufacturing processes
- Semiconductor device manufacture
- Data analysis
- Data warehouses
- Quality control
- Process control
- Decision trees
5.abstract:
- LED is one of the most widely used components in electric products. And the LED
  packaging is a very important process between semiconductor manufacturers and the
  electric product manufacturers. Based on the analysis of the characteristics of
  the LED packaging processes, a quality control model based on SPC (statistical process
  control) and data mining is put forward. The data mining is used as the quality
  data analysis tool and the quality diagnosis method. Then an infrastructure of the
  integrated continuous quality improvement systems of the LED packaging is put forward.
  In this infrastructure, there are three layers of the data collection layer, the
  data analysis layer and the result viewer layer. Furthermore, the data warehouse
  of LED packaging is designed with the snowflake schema. A 3 layer yield rate SPC
  is studied and the decision tree method is used as a quality diagnosis method based
  on the designed data warehouse. Finally, a prototype of the continuous quality improvement
  system is developed.
6.year:
- 2007
7.type_publication:
- inproceedings
8.doi:
- 10.1109/WICOM.2007.1378
1.chave:
- 8985803
2.author:
- Rahmawati, Sinta
- Ruldeviyani, Yova
3.title:
- 'Data Quality Management Strategy to Improve the Quality of Worker''s Wage and Income
  Data: A Case Study in BPS-Statistics Indonesia, 2018'
4.keywords:
- Data Quality Management
- DQM
- Loshin's framework
- DMBOK
5.abstract:
- Data quality was a problem for professionals and academics in their research. The
  issues of poor data quality will harm the organization's business. Based on the
  quality target of social statistics in the strategic plan of BPS-Statistics Indonesia
  in 2015-2019 and worker's wage and income data from the Survey of Data Requirement
  (SKD) in 2017, there is 59 percent gap between expectations and realization of data
  quality satisfaction. Therefore, the assessment of the data quality management maturity
  model is important because it will be the basis for making recommendations to improve
  data quality. The framework used in this study is the data quality framework by
  David Loshin and DQM DMBOK. Overall, the average of data quality maturity level
  is still at level 3 (defined). From the results of the data quality maturity level,
  a list of characteristic gaps that have not been implemented is obtained. The results
  of the gap mapping get several DQM activities based on DMBOK. This activity is described
  as several strategy recommendations for improving the quality of worker's wage and
  income data.
6.year:
- 2019
7.type_publication:
- inproceedings
8.doi:
- 10.1109/ICIC47613.2019.8985803
1.chave:
- 7046920
2.author:
- Fan, Lang
- Ma, Hui
3.title:
- Comparative Study of Products Quality Control System of Countries in the Era of
  Big Data
4.keywords:
- Safety
- Quality assessment
- Product design
- Standards
- Control systems
- Big data
- Educational institutions
- Product quality control
- Big data
- Regulatory regime
5.abstract:
- Product quality control is an important outcome of the development of human society
  and production management of core areas, while its system is an important content
  of the quality and safety system. National regulatory system for product quality
  very seriously, however, product quality and safety are occurring. What is a quality
  management system? Regulatory information and what is the relationship between implementation
  of the system? Internet and "big data" but also can lead to changes in the regulatory
  process and innovation of social governance and strict regulatory regime covering
  the whole process? This series of quality control problems have been highlighted.
  Thus, drawing on the experience of other countries, along the historical context,
  reflections on both sorting and summarizing, and building more accurate inference
  and description are very urgent and necessary.
6.year:
- 2014
7.type_publication:
- inproceedings
8.doi:
- 10.1109/ICMeCG.2014.51
1.chave:
- 9026025
2.author:
- Xinrui, Yang
- Lei, Wu
- Ruiyi, Li
3.title:
- Data Quality Evaluation of Chinese Wind Profile Radar Network in 2018
4.keywords:
- Data integrity
- Radar detection
- Wind
- Standards
- Indexes
- Meteorological radar
- '2018'
- Chinese wind profile radar network
- Data quality
- Evaluation
5.abstract:
- "In 2018, a total of 98 wind profile radars are stable and upload data to CMA. Based\
  \ on the \u201D Data quality control and evaluation system of wind profile radar\
  \ network (V1.0)\u201D independently designed and developed by Meteorological Observation\
  \ Center of CMA, the quality of data is evaluated and analyzed with the following\
  \ contents: detection capabilities and data accuracy. The main conclusions are as\
  \ follows: (1) The effective detection height of over 80% of the wind profile radars\
  \ in the whole network has met the design indicators. The main reasons causing non-compliance\
  \ of the effective detection height are system failure and low system performance.\
  \ (2) The data of wind profile radars is compared with the data of GRAPES forecasting\
  \ field, more than 90% of the data participating in the evaluation is relatively\
  \ accurate. After data quality control, the standard deviations of U and V components\
  \ are both within 3.05 m/s, compared with the non-quality control, they are decreased\
  \ by 19.3 % and 19.5 %,furthermore, they are decreased by 4.1 % and 2.9 % compared\
  \ with 2017. The reasons leading to the poor accuracy of data mainly are system\
  \ failure, system observation parameter setting errors and data non-compliance.\
  \ Based on the above two assessments, 19 sites have poor data availability throughout\
  \ the year, accounting for 19.4% of all the wind profile radars, and 18 sites have\
  \ poor data availability during part time of the year. The factors that affect data\
  \ quality have the following characteristics: the number of sites with low system\
  \ performance has increased year by year; the system observation parameter setting\
  \ errors and the system failure that can not be repaired for a long time remain;\
  \ due to signal interference shutdown, the lack of observation data is serious.\
  \ In the future, China will improve data quality and enhance the efficiency of data\
  \ application by strengthening management of business operation at assessment sites\
  \ and the equipment operation at non-assessment sites, upgrading equipments with\
  \ low system performance, and improving data quality control algorithms and so on."
6.year:
- 2019
7.type_publication:
- inproceedings
8.doi:
- 10.1109/ICMO49322.2019.9026025
1.chave:
- 7848472
2.author:
- Yeo, Yvonne
- Xue, Feng
- Low, Wen
- Yoon, Jung
- Gold, Steve
3.title:
- A big data approach for memory quality management
4.keywords:
- Data analysis
- Memory management
- Manufacturing
- Engines
- Random access memory
- Quality management
- Data mining
- Big data
- Analytics
- Memory Quality
- Supplier Quality
5.abstract:
- As memory technology scaling continues to advance to sub 20nm technology and memory
  capacity becomes higher, memory quality management becomes more challenging to achieve
  client's quality expectations especially in this new era of computing. Transformation
  of traditional quality management approaches becomes necessary to drive memory quality
  improvements. A big data analytics approach is presented in this paper to demonstrate
  its application on end to end quality management process to drive continuous memory
  quality improvements.
6.year:
- 2016
7.type_publication:
- inproceedings
8.doi:
- 10.1109/TENCON.2016.7848472
1.chave:
- 9261414
2.author:
- Duan, Gui-Jiang
- Yan, Xin
3.title:
- A Real-Time Quality Control System Based on Manufacturing Process Data
4.keywords:
- Manufacturing processes
- Production
- Quality control
- Product design
- Real-time systems
- Manufacturing
- Quality assessment
- Quality management
- production control
- prediction methods
5.abstract:
- Quality prediction is one of the key links of quality control. Benefitting from
  the development of digital manufacturing, manufacturing process data have grown
  rapidly, which allows product quality predictions to be made based on a real-time
  manufacturing process. A real-time quality control system (RTQCS) based on manufacturing
  process data is presented in this paper. In this study, the relationship between
  the product real-time quality status and processing task process was established
  by analyzing the relationship between the product manufacturing resources and the
  quality status. The key quality characteristics of the product were identified by
  analyzing the similarity of the product quality characteristic variations in the
  manufacturing process based on the big data technology, and a quality-resource matrix
  was constructed. Based on the quality-resource matrix, the RTQCS was established
  by introducing an association-rule incremental-update algorithm. Finally, the RTQCS
  was applied in actual production, and the performance of RTQCS was verified by experiments.
  The experiments showed that the RTQCS can effectively guarantee the quality of product
  manufacturing and improve the manufacturing efficiency during production.
6.year:
- 2020
7.type_publication:
- article
8.doi:
- 10.1109/ACCESS.2020.3038394
1.chave:
- 6680518
2.author:
- Rajan, Naresh
- Gouripeddi, Ramkiran
- Facelli, Julio
3.title:
- A Service Oriented Framework to Assess the Quality of Electronic Health Data for
  Clinical Research
4.keywords:
- Quality assessment
- Data models
- Medical services
- Terminology
- Computer architecture
- Engines
- Conferences
- Data Quality
- Secondar use of EHR data
- Service-Oriented Architecture
- Comparative Rffectiveness Research
- Data Analytics for Healthcare
- Data mining
5.abstract:
- Retrospective/observational clinical research studies are dependent on the secondary
  use of electronic health record (EHR) data for obtaining important results about
  the effectiveness of different medical interventions. In contrast to traditional
  clinical trials these studies provide results from real-world clinical settings,
  but suffer from data quality issues. Therefore, it is important to take into account
  the nature and quality of data when designing these studies in order to differentiate
  between true and artifactual variations [1]. We are developing a service-oriented
  framework to assess the quality of EHR data.
6.year:
- 2013
7.type_publication:
- inproceedings
8.doi:
- 10.1109/ICHI.2013.70
1.chave:
- 8622388
2.author:
- Norman, Ryan
- Bolin, Jason
- Powell, Edward
- Amin, Sanket
- Nacker, John
3.title:
- Using Big Data Analytics to Create a Predictive Model for Joint Strike Fighter
4.keywords:
- Big Data
- Knowledge management
- Tools
- US Department of Defense
- Cloud computing
- Computer architecture
- Data analysis
- Big Data
- Data Analytics
- Knowledge Management
- Data Management
- Virtualization
- Cloud Computing
- Predictive Maintainance
- Department of Defense
- Test and Evaluation
5.abstract:
- The amount of information needed to acquire knowledge on today's acquisition systems
  is growing exponentially due to more complex, higher resolution, software-intensive
  acquisition systems that need to operate in System-of-Systems (SoS), Family-of-Systems
  (FoS), Joint, and Coalition environments. Unfortunately, the tools and methods necessary
  to rapidly collect, aggregate, and analyze this information have not evolved as
  a whole in conjunction with this increased system complexity and, therefore, has
  made analysis and evaluation increasingly deficient and ineffective. The Test Resource
  Management Center's (TRMC's) vision is to build a DoD test and evaluation (T&E)
  knowledge management (KM) and analysis capability that leverages commercial big
  data analysis and cloud computing technologies to improve evaluation quality and
  reduce decision-making time. An evaluation revolution, starting with the Joint Strike
  Fighter (JSF) program, is underway to ensure the T&E community can support the demands
  of next-generation weapon systems.The true product of T&E is knowledge ascertained
  through the collection of information about a system or item under test. However,
  the T&E community's ability to provide this knowledge is hampered by more complex
  systems, more complex environments, and the need to be more agile in support of
  strategic initiatives, such as agile acquisition and the 3rd Offset Strategy. This
  increased complexity and need for speed cause delayed analysis and problems that
  go undetected during T&E. The primary reason for these shortfalls is antiquated
  tools and processes that make data hard to locate, aggregate, and convert into knowledge.
  In short, DoD has not evolved its evaluation infrastructure as its weapon systems
  have evolved.Conversely, commercial entities, such as medical observation and diagnosis,
  electric power distribution, retail, and industrial manufacturing, have embraced
  agility in their methodologies while modernizing analytics capabilities to keep
  up with the massive influx of data. Raw physical sensors could provide data, higher-quality
  image or video cameras, radio frequency identification (RFID) devices, faster data
  collectors, more detailed point-of-sale information or digitized records, and ultimately
  is providing more data to analysts in size and complexity than ever before. As more
  data has become available, an interrelated phenomenon is the desire of analysts
  to ask more detailed questions about their consumers and their business infrastructure.
  To drive the process of implementing big data analytics, businesses have begun establishing
  analytics centers which either take pre-defined business cases and apply methods
  to address them or implement existing knowledge within the data architecture to
  create a higher level of awareness to business groups or the company at-large. To
  meet these demands, data storage and computation architectures have become more
  sophisticated, dozens of technologies were developed for large-scale processing
  (such as Apache Hadoop or GreenPlum), and streaming architectures which allow data
  to be processed and actioned on in real-time as it is collected have become commonplace.
  The net result of these commercial best practices is a solid foundation for the
  DoD to transform how it uses data to achieve faster, better, and smarter decisions
  throughout the acquisition lifecycle.
6.year:
- 2018
7.type_publication:
- inproceedings
8.doi:
- 10.1109/BigData.2018.8622388
1.chave:
- 8731462
2.author:
- Schelter, Sebastian
- Grafberger, Stefan
- Schmidt, Philipp
- Rukat, Tammo
- Kiessling, Mario
- Taptunov, Andrey
- Biessmann, Felix
- Lange, Dustin
3.title:
- Differential Data Quality Verification on Partitioned Data
4.keywords:
- Computational modeling
- Data integrity
- Data models
- Frequency estimation
- Analytical models
- Libraries
- data quality
- data validation
- incremental data processing
5.abstract:
- Modern companies and institutions rely on data to guide every single decision. Missing
  or incorrect information seriously compromises any decision process. In previous
  work, we presented Deequ, a Spark-based library for automating the verification
  of data quality at scale. Deequ provides a declarative API, which combines common
  quality constraints with user-defined validation code, and thereby enables "unit
  tests for data". However, we found that the previous computational model of Deequ
  is not flexible enough for many scenarios in modern data pipelines, which handle
  large, partitioned datasets. Such scenarios require the evaluation of dataset-level
  quality constraints after individual partition updates, without having to re-read
  already processed partitions. Additionally, such scenarios often require the verification
  of data quality on select combinations of partitions. We therefore present a differential
  generalization of the computational model of Deequ, based on algebraic states with
  monoid properties. We detail how to efficiently implement the corresponding operators
  and aggregation functions in Apache Spark. Furthermore, we show how to optimize
  the resulting workloads to minimize the required number of passes over the data,
  and empirically validate that our approach decreases the runtimes for updating data
  metrics under data changes and for different combinations of partitions.
6.year:
- 2019
7.type_publication:
- inproceedings
8.doi:
- 10.1109/ICDE.2019.00210
1.chave:
- 9660802
2.author:
- Nikiforova, Anastasija
- Kozmina, Natalija
3.title:
- 'Stakeholder-centred Identification of Data Quality Issues: Knowledge that Can Save
  Your Business'
4.keywords:
- Data integrity
- Government
- Data science
- Stakeholders
- Open data
- Business
- data quality
- stakeholder
- Delphi
- brainstorming
- open government data
- open data
5.abstract:
- "The paper presents a study aimed at identifying the most widely occurring data\
  \ quality issues that affect users\u2019 experience with data and their reuse, and\
  \ presence of which may not only disrupt the willingness to work with data but also\
  \ cause losses for businesses. The list of defects is intended to be identified\
  \ as a result of the following activities: first, the list of the most widely occurring\
  \ data quality requirements and/or dimensions should be established by means of\
  \ literature analysis. Second, given the diversity and quantity of different data\
  \ quality requirements and dimensions, this list should be reduced by means of the\
  \ brainstorming session and DELPHI analysis, which involves 12 experts. The validity\
  \ of the resulting list should then be verified by applying these requirements to\
  \ real-world data, more precisely, open government data, which are freely available\
  \ to every stakeholder. This activity involves 30 users with advanced data quality\
  \ knowledge. This allows us to define a list of key data quality issues. Both the\
  \ data holder and the data user with higher degree of confidence can make use of\
  \ it to make sure that the data are error-free and are a trusted source to be used\
  \ without losses for business. These requirements serve as part of a specification\
  \ for the web-based data quality analysis tool to be developed."
6.year:
- 2021
7.type_publication:
- inproceedings
8.doi:
- 10.1109/IDSTA53674.2021.9660802
1.chave:
- 5591731
2.author:
- Lihong, Dong
- Yunbing, Hou
3.title:
- Study on Data Quality Evaluation of Coal and Gas Outburst
4.keywords:
- Accuracy
- Indexes
- Information services
- Data mining
- Data models
- Quality assessment
- Computational modeling
- coal and gas outburst
- data quality
- dimension
- assessment metadata
- data warehousing
5.abstract:
- 'Data quality evaluation is an important part of the process of data mining. This
  article has build the information quality evaluation index system and evaluation
  model, determines the quantitative index for each quality dimension, and also demonstrates
  the formulas to calculate them. The article takes the completeness as an example:
  Evaluate the information quality completeness dimension of key factors about coal
  and gas outburst, and, accordingly, finish the completeness evaluations of other
  dimensions, which can provide effective approaches and basis for DM data acquisition
  and pre-processing in predicting coal and gas outburst.'
6.year:
- 2010
7.type_publication:
- inproceedings
8.doi:
- 10.1109/ICEE.2010.215
1.chave:
- 4284163
2.author:
- Bhatia, MPS
- Singh, Harender
- Kumar, Naresh
3.title:
- A Proposal for the Management of Mobile Network's Quality of Service (QoS) using
  Data Mining Methods
4.keywords:
- Quality of service
- Proposals
- Quality management
- Data mining
- Performance gain
- Personnel
- Gain measurement
- Information analysis
- Performance analysis
- Set theory
- Data Mining
- Rough Set Theory
- CART
- SOM
- Quality of Service (QoS)
- Mobile Network
5.abstract:
- 'Today, the challenge for the service operators is not only to attract and subscribe
  new users but to retain already subscribed users. To gain a competitive edge over
  other service operators, the operating personnel have to measure the services provided
  to their users and the network performance in terms of Quality of Service (QoS)
  at regular periods. By analyzing the information in these measurements, they can
  manage the quality of service, which helps to improve their service and network
  performance. But due to the heavy increase in the number of users in recent years,
  they find it difficult to elicit essential information from such a large and complex
  data to manage the QoS using the existing methods. It is here that the recently
  developed and more powerful data mining methods come in handy. In this paper we
  proposed how data mining methods can be used to manage the mobile network QoS. We
  describe three data mining methods: Rough Set Theory, Classification and Regression
  Tree (CART), and Self Organizing Map (SOM).'
6.year:
- 2007
7.type_publication:
- inproceedings
8.doi:
- 10.1109/WOCN.2007.4284163
1.chave:
- 9599192
2.author:
- Zhang, Zhenwei
- Wu, Wenyan
- Wu, Dongjie
3.title:
- A Multi-Mode Learning Behavior Real-time Data Acquisition Method Based on Data Quality
4.keywords:
- Measurement
- Data integrity
- Data acquisition
- Learning (artificial intelligence)
- Interference
- Data models
- Real-time systems
- multi-mode Data
- Learning behavior
- data quality
- data acquisition
5.abstract:
- With the rapid development of new technologies such as artificial intelligence,
  big data, and the Internet of Things, many researchers have probed into the study
  of learning analysis, trying to solve the problems of teaching by analyzing the
  learning behavior data from learning process. And in many learning behavior research,
  the sensor network usually consists of a host of mutually independent data sources,
  which can be used to monitor measured objects from multiple dimensions thereby obtaining
  the multi-source multi-modal sensory data. However, there still exist false negative
  readings, false positive readings and environmental interference, etc. Therefore,
  we propose a multi-source multimode sensory data acquisition method based on Date
  Quality(DQ). We first define the data quality in terms of four aspects-accuracy,
  integrity, consistency and instantaneity. Then, by the modeling there aspects respectively,
  we propose metrics to estimate the comprehensive data quality method of multi-source
  multi-mode sensory data. Finally, a data acquisition method is presented based on
  data quality, which selects a part of data sources for data transmission according
  to the given precision. This method aims at reducing the consumption of the sensory
  network on the premise of the data quality guarantee. An extensive experimental
  evaluation demonstrates the efficiency and effectiveness of the algorithm.
6.year:
- 2021
7.type_publication:
- inproceedings
8.doi:
- 10.1109/ISCEIC53685.2021.00021
1.chave:
- 7527846
2.author:
- Kathiravelu, Pradeeban
3.title:
- Software-Defined Networking-Based Enhancements to Data Quality and QoS in Multi-tenanted
  Data Center Clouds
4.keywords:
- Distributed databases
- Quality of service
- Cloud computing
- Emulation
- Conferences
- Routing
- Biomedical imaging
- Software-Defined Networking (SDN)
- Quality of Service (QoS)
- Data Quality
- Data Centers
- Multi-Tenancy
- Message-Oriented Middleware (MOM)
5.abstract:
- 'Tenants assume various roles in the enterprise data center networks, requiring
  a differentiated Quality of Service (QoS), data quality and isolation guarantees
  among them. Traditionally, data storage and processing are handled in either distributed,
  or centralized manner. While distributed execution offers a higher horizontal scalability,
  it often comes with a trade-off of lack of centralized control, and hence often
  with a decreased accuracy and management efficiency. Software-Defined Networking
  (SDN) offers a global view of the entire data center network to a logically centralized
  controller. Hence, it provides the best of both worlds with minimal compromises:
  (i) scalability of the large-scale distributed systems. (ii) unified management
  capabilities of the traditional centralized systems. By deploying an extended SDN
  controller architecture, we attempt to enhance the data quality of stored and processed
  data and increase the QoS of the multi-tenanted data center network clouds.'
6.year:
- 2016
7.type_publication:
- inproceedings
8.doi:
- 10.1109/IC2EW.2016.19
1.chave:
- 8898129
2.author:
- Gupta, Maneesha
- Malhotra, Vaibhav
- Shah, Bankim
- Prakash, Shilpa
- Sharma, Anuja
- Kartikeyan, B.
3.title:
- RISAT-1 SAR HRS Mode Data Quality Evaluation
4.keywords:
- Image resolution
- Synthetic aperture radar
- Azimuth
- Data integrity
- Radiometry
- Orbits
- Image quality
5.abstract:
- This paper presents the quality evaluation of the data products from India's first
  SAR viz. Radar Imaging Satellite (RISAT-1) in High Resolution Spotlight (HRS) Mode
  having circular polarimetry from space. Sample scenes of Level-2 terrain corrected
  georeferenced products are acquired in different regions to observe geometric data
  quality in terms of Location accuracy and Internal Distortion. Further, parameters
  are identified to evaluate the image and radiometric data quality, such as Background
  to Peak Ratio (BPRatio), Integrated Side Lobe ratio (ISLR), Peak to Side Lobe ratio
  (PSLR), Radar Cross Section (RCS) of the Corner Reflector (CR), geometric resolution
  in the circular polarization mode (Level-1 Single Look Complex products). Results
  of the analysis are encouraging and meets the specifications. Location accuracy
  is better than 80m across track and 110m along-track, which is better than the SAR
  processor specifications. Further, geometric resolution achieved is within the range
  of 1.18 m for azimuth direction and 0.75 meter for range direction. RCS from image
  matched well with the CR's.
6.year:
- 2019
7.type_publication:
- inproceedings
8.doi:
- 10.1109/IGARSS.2019.8898129
1.chave:
- 9209633
2.author:
- Byabazaire, John
- "O\u2019Hare, Gregory"
- Delaney, Declan
3.title:
- Using Trust as a Measure to Derive Data Quality in Data Shared IoT Deployments
4.keywords:
- Data integrity
- Data models
- Big Data
- Biological system modeling
- Measurement
- Standards
- Internet of Things
- Data Quality
- Internet of Things (IoT)
- Trust
- Big Data Model
- Machine learning
5.abstract:
- Recent developments in Internet of Things have heightened the need for data sharing
  across application domains to foster innovation. As most of these IoT deployments
  are based on heterogeneous sensor types, there is increased scope for sharing erroneous,
  inaccurate or inconsistent data. This in turn may lead to inaccurate models built
  from this data. It is important to evaluate this data as it is collected to establish
  its quality. This paper presents an analysis of data quality as it is represented
  in Internet of Things (IoT) systems and some of the limitations of this representation.
  The paper then introduces the use of trust as a heuristic to drive data quality
  measurements. Trust is a well-established metric that has been used to determine
  the validity of a piece or source of data in crowd sourced or other unreliable data
  collection techniques. The analysis extends to detail an appropriate framework for
  representing data quality within the big data model. To demonstrate the application
  of a trust backed framework, we used data collected from a IoT deployment of sensors
  to measure air quality in which a low cost sensor was co-located with a gold reference
  sensor. Using data streams modeled based on a dataset from an IoT deployment, our
  initial results show that the framework's trust score are consistent with the accuracy
  measure of the machine learning models.
6.year:
- 2020
7.type_publication:
- inproceedings
8.doi:
- 10.1109/ICCCN49398.2020.9209633
1.chave:
- 7981158
2.author:
- Kattmann, Christoph
- Tenbohlen, Stefan
3.title:
- Visualization of power quality data
4.keywords:
- Data visualization
- Power quality
- Visualization
- Power measurement
- Voltage measurement
- Time measurement
- Image color analysis
- Power quality
- Data visualization
- Power grids
5.abstract:
- The visualization of massive amounts of data is a challenge for the evaluation of
  power quality, where hundreds of data points per second and location can be generated.
  Based on the theoretic foundations of data visualization, common visualizations
  for the current state of power quality as well as aggregated values are reviewed
  and analyzed. For several examples, the mapping from data point properties to visual
  dimensions is shown and discussed, highlighting the importance of the definition
  of exact goals for a visualization and illustrating possible pitfalls. In particular,
  the challenge of visualizing norm compliance is discussed and a proposal for a suitable
  plot type is made.
6.year:
- 2017
7.type_publication:
- inproceedings
8.doi:
- 10.1109/PTC.2017.7981158
1.chave:
- 7382218
2.author:
- Dai, Tao
- Hu, Hongpu
- Wan, Yanli
- Chen, Quan
- Wang, Yan
3.title:
- A data quality management and control framework and model for health decision support
4.keywords:
- Inspection
- Data models
- Quality control
- Process control
- Quality assessment
- Complexity theory
- health decision support
- data quality control
- quality inspection
- quality assessment
5.abstract:
- Health data quality issues are important influencing factors on the validity and
  scientific nature of health decisions. However, a series of health data quality
  issues have emerged, such as serious lack of key data terms and non-uniform data
  standards because of the complexity and diversity of health data. Based on the current
  state of data quality for health decision support, the paper proposes a framework
  and model for health data quality management and control. And a method of data quality
  inspection, processing and assessment applicable for the health data with complex
  types is designed based on the proposed model. On the one hand, it defines the inspection
  model of the semi-structured health data, designs the quick calling strategy of
  the inspection rules, and makes clear the data quality inspection method and problematic
  data processing method. On the other hand, it proposes a mathematic model for objective
  and quantitative evaluation of the health data. This paper provides future reference
  for addressing the data quality management of health decision support systems and
  to protect for the practical value thereof.
6.year:
- 2015
7.type_publication:
- inproceedings
8.doi:
- 10.1109/FSKD.2015.7382218
1.chave:
- 9681424
2.author:
- Dakay, Irish
- Canillo, Angie
- Ferolin, Rosana
3.title:
- 'Improving Integration of Databases and Data Sets Supporting Quality Management
  in a Higher Education Institution: A Project Post Mortem Analysis'
4.keywords:
- Quality assurance
- Databases
- Education
- Data visualization
- Data integration
- Hardware
- Distance measurement
- data integration
- quality management
- higher education
- project management
5.abstract:
- Amidst various quality assurance agencies seeking evidence of compliance in different
  reporting formats, the university in this particular case, finds itself seeking
  for efficient ways to generate and make use of data and consequently bring the organisation
  beyond a mere compliance culture to a continuous improvement culture. Aware of the
  different operational contexts of its units and disciplines, a project was carried
  out not to centralise or make uniform its operations but to find a common ground
  to see how these units collectively contribute to the attainment of institutional
  goals. The project measured its success on the degree of data integration, efficiency
  of data visualisation and the availability of action plans generated out of the
  visualised data. From the level of success attained, a retrospection was made to
  determine the impediments encountered. The analysis revealed causes ranging from
  the nature of the data requirements, how the integration project was operationalised,
  and the remaining issues despite the deployed integration procedure, organisational
  factors, hardware requirements and system compatibility. It concluded with key points
  necessary to not only celebrate the project success but also for the next improvement
  cycle or related project initiatives. It also provided insights about the related
  operational processes and quality management system for the areas being covered.
6.year:
- 2021
7.type_publication:
- inproceedings
8.doi:
- 10.1109/iCORE54267.2021.00052
1.chave:
- 7840586
2.author:
- Challa, Jagat
- Goyal, Poonam
- Nikhil, S.
- Mangla, Aditya
- Balasubramaniam, Sundar
- Goyal, Navneet
3.title:
- 'DD-Rtree: A dynamic distributed data structure for efficient data distribution
  among cluster nodes for spatial data mining algorithms'
4.keywords:
- Data structures
- Clustering algorithms
- Data mining
- Indexing
- Distributed databases
- Algorithm design and analysis
- Data mining
- data distribution
- spatial locality
- neighborhood queries
- k-NN queries
- density based clustering
5.abstract:
- Parallelizing data mining algorithms has become a necessity as we try to mine ever
  increasing volumes of data. Spatial data mining algorithms like Dbscan, Optics,
  Slink, etc. have been parallelized to exploit a cluster infrastructure. The efficiency
  achieved by existing algorithms can be attributed to spatial locality preservation
  using spatial indexing structures like k-d-tree, quad-tree, grid files, etc. for
  distributing data among cluster nodes. However, these indexing structures are static
  in nature, i.e., they need to scan the entire dataset to determine the partitioning
  coordinates. This results in high data distribution cost when the data size is large.
  In this paper, we propose a dynamic distributed data structure, DD-Rtree, which
  preserves spatial locality while distributing data across compute nodes in a shared
  nothing environment. Moreover, DD-Rtree is dynamic, i.e., it can be constructed
  incrementally making it useful for handling big data. We compare the quality of
  data distribution achieved by DD-Rtree with one of the recent distributed indexing
  structure, SD-Rtree. We also compare the efficiency of queries supported by these
  indexing structures along with the overall efficiency of DBSCAN algorithm. Our experimental
  results show that DD-Rtree achieves better data distribution and thereby resulting
  in improved overall efficiency.
6.year:
- 2016
7.type_publication:
- inproceedings
8.doi:
- 10.1109/BigData.2016.7840586
1.chave:
- 8088200
2.author:
- "Fern\xE1ndez, Marshall"
- "D\xE1vila, Abraham"
- Angeleri, Paula
3.title:
- 'Data quality applied to an academic business intelligence solution: Lesson learned'
4.keywords:
- IEC Standards
- ISO Standards
- Data models
- Business intelligence
- Data warehouses
- IEC
- Business intelligence
- Data warehouse
- SQuaRE
- ISO/IES 25000
- ISO/IEC 25012
- ISO/IEC 25024
- data quality model
- data quality metrics
5.abstract:
- Business intelligence covers a set of technologies allowing the extraction, transformation
  and loading of data into a data warehouse, and presents the information in a way
  that allows managers of an organization to make decisions. However, stored data
  in a warehouse does not always have the expected quality required and this can lead
  managers to make wrong decisions. The objective of this work is the development
  of a quality model applicable to an academic business intelligence solution, based
  on the ISO/IEC 25000 series of standards. In this research, an analysis of the data
  quality requirements was done, as an input for the respective evaluation. The results
  obtained were presented in a radar chart, along with a textual analysis. It could
  be concluded that data quality models and standards could be used successfully for
  evaluating the quality of an academic BI system, and this information is important
  for identifying and mitigating risks inherent to data and risks that depend on an
  information system.
6.year:
- 2017
7.type_publication:
- inproceedings
8.doi:
- 10.1109/ColComCon.2017.8088200
1.chave:
- 7207219
2.author:
- Taleb, Ikbal
- Dssouli, Rachida
- Serhani, Mohamed
3.title:
- 'Big Data Pre-processing: A Quality Framework'
4.keywords:
- Big data
- Data integration
- Accuracy
- Distributed databases
- Data analysis
- Business
- Big Data
- Data Quality
- pre-processing
5.abstract:
- With the abundance of raw data generated from various sources, Big Data has become
  a preeminent approach in acquiring, processing, and analyzing large amounts of heterogeneous
  data to derive valuable evidences. The size, speed, and formats in which data is
  generated and processed affect the overall quality of information. Therefore, Quality
  of Big Data (QBD) has become an important factor to ensure that the quality of data
  is maintained at all Big data processing phases. This paper addresses the QBD at
  the pre-processing phase, which includes sub-processes like cleansing, integration,
  filtering, and normalization. We propose a QBD model incorporating processes to
  support Data quality profile selection and adaptation. In addition, it tracks and
  registers on a data provenance repository the effect of every data transformation
  happened in the pre-processing phase. We evaluate the data quality selection module
  using large EEG dataset. The obtained results illustrate the importance of addressing
  QBD at an early phase of Big Data processing lifecycle since it significantly save
  on costs and perform accurate data analysis.
6.year:
- 2015
7.type_publication:
- inproceedings
8.doi:
- 10.1109/BigDataCongress.2015.35
1.chave:
- 7603605
2.author:
- Lin, Shunfu
- Xie, Chao
- Tang, Bo
- Liu, Ronghui
- Pan, Aiqiang
3.title:
- The data mining application in the power quality monitoring data analysis
4.keywords:
- Power quality
- Correlation
- Monitoring
- Indexes
- Data mining
- Voltage fluctuations
- Data analysis
- power quality
- data mining
- cluster analysis
- correlation analysis
5.abstract:
- It is a main issue to find valuable information from the power quality data because
  of its big volume, heterogeneity and low value density in the power quality monitoring
  system of the grid. An analysis system of the power quality analysis based on the
  data mining technologies is presented in this paper, consisting of the technologies
  of data cleaning, data fusion, cluster analysis, correlation analysis, and etc.
  The proposed analysis system is applied in the power quality data analysis of a
  certain city power quality monitoring system. The meaningful variation laws of the
  power quality indices are obtained, which can provide valuable reference to the
  grid planning, dispatch and operation.
6.year:
- 2016
7.type_publication:
- inproceedings
8.doi:
- 10.1109/ICIEA.2016.7603605
1.chave:
- 9297009
2.author:
- Juddoo, Suraj
- George, Carlisle
3.title:
- A Qualitative Assessment of Machine Learning Support for Detecting Data Completeness
  and Accuracy Issues to Improve Data Analytics in Big Data for the Healthcare Industry
4.keywords:
- Industries
- Machine learning algorithms
- Data integrity
- Clustering algorithms
- Medical services
- Big Data
- Tools
- Big Data
- Data Quality
- Data Inaccuracy
- Data incompleteness
- Machine Learning
5.abstract:
- Tackling Data Quality issues as part of Big Data can be challenging. For data cleansing
  activities, manual methods are not efficient due to the potentially very large amount
  of data.. This paper aims to qualitatively assess the possibilities for using machine
  learning in the process of detecting data incompleteness and inaccuracy, since these
  two data quality dimensions were found to be the most significant by a previous
  research study conducted by the authors. A review of existing literature concludes
  that there is no unique machine learning algorithm most suitable to deal with both
  incompleteness and inaccuracy of data. Various algorithms are selected from existing
  studies and applied against a representative big (healthcare) dataset. Following
  experiments, it was also discovered that the implementation of machine learning
  algorithms in this context encounters several challenges for Big Data quality activities.
  These challenges are related to the amount of data particualar machine learning
  algorithms can scale to and also to certain data type restrictions imposed by some
  machine learning algorithms. The study concludes that 1) data imputation works better
  with linear regression models, 2) clustering models are more efficient to detect
  outliers but fully automated systems may not be realistic in this context. Therefore,
  a certain level of human judgement is still needed.
6.year:
- 2020
7.type_publication:
- inproceedings
8.doi:
- 10.1109/ELECOM49001.2020.9297009
1.chave:
- 4280171
2.author:
- Shu-guang, He
- Li, Li
- Er-shi, Qi
3.title:
- Study on the Continuous Quality Improvement of Telecommunication Call Centers Based
  on Data Mining
4.keywords:
- Data mining
- Telephony
- Data analysis
- Performance analysis
- Process control
- Helium
- Quality management
- Engineering management
- Data engineering
- Educational institutions
- Data mining
- Telecommunication call center
- Continuous quality improvement
- Statistical process control
5.abstract:
- Based on the study of the processes of telecommunication call centers, the service
  quality metrics of the call centers are put forward. And the mode of the continuous
  service quality improvement of the call centers based on data warehouse and data
  mining is studied. Then the process of the IVR (Interactive Voice Response) is analyzed
  and a mode for the efficiency improvement of IVR is put forward based on the exchange
  of the orders of the service items in the IVR. Then a service quality metrics of
  the agents, the ratio of recall in one hour, is put forward. This metrics can be
  used in the performance analysis of the agents. Furthermore, the model of the performance
  analysis and control of the ASA (Average Speed of Answer) based on data mining and
  SPC (Statistical Process Control) is put forward. At last, a method for forecasting
  the call arriving in is put forward based the time series analysis using dynamic
  data mining. The result certified that the efficiency and service quality of the
  telecommunication call center can be improved obviously using the method in this
  paper.
6.year:
- 2007
7.type_publication:
- inproceedings
8.doi:
- 10.1109/ICSSSM.2007.4280171
1.chave:
- 8757524
2.author:
- OZPOLAT, Zeynep
- KARABATAK, Murat
3.title:
- Temperature Estimation with Time Series Analysis from Air Quality Data Set
4.keywords:
- Time series analysis
- Estimation
- Artificial neural networks
- Air quality
- Data mining
- Training
- Mathematical model
- Data Mining
- Time Series Analysis
- Air Quality
5.abstract:
- With the expansion of the data size, data mining techniques are gaining more and
  more importance. Data mining consists of methods such as classification, clustering,
  time series estimation and association rule. In this study, a time series analysis
  is carried out in order to make an estimation for the future in accordance with
  the structure of the data set. Time series are series in which the variables are
  recorded in chronological order. The data set was created by recording the gas concentrations
  in the air at a time interval. These data are used to estimate the changes in air
  quality. Three types of time series analysis training algorithm are used in the
  study. The results given by the algorithms are close to each other and high performance
  has been determined. As a result of experimental studies, it is observed that time
  series analysis is sufficient to estimate air quality.
6.year:
- 2019
7.type_publication:
- inproceedings
8.doi:
- 10.1109/ISDFS.2019.8757524
1.chave:
- 7404613
2.author:
- Bushnell, Mark
3.title:
- Quality Assurance/Quality Control of Real-Time Oceanographic Data
4.keywords:
- Manuals
- Real-time systems
- Quality control
- Ocean temperature
- Salinity (geophysical)
- Quality assurance
- QARTOD
- data quality control
- real-time data
5.abstract:
- 'The Quality Assurance/Quality Control of Real-Time Oceanographic Data (QARTOD)
  project was formally adopted in 2012 as a part of the U.S. Integrated Ocean Observing
  System (IOOS) Data Management and Communication (DMAC) system. A well-established
  process has resulted in eight manuals that provide specific quality control tests
  for a variety of U.S. IOOS core variables of interest. Specifically, the manuals
  address quality control (QC) for observations of dissolved oxygen, currents, waves,
  water levels, winds, temperature and salinity, ocean optics, and dissolved nutrients.
  Regional Associations within the U.S. are now working toward implementing these
  test procedures, which are also being incorporated by international ocean-observing
  organizations, the private sector, and manufacturers developing sensor improvements.
  The manuals are initially drafted and reviewed by a committee of subject matter
  experts. The resulting draft is distributed to the U.S. Regional Associations for
  a second round of reviews, followed by a third review from the wider international
  community. As such, the manuals represent the best processes desired by those who
  will implement the tests. An important aspect of the tests is the selection of thresholds
  and other test criteria by local operators, who are in the best position to understand
  and determine such limits. The tests are neither overly prescriptive nor overly
  generic and are designed to support a wide range of sensors and operator capabilities.
  Tests are identified as required, strongly recommended, or suggested, with test
  results falling into one of three categories: pass, suspect/of high interest, or
  fail. Test examples are provided in the manuals, and the instructions are sufficiently
  explicit for a programmer to use them to write software code for automated QC processes.
  The manuals are living documents. They are updated as the QARTOD project evolves
  and as input is received from operators, data users, and manufacturers. The dissolved
  oxygen, waves, and currents manuals received updates in 2015.'
6.year:
- 2015
7.type_publication:
- inproceedings
8.doi:
- 10.23919/OCEANS.2015.7404613
1.chave:
- 7724284
2.author:
- Swapna, S.
- Niranjan, P.
- Srinivas, B.
- Swapna, R.
3.title:
- Data cleaning for data quality
4.keywords:
- Cleaning
- Data mining
- Electronic mail
- Data warehouses
- Decision trees
- Conferences
- Databases
- Data Warehouse (DW)
- Data Profiling
- OLTP
- Data Quality (DQ)
- ETL
5.abstract:
- Now a day's every second data is being generated rapidly through internet and hence
  for making proper decision has become a huge task. We are surrounded by data but
  starving for knowledge, to achieve more profits in business, knowledge plays a key
  role for decision makers. Data warehouse provides a feasible solution to manage
  data which should be elegant and accurate for proper analysis and decision making.
  The data which is collected from different sources may have dirty data, cleaning
  of data should be done before the data is loaded into warehouse in order to achieve
  quality data. Once the data have been cleaned it will produce precise results, when
  the data mining query is applied. Hence consistent data is essential and reliable
  for decision making. We proposed an algorithm which constructs a Decision for every
  attribute and Missing values are to be replaced with leaf Node values by which we
  can achieve more quality data on which mining techniques can be applied for a quality
  analysis.
6.year:
- 2016
7.type_publication:
- inproceedings
8.doi:
- ''
1.chave:
- 9671890
2.author:
- Geronazzo, Angela
- Ziegler, Markus
3.title:
- 'QMLEx: Data Driven Digital Transformation in Marketing Analytics'
4.keywords:
- Itemsets
- Soft sensors
- Digital transformation
- Data integrity
- Conferences
- Big Data
- Feature extraction
- Digital transformation
- entity linking
- topic extraction
- word embedding
- pattern search
- frequent itemset mining
- data quality
5.abstract:
- This paper presents a data driven approach to replace expert driven business processes.
  The QMLEx methodology combines NLP algorithms to improve data quality, ML techniques
  to perform the task and exploits external data sources to eliminate the need for
  the expert input. The methodology is applied to our internal process devoted to
  creating groups of products with similar features, one of the most relevant use
  case in marketing analytics.
6.year:
- 2021
7.type_publication:
- inproceedings
8.doi:
- 10.1109/BigData52589.2021.9671890
1.chave:
- 9314391
2.author:
- Faroukhi, Abou
- El, Imane
- Gahi, Youssef
- Amine, Aouatif
3.title:
- 'Big Data Value Chain: A Unified Approach for Integrated Data Quality and Security'
4.keywords:
- Big Data
- Security
- Data integrity
- Data models
- Organizations
- Decision making
- Reliability
- Big Data Value Chain
- Data Management
- Data Quality
- Data Security
- Process Integration
- Orchestration
5.abstract:
- Big Data has grown significantly in recent years. This growth has led organizations
  to adopt Big Data Value Chains (BDVC) as the appropriate framework for unlocking
  the value to make suitable decisions. Despite its promising opportunities, Big Data
  raises new concerns such as data quality and security that could radically impact
  the effectiveness of the BDVC. These two essential aspects have become an urgent
  need for any Big Data project to provide meaningful datasets and reliable insights.
  In this contribution, we highlight the importance of considering data quality and
  security requirements. Then, we propose a coherent, unified framework that extends
  BDVC with security and quality aspects. Through quality and security reports, the
  model can self-evaluate and arrange tasks according to orchestration and monitoring
  process, allowing the BDVC to evolve at the organization pace and to align strategically
  with its objectives as well as to federate a sustainable ecosystem.
6.year:
- 2020
7.type_publication:
- inproceedings
8.doi:
- 10.1109/ICECOCS50124.2020.9314391
1.chave:
- 7013706
2.author:
- Gao, Jie
- Zheng, Changbao
- Hu, Cungang
- Ma, Xinchen
3.title:
- An enhanced method based on wavelet for power quality compression
4.keywords:
- Power quality
- Wavelet coefficients
- Data compression
- Transient analysis
- Educational institutions
- Power quality
- Compression
- Wavelet transform
- Zero-crossing
5.abstract:
- The high sampling rate of the power quality detection will produce large amounts
  of data in a long time. It's inconvenient for data transmission and storage. The
  technology of power quality compression is increasingly important. An enhanced method
  which based on wavelet is applied to the compression of power quality detective
  data is proposed in this paper. The compression of fault data from different type
  is performed by Matlab. The algorithm is realizable and has a good performance.
6.year:
- 2014
7.type_publication:
- inproceedings
8.doi:
- 10.1109/ICEMS.2014.7013706
1.chave:
- 6972274
2.author:
- Pastorello, Gilberto
- Agarwal, Deb
- Papale, Dario
- Samak, Taghrid
- Trotta, Carlo
- Ribeca, Alessio
- Poindexter, Cristina
- Faybishenko, Boris
- Gunter, Dan
- Hollowgrass, Rachel
- Canfora, Eleonora
3.title:
- Observational Data Patterns for Time Series Data Quality Assessment
4.keywords:
- Quality assessment
- Soil
- Wind speed
- Calibration
- Heating
- Instruments
- observational data patterns
- data quality
- time series data
- FLUXNET
5.abstract:
- 'Observational data are fundamental for scientific research in almost any domain.
  Recent advances in sensor and data management technologies are enabling unprecedented
  amounts of observational data to be collected and analyzed. However, an essential
  part of using observational data is not currently as scalable as data collection
  and analysis methods: data quality assurance and control. While specialized tools
  for very narrow domains do exist, general methods are harder to create. This paper
  explores the identification of data issues that lead to the creation of data tests
  and tools to perform data quality control activities. Developing this identification
  step in a systematic manner allows for better and more general quality control tools.
  As our case study, we use carbon, water, and energy fluxes as well as micro-meteorological
  data collected at field sites that are part of FLUXNET, a network of over 400 ecosystem-level
  monitoring stations. In an effort toward the release of a new global data set of
  fluxes, we are doing data quality control for these data. The experience from this
  work led to the creation of a catalog of issues identified in the data. This paper
  presents this catalog and its generalization into a set of patterns of data quality
  issues that can be detected in observational data.'
6.year:
- 2014
7.type_publication:
- inproceedings
8.doi:
- 10.1109/eScience.2014.45
1.chave:
- 8525075
2.author:
- Aleksandrova, Svetlana
- Vasiliev, Victor
- Letuchev, Gennady
3.title:
- Digital Technology and Quality Management
4.keywords:
- Quality management
- Production
- Maintenance engineering
- Information technology
- Process control
- Software
- quality
- quality management system
- integrated management system
- model of the integrated management system
- TQM
- EAM
- MES
- ERP
- PLM
- CALS
5.abstract:
- Development of science and technology requires the development of new methods of
  quality management. Along with already existing methods and quality management systems
  new approaches. Article showing some areas for improvement of the existing and creation
  of new tools, techniques and quality management systems in the light of the development
  of digital technologies. In particular, the combination of known methods of quality
  management (TQM, Lean Production and other) and information technology (methods
  of Product Lifecycle Management (PLM), CALS-technologies, ERP ( Enterprise Resource
  Planning), PDM-system (Product Data Management), MES (manufacturing execution system),
  LIMS (Laboratory Information Management System), EAM (Enterprise Asset Management
  systems) and others) allows you to create new principles for a modern quality management
  system.
6.year:
- 2018
7.type_publication:
- inproceedings
8.doi:
- 10.1109/ITMQIS.2018.8525075
1.chave:
- 8538098
2.author:
- Liu, Yang
- Lingling, Xv
- Jiang, Peng
- Yutong, Li
- Mingtai, Shi
- Haosong, Li
- Zhongping, Xv
- Jin, Li
- Shuai, Wang
- Dongliang, Hu
- Jia, Wu
- Dan, Su
3.title:
- Study of Data Integration Architecture for WideArea Distributed Power Quality of
  Power Grid
4.keywords:
- Power quality
- Data integration
- Distributed databases
- Monitoring
- Companies
- Web services
- Real-time systems
- Power Quality Wide-Area Distribution Data Integration
5.abstract:
- With the increasing degree of interconnection between regional power grids and the
  diversification of power quality interference sources, the problem of power quality
  has become a complex problem across provinces and regions. It is necessary to provide
  an analysis method for solving complex power quality problems between regions by
  carrying out analysis technology research based on the monitoring data of whole
  network power quality and exploring the correlation of interregional power quality
  problems. A new wide-area distributed power quality data fusion architecture is
  proposed in this paper. It solves the data source problem of the big data analysis
  of the power quality, and realizes the sharing of the data and information of the
  whole network power quality, and lays the theoretical foundation for the depth application
  of the power quality data by researching and designing the architecture aiming at
  multi-source, heterogeneous and distributed data integration technology and wide
  area distributed data storage technology.
6.year:
- 2018
7.type_publication:
- inproceedings
8.doi:
- 10.1109/ISSI.2018.8538098
1.chave:
- 9871919
2.author:
- Pezoulas, Vasileios
- Tachos, Nikolaos
- Olivotto, Iacopo
- Barlocco, Fausto
- Fotiadis, Dimitrios
3.title:
- "A \u201Csmart\u201D Imputation Approach for Effective Quality Control Across Complex\
  \ Clinical Data Structures"
4.keywords:
- Heating systems
- Correlation
- Data integrity
- Sociology
- Semantics
- Quality control
- Data structures
- data imputation
- virtual profiles
- complex clinical data structures
- in silico trials
5.abstract:
- "The overwhelming need to improve the quality of complex data structures in healthcare\
  \ is more important than ever. Although data quality has been the point of interest\
  \ in many studies, none of them has focused on the development of quantitative and\
  \ explainable methods for data imputation. In this work, we propose a \u201Csmart\u201D\
  \ imputation workflow to address missing data across complex data structures in\
  \ the context of in silico clinical trials. AI algorithms were utilized to produce\
  \ high-quality virtual patient profiles. A search algorithm was then developed to\
  \ extract the best virtual patient profiles through the definition of a profile\
  \ matching score (PMS). A case study was conducted, where the real dataset was randomly\
  \ contaminated with multiple missing values (e.g., 10 to 50%). In total, 10000 virtual\
  \ patient profiles with less than 0.02 Kullback-Leibler (KL) divergence were produced\
  \ to estimate the PMS distribution. The best generator achieved the lowest average\
  \ squared absolute difference (0.4) and average correlation difference (0.02) with\
  \ the real dataset highlighting its increased effectiveness for data imputation\
  \ across complex clinical data structures."
6.year:
- 2022
7.type_publication:
- inproceedings
8.doi:
- 10.1109/EMBC48229.2022.9871919
1.chave:
- 4155473
2.author:
- Cardoso, Jorge
3.title:
- Workflow Quality of Service Management using Data Mining Techniques
4.keywords:
- Quality of service
- Quality management
- Data mining
- Costs
- Workflow management software
- Customer satisfaction
- Prediction algorithms
- Business process re-engineering
- Runtime
- Monitoring
- Quality of Service
- Data Mining
- Business Process
- Workflow
5.abstract:
- Organizations have been aware of the importance of quality of service (QoS) for
  competitiveness for some time. It has been widely recognized that workflow systems
  are a suitable solution for managing the QoS of processes and workflows. The correct
  management of the QoS of workflows allows for organizations to increase customer
  satisfaction, reduce internal costs, and increase added value services. In this
  paper we show a novel method, composed of several phases, describing how organizations
  can apply data mining algorithms to predict the QoS for their running workflow instances.
  Our method has been validated using experimentation by applying different data mining
  algorithms to predict the QoS of workflow
6.year:
- 2006
7.type_publication:
- inproceedings
8.doi:
- 10.1109/IS.2006.348466
1.chave:
- 8419262
2.author:
- Fox, Frank
- Aggarwal, Vishal
- Whelton, Helen
- Johnson, Owen
3.title:
- A Data Quality Framework for Process Mining of Electronic Health Record Data
4.keywords:
- Data mining
- Data integrity
- Dentistry
- Registers
- Systematics
- Data visualization
- EHR, research data, process mining, data quality
5.abstract:
- 'Reliable research demands data of known quality. This can be very challenging for
  electronic health record (EHR) based research where data quality issues can be complex
  and often unknown. Emerging technologies such as process mining can reveal insights
  into how to improve care pathways but only if technological advances are matched
  by strategies and methods to improve data quality. The aim of this work was to develop
  a care pathway data quality framework (CP-DQF) to identify, manage and mitigate
  EHR data quality in the context of process mining, using dental EHRs as an example.
  Objectives: To: 1) Design a framework implementable within our e-health record research
  environments; 2) Scale it to further dimensions and sources; 3) Run code to mark
  the data; 4) Mitigate issues and provide an audit trail. Methods: We reviewed the
  existing literature covering data quality frameworks for process mining and for
  data mining of EHRs and constructed a unified data quality framework that met the
  requirements of both. We applied the framework to a practical case study mining
  primary care dental pathways from an EHR covering 41 dental clinics and 231,760
  patients in the Republic of Ireland. Results: Applying the framework helped identify
  many potential data quality issues and mark-up every data point affected. This enabled
  systematic assessment of the data quality issues relevant to mining care pathways.
  Conclusion: The complexity of data quality in an EHR-data research environment was
  addressed through a re-usable and comprehensible framework that met the needs of
  our case study. This structured approach saved time and brought rigor to the management
  and mitigation of data quality issues. The resulting metadata is being used within
  cohort selection, experiment and process mining software so that our research with
  this data is based on data of known quality. Our framework is a useful starting
  point for process mining researchers to address EHR data quality concerns.'
6.year:
- 2018
7.type_publication:
- inproceedings
8.doi:
- 10.1109/ICHI.2018.00009
1.chave:
- 6204995
2.author:
- Sidi, Fatimah
- Shariat, Payam
- Affendey, Lilly
- Jabar, Marzanah
- Ibrahim, Hamidah
- Mustapha, Aida
3.title:
- 'Data quality: A survey of data quality dimensions'
4.keywords:
- Organizations
- Accuracy
- Information systems
- Process control
- Reliability
- Data mining
- Data Quality
- Data Quality Dimensions
- Types of Data
5.abstract:
- Nowadays, activities and decisions making in an organization is based on data and
  information obtained from data analysis, which provides various services for constructing
  reliable and accurate process. As data are significant resources in all organizations
  the quality of data is critical for managers and operating processes to identify
  related performance issues. Moreover, high quality data can increase opportunity
  for achieving top services in an organization. However, identifying various aspects
  of data quality from definition, dimensions, types, strategies, techniques are essential
  to equip methods and processes for improving data. This paper focuses on systematic
  review of data quality dimensions in order to use at proposed framework which combining
  data mining and statistical techniques to measure dependencies among dimensions
  and illustrate how extracting knowledge can increase process quality.
6.year:
- 2012
7.type_publication:
- inproceedings
8.doi:
- 10.1109/InfRKM.2012.6204995
1.chave:
- 9159907
2.author:
- Wu, Di
- Luo, Xin
- Shang, Mingsheng
- He, Yi
- Wang, Guoyin
- Wu, Xindong
3.title:
- A Data-Characteristic-Aware Latent Factor Model for Web Services QoS Prediction
4.keywords:
- Quality of service
- Predictive models
- Web services
- Clustering algorithms
- Data models
- Sparse matrices
- Reliability
- Web Service
- quality-of-service
- QoS
- latent factor analysis
- density peak
- data-characteristic-aware
- missing data
- big data
- topological neighborhood
- noise data
- service selection
- data science
5.abstract:
- "How to accurately predict unknown quality-of-service (QoS) data based on observed\
  \ ones is a hot yet thorny issue in Web service-related applications. Recently,\
  \ a latent factor (LF) model has shown its efficiency in addressing this issue owing\
  \ to its high accuracy and scalability. An LF model can be improved by identifying\
  \ user and service neighborhoods based on user and service geographical information.\
  \ However, such information can be difficult to acquire in most applications with\
  \ the considerations of information security, identity privacy, and commercial interests\
  \ in a real system. Besides, the existing LF model-based QoS predictors mostly ignore\
  \ the reliability of given QoS data where noises commonly exist to cause accuracy\
  \ loss. To address the above issues, this paper proposes a data-characteristic-aware\
  \ latent factor (DCALF) model to implement highly accurate QoS predictions, where\
  \ \u2018data-characteristic-aware\u2019 indicates that it can appropriately implement\
  \ QoS prediction according to the characteristics of given QoS data. Its main idea\
  \ is two-fold: a) it detects the neighborhoods and noises of users and services\
  \ based on the dense LFs extracted from the original sparse QoS data, b) it incorporates\
  \ a density peaks-based clustering method into its modeling process for achieving\
  \ the simultaneous detections of both neighborhoods and noises of QoS data. With\
  \ such designs, it precisely represents the given QoS data in spite of their sparsity,\
  \ thereby achieving highly accurate predictions for unknown ones. Experimental results\
  \ on two QoS datasets generated by real-world Web services demonstrate that the\
  \ proposed DCALF model outperforms state-of-the-art QoS predictors, making it highly\
  \ competitive in addressing the issue of Web service selection and recommendation."
6.year:
- 2022
7.type_publication:
- article
8.doi:
- 10.1109/TKDE.2020.3014302
1.chave:
- 9239752
2.author:
- Wang, Wenjing
- Yang, Shengquan
3.title:
- Research on Air Quality Forecasting Based on Big Data and Neural Network
4.keywords:
- Air quality
- Predictive models
- Atmospheric modeling
- Data models
- Big Data
- Biological neural networks
- AQI Prediction
- Big Data
- Neural Network
5.abstract:
- Aiming at the problem that existing air quality prediction models cannot efficiently
  and accurately predict air quality in a big data environment, an air quality prediction
  method based on a big data platform to implement a distributed neural network is
  proposed. Collect historical data of the six pollutant concentrations that affect
  the air quality index and use it as input to a neural network model; A distributed
  neural network model containing the AQI change rule in the distributed neural network
  structure is adopted to realize the short-term prediction of the AQI. Experimental
  results show that air quality prediction models based on big data and neural networks
  can reveal the development trend of air quality through self-learning characteristics.
  And has higher prediction accuracy, It can provide a scientific basis for the degree
  of urban air pollution and help people make appropriate measures for different AQI
  levels.
6.year:
- 2020
7.type_publication:
- inproceedings
8.doi:
- 10.1109/ICCNEA50255.2020.00045
1.chave:
- 8787092
2.author:
- Guan, Zhibin
- Ji, Tongkai
- Qian, Xu
- Ma, Yan
- Hong, Xuehai
3.title:
- A Survey on Big Data Pre-processing
4.keywords:
- Big Data
- Data integration
- Data analysis
- Dimensionality reduction
- Feature extraction
- Data integrity
- big data processing
- data pre-processing
- data cleansing
- dimension reduction
- data quality
5.abstract:
- In this paper, we briefly introduce some basic concepts and characteristics of big
  data. We are surrounded by massive amount of data but starving for knowledge. In
  the era of Big Data, how to quickly obtain high-quality and valuable information
  from massive amounts of data has become an important research direction. Hence,
  we focus our attention to the data pre-processing which is a sub-content of the
  data processing workflow. In this paper, the four phases of data pre-processing,
  including data cleansing, data integration, data reduction, and data transformation,
  have been discussed. And different approaches for a variety of purposes have been
  presented, which show current methods and techniques need to be further modified
  in order to improve the quality of data before data analysis.
6.year:
- 2017
7.type_publication:
- inproceedings
8.doi:
- 10.1109/ACIT-CSII-BCD.2017.49
1.chave:
- 5170977
2.author:
- Xiao, Yongkang
- Ji, Cuiling
3.title:
- Management of Air Quality Monitor Data with Data Warehouse and GIS
4.keywords:
- Quality management
- Monitoring
- Data warehouses
- Geographic Information Systems
- Protection
- Cities and towns
- Prototypes
- Data visualization
- Air pollution
- Decision making
- air quality
- data warehouse
- web spatial OLAP system
- GIS
5.abstract:
- Air quality status is an important problem focused by all people. This paper utilizes
  Oracle 10 g to design and implement a prototype system of air quality data warehouse
  with the monitor data of 86 main cities from the year of 2000 to 2007 in China.
  To query and analyze the data in the data warehouse conveniently and effectively,
  it extends the star model to manage the spatial data with ArcGIS 9.0, and implements
  a Web spatial OLAP system to improve the ability of spatial analysis and visualization
  of traditional OLAP systems synchronously. Our work will help to evaluate air quality
  status, analyze its spatio-temporal characteristic, forecast and provide decision-making
  support for improvement of air quality.
6.year:
- 2009
7.type_publication:
- inproceedings
8.doi:
- 10.1109/CSIE.2009.280
1.chave:
- 4352911
2.author:
- Giansanti, D.
- Morelli, S.
- Macellari, V.
3.title:
- 'Experience at Italian National Institute of Health in the quality control in telemedicine:
  tools for gathering data information and quality assessing'
4.keywords:
- Quality control
- Telemedicine
- Medical services
- Quality assessment
- Application software
- Manufacturing processes
- Performance evaluation
- Documentation
- Certification
- Control systems
5.abstract:
- 'The authors proposed a set of tools and procedures to perform a Telemedicine Quality
  Control process (TM-QC) to be submitted to the telemedicine (TM) manufacturers.
  The proposed tools were: the Informative Questionnaire (InQu), the Classification
  Form (ClFo), the Technical File (TF), the Quality Assessment Checklist (QACL). The
  InQu served to acquire the information about the examined TM product/service; the
  ClFo allowed to classify a TM product/service as belonging to one application area
  of TM. The TF was intended as a technical dossier of product and forced the TM supplier
  to furnish the only requested documentation of its product, so to avoid redundant
  information. The QACL was a checklist of requirements, regarding all the essential
  aspects of the telemedical applications, that each TM products/services must be
  met. The final assessment of the TM product/service was carried out via the QACL,
  by computing the number of agreed requirements: on the basis of this computation,
  a Quality Level (QL) was assigned to the telemedical application. Seven levels were
  considered, ranging from the Basic Quality Level (QL1- B) to the Excellent Quality
  Level (QL7-E). The TM-QC process resulted a powerful tool to perform the quality
  control of the telemedical applications and should be a guidance to all the TM practitioners,
  from the manufacturers to the expert evaluators. The quality control process procedures
  proposed thus could be adopted in future as routine procedures and could be useful
  in the assessing the TM delivering into the National Health Service versus the traditional
  face to face healthcare services.'
6.year:
- 2007
7.type_publication:
- inproceedings
8.doi:
- 10.1109/IEMBS.2007.4352911
1.chave:
- 8669638
2.author:
- Li, Lianzhi
3.title:
- Evaluation Model of Education Service Quality Satisfaction in Colleges and Universities
  Dependent on Classification Attribute Big Data Feature Selection Algorithm
4.keywords:
- Education
- Correlation
- Data models
- Encyclopedias
- Big Data
- Mutual information
- Compounds
- Classification Attribute Big Data Feature Selection Algorithm
- Education Service Quality in Colleges and Universities
- Education Service in Colleges and Universities
- Satisfaction Evaluation
5.abstract:
- In view of the insufficiency in the education service quality in colleges and universities,
  a kind of evaluation model of the education service quality satisfaction in the
  colleges and universities that is dependent on the classification attribute big
  data feature selection algorithm is put forward in this paper based on the existing
  work. On the basis of detailed description of the model components, further study
  on the evaluation method of the proposed model for the education service quality
  satisfaction in the colleges and universities is carried out. Under the guidance
  of the evaluation model of the education service quality satisfaction in the colleges
  and universities, the method for the construction of the evaluation model of the
  education service quality satisfaction in the colleges and universities is studied
  with the orientation to the education service resources in the colleges and universities
  under the open big data environment. In addition, experimental verification is carried
  out on the basis of the evaluation data in the 360 Encyclopedia on the education
  service quality satisfaction in the colleges and universities. The experimental
  results show that the model and method put forward in this paper can effectively
  evaluate the quality of the education service in the colleges and universities.
6.year:
- 2019
7.type_publication:
- inproceedings
8.doi:
- 10.1109/ICITBS.2019.00160
1.chave:
- 8258267
2.author:
- Catarci, Tiziana
- Scannapieco, Monica
- Console, Marco
- Demetrescu, Camil
3.title:
- My (fair) big data
4.keywords:
- Big Data
- Pipelines
- Ontologies
- Metadata
- Google
- Quality-driven policies
- Big Data pipeline
- ontology-based quality checking
- Big Data confidentiality
5.abstract:
- Policy making has the strict requirement to rely on quantitative and high quality
  information. This paper will address the data quality issue for policy making by
  showing how to deal with Big Data quality in the different steps of a processing
  pipeline, with a focus on the integration of Big Data sources with traditional sources.
  In this respect, a relevant role is played by metadata and in particular by ontologies.
  Integration systems relying on ontologies enable indeed a formal quality evaluation
  of inaccuracy, inconsistency and incompleteness of integrated data. The paper will
  finally describe data confidentiality as a Big Data quality dimension, showing the
  main issues to be faced for its assurance.
6.year:
- 2017
7.type_publication:
- inproceedings
8.doi:
- 10.1109/BigData.2017.8258267
1.chave:
- 4586220
2.author:
- Tamagawa, Katsunori
- Kitsuregawa, Masaru
- Ikoma, Eiji
- Ohta, Tetsu
- Williams, Steve
- Koike, Toshio
3.title:
- An Advanced Quality Control System for the CEOP/CAMP In-Situ Data Management
4.keywords:
- Quality control
- Quality management
- Satellites
- Atmospheric modeling
- Energy management
- Water resources
- Information systems
- Predictive models
- Data assimilation
- CD recording
- Data management
- observers
- quality control
5.abstract:
- The Coordinated Enhanced Observing Period (CEOP) was proposed in 1997 as an initial
  step for establishing an integrated observation system for the global water cycle.
  The Enhanced Observing Period was conducted from October 2002 to December 2004,
  with satellite data, in-situ data, and model output data collected and available
  for integrated analysis. Under the framework of CEOP, the CEOP Asia-Australia Monsoon
  Project (CAMP) was organized and provided the in-situ dataset in the Asian region.
  CAMP included 13 different reference sites in the Asian monsoon region during Phase
  1 (October 2002 to December 2004). These reference sites were operated by individual
  researchers for their own research objectives. Therefore, the various sites' data
  had important differences in observational elements, data formats, recording intervals,
  etc. This usually requires substantial manual data processing to use these data
  for scientific research which consumes a great deal of researcher time and energy.
  To reduce the time and effort for data quality checking and format conversion, the
  CAMP Data Center (CDC) established a Web-based quality control (QC) system. This
  paper introduces this in-situ data management and quality control system for the
  Asian region data under the framework of CEOP.
6.year:
- 2008
7.type_publication:
- article
8.doi:
- 10.1109/JSYST.2008.927710
1.chave:
- 5720691
2.author:
- Qing, An
- Hongtao, Zhang
- Zhikun, Hu
- Zhiwen, Chen
3.title:
- A Compression Approach of Power Quality Monitoring Data Based on Two-dimension DCT
4.keywords:
- Power quality
- Voltage fluctuations
- Monitoring
- Discrete cosine transforms
- Data compression
- Wavelet transforms
- Discrete cosine transform
- Power quality monitoring
- Data compression
5.abstract:
- "A compression approach of power quality monitoring data based on two-dimension\
  \ Discrete Cosine Transform (DCT) is presented to deal with huge data about power\
  \ quality event detection. The monitoring data was truncated and recomposed in multiple\
  \ cycles to transform the one-dimension data into the two-dimension data, which\
  \ was a matrix in essence. The matrix was divided into some sub-blocks all of which\
  \ were 8\xD78 matrices. These matrices were performed two-dimension DCT. The elements\
  \ at the same location of all sub-matrices form a new matrix, and the elements were\
  \ at the equivalent energy level. The energy levels of new matrices were measured\
  \ by average energy, and quantitative matrix was obtained by a threshold of average\
  \ energy. The new matrices and quantitative matrix were used to represent the monitoring\
  \ data set."
6.year:
- 2011
7.type_publication:
- inproceedings
8.doi:
- 10.1109/ICMTMA.2011.12
1.chave:
- 897710
2.author:
- Domijan, A.
- Song, Z.
- Baptista, G.N.
- Montenegro, A.
- Wang, X.
- Gurlaskie, G.T.
- Mattern, K.E.
3.title:
- Power quality monitoring and analysis in a customized power industrial area using
  the FRIENDS concept
4.keywords:
- Power quality
- Monitoring
- Power system reliability
- Electrical equipment industry
- Voltage
- Communication system control
- Power system harmonics
- Control systems
- Surges
- Costs
5.abstract:
- As one of the steps towards development of a FRIENDS (flexible reliable and intelligent
  electric energy delivery system) network for improving power quality, the concept
  of a customized power industrial area (CPIA) is being undertaken. The first phase
  of this development involves power quality monitoring and analysis as presented
  in this paper. Data collected from power quality records installed at various points
  in a customized power industrial area are processed to identify the causes and effects
  of various disturbances on power quality and to identify the power quality needs
  of facilities with the customized power industrial area.
6.year:
- 2000
7.type_publication:
- inproceedings
8.doi:
- 10.1109/ICHQP.2000.897710
1.chave:
- 7821610
2.author:
- Kush, Ashwani
- Hwang, C.
- Dattana, Vishal
3.title:
- Big data analytics on MANET routing standardization using quality assurance metrics
4.keywords:
- Routing protocols
- Routing
- Big data
- Mobile ad hoc networks
- Quality assurance
- AODV
- Big Data
- PDR
- Metrics
- Quality
5.abstract:
- An ad hoc network is a collection of wireless mobile nodes dynamically forming a
  temporary network without the use of any existing network infrastructure or centralized
  administration. Routing in ad-hoc network is a challenging issue. Big data analytics
  have been suggested for proper evaluation and decision making in routing and placement
  of ad hoc network nodes. This Paper analyses the performance of AODV and DSR routing
  protocols for the quality assurance metrics. The performance differentials of AODV
  and DSR protocols are analyzed using NS-2 which is the main network simulator, NAM
  (Network Animator) and compared in terms of scales applied on Packet Delivery Ratio
  (PDR), in different environments specified by varying pause time, speed and number
  of nodes.
6.year:
- 2016
7.type_publication:
- inproceedings
8.doi:
- 10.1109/FTC.2016.7821610
1.chave:
- 9828956
2.author:
- Cao, Jie
- Zhang, Ju
- Lin, Xiao
- Sun, An
3.title:
- Design and Implementation of a Perioperative Medical Data Quality Management Platform
4.keywords:
- Systematics
- Data integrity
- Soft sensors
- Data preprocessing
- Surgery
- Machine learning
- Quality control
- data quality management
- perioperative medical data
- temporal logic
5.abstract:
- At present, there are more than 60 million hospitalized surgeries each year in China,
  and hundreds of millions of medical data records have been accumulated. The diversity,
  speed and other characteristics make it confounding for perioperative medical data
  to comply with consistent standards, resulting in widespread quality problems. Many
  issues escape simple inspections because the data generated for surgeries are from
  multiple data streams. Hence perioperative medical data quality management platform
  is designed in this paper to unite data from multiple sources and address issues
  discovered from cross-referencing. By representing cross-referencing data rules
  with temporal logic, it implements a comprehensive work platform for data quality
  inspection, data quality control and data annotation of perioperative medical data.
6.year:
- 2022
7.type_publication:
- inproceedings
8.doi:
- 10.1109/ICICSE55337.2022.9828956
1.chave:
- 8365994
2.author:
- Gschwandtner, Theresia
- Erhart, Oliver
3.title:
- 'Know Your Enemy: Identifying Quality Problems of Time Series Data'
4.keywords:
- Data integrity
- Time series analysis
- Data visualization
- Task analysis
- Prototypes
- Bars
- Taxonomy
- data quality
- data profiling
- time series
- visual analytics
5.abstract:
- Sensible data analysis requires data quality control. An essential part of this
  is data profiling, which is the identification and assessment of data quality problems
  as a prerequisite for adequately handling these problems. Differentiating between
  actual quality problems and unusual, but valid data values requires the "human-in-the-loop"
  through the use of visual analytics. Unfortunately, existing approaches for data
  profiling do not adequately support the special characteristics of time, which is
  imperative to identify quality problems in time series data - a data type prevalent
  in a multitude of disciplines. In this design study paper, we outline the design,
  implementation, and evaluation of "Know Your Enemy" (KYE) - a visual analytics approach
  to assess the quality of time series data. KYE supports the task of data profiling
  with (1) predefined data quality checks, (2) user-definable, customized quality
  checks, (3) interactive visualization to explore and reason about automatically
  detected problems, and (4) the visual identification of hidden quality problems.
6.year:
- 2018
7.type_publication:
- inproceedings
8.doi:
- 10.1109/PacificVis.2018.00034
1.chave:
- 9212069
2.author:
- Schmihing, Frederik
- Schindler, Matthias
- Jochem, Roland
3.title:
- Paradigm change towards Visual Analytics for data-driven quality improvement in
  highly flexible production systems
4.keywords:
- Training
- Lean production
- Data analysis
- Visual analytics
- Key performance indicator
- Data visualization
- Complexity theory
- production
- quality
- visual analytics
- visualization
- self-service
5.abstract:
- Especially in highly flexible production systems, quality management is confronted
  with immense challenges. Data Analytics is regarded as a technology for coping with
  increasing complexity. Currently a lack of value-added orientation impairs the effectiveness
  of such initiatives. This is because the workers do not have the capabilities to
  process Data Analytics. One approach is to train these workers to enable them for
  this new technology. These experts are highly busy with their core tasks, so that
  the other approach is to reduce the complexity for the user of Analytics by providing
  an interface with a high and interactive usability to have a minimum of training
  efforts. Today the usage of Visual Management within the lean production paradigms
  is used to provide Key Performance Indicators and other static information to the
  workers in an analogue or even digital way. The main problem is that there is no
  possibility for the worker to analyze why Key Performance Indicators changed and
  to find the root causes for the development so that they can improve their processes
  based on the analysis they have done. This paper shows a concept how Visual Analytics
  can be applied to enable production and quality experts to perform their analysis
  directly at the value-added process and thus continuously improve process and product
  quality.
6.year:
- 2020
7.type_publication:
- inproceedings
8.doi:
- 10.1109/ETFA46521.2020.9212069
1.chave:
- 4424178
2.author:
- Bilik, Petr
- Koval, Ludvik
- Hula, Jiri
3.title:
- Modular system for distributed power quality monitoring
4.keywords:
- Power quality
- Monitoring
- Instruments
- Data acquisition
- IEC standards
- Power measurement
- Voltage
- Frequency
- Hardware
- Application software
- Power Quality
- Virtual Instrumentation
- EN50160
- IEC 61000-4-30
- IEC 61000-4-7
- IEC 61000-4-15
- EDF
5.abstract:
- BK-ELCOM is a modular HW & SW platform for power quality monitoring and analysis
  based on virtual instrumentation technology. The analyzer of BK-ELCOM product line
  is based on PC hardware equipped by 16-bit NI data acquisition board running the
  instrument firmware fully written in LabVIEWtrade 8.2. Implemented algorithms follow
  the requirements of the latest power quality standards like IEC61000-4-30, IEC61000-4-15,
  IEC61000-4-7, EN 50160. Newly developed data storage format EDF (Extended Data Format)
  brings open platform for storing any type data for power quality analyzer. EDF brings
  also new possibilities for the postprocessing software package.
6.year:
- 2007
7.type_publication:
- inproceedings
8.doi:
- 10.1109/EPQU.2007.4424178
1.chave:
- 5538292
2.author:
- Su, Ying
- Al-Hakim, Latif
3.title:
- Intelligent control model for Checking data quality in hospital process management
4.keywords:
- Intelligent control
- Hospitals
- Quality management
- Automatic control
- Control systems
- Automation
- Electronic mail
- Information systems
- Design for disassembly
- Mathematical model
- Information Gap
- Intelligent Control
- hospital process
- Checking Mechanism
- Information Quality
- Data Quality
5.abstract:
- Hospital process management ought to deliver better value in terms of end-to-end
  services. Making managerial decisions based on electronic data generated by the
  Hospital Information System (HIS) to be more easily manipulated and destroyed than
  paper document. It is therefore important for auditors to assure that the information
  in HIS is well-controlled and high quality. This research aims to develop a control
  model, namely the Information Gap Checking Mechanism (IGAP Checking Mechanism),
  to automatically check the information gap between computerized process flows and
  workflow. This study also justifies the feasibility of IGAP-Checking Mechanism by
  providing a real case study. The result indicates that the IGAP-Checking Mechanism
  can assist the case hospital in resolving information quality problems that have
  occurred in its HIS and can also provide value service for the hospital.
6.year:
- 2010
7.type_publication:
- inproceedings
8.doi:
- 10.1109/ICINDMA.2010.5538292
1.chave:
- 867593
2.author:
- Santoso, S.
- Lamoree, J.D.
3.title:
- 'Power quality data analysis: from raw data to knowledge using knowledge discovery
  approach'
4.keywords:
- Power quality
- Data analysis
- Instruments
- Extraterrestrial measurements
- Power measurement
- Voltage
- Memory
- Computerized monitoring
- Data mining
- Energy capture
5.abstract:
- Power quality instrumentation has advanced significantly and allows continuous monitoring
  and the capability of capturing various power quality measurements. As a result
  more and more data is being collected, however, there is no practical method to
  conveniently convert the collected raw data into specific knowledge desired by end-users.
  In this panel session, a method of converting raw data into knowledge using the
  so-called knowledge discovery approach is presented. The motivation and background
  to automating the data converting process along with a real-world example implemented
  in an industrial power monitoring system is presented.
6.year:
- 2000
7.type_publication:
- inproceedings
8.doi:
- 10.1109/PESS.2000.867593
1.chave:
- 4099171
2.author:
- Crout, Richard
- Conlee, Don
3.title:
- 'Quality Control of Minerals Management Service - Oil Company ADCP Data at NDBC:
  A Successful Partnership Implementation'
4.keywords:
- Quality control
- Minerals
- Quality management
- Petroleum
- Production
- Industrial control
- Government
- Acoustic beams
- Oceans
- Oil drilling
5.abstract:
- The Minerals Management Service (MMS) requires that deep water oil drilling and
  production platforms in the northern Gulf of Mexico collect and provide current
  profile data to the National Data Buoy Center (NDBC). NDBC processes and displays
  the resulting currents on the NDBC website. NDBC has recently implemented quality
  control algorithms agreed upon by industry and the government. The resulting imagery
  and data, including quality control flags, are available on the publicly available
  NDBC website. The quality control algorithms and flags are presented and comparisons
  of the resulting files are described. Oil companies must collect current profile
  data when drilling wells or operating production platforms in water greater than
  400 meters deep. They are required to collect the data at 20 minute intervals and
  transmit the data via FTP to NDBC. The data are received, decoded, and quality controlled
  at NDBC. The current profiles are then formatted in TEmperature Salinity and Current
  (TESAC) messages and transmitted over the Global Telecommunications System (GTS).
  The data are also viewed over the NDBC website as columnar listings and current
  vector stick plots. In order to determine the quality control algorithms for the
  current profiles, a committee of oil company, industry, and government representatives
  determined an approach that includes both individual bin (depth level) and profile
  algorithms. The algorithms take advantage of the fact that the Teledyne RDI Acoustic
  Doppler Current Profiler (ADCP) collects error velocity, percent good statistics
  for 3 and 4 beams, and correlation matrices and echo amplitudes for each beam. The
  algorithms described in this presentation were then implemented and flags generated
  for each quality control test. A total of nine flags are assigned within the NDBC
  database. The flags indicate good data (3), suspect data (2), or bad (1) data. Only
  bad data are not reproduced or plotted on the NDBC real-time webpage. Results from
  the implementation are being reviewed, but a quick look indicates that the algorithms
  are returning accurate descriptions of the ADCP data. The stick plots of ocean current
  with depth are much "cleaner" following the quality control implementation. The
  implementation of the quality control algorithms was delayed by Hurricanes Katrina
  and Rita, which impacted both the NDBC and the oil industry in the Gulf of Mexico.
  NDBC is now resubmitting past data files through the quality control algorithms
  to insure that all data at NDCB have been quality controlled. The results of this
  effort (including the quality control algorithms) are being shared with Integrated
  Ocean Observing System (IOOS) partners in an effort to standardize quality control
  of oceanographic data
6.year:
- 2006
7.type_publication:
- inproceedings
8.doi:
- 10.1109/OCEANS.2006.307072
1.chave:
- 9540154
2.author:
- Buelvas, Julio
- Avila, Fernando
- Gaviria, Natalia
- Munera, Danny
3.title:
- Data Quality Estimation in a Smart City's Air Quality Monitoring IoT Application
4.keywords:
- Smart cities
- Data integrity
- Estimation
- Tools
- Air quality
- Sensor systems
- Internet of Things
- Smart City
- Internet of Things
- Air Quality
- Environmental Monitoring
- Low-cost Sensor
- Data Quality
5.abstract:
- With the upcoming growth of the Internet of Things (IoT), which is translated into
  millions of interconnected devices reporting a high volume of data coming from heterogeneous
  sources (sensors), it is necessary to assess the confidence of data in order to
  provide the system with trustable information that can be used to get real insights
  from the physical world and thus take proper decisions or actions over it. Having
  in mind that ensuring data quality is key to ease user engagement, acceptance of
  IoT services and large scale deployments [1], a new critical issue arises which
  is related to the quality of the data in IoT. Some applications might have a different
  definitions and indicators for data quality (DQ) and thus different threshold for
  acceptance of the data. In this work, we explore a smart city application in the
  field of environmental monitoring and identify the related DQ indicators that apply
  within this context. Our approach is evaluated over a real dataset retrieved from
  SIATA's citizen scientist low-cost sensor network, an air quality monitoring system
  that can be encompassed within the IoT paradigm and that is composed by more than
  200 nodes deployed all over the Aburra Valley in Antioquia, Colombia. The results
  show that feasibility assessing data quality and importance data quality awareness
  for an IoT application, as a tool for it to take proper actions on the real world.
  Our approach is evaluated over a real dataset retrieved from SIATA's citizen scientist
  low-cost sensor network, an air quality monitoring system that can be encompassed
  within the IoT paradigm and that is composed by more than 200 nodes deployed all
  over the Aburra Valley in Antioquia, Colombia. The results show that feasibility
  assessing data quality and importance data quality awareness for an IoT application,
  as a tool for it to take proper actions on the real world.
6.year:
- 2021
7.type_publication:
- inproceedings
8.doi:
- 10.1109/SCLA53004.2021.9540154
1.chave:
- 7364064
2.author:
- Becker, David
- King, Trish
- McMullen, Bill
3.title:
- Big data, big data quality problem
4.keywords:
- Big data
- Biomedical imaging
- Personnel
- Complexity theory
- Bioinformatics
- Process control
- Instruments
- Big Data
- Data Quality
- Returns to Scale
5.abstract:
- A USAF sponsored MITRE research team undertook four separate, domain-specific case
  studies about Big Data applications. Those case studies were initial investigations
  into the question of whether or not data quality issues encountered in Big Data
  collections are substantially different in cause, manifestation, or detection than
  those data quality issues encountered in more traditionally sized data collections.
  The study addresses several factors affecting Big Data Quality at multiple levels,
  including collection, processing, and storage. Though not unexpected, the key findings
  of this study reinforce that the primary factors affecting Big Data reside in the
  limitations and complexities involved with handling Big Data while maintaining its
  integrity. These concerns are of a higher magnitude than the provenance of the data,
  the processing, and the tools used to prepare, manipulate, and store the data. Data
  quality is extremely important for all data analytics problems. From the study's
  findings, the "truth about Big Data" is there are no fundamentally new DQ issues
  in Big Data analytics projects. Some DQ issues exhibit return-s-to-scale effects,
  and become more or less pronounced in Big Data analytics, though. Big Data Quality
  varies from one type of Big Data to another and from one Big Data technology to
  another.
6.year:
- 2015
7.type_publication:
- inproceedings
8.doi:
- 10.1109/BigData.2015.7364064
1.chave:
- 8109143
2.author:
- Chengzan, Li
- Yanfei, Hou
- Jianhui, Li
- Lili, Zhang
3.title:
- 'ScienceDB: A Public Multidisciplinary Research Data Repository for eScience'
4.keywords:
- Metadata
- Business
- Data visualization
- Data mining
- Distributed databases
- Collaboration
- Computer architecture
- research data repository
- technical framework
- multidiscipline
- data recommendation
- data collaboration
- open data
5.abstract:
- Research data repositories are necessary infrastructures that ensure the data generated
  for research are accessible, stable, reliable, and reusable. Based on years of accumulated
  data work experience, the Computer Network Information Center of the Chinese Academy
  of Sciences has built a multi-disciplinary data repository ScienceDB for research
  users and teams using its big data storage, analysis and computing environments.
  This paper firstly introduces the motivation to develop ScienceDB and gives a profile
  to it. Then the overall technical framework of ScienceDB is introduced, and the
  key technologies such as the support for multidiscipline extensibility, data collaboration
  and data recommendation are analyzed deeply. And then this paper presents the functions
  and features of ScienceDB's current version and discusses some issues such as its
  data policy, data quality assurance measures, and current application status. Finally,
  it summarizes and puts forward that it needs to carry out more in-depth research
  and practice of ScienceDB in order to meet the higher requirements of eScience in
  terms of thorough data association and fusion, data analysis and mining, data evaluation,
  and so on.
6.year:
- 2017
7.type_publication:
- inproceedings
8.doi:
- 10.1109/eScience.2017.38
1.chave:
- 9700829
2.author:
- Cheng, Wei
- Liu, Kun
- Wang, Shenliang
- Li, Li
3.title:
- Design and Implementation of GIS Basic Data Quality Management Tools for Power Network
4.keywords:
- Seminars
- Data integrity
- Soft sensors
- Decision making
- Production
- Power grids
- Personnel
- GIS
- power grid
- basic data
5.abstract:
- The GIS based data quality is the premise of all data processing and analysis, and
  it has great significance for carrying out GIS business applications and improving
  the level of practical system. However, due to the adjustment of the platform model,
  the new and old system data migration, wrong data source input and other reasons,
  the power grid GIS basic data quality issues are serious. In order to solve the
  problem of quality management of stock data, this paper designs and implements a
  GIS data quality management tool based on graph logic processing algorithm, which
  includes data source management, configuration management, query management and
  so on. Test results show that the use of the tool technology and optimization of
  the relevant management process can continuously improve the power grid GIS based
  data quality, and enhance the practical application of GIS large data.
6.year:
- 2022
7.type_publication:
- inproceedings
8.doi:
- 10.1109/SCSET55041.2022.00085
1.chave:
- 8290178
2.author:
- Mukwakungu, S.
- Mbohwa, C.
3.title:
- The impact and effectiveness of participating in external quality assurance programmes
  in quality management and improvement at a local institute medical laboratory, south
  africa
4.keywords:
- Medical diagnostic imaging
- ISO Standards
- Total quality management
- Testing
- Quality assessment
- Quality
- Total Quality Management
- External Quality Assessment
5.abstract:
- This study was conducted in a Medical Laboratory in Johannesburg, South Africa,
  to evaluate the effectiveness and impact of participating in External Quality Assurance
  (EQA) programs towards improving the correctness of lab results and continuous quality
  improvement. The study followed a quantitative approach whereby survey questionnaires
  were emailed and handed out to laboratory personnel. The participant's responses
  were summarized and analysed using frequency tables and histograms. The data analysis
  results indicated that the EQA programs play a vital role in quality management
  and improvement. Most participants indicated that they understood the role the EQA
  programs play and felt that it is necessary for a medical laboratory to participate
  in such programs, coupled with other quality assurance and quality control procedures
  such as IQC, daily QC procedures, corrective action and continuous education. This
  research showed that EQA plays a vital role in the correct interpretation and reporting
  of the lab results.
6.year:
- 2017
7.type_publication:
- inproceedings
8.doi:
- 10.1109/IEEM.2017.8290178
1.chave:
- 8697192
2.author:
- none
3.title:
- IEEE Recommended Practice for Power Quality Data Interchange Format (PQDIF)
4.keywords:
- IEEE Standards
- Power distribution
- Power quality
- Power measurement
- File systems
- Information exchange
- data interchange
- file format
- IEEE 1159.3
- measurement
- monitoring
- power quality
- PQDIF
5.abstract:
- A file format suitable for exchanging power quality related measurement and simulation
  data in a vendor independent manner is defined in this recommended practice. The
  format is designed to represent all power quality phenomena identified in IEEE Std
  1159-2009, IEEE Recommended Practice on Monitoring Electric Power Quality, other
  power related measurement data, and is extensible to other data types as well. The
  recommended file format utilizes a highly compressed storage scheme to help reduce
  disk space and transmission times. The utilization of Globally Unique Identifiers
  (GUID) to represent each element in the file permits the format to be extensible
  without the need for a central registration authority.
6.year:
- 2019
7.type_publication:
- article
8.doi:
- 10.1109/IEEESTD.2019.8697192
1.chave:
- 9678209
2.author:
- Yalaoui, Mehdi
- Boukhedouma, Saida
3.title:
- 'A survey on data quality: principles, taxonomies and comparison of approaches'
4.keywords:
- Data integrity
- Taxonomy
- Standards organizations
- Decision making
- Organizations
- Big Data
- Data models
- Data Quality
- Big Data
- Quality Dimensions
- Quality Metrics
- Metamodel
- Assessment process
- Improvement
5.abstract:
- "Nowadays, data generation keeps increasing exponentially due to the emergence of\
  \ the Internet of Things (IoT) and Big data technologies. The manipulation of such\
  \ Big amount of data becomes more and more difficult because of its size and its\
  \ variety. For better governance of organizations (decision making, data analysis,\
  \ earnings increase \u2026), data quality and data governance at present of Big\
  \ data are two major pillars for the design of any system handling data within the\
  \ organization. This explains the number of researches conducted as it constitutes\
  \ a research subject with several gaps and opportunities. Many works were conducted\
  \ to define and standardize Data Quality (DQ) and its dimensions, others were directed\
  \ to design and propose data quality assessment and improvement models or frameworks.\
  \ This work aims to recall the data quality principles starting by the needed background\
  \ knowledge, then identify and compare the relevant taxonomies existing in the literature,\
  \ next surveys and compares the available Data quality assessment and improvement\
  \ approaches. After that, we propose a metamodel highlighting the main concepts\
  \ of DQ assessment and we describe a generic process for DQ assessment and improvement.\
  \ Finally, we evoke the main challenges in the field of DQ before and after the\
  \ emergence of Big Data."
6.year:
- 2021
7.type_publication:
- inproceedings
8.doi:
- 10.1109/ICISAT54145.2021.9678209
1.chave:
- 8667300
2.author:
- Lee, Doyoung
3.title:
- 'Big Data Quality Assurance Through Data Traceability: A Case Study of the National
  Standard Reference Data Program of Korea'
4.keywords:
- Standards
- Uncertainty
- Big Data
- Metrology
- Reliability
- Measurement uncertainty
- Biomedical measurement
- Big data
- data quality
- data traceability
- metrology
- standard reference data
- uncertainty
5.abstract:
- "In the era of big data, the scientific and social demand for quality data is aggressive\
  \ and urgent. This paper sheds light on the expanded role of metrology of verifying\
  \ validated procedures of data production and developing adequate uncertainty evaluation\
  \ methods to ensure the trustworthiness of data and information. In this regard,\
  \ I explore the mechanism of the national standard reference data (SRD) program\
  \ of Korea, which connects various scientific and social sectors to metrology by\
  \ applying useful metrological concepts and methods to produce reliable data and\
  \ convert such data into national standards. In particular, the changing interpretation\
  \ of metrological key concepts, such as \u201Cmeasurement,\u201D \u201Ctraceability,\u201D\
  \ and \u201Cuncertainty,\u201D will be explored and reconsidered from the perspective\
  \ of data quality assurance. As a result, I suggest the concept of \u201Cdata traceability\u201D\
  \ with \u201Cthe matrix of data quality evaluation\u201D according to the elements\
  \ of a data production system and related evaluation criteria. To conclude, I suggest\
  \ social and policy implications for the new role of metrology and standards for\
  \ producing and disseminating reliable knowledge sources from big data."
6.year:
- 2019
7.type_publication:
- article
8.doi:
- 10.1109/ACCESS.2019.2904286
1.chave:
- 404034
2.author:
- Wang, R.Y.
- Storey, V.C.
- Firth, C.P.
3.title:
- A framework for analysis of data quality research
4.keywords:
- Data analysis
- Pulp manufacturing
- Research and development management
- Research and development
- Production
- Databases
- Organizing
- Cost function
- Personnel
- Law
5.abstract:
- 'Organizational databases are pervaded with data of poor quality. However, there
  has not been an analysis of the data quality literature that provides an overall
  understanding of the state-of-art research in this area. Using an analogy between
  product manufacturing and data manufacturing, this paper develops a framework for
  analyzing data quality research, and uses it as the basis for organizing the data
  quality literature. This framework consists of seven elements: management responsibilities,
  operation and assurance costs, research and development, production, distribution,
  personnel management, and legal function. The analysis reveals that most research
  efforts focus on operation and assurance costs, research and development, and production
  of data products. Unexplored research topics and unresolved issues are identified
  and directions for future research provided.<>'
6.year:
- 1995
7.type_publication:
- article
8.doi:
- 10.1109/69.404034
1.chave:
- 8589081
2.author:
- "M\xFCnzberg, Alexander"
- Sauer, Janina
- Hein, Andreas
- "R\xF6sch, Norbert"
3.title:
- The use of ETL and data profiling to integrate data and improve quality in food
  databases
4.keywords:
- Databases
- Food products
- Data integrity
- Measurement
- ISO Standards
- Europe
- Conferences
- food data
- food information service
- ETL process
- data warehousing
- data profiling
- data quality improvement
- data quality metrics
5.abstract:
- This paper focuses on integrating food data sources into a central database using
  extract, transform and load processing and the subsequent data quality enhancement.
  The obtained data will be transmitted by a food data web service to certain health
  apps for further use. Furthermore, it is planned to identify inconsistent, incorrect,
  duplicate and incomplete data using methods of data profiling so that they can be
  corrected. In order to quantify the data quality purposefully and appropriately,
  certain quality metrics were used. These metrics were calculated and evaluated using
  random test data selected from the food data.
6.year:
- 2018
7.type_publication:
- inproceedings
8.doi:
- 10.1109/WiMOB.2018.8589081
1.chave:
- 273780
2.author:
- Dabbs, W.W.
- Sabin, D.D.
- Grebe, T.E.
- Mehta, H.
3.title:
- Probing power quality data
4.keywords:
- Power quality
- Power system management
- Data analysis
- Monitoring
- Instruments
- Databases
- Software systems
- Energy management
- Quality management
- Steady-state
5.abstract:
- A power quality problem can best be described as any variation in the electric power
  service resulting in misoperation or failure of end-use equipment. As customers
  seek to increase utilisation and efficiency, utilities strive to better understand
  power quality events and their effects on these customers. Utilities are creating
  special programs and organisations to deal with customer power quality needs. A
  problem shared by both parties is the need for improved methods in the collection,
  analysis, and reporting of very large amounts of measured power quality data. This
  article presents one method currently being used to characterise power quality levels
  on distribution systems throughout the United States. The method utilises a number
  of software systems, one of which is the data management and analysis program described
  in the article.<>
6.year:
- 1994
7.type_publication:
- article
8.doi:
- 10.1109/67.273780
1.chave:
- 9845662
2.author:
- Olaniyan, Folashikemi
- Owoseni, Adebowale
3.title:
- 'Toward Improved Data Quality in Public Health: Analysis of Anomaly Detection Tools
  applied to HIV/AIDS Data in Africa'
4.keywords:
- Analytical models
- Data integrity
- Africa
- Programming
- Data models
- Reliability
- Public healthcare
- Data quality review
- anomaly detection
- data quality assessment
- public health
- low-income country
5.abstract:
- The study examined the data quality efficiency of the WHO Data Quality Review (DQR)
  toolkit and PyCaret anomaly detection algorithms. The tools were applied to the
  African HIV/AIDS data (2015-2021) extracted from a public data repository (data.pepfar.gov).
  The research outcome suggests that unsupervised anomaly detection algorithms could
  complement the efficiency of the WHO DQR toolkit and improve Data Quality Assessment
  (DQA). In particular, the study showed that anomaly detection algorithms through
  python programming provide a more straightforward and more reliable process for
  detecting data inconsistencies, incompleteness, and timeliness appears more accurate
  than the WHO tool. Consequently, the study contributed to ongoing debates on improving
  health data quality in low-income African countries.
6.year:
- 2022
7.type_publication:
- inproceedings
8.doi:
- 10.23919/IST-Africa56635.2022.9845662
1.chave:
- 5403316
2.author:
- Ruan, Hongyong
- Yu, Dongjin
- Cao, Yong
3.title:
- Research and Implementation of the Platform for Analyzing Data Quality
4.keywords:
- Data analysis
- Information analysis
- Management information systems
- Database systems
- Quality assessment
- Visual databases
- Computer science
- Data engineering
- Computer security
- Data security
- data quality
- metamodel
- analysis indexes
- information system
5.abstract:
- With more and more redundant and dirty data accumulating in information systems
  nowadays, the problem of data quality is getting increasingly urgent. People usually
  analyze the data quality through the tools provided by the database management systems,
  which however bring much inconvenience and inefficiency. This article introduces
  a novel integrated platform for the data quality analysis, which loads, compares,
  and verifies the business data through the predefined regular expressions and the
  metamodel. Moreover, it presents a detailed analysis indexes containing the rules
  for evaluating the data quality. The implementation conducted in the labor market
  information system proves that the platform is quite applicable for the data quality
  analysis.
6.year:
- 2009
7.type_publication:
- inproceedings
8.doi:
- 10.1109/WCSE.2009.669
1.chave:
- 8463402
2.author:
- Tamboli, Roopak
- Reddy, M.
- Kara, Peter
- Martini, Maria
- Channappayya, Sumohana
- Jana, Soumya
3.title:
- A High-angular-resolution Turntable Data-set for Experiments on Light Field Visualization
  Quality
4.keywords:
- Cameras
- Three-dimensional displays
- Image resolution
- Calibration
- Visualization
- Quality assessment
- Light field
- turntable data-set
- angular resolution
- quality assessment
5.abstract:
- In this paper, we present a high-angular-resolution data-set created using a turntable
  arrangement. Seven distinct objects, positioned on an automated turntable, were
  captured from three camera positions for every half degree of rotation, generating
  720 images for each camera position. For each object, the camera positions were
  registered to the coordinate system of the middle camera. Intrinsic parameters of
  the camera were also estimated. A data-set of this kind is instrumental for research
  in a variety of areas, such as light field visualization, manifold learning, visual
  quality assessment, evaluation of preferred object orientation etc. Due to the availability
  of three-view stereo, this data-set could be useful for studying view interpolation
  techniques.
6.year:
- 2018
7.type_publication:
- inproceedings
8.doi:
- 10.1109/QoMEX.2018.8463402
1.chave:
- 9347363
2.author:
- Xie, Hanyang
- Hu, Xiaoqi
- Peng, Zewu
- Jiang, Jiang
- Wen, Bojian
- Yang, Qiuyong
3.title:
- Energy System Time Series Data Quality Maintenance System Based on Data Mining Technology
4.keywords:
- Data integrity
- Time series analysis
- System integration
- Maintenance engineering
- Internet
- Reliability
- Decision trees
- energy system time series data
- data mining
- decision tree
- outlier detection
- data quality maintenance
5.abstract:
- With improvement of science and technology, energy system becomes more intelligent
  and data scale becomes larger, so an efficient and reliable quality data management
  has implications for energy system. This paper proposes a grid time series data
  quality maintenance system based on data mining technology. In view of different
  characteristics of data structure in different systems, decision tree algorithm
  and data outlier detection methods are combined to improve efficiency of data detection
  while quickly locating issue type, which is convenient for data repair and improvement.
6.year:
- 2020
7.type_publication:
- inproceedings
8.doi:
- 10.1109/EI250167.2020.9347363
1.chave:
- 4746551
2.author:
- Yan, Hao
- Diao, Xing-chun
- Li, Kai-qi
3.title:
- Research on Information Quality Driven Data Cleaning Framework
4.keywords:
- Cleaning
- Data analysis
- Information management
- Quality management
- Software tools
- Seminars
- Information technology
- Technology management
- Engineering management
- Data engineering
- data cleaning framework
- information quality
5.abstract:
- Considering the limited extensibility and interactivity of the current data cleaning
  work, this paper proposes a new extensible and interactive data cleaning framework
  driven by information quality. The framework implements the formation of data quality
  analysis strategy, data transformation strategy and cleaning result assessment strategy.
  It also realizes the control of the cleaning process. The framework has the significant
  features of extensibility and interactivity.
6.year:
- 2008
7.type_publication:
- inproceedings
8.doi:
- 10.1109/FITME.2008.126
1.chave:
- 5556606
2.author:
- Lucas, Ana
3.title:
- 'Corporate data quality management: From theory to practice'
4.keywords:
- Quality management
- Data models
- Companies
- Context
- Process control
- data quality management
- framework
- data quality initiative
- case study
5.abstract:
- "It is now assumed that poor quality data is costing large amounts of money to corporations\
  \ all over the world. Although research on methods and techniques for data quality\
  \ assessment and improvement have begun in the early nineties of the past century\
  \ and being currently abundant and innovative, it is noted that the academic and\
  \ professional communities virtually have no dialogue, which turns out to be harmful\
  \ to both of them. The challenge of promoting the relevance in information systems\
  \ research, without compromising the necessary rigor, is still present in the various\
  \ disciplines of information systems scientific area, including the data quality\
  \ one. In this paper we present \u201Cdata as a corporate asset\u201D as a business\
  \ philosophy, and a framework for the concepts related to that philosophy, derived\
  \ from the academic and professional literature. According to this framework, we\
  \ present, analyze and discuss a single explanatory case study, developed in a fixed\
  \ and mobile telecommunications company, operating in one of the European Union\
  \ Countries. The results show that, in the absence of data stewardship roles, data\
  \ quality problems become more of an \"IT problem\" than typically is considered\
  \ in the literature, owing to Requirements Analysis Teams of the IS Development\
  \ Units, to become a \u201Cquality negotiator\u201D between the various stakeholders.\
  \ Other findings are their bottom-up approach to data quality management, their\
  \ biggest focus on motivating employees through innovative forms of communication,\
  \ which appears to be a critical success factor (CSF) for data quality management,\
  \ as well as the importance of a data quality champion leadership."
6.year:
- 2010
7.type_publication:
- inproceedings
8.doi:
- ''
1.chave:
- 8701948
2.author:
- Ida, Masaaki
3.title:
- Consideration on the variation of financial data of institutions for canonical correlation
  analysis
4.keywords:
- Correlation
- Education
- Data analysis
- Databases
- Correlation coefficient
- Data visualization
- Quality assurance
- data analysis
- canonical correlation analysis
- data variation
- financial data
- higher education institution
5.abstract:
- In these days, progress of e-government and spread of electrical data lead to the
  prevail of public open databases, and also lead to the large amount of data analysis
  applications applying to these official open data. With regard to data analysis
  method, Canonical Correlation Analysis, which is one of the basic data analysis
  method and also data visualization method, is becoming the requisite skill for data
  scientists in this Big Data era. This paper examines the open data of financial
  data of education institutions. Especially, we focus on the higher education institutions
  and their financial data. In addition, we examine the variation of data and its
  problem to the data analysis. We aim to apply this analysis method and the result
  of consideration for supporting the improvement of quality assurance of higher education
  institutions.
6.year:
- 2019
7.type_publication:
- inproceedings
8.doi:
- 10.23919/ICACT.2019.8701948
1.chave:
- 9669614
2.author:
- Pan, Weidong
- Xie, Jiadong
- Zhao, Yufeng
- Liu, Baoyan
- Hu, Kongfa
3.title:
- Research On the Data Quality Control Model of the Traditional Chinese Medicine Inpatient
  Medical Record Home Page Based on XGBoost
4.keywords:
- Correlation
- Dictionaries
- Hospitals
- Data integrity
- Quality control
- Predictive models
- Data models
- Traditional Chinese Medicine Inpatient Medical Record Home Page
- Data Quality Control
- XGBoost
- Correlation Analysis
- Model Construction
5.abstract:
- 'Objective: Designs a XGBoost-based data quality control model for the traditional
  chinese medicine (TCM) inpatient medical record home page. Exploring the method
  of data normalization on the TCM inpatient medical record home page. Methods: Taking
  the data on the TCM inpatient medical record home page of a hospital in Jiangsu
  Province as the original data. Using correlation analysis to filter out some data
  items that have a higher degree of correlation with the data items to be quality
  control. Establishing a XGBoost-based data quality control model for the TCM inpatient
  medical record home page. Using the hierarchical 10-fold cross-validation to evaluate
  the model. Results: The experimental results show that the accuracy rate of the
  model can reach 88.60%. Conclusion: The data quality control model on the TCM inpatient
  medical record home page is conducive to improving the quality of the data on the
  TCM inpatient medical record home page and provides data support for medical research.'
6.year:
- 2021
7.type_publication:
- inproceedings
8.doi:
- 10.1109/BIBM52615.2021.9669614
1.chave:
- 4154483
2.author:
- Yunus, Bahisham
- Li, Haiyu
3.title:
- Analysis of Power Quality Waveform for Data Transmission Efficiency over IEC 61850
  Communication Standard
4.keywords:
- Power quality
- Data communication
- IEC standards
- Communication standards
- Voltage fluctuations
- Monitoring
- Substations
- IP networks
- Protocols
- Bandwidth
- efficiency
- IEC 61850 standards
- power quality waveforms
- voltage sags
- wavelet transform
5.abstract:
- In power utilities, grid operators monitor power-quality phenomena such as voltage
  sag at several substations to obtain an overview of the general network state and
  evolution of power quality. Collections of these huge online data from disturbance
  recorders involved long and tedious downloading process, usage of Internet protocols
  consumed bandwidth and causes traffic congestion and collisions with other users
  of the wide area network. This paper describes an integrated application of power
  quality monitoring system for a modeled substation over the IEC 61850 standard developed
  in Java programming language. The application used wavelet compression technique
  for transmission of voltage sag waveform over IEC 61850 standard and then Internet
  to a control centre. Analysis done shows that the correct selection of mother wavelet
  can achieve over 90% data compression ability while preserving all significant power
  quality features required for power quality waveform analysis required by utilities.
6.year:
- 2006
7.type_publication:
- inproceedings
8.doi:
- 10.1109/PECON.2006.346639
1.chave:
- 5489992
2.author:
- "G\u01CE\u015Fp\u01CEresc, Gabriel"
3.title:
- Data compression of power quality disturbances using wavelet transform and spline
  interpolation method
4.keywords:
- Data compression
- Power quality
- Wavelet transforms
- Spline
- Interpolation
- Frequency
- Signal resolution
- Signal reconstruction
- Costs
- White noise
- data compression
- power quality disturbance
- Wavelet transform
- interpolation
5.abstract:
- In this paper is described a data compression technique based on wavelet decomposition
  and spline interpolation for power quality disturbance. The technique consist in
  signal decomposition, thresholding of wavelet transform coefficients, decimation
  of the last coefficient and signal reconstruction using spline interpolation for
  the last coefficient. The optimal order of Daubechies scaling function recommended
  in order to achieve the best compression ratio and comparative compression results
  using different types of power quality disturbances are also presented. This technique
  is suitable mainly for data acquired at high sample rates.
6.year:
- 2010
7.type_publication:
- inproceedings
8.doi:
- 10.1109/EEEIC.2010.5489992
1.chave:
- 8782595
2.author:
- Wang, Songyun
- Yuan, Jiabin
- Li, Xin
- Qian, Zhuzhong
- Arena, Fabio
- You, Ilsun
3.title:
- Active Data Replica Recovery for Quality-Assurance Big Data Analysis in IC-IoT
4.keywords:
- Nonvolatile memory
- Quality of service
- Data analysis
- Data centers
- Bandwidth
- Big Data
- Robustness
- Big data analysis
- data recovery
- IC-IoT
- NVM
- QoS improvement
5.abstract:
- QoS-aware big data analysis is critical in Information-Centric Internet of Things
  (IC-IoT) system to support various applications like smart city, smart grid, smart
  health, intelligent transportation systems, and so on. The employment of non-volatile
  memory (NVM) in cloud or edge system provides good opportunity to improve quality
  of data analysis tasks. However, we have to face the data recovery problem led by
  NVM failure due to the limited write endurance. In this paper, we investigate the
  data recovery problem for QoS guarantee and system robustness, followed by proposing
  a rarity-aware data recovery algorithm. The core idea is to establish the rarity
  indicator to evaluate the replica distribution and service requirement comprehensively.
  With this idea, we give the lost replicas with distinguishing priority and eliminate
  the unnecessary replicas. Then, the data replicas are recovered stage by stage to
  guarantee QoS and provide system robustness. From our extensive experiments and
  simulations, it is shown that the proposed algorithm has significant performance
  improvement on QoS and robustness than the traditional direct data recovery method.
  Besides, the algorithm gives an acceptable data recovery time.
6.year:
- 2019
7.type_publication:
- article
8.doi:
- 10.1109/ACCESS.2019.2932259
1.chave:
- 5287757
2.author:
- Hai-ying, Liu
3.title:
- Research on Process Quality Data Collection and Control Based on Computer Technology
4.keywords:
- Quality control
- Testing
- Production
- Computer aided manufacturing
- Statistical analysis
- Error correction
- Process control
- Computerized monitoring
- Application software
- Sampling methods
- Process quality
- Data collection and control
- Computer technology
5.abstract:
- Quality data collection processes and process parameters adjustment is the key element
  of Quality control processes. The content of process quality data collection and
  control include process monitoring and product quality testing. Process parameter
  monitoring can be achieved by measuring the characteristics of the related parameters.
  Process product quality testing can be achieved by direct measuring product quality
  characteristics in the process or processes interval. Process quality data collection
  and control system have been studied in this paper.
6.year:
- 2009
7.type_publication:
- inproceedings
8.doi:
- 10.1109/ICICTA.2009.383
1.chave:
- 8823437
2.author:
- Zeng, Yu-Ren
- Chang, Yue
- Fang, You
3.title:
- Data Visualization for Air Quality Analysis on Bigdata Platform
4.keywords:
- Data visualization
- Air quality
- Big Data
- Forecasting
- Monitoring
- Web servers
- Air Quality
- Big Data
- Forecasting
- Cloud Environment
- Data Visualization
5.abstract:
- With the advances of industry, air pollution is increasingly becoming serious, and
  most of governments in the world has deployed many devices to monitor daily air
  quality. Monitoring and forecasting of air quality has also become an important
  issue to improve the quality of people's lives. As far as we know, bad air quality
  does not only affect the health of the respiratory tract, it may but also even cause
  mental illness. Many researchers have investigated different approaches to work
  on air quality forecast, and the visualization of forecasting becomes important.
  In this paper, we present an architecture for visualizing forecasted air quality
  on a big data platform. We implemented an ETL (Extract-Transform-Load) based framework
  in the platform, which includes computing nodes and storage nodes. Computational
  nodes are used for data collection and for air quality forecasting over the next
  1 to 8 hours through machine learning and deep learning. Storage nodes are used
  to retrieve, analyze, and preprocess of collected data. We use the RESTful Web Service
  as an API, and finally we use the browser to get the data by predefined API and
  to present the forecasted and monitored results with Google Map API and D3 JavaScript
  library. It reveals that the visualization on big data framework can work well for
  air quality analysis.
6.year:
- 2019
7.type_publication:
- inproceedings
8.doi:
- 10.1109/ICSSE.2019.8823437
1.chave:
- 9391025
2.author:
- Wang, Fengling
- Wang, Han
- Xue, Liang
3.title:
- Research on Data Security in Big Data Cloud Computing Environment
4.keywords:
- Cloud computing
- Data security
- Data integrity
- Big Data
- Maintenance engineering
- Virtualization
- Information technology
- big data
- cloud computing
- data security
- big data cloud computing
- security policy
5.abstract:
- In the big data cloud computing environment, data security issues have become a
  focus of attention. This paper delivers an overview of conceptions, characteristics
  and advanced technologies for big data cloud computing. Security issues of data
  quality and privacy control are elaborated pertaining to data access, data isolation,
  data integrity, data destruction, data transmission and data sharing. Eventually,
  a virtualization architecture and related strategies are proposed to against threats
  and enhance the data security in big data cloud environment.
6.year:
- 2021
7.type_publication:
- inproceedings
8.doi:
- 10.1109/IAEAC50856.2021.9391025
1.chave:
- 8621924
2.author:
- Guntupally, Kavya
- Devarakonda, Ranjeet
- Kehoe, Kenneth
3.title:
- 'Spring Boot based REST API to Improve Data Quality Report Generation for Big Scientific
  Data: ARM Data Center Example'
4.keywords:
- Springs
- Databases
- Service-oriented architecture
- Data integrity
- Tools
- Servers
- Big Data
- auto configuration
- CRUD
- java framework
- service-oriented architecture
- REST
- spring boot
5.abstract:
- Web application technologies are growing rapidly with continuous innovation and
  improvements. This paper focuses on the popular Spring Boot [1] java-based framework
  for building web and enterprise applications and how it provides the flexibility
  for service-oriented architecture (SOA). One challenge with any Spring-based applications
  is its level of complexity with configurations. Spring Boot makes it easy to create
  and deploy stand-alone, production-grade Spring applications with very little Spring
  configuration. Example, if we consider Spring Model-View-Controller (MVC) framework
  [2], we need to configure dispatcher servlet, web jars, a view resolver, and component
  scan among other things. To solve this, Spring Boot provides several Auto Configuration
  options to setup the application with any needed dependencies. Another challenge
  is to identify the framework dependencies and associated library versions required
  to develop a web application. Spring Boot offers simpler dependency management by
  using a comprehensive, but flexible, framework and the associated libraries in one
  single dependency, which provides all the Spring related technology that you need
  for starter projects as compared to CRUD web applications. This framework provides
  a range of additional features that are common across many projects such as embedded
  server, security, metrics, health checks, and externalized configuration. Web applications
  are generally packaged as war and deployed to a web server, but Spring Boot application
  can be packaged either as war or jar file, which allows to run the application without
  the need to install and/or configure on the application server. In this paper, we
  discuss how Atmospheric Radiation Measurement (ARM) Data Center (ADC) at Oak Ridge
  National Laboratory, is using Spring Boot to create a SOA based REST [4] service
  API, that bridges the gap between frontend user interfaces and backend database.
  Using this REST service API, ARM scientists are now able to submit reports via a
  user form or a command line interface, which captures the same data quality or other
  important information about ARM data.
6.year:
- 2018
7.type_publication:
- inproceedings
8.doi:
- 10.1109/BigData.2018.8621924
1.chave:
- 5361869
2.author:
- Chen, Bing
- Wang, Beizhan
- Zheng, Chengman
- Hu, Xueqin
3.title:
- Research and Implementation of Information Quality Improvement
4.keywords:
- Information systems
- Quality management
- Decision making
- Information analysis
- Management information systems
- Quality assessment
- Information resources
- Artificial intelligence
- Software quality
- Standardization
- total data quality management
- goal-question-metric approach
- information quality
- metadata
- data quality
5.abstract:
- Information quality is the premise for scientific decision making. Along with the
  development of various types of information-sharing project, problems of information
  quality are increasingly apparent. This paper firstly analyzed the criteria of information
  quality from the relationship between information quality and data quality, and
  then proposed an improved scheme based on metadata to resolve the issue of information
  quality from the source. Finally, combined with the business of China Construction
  Bank this scheme has been implemented successfully. The practice proven that this
  scheme can reflect the system design changes and data quality real-timely and moreover
  provided some research direction for the improvement of information quality.
6.year:
- 2009
7.type_publication:
- inproceedings
8.doi:
- 10.1109/COINFO.2009.17
1.chave:
- 8622590
2.author:
- Rueda, Diego
- Vergara, Dahyr
- Reniz, David
3.title:
- 'Big Data Streaming Analytics for QoE Monitoring in Mobile Networks: A Practical
  Approach'
4.keywords:
- Quality of experience
- Big Data
- Monitoring
- Streaming media
- Real-time systems
- Tools
- Big Data Analytics
- customer experience management
- mobile networks
- quality of experience
- streaming data processing
5.abstract:
- Traditionally, Mobile Network Operators (MNOs) use a set of Key Performance Indicators
  (KPIs) to measure the quality offered to their customers. However, these KPIs do
  not reflect the quality perceived by the customers because they are high-level and
  network-based metrics. Instead, Quality of Experience (QoE) monitoring of the most
  common mobile applications can help MNOs to determine when and where customer experience
  is degraded. In this paper, a customized tool based on Big Data Streaming is proposed
  to solve the needs of customer experience monitoring in a real-life MNO and to overcome
  the challenges of processing a large amount of data collected in 3G and 4G mobile
  networks. Moreover, real-life case studies of value creation through Big Data Analytics
  for telecommunication industry are also defined. Results show that the streaming
  data processing enables new opportunities for the MNO to take actions focused on
  customer experience improvement in near real-time.
6.year:
- 2018
7.type_publication:
- inproceedings
8.doi:
- 10.1109/BigData.2018.8622590
1.chave:
- 7994799
2.author:
- Urrutia, Angelica
- Chavez, Emma
- Motz, Regina
- Gajardo, Rosa
3.title:
- An Ontology to Assess Data Quality Domains. A Case Study Applied to a Health Care
  Entity.
4.keywords:
- Ontologies
- Measurement
- IEEE transactions
- Medical services
- Decision support systems
- Databases
- Semantic Web
- Data quality
- Health informatics
- Ontologies
- Quality metrics
5.abstract:
- Data integration can extend scenarios in applications designed to fulfill the requirements
  of decision support systems. However, a critical aspect in database development
  is to determine the quality of the data sources. Thus, it is important to have an
  automatic mechanism to be able to measure the quality of the data source, where
  the level of quality depends on the amount of data with technical anomalies that
  exist in the upgrade process. This article aims to define a flexible process to
  asses data quality, which does not depend of the application domain, and has a declarative
  specification based on the use of domain ontologies and data quality metrics. As
  a prove of concepts a description of the use of the ontology and the possible metrics
  to use are proposed in a case study that applies to the health care domain. For
  the case a Semantic Web Rule Language (SWRL) was used.
6.year:
- 2017
7.type_publication:
- article
8.doi:
- 10.1109/TLA.2017.7994799
1.chave:
- 7840729
2.author:
- Xu, Yanan
- Zhu, Yanmin
3.title:
- 'When remote sensing data meet ubiquitous urban data: Fine-grained air quality inference'
4.keywords:
- Air quality
- Remote sensing
- Monitoring
- Tensile stress
- Sensors
- Urban areas
- Pollution measurement
- Remote sensing
- AOT
- air quality
- inference
- neural network
- tensor decomposition
5.abstract:
- With the growth of the economy, the air quality is becoming a serious issue, especially
  for those developing countries, such as China. Therefore, it is very important for
  the public and the government to access real-time air quality information. Unfortunately,
  the limited number of air quality monitoring stations is unable to provide fine-grained
  air quality information in a huge city, such as Beijing. One cost-effective approach
  for obtaining fine-grained air quality information is to infer air quality with
  those measured data at the monitoring stations. However, existing inference techniques
  have poor performance because of the extreme data sparsity problem (e.g., only 0.2%
  data are known). We observe that remote sensing has been a high-quality data source
  about urban dynamics. In this paper, we propose to integrate remote sensing data
  and ubiquitous urban data for air quality inference. There are two main challenges,
  i.e., data heterogeneity and incomplete remote sensing data. In response to the
  challenges, we propose a two-stage inference approach. In the first stage, we use
  the AOT remote sensing data and the meteorological data to infer the air quality
  values with an Artificial Neural Network (ANN). After this stage, we significantly
  reduce the percentage of empty cells in the tensor representing the spatio-temporal
  air quality values. In the second stage, we propose a tensor decomposition method
  to infer the complete set of air quality values. We use the spatial features (i.e.,
  road features and POI features) and the temporal features (i.e., meteorological
  features) as the constraints in the tensor decomposition process. Experiments with
  real data sets show that our approach has profound performance advantage over the
  state-of-the-art methods, such as U-Air.
6.year:
- 2016
7.type_publication:
- inproceedings
8.doi:
- 10.1109/BigData.2016.7840729
1.chave:
- 9433875
2.author:
- Tummala, Sudhakar
- Focke, Niels
3.title:
- Machine Learning Framework For Fully Automatic Quality Checking Of Rigid And Affine
  Registrations In Big Data Brain MRI
4.keywords:
- Training
- Magnetic resonance imaging
- Supervised learning
- Quality control
- Big Data
- Brain modeling
- Data models
- structural MRI
- big data
- machine learning
- quality control
- image registration
5.abstract:
- Rigid and affine registrations to a common template are the essential steps during
  pre-processing of brain structural magnetic resonance imaging (MRI) data. Manual
  quality control (QC) of these registrations is quite tedious if the data contains
  several thousands of images. Therefore, we propose a machine learning (ML) framework
  for fully automatic QC of these registrations via global and local computation of
  the similarity functions such as normalized cross-correlation, normalized mutual-information,
  and correlation ratio, and using these as features for training of different ML
  classifiers. To facilitate supervised learning, misaligned images are generated.
  A structural MRI dataset consisting of 215 subjects from autism brain imaging data
  exchange is used for 5-fold cross-validation and testing. ML models based on local
  costs performed better than the models with global costs. Local cost based random
  forest, and AdaBoost models reached testing F1-scores and balanced accuracies of
  0.98 and 0.95 respectively for QC of both rigid and affine registrations.
6.year:
- 2021
7.type_publication:
- inproceedings
8.doi:
- 10.1109/ISBI48211.2021.9433875
1.chave:
- 6329792
2.author:
- Madraky, Abbas
- Othman, Zulaiha
- Hamdan, Abdul
3.title:
- 'Hair data model: A new data model for Spatio-Temporal data mining'
4.keywords:
- Data models
- Hair
- Object oriented modeling
- Data mining
- Security
- Mathematical model
- Analytical models
- hair data model
- spatio-temporal data models
- data warehouse model
5.abstract:
- Spatio-Temporal data is related to many of the issues around us such as satellite
  images, weather maps, transportation systems and so on. Furthermore, this information
  is commonly not static and can change over the time. Therefore the nature of this
  kind of data are huge, analysing data is a complex task. This research aims to propose
  an intermediate data model that can represented suitable for Spatio-Temporal data
  and performing data mining task easily while facing problem in frequently changing
  the data. In order to propose suitable data model, this research also investigate
  the analytical parameters, the structure and its specifications for Spatio-Temporal
  data. The concept of proposed data model is inspired from the nature of hair which
  has specific properties and its growth over the time. In order to have better looking
  and quality, the data is needed to maintain over the time such as combing, cutting,
  colouring, covering, cleaning etc. The proposed data model is represented by using
  mathematical model and later developed the data model tools. The data model is developed
  based on the existing relational and object-oriented models. This paper deals with
  the problems of available Spatio-Temporal data models for utilizing data mining
  technology and defines a new model based on analytical attributes and functions.
6.year:
- 2012
7.type_publication:
- inproceedings
8.doi:
- 10.1109/DMO.2012.6329792
1.chave:
- 7872917
2.author:
- Rahman, H.
- Ahmed, N.
- Hussain, Md.
3.title:
- A hybrid data aggregation scheme for provisioning Quality of Service (QoS) in Internet
  of Things (IoT)
4.keywords:
- Data aggregation
- Sensors
- Protocols
- Wireless sensor networks
- Power demand
- Data communication
- Time division multiple access
- Data Aggregation
- Internet of Things
- Quality of Service
- Wireless Sensor Network
5.abstract:
- Internet of Things (IoT) is a new paradigm which is enormously gaining ground in
  today's world. In order to maintain desirable service quality in the transmission
  of sensed data, data aggregation schemes are highly used. The main goal of data
  aggregation scheme is to collect and aggregate data packets in an efficient manner
  so as to reduce power consumption, traffic congestion, and to increase network lifetime,
  data accuracy, etc. In this paper, a hybrid Quality of service-Aware Data Aggregation
  (QADA) scheme is proposed. This scheme combines the features of the cluster and
  tree-based data aggregation schemes and addresses some of their important limitations.
  Simulation results show that QADA outperforms cluster and tree-based aggregation
  schemes in terms of power consumption, network lifetime and bearing higher traffic
  load.
6.year:
- 2016
7.type_publication:
- inproceedings
8.doi:
- 10.1109/CIOT.2016.7872917
1.chave:
- 9060361
2.author:
- Woodbridge, Diane
- Hua, Nina
- Suarez, Victoria
- Reilly, Rebecca
- Trinh, Philip
- Intrevado, Paul
3.title:
- 'The Impact of Bike-Sharing Ridership on Air Quality: A Scalable Data Science Framework'
4.keywords:
- Air quality
- Distributed databases
- Servers
- Urban areas
- Cloud computing
- Vegetation
- Indexes
- Distributed computing
- Distributed information systems
- Distributed databases
- Machine learning
- Air pollution
- Air quality
- Intelligent transportation systems
5.abstract:
- This research explores the relationship between daily air quality indicator (AQI)
  values and the daily intensity of bike-share ridership in New York City. The authors
  designed and deployed a distributed data science framework on which to process and
  run Elastic Net, Random Forest Regression, and Gradient Boosted Regression Trees.
  Nine gigabytes of CitiBike ridership data, along with one gigabyte of air quality
  indicator (AQI) data were employed. All machine learning algorithms identified bike-share
  ridership intensity as either the most important or the second most important feature
  in predicting future daily AQIs. The authors also empirically demonstrated that
  although a distributed platform was necessary to ingest and pre-process the raw
  10 gigabytes of data, the actual execution time of all three machine learning algorithms
  on cleaned, joined, and aggregated data was far faster on a local, commodity computer
  than on its distributed counterpart.
6.year:
- 2019
7.type_publication:
- inproceedings
8.doi:
- 10.1109/SmartWorld-UIC-ATC-SCALCOM-IOP-SCI.2019.00341
1.chave:
- 9320548
2.author:
- Luo, Xin
- Chen, Minzhi
- Wu, Hao
- Liu, Zhigang
- Yuan, Huaqiang
- Zhou, MengChu
3.title:
- Adjusting Learning Depth in Nonnegative Latent Factorization of Tensors for Accurately
  Modeling Temporal Patterns in Dynamic QoS Data
4.keywords:
- Tensors
- Big Data
- Quality of service
- Computational efficiency
- Machine learning
- Web services
- Algorithm
- big data
- dynamics
- high-dimensional and incomplete (HDI) data
- machine learning
- missing data estimation
- multichannel data
- nonnegative latent factorization of tensors (NLFT)
- temporal pattern
- quality of service (QoS)
- web service
5.abstract:
- "A nonnegative latent factorization of tensors (NLFT) model precisely represents\
  \ the temporal patterns hidden in multichannel data emerging from various applications.\
  \ It often adopts a single latent factor-dependent, nonnegative and multiplicative\
  \ update on tensor (SLF-NMUT) algorithm. However, learning depth in this algorithm\
  \ is not adjustable, resulting in frequent training fluctuation or poor model convergence\
  \ caused by overshooting. To address this issue, this study carefully investigates\
  \ the connections between the performance of an NLFT model and its learning depth\
  \ via SLF-NMUT to present a joint learning-depth-adjusting scheme for it. Based\
  \ on this scheme, a Depth-adjusted Multiplicative Update on tensor algorithm is\
  \ innovatively proposed, thereby achieving a novel depth-adjusted nonnegative latent-factorization-of-tensors\
  \ (DNL) model. Empirical studies on two industrial data sets demonstrate that compared\
  \ with the state-of-the-art NLFT models, a DNL model achieves significant accuracy\
  \ gain when performing missing data estimation on a high-dimensional and incomplete\
  \ tensor with high efficiency. Note to Practitioners\u2014Multichannel data are\
  \ often encountered in various big-data-related applications. It is vital for a\
  \ data analyzer to correctly capture the temporal patterns hidden in them for efficient\
  \ knowledge acquisition and representation. This article focuses on analyzing temporal\
  \ QoS data, which is a representative kind of multichannel data. To correctly extract\
  \ their temporal patterns, an analyzer should correctly describe their nonnegativity.\
  \ Such a purpose can be achieved by building a nonnegative latent factorization\
  \ of tensors (NLFT) model relying on a single latent factor-dependent, nonnegative\
  \ and multiplicative update on tensor (SLF-NMUT) algorithm. But its learning depth\
  \ is not adjustable, making an NLFT model frequently suffer from severe fluctuations\
  \ in its training error or even fail to converge. To address this issue, this study\
  \ carefully investigates the learning rules for an NLFT model\u2019s decision parameters\
  \ using an SLF-NMUT and proposes a joint learning-depth-adjusting scheme. This scheme\
  \ manipulates the multiplicative terms in SLF-NMUT-based learning rules linearly\
  \ and exponentially, thereby making the learning depth adjustable. Based on it,\
  \ this study builds a novel depth-adjusted nonnegative latent-factorization-of-tensors\
  \ (DNL) model. Compared with the existing NLFT models, a DNL model better represents\
  \ multichannel data. It meets industrial needs well and can be used to achieve high\
  \ performance in data analysis tasks like temporal-aware missing data estimation"
6.year:
- 2021
7.type_publication:
- article
8.doi:
- 10.1109/TASE.2020.3040400
1.chave:
- 7881388
2.author:
- El, Nadia
- Peinsipp-Byma, Elisabeth
3.title:
- Assuring Data Quality by Placing the User in the Loop
4.keywords:
- Classification algorithms
- Quality assurance
- Data mining
- Training data
- Decision support systems
- Cleaning
- Algorithm design and analysis
- Data quality
- User in the loop
- Data Mining
5.abstract:
- Advanced analytical techniques such as data mining, text mining or predictive analytics
  are concepts that are increasingly important in the area of discovering large data
  sets. Various business areas recognize that data in all formats and sizes can provide
  significant support for decision-making. Large amounts of data can contain explicit
  knowledge in form of patterns. Errors within the data can falsify extracted patterns.
  Data is useful if it is correct, organized and interpreted correctly. Data mining
  algorithms can help improve data quality. Algorithms can suggest hints on possible
  errors. Possible errors need a mechanism that decides whether the error is true
  or false. The solution this paper introduces is to integrate users in the quality
  assurance process for decision support systems. The user can assess whether an error
  is true or false.
6.year:
- 2016
7.type_publication:
- inproceedings
8.doi:
- 10.1109/CSCI.2016.0095
1.chave:
- 8078801
2.author:
- Song, Huaming
- Cao, Zhexiu
3.title:
- Research on product quality evaluation based on big data analysis
4.keywords:
- Quality assessment
- Product design
- Feature extraction
- Dictionaries
- Toy manufacturing industry
- Data mining
- Sentiment analysis
- quality evaluation
- online reviews
- big data analysis
- machine learning
5.abstract:
- In order to evaluate product quality from nonnumerical data, we propose the product
  quality evaluation model based on big data analysis including data collecting, data
  preprocessing, quality feature extraction, vector quantization and quality classification.
  Quality feature word extension algorithm, reviews quantization algorithm and machine
  learning algorithm are applied. We finally obtain the qualified rate(88.94%) and
  7 features that most concerned by consumers through the analysis of 184,967 effective
  product reviews of wooden toys. In the end, we compare the SVM machine learning
  algorithm with decision tree and naive bayes, and discuss the credibility of the
  results. Our research on product quality evaluation extends the application of big
  data analysis, and also presents a new method to evaluate product quality in the
  field of manufacture.
6.year:
- 2017
7.type_publication:
- inproceedings
8.doi:
- 10.1109/ICBDA.2017.8078801
1.chave:
- 7483222
2.author:
- Zellal, Nouha
- Zaouia, Abdellah
3.title:
- An exploratory investigation of Factors Influencing Data Quality in Data Warehouse
4.keywords:
- Data warehouses
- Warehousing
- Quality management
- Context
- Data models
- Companies
- Data Quality
- Data Warehouse
- Business Intelligence
5.abstract:
- Data quality is of paramount importance in any Business Intelligence (BI) project.
  In fact, the Decision Support System can be the reference decision unless the data
  is consistent, updated, completed and fit for use for the decision makers and the
  decision tasks. Otherwise, the non-quality data may lead policy makers to make bad
  decisions, to miss opportunities or even commit very serious business mistakes.
  Considering this issue, we propose to study in this paper the factors that influence
  the quality of intelligence data, or rather the factors moderating the influence
  of the quality of source data since this last one is inevitable. We conduct an exploratory
  research in order to gather information from the literature review that helps us
  to suggest hypothesis of our research model "Factors Influencing Data Quality in
  a Data Warehouse". The objective of identifying the critical ones throw a confirmatory
  research (that we don't expose in this paper) will be to enable stakeholders to
  better use their scare resources while implementing a BI project by focusing on
  these key areas that are most likely to have a greater impact.
6.year:
- 2015
7.type_publication:
- inproceedings
8.doi:
- 10.1109/ICoCS.2015.7483222
1.chave:
- 8273343
2.author:
- Mahecha, John
- "L\xF3pez, Nicol\xE1s"
- Velandia, John
3.title:
- 'Assessing data quality in open data: A case study'
4.keywords:
- Measurement
- Prototypes
- Data integrity
- Data mining
- Databases
- Metadata
- Software
- Open data
- Data Quality
- Traceability
- Completeness
- Compliance
- Software architecture and Business process model
5.abstract:
- 'This article focuses on measuring the data quality extracted from the API www.datos.gov.co
  in the area of contracts. This platform allows public entities obey law 1712 of
  2014 [2] but this does not have a validation system of minimum data quality. Three
  metrics are taken as a reference: completeness, traceability and compliance. The
  measuring of the metrics is done with a software called RapidMiner. This software
  allows to do data mining, in this specific case the processes for the measuring
  of the data stored are done and thus determine if the platform www.datos.gov.co
  has problems. For the programming of the prototype spiral methodology [7] is used.
  Calculations is created in each phase. Within the data found, serious inconsistencies
  are found within the platform and within the data, recalling the law 1712 of 2014,
  it says that the public data must be complete, which shows a breach of it.'
6.year:
- 2017
7.type_publication:
- inproceedings
8.doi:
- 10.1109/CONIITI.2017.8273343
1.chave:
- 9006814
2.author:
- Chen, Wuhui
- Liu, Baichuan
- Paik, Incheon
- Li, Zhenni
- Zheng, Zibin
3.title:
- QoS-Aware Data Placement for MapReduce Applications in Geo-Distributed Data Centers
4.keywords:
- Data centers
- Quality of service
- Data transfer
- Distributed databases
- Data models
- Optimization
- Network topology
- Big-data processing
- data placement
- geo-distributed data centers
- QoS aware
5.abstract:
- With growing data volumes and the scaling of data center clusters, communication
  resources often become a bottleneck in service provisioning for many MapReduce applications
  (e.g., training machine learning models). Therefore, data placements that bring
  data blocks closer to data consumers (e.g., MapReduce applications) are seen as
  a promising solution. In this article, we propose an efficient data-placement technique
  that considers network traffic reduction as well as QoS guarantees for the data
  blocks to optimize the communication resources. We first formulate the joint optimization
  of the data-placement problem, propose a generic model for minimizing communication
  costs, and show that the joint data-placement problem is NP-hard. To solve this
  problem, we propose a heuristic algorithm considering traffic flows in the network
  topology of data centers by first seeking optimal QoS-aware data placement based
  on golden division on a Zipflike replica distribution, then transforming the joint
  data-placement problem into a block-dependence tree (BDT) construction problem,
  and finally reducing the BDT construction to a graph-partitioning problem. The experimental
  results demonstrate that our data-placement approach could effectively improve the
  performance of MapReduce jobs with lower communication costs and less job execution
  time for big-data processing.
6.year:
- 2021
7.type_publication:
- article
8.doi:
- 10.1109/TEM.2020.2971717
1.chave:
- 5567975
2.author:
- Fang, Li
- Yue, Jianwei
- Yu, Zhuoyuan
3.title:
- A spatial data checking system based on quality rules
4.keywords:
- Spatial databases
- Construction industry
- Quality control
- XML
- Cities and towns
- User interfaces
- data quality
- data checking
- quality control
- quality rules
- spatial databases
5.abstract:
- This paper describes the design and implementation of a spatial data checking system
  based on quality rules. A relational expression of defining quality rules is presented
  and new quality rules can be extended by using this structure. Methods of storing
  and parsing these quality rules for data checking are discussed. Based on this,
  a spatial data checking system is developed, in which extended data checking types
  can be defined and data checking tasks are made to execute automatic checking for
  spatial databases. The system has been applied to the construction of spatial databases
  in Jilin city and proved to be effective.
6.year:
- 2010
7.type_publication:
- inproceedings
8.doi:
- 10.1109/GEOINFORMATICS.2010.5567975
1.chave:
- 9849309
2.author:
- Bai, Jin
- Gao, Jingnan
- Ma, Wei
- Dang, Yu
- Lu, Chenni
- Liu, Chang
- Dong, Shuai
- Li, Hai
3.title:
- Research on Quality Evaluation of Large Scale Topographic Update Database
4.keywords:
- Economics
- Databases
- Geology
- Urban planning
- Buildings
- Production
- Inspection
- Topographic data
- Quality evaluation
- Field sampling inspection
- Basic surveying and mapping
- Factor statistics
5.abstract:
- "Basic surveying and mapping is a public welfare undertaking. It provides indispensable\
  \ geographic information for economic development, social people\u2019s livelihood,\
  \ urban planning and national defense construction. It is a national strategic resource\
  \ and an important basis for implementing development planning, macro management,\
  \ safeguarding national security, and building ecological civilization. In order\
  \ to ensure the quality of the updated results of basic surveying and mapping topographic\
  \ data and the scientific planning and smooth implementation of national economic\
  \ construction and social development, inspection, acceptance and quality evaluation\
  \ have become important means to ensure the quality of results. Starting from the\
  \ field sampling inspection of large-scale topographic data updating results participated\
  \ by the author in the whole process, this paper leads out and discusses the problems,\
  \ causes, and solutions that are easy to occur in the production process of large-scale\
  \ topographic data updating results from the quality problems, and summarizes and\
  \ considers them in combination with typical problems. It can provide a reference\
  \ for the future sampling of large scale topographic data and the subsequent quality\
  \ analysis and evaluation."
6.year:
- 2022
7.type_publication:
- inproceedings
8.doi:
- 10.1109/ICGMRS55602.2022.9849309
1.chave:
- 8843909
2.author:
- Bors, Christian
- Gschwandtner, Theresia
- Miksch, Silvia
3.title:
- Capturing and Visualizing Provenance From Data Wrangling
4.keywords:
- Data integrity
- Measurement
- Data visualization
- History
- Data models
- Data Wrangling
- Data Cleansing
- Data Quality
- Quality Metrics
- Data Provenance
- Sensemaking
5.abstract:
- 'Data quality management and assessment play a vital role for ensuring the trust
  in the data and its fitness-of-use for subsequent analysis. The transformation history
  of a data wrangling system is often insufficient for determining the usability of
  a dataset, lacking information how changes affected the dataset. Capturing workflow
  provenance along the wrangling process and combining it with descriptive information
  as data provenance can enable users to comprehend how these changes affected the
  dataset, and if they benefited data quality. We present DQProv Explorer, a system
  that captures and visualizes provenance from data wrangling operations. It features
  three visualization components: allowing the user to explore the provenance graph
  of operations and the data stream, the development of quality over time for a sequence
  of wrangling operations applied to the dataset, and the distribution of issues across
  the entirety of the dataset to determine error patterns.'
6.year:
- 2019
7.type_publication:
- article
8.doi:
- 10.1109/MCG.2019.2941856
1.chave:
- 9794846
2.author:
- Chistyakova, Tamara
- Makaruk, Roman
- Tedtoev, Azamat
3.title:
- Methods and Technologies of Application of Fuzzy Models for Processing Industrial
  Data and Quality Management of Polymer Materials
4.keywords:
- Analytical models
- Image color analysis
- Software packages
- Computational modeling
- Quality control
- Predictive models
- Data models
- processing of large industrial data
- methods and technologies
- polymer film
- analysis
- film color characteristics
- fuzzy models
- quality indicators
- expert system
- quality control
5.abstract:
- The paper presents methods and technologies for the use of fuzzy models for the
  analysis and quality control of polymer materials. In accordance with the approaches
  under consideration, a single reconfigurable computer system was developed that
  allows for a comprehensive analysis of the quality of products of multi-assortment
  polymer industries. The methods and technologies that allow using fuzzy models to
  evaluate the color characteristics of polymer materials are described, which makes
  it possible to eliminate the incompleteness of the input data about the object of
  study and improve the quality of the results of the system as a whole
6.year:
- 2022
7.type_publication:
- inproceedings
8.doi:
- 10.1109/SCM55405.2022.9794846
1.chave:
- 9643782
2.author:
- Wenz, Viola
- Kesper, Arno
- Taentzer, Gabriele
3.title:
- Detecting Quality Problems in Data Models by Clustering Heterogeneous Data Values
4.keywords:
- Uncertainty
- Quality assurance
- Documentation
- Syntactics
- Data models
- Model driven engineering
- Encoding
- Data model
- Model quality
- Clustering
- Semi structured data
5.abstract:
- "Data is of high quality if it is fit for its intended use. The quality of data\
  \ is influenced by the underlying data model and its quality. One major quality\
  \ problem is the heterogeneity of data as quality aspects such as understandability\
  \ and interoperability are impaired. This heterogeneity may be caused by quality\
  \ problems in the data model. Data heterogeneity can occur in particular when the\
  \ information given is not structured enough and just captured in data values, often\
  \ due to missing or non-suitable structure in the underlying data model. We propose\
  \ a bottom-up approach to detecting quality problems in data models that manifest\
  \ in heterogeneous data values. It supports an explorative analysis of the existing\
  \ data and can be configured by domain experts according to their domain knowledge.\
  \ All values of a selected data field are clustered by syntactic similarity. Thereby\
  \ an overview of the data values\u2019 diversity in syntax is provided. It shall\
  \ help domain experts to understand how the data model is used in practice and to\
  \ derive potential quality problems of the data model. We outline a proof-of-concept\
  \ implementation and evaluate our approach using cultural heritage data."
6.year:
- 2021
7.type_publication:
- inproceedings
8.doi:
- 10.1109/MODELS-C53483.2021.00027
1.chave:
- 5523332
2.author:
- He, Yihai
- Chang, Wenbing
3.title:
- Research on the Computer Integrated Quality System for Product Design Process
4.keywords:
- Product design
- Quality control
- Quality management
- Process design
- Design engineering
- Technology management
- Assembly
- Reliability engineering
- Manufacturing processes
- Fixtures
- product design
- computer integrated quality system
- quality characteristics
- design quality data
- quality data management model
5.abstract:
- Aiming at the fact that there is no computer integrated quality system for quality
  control in product design phase, a data management model of design quality based
  on quality characteristics (QCs) is proposed, and based on this model an integrated
  quality control system suitable for product design phase is developed. Firstly,
  the content of QCs is defined and its key function in the process of product design
  quality control is emphasized, and the concept of design quality data is declared
  and the mapping relationships of product QCs and design quality data are given.
  Secondly, the data management model of design quality based on QCs is proposed,
  which is consistent with the standard of product data management (PDM). Finally,
  a computer Integrated Quality System for Design Process (DP-CIQS) is developed,
  the validity and effectiveness of DP-CIQS is verified via a preliminary application.
6.year:
- 2010
7.type_publication:
- inproceedings
8.doi:
- 10.1109/ICICTA.2010.283
1.chave:
- 9232648
2.author:
- Hossen, Md
- Goh, Michael
- Hossen, Abid
- Rahman, Md.
3.title:
- A Study on the Aspects of Quality of Big Data on Online Business and Recent Tools
  and Trends Towards Cleaning Dirty Data
4.keywords:
- Data integrity
- Tools
- Companies
- Cleaning
- Task analysis
- Machine learning
- Regulation
- E-business
- Big data
- data quality
- dirty data
- machine learning
5.abstract:
- The reliability, efficiency, and accuracy of e-business depend on the quality of
  data that is associated with a buyer, seller, brokers, e-business portals, admins,
  managers, decision-makers and so on. However, maintaining the quality of data in
  e-business is very challenging. It is because e-business data typically comes from
  different communication channels and sources. Integrating and managing the data
  quality of different sources is generally much troublesome than dealing with traditional
  business data. Even though there are several data cleaning methods and tools exist
  those methods and tools have some constraints. None of them directly working, particularly
  on e-business data that motivates to do research to highlight the aspects of big
  data quality related to e-business. Therefore, this research demonstrates the problems
  related to data quality related to online business, discusses the existing literature
  of data quality, the current tools and techniques that are being used for data quality
  and provides a research finding highlighting the weaknesses of current tools to
  address the problem of online business.
6.year:
- 2020
7.type_publication:
- inproceedings
8.doi:
- 10.1109/ICSGRC49013.2020.9232648
1.chave:
- 6409222
2.author:
- Chen, Mengjie
- Song, Meina
- Han, Jing
- Haihong, E.
3.title:
- Survey on data quality
4.keywords:
- Data models
- Quality assessment
- Data mining
- Standards organizations
- data quality
- data quality management
- quality assessment
- quality tool
5.abstract:
- With high quality of data, enterprises will add value of business. How-ever, poor
  data has resulted in waste of resource, low service efficiency and high costs in
  every area. In this paper, we firstly focus on basic issues of data quality, like
  where the quality problems come from and how to describe it. Then we study some
  cases of data quality management from a holistic enterprise perspective to the details
  of perspective, that is, hierarchical management architecture, frameworks, approaches
  and algorithm. Quality assessment of data is also an important theme, which is used
  to show whether data is good enough and to help people master credibility of data
  quality. We study some assessment algorithm and models. When problem occur, what
  tools should be used in a project is also included in the paper. Finally, we provide
  some outstanding research topics and unresolved issues for future.
6.year:
- 2012
7.type_publication:
- inproceedings
8.doi:
- 10.1109/WICT.2012.6409222
1.chave:
- 8760967
2.author:
- Barbosa, Wesley
- Alves-Souza, Solange
- Correa-Pizzigatti, Pedro
- DeSouza, Luiz
3.title:
- Data Quality Problems Identified in the Bioclimatic Data Collection Process - A
  Survey
4.keywords:
- Biological system modeling
- Bioinformatics
- Data integrity
- Biodiversity
- Meteorology
- Planets
- data quality
- data issues
- climatic
- biodiversity
- bioclimatic
5.abstract:
- Bioclimatic data support several researches that aim to identify the influence of
  climatic factors on biodiversity on the planet. In order to make these studies possible,
  the quality of the data that supports the analyzes must be guaranteed from the beginning
  of the data life cycle so that the results and models generated reflect the real
  scenario of the investigated phenomena. However, the collection of climate and biodiversity
  data presents significant challenges. This work performs a survey of the main quality
  problems identified in the bioclimatic data collection process. The methodological
  procedure consisted in identifying the problems, assigning the data quality dimension
  affected, and suggestions for possible solutions to the problems. The results of
  this survey showed that ambiguous methodological procedures during the data gathering
  and human interference are important factors for data quality impairment and the
  information obtained from these data. Thus, the correction of the data should focus
  on the collection processes and procedures, not the raw data itself.
6.year:
- 2019
7.type_publication:
- inproceedings
8.doi:
- 10.23919/CISTI.2019.8760967
1.chave:
- 9458779
2.author:
- Foroni, Daniele
- Lissandrini, Matteo
- Velegrakis, Yannis
3.title:
- The F4U System for Understanding the Effects of Data Quality
4.keywords:
- Measurement
- Correlation
- Data integrity
- Conferences
- Data engineering
- Task analysis
- Monitoring
- Data Quality
- Data Profiling
- Data Mining
- Data Cleaning
5.abstract:
- We demonstrate a system that enables a data-centric approach in understanding data
  quality. Instead of directly quantifying data quality as traditionally done, it
  disrupts the quality of the dataset and monitors the deviations in the output of
  an analytic task at hand. It computes the correlation factor between the disruption
  and the deviation and uses it as the quality metric. This allows users to understand
  not only the quality of their dataset but also the effect that present and future
  quality issues have to the intended analytic tasks. This is a novel data-centric
  approach aimed at complementing existing solutions. On top of the new information
  that it provides, and in contrast to existing techniques of data quality, it neither
  requires knowledge of the clean datasets, nor of the constraints on which the data
  should comply.
6.year:
- 2021
7.type_publication:
- inproceedings
8.doi:
- 10.1109/ICDE51399.2021.00312
1.chave:
- 6092314
2.author:
- Woodring, Jonathan
- Mniszewski, Susan
- Brislawn, Christopher
- DeMarle, David
- Ahrens, James
3.title:
- Revisiting wavelet compression for large-scale climate data using JPEG 2000 and
  ensuring data precision
4.keywords:
- Data visualization
- Transform coding
- Image coding
- Bit rate
- Quantization
- Wavelet transforms
5.abstract:
- We revisit wavelet compression by using a standards-based method to reduce large-scale
  data sizes for production scientific computing. Many of the bottlenecks in visualization
  and analysis come from limited bandwidth in data movement, from storage to networks.
  The majority of the processing time for visualization and analysis is spent reading
  or writing large-scale data or moving data from a remote site in a distance scenario.
  Using wavelet compression in JPEG 2000, we provide a mechanism to vary data transfer
  time versus data quality, so that a domain expert can improve data transfer time
  while quantifying compression effects on their data. By using a standards-based
  method, we are able to provide scientists with the state-of-the-art wavelet compression
  from the signal processing and data compression community, suitable for use in a
  production computing environment. To quantify compression effects, we focus on measuring
  bit rate versus maximum error as a quality metric to provide precision guarantees
  for scientific analysis on remotely compressed POP (Parallel Ocean Program) data.
6.year:
- 2011
7.type_publication:
- inproceedings
8.doi:
- 10.1109/LDAV.2011.6092314
1.chave:
- 8416208
2.author:
- Blanquer, Ignacio
- Meira, Wagner
3.title:
- EUBra-BIGSEA, A Cloud-Centric Big Data Scientific Research Platform
4.keywords:
- Big Data
- Data analysis
- Data privacy
- Quality of service
- Data models
- Security
- Biological system modeling
- Cloud-Computing,-Big-Data,-Quality-of-Service,-Data-Analytics
5.abstract:
- This paper describes the achievements of project EUBra-BIGSEA, which has delivered
  programming models and data analytics tools for the development of distributed Big
  Data applications. As framework components, multiple data models are supported (e.g.
  data streams, multidimensional data, etc.) and efficient mechanisms to ensure privacy
  and security, on top of a QoS-aware layer for the smart and rapid provisioning of
  resources in a cloud-based environment.
6.year:
- 2018
7.type_publication:
- inproceedings
8.doi:
- 10.1109/DSN-W.2018.00023
1.chave:
- 8861049
2.author:
- Kim, Sunho
- Castillo, Ricardo
- Caballero, Ismael
- Lee, Jimwoo
- Lee, Changsoo
- Lee, Downgwoo
- Lee, Sangyub
- Mate, Alejandro
3.title:
- Extending Data Quality Management for Smart Connected Product Operations
4.keywords:
- Internet of Things
- Data integrity
- Business
- Metadata
- ISO Standards
- Wireless sensor networks
- Data analysis
- IoT
- Internet of Things
- SCP
- smart connected product
- data quality
- data quality management
- process reference model
- ISO 8000-61
- DQM
- PRM
5.abstract:
- 'Smart connected product (SCP) operation embodies the concept of the internet of
  things (IoT). To increase the probability of success of SCP operations for customers,
  the high quality of the IoT data across operations is imperative. IoT data go beyond
  sensor data, as integrate some other various type of data such as timestamps, device
  metadata, business data, and external data through SCP operation processes. Therefore,
  traditional data-centric approaches that analyze sensor data and correct their errors
  are not enough to preserve, in long-term basis, adequate levels of quality of IoT
  data. This research provides and alternative framework of data quality management
  as a process-centric approach to improve the quality of IoT data. The proposed framework
  extends the process reference model (PRM) for data quality management (DQM) defined
  in ISO 8000-61, and tailored to fully adapt to the special requirements of the IoT
  data management. These involve several adaptations: first, the scope of the SCP
  operations for data quality management is determined, and the processes required
  for SCP operations are defined following the process description format of ISO 8000-61.
  Second, the relationship between the processes and the structure of the processes
  in the technology stack of the SCP operations are described to cover the actual
  nature of the IoT data flows. Finally, a new IoT DQM-PRM is proposed by integrating
  the processes for the SCP operations with DQM-PRM. When these processes are executed
  in the organization, the quality of IoT data composed of data of various types can
  be continuously improved and the utilization rate of SCP operations is expected
  to increase.'
6.year:
- 2019
7.type_publication:
- article
8.doi:
- 10.1109/ACCESS.2019.2945124
1.chave:
- 8607406
2.author:
- Mukwakungu, S.
- Bakama, E.
- Lumbwe, A.
- Bolipombo, M.
- Niati, D.
- Ibrahimu, K.
- Kasongo, J.
- Mbohwa, C.
3.title:
- Assessment of Quality of Service at the Main Laboratory of the LAB Aimed at Satisfying
  Internal Customer Needs
4.keywords:
- Medical services
- Quality of service
- Tools
- Reliability
- Organizations
- Standards
- Service quality
- SERVQUAL
- Quality Management System (QMS)
5.abstract:
- "This paper's objectives are to establish and document internal clients' perception\
  \ about the quality of service received at the Main Laboratory of the LAB, a national\
  \ laboratory dealing with transmittable diseases in South Africa. The study followed\
  \ a quantitative design approach with cross functional examinations. Data collection\
  \ tool was based on \u201CSERVQUAL\u201D model. Findings show that in terms of the\
  \ quality dimensions, the LAB's centres performed variably in many aspects and to\
  \ a varying degree in different quality dimensions measured. Each centre had its\
  \ own unique set of challenges. The recommendations made in this study can be implemented\
  \ as a solution to the problems faced by the LAB and other similar departments.\
  \ This study viewed from a South African perspective, is first of its kind as it\
  \ explores the effectiveness of the implementation of a Quality Management System\
  \ at a biosafety level 4, the only one on the African continent."
6.year:
- 2018
7.type_publication:
- inproceedings
8.doi:
- 10.1109/IEEM.2018.8607406
1.chave:
- 8806407
2.author:
- Wang, Yongzhi
3.title:
- Research on Evaluation Method of Tourism Quality of Characteristic Towns in Southwest
  Minority Areas Based on ORA Network Data Analysis
4.keywords:
- Indexes
- Data analysis
- Cultural differences
- Industries
- Quality assessment
- Analytic hierarchy process
- Standards
- ORA
- Data analysis
- Southwest ethnic region
- Town tourism
- Quality assessment
5.abstract:
- In order to better promote the development of tourism scenic spots in small towns
  with ethnic characteristics, this paper proposes a tourism quality evaluation method
  for small towns with ethnic characteristics in southwest China based on ORA network
  data analysis. Based on ORA network data analysis principle, this paper integrates
  and perfects the content of tourism scenic spot quality evaluation of small towns
  with ethnic characteristics, and sets up the tourism quality evaluation standard
  of small towns with ethnic characteristics. According to the standard, the integrated
  content is graded and data evaluation is carried out according to different grade
  type standards, thus effectively completing the evaluation of tourism quality of
  small towns with ethnic characteristics in southwest China. Finally, through experiments,
  it is proved that the tourism quality evaluation method based on ORA network data
  analysis has higher accuracy and practicability than the traditional method.
6.year:
- 2019
7.type_publication:
- inproceedings
8.doi:
- 10.1109/ICRIS.2019.00085
1.chave:
- 9006446
2.author:
- Homayouni, Hajar
- Ghosh, Sudipto
- Ray, Indrakshi
- Kahn, Michael
3.title:
- An Interactive Data Quality Test Approach for Constraint Discovery and Fault Detection
4.keywords:
- Fault detection
- Data integrity
- Data models
- Semantics
- Decision trees
- Self-organizing feature maps
- Inspection
- Big Data
- Data quality tests
- Explainable learning
- Interactive learning
- Unsupervised learning
5.abstract:
- Data quality tests validate heterogeneous data to detect violations of syntactic
  and semantic constraints. The specification of these constraints can be incomplete
  because domain experts typically specify them in an ad hoc manner. Existing automated
  test approaches can generate false alarms and do not explain the constraint violations
  while reporting faulty data records. In previous work, we proposed ADQuaTe, which
  is an automated data quality test approach that uses an unsupervised deep learning
  techni que (1) to discover constraints from big datasets that may have been missed
  by experts, and (2) to label as suspicious those records that violate the constraints.
  These records are grouped and explanations for constraint violations are presented
  to domain experts who determine whether or not the groups are actually faulty. This
  paper presents ADQuaTe2, which extends ADQuaTe to use an interactive learning technique
  that incorporates expert feedback to retrain the learning model and improve the
  accuracy of constraint discovery and fault detection. We evaluate the effectiveness
  of the approach on real-world datasets from a health data warehouse and a plant
  diagnosis database. We also use datasets with known faults from the UCI repository
  to evaluate the improvement in the accuracy of the approach after incorporating
  ground truth knowledge.
6.year:
- 2019
7.type_publication:
- inproceedings
8.doi:
- 10.1109/BigData47090.2019.9006446
1.chave:
- 8601548
2.author:
- Zhong, Qing
- Yao, Wenlin
- Lin, Linxue
- Wang, Gang
- Xu, Zhong
3.title:
- Data Analysis and Applications of the Power Quality Monitoring
4.keywords:
- Monitoring
- Power quality
- Big Data
- Standards
- Correlation coefficient
- Correlation
- Big data
- Power quality
- Data analysis
- Correlation coefficient
5.abstract:
- This paper presents three applications to transform the power quality (PQ) monitoring
  data into the useful information. With the increasing volume of the PQ monitoring
  data, mining the values of the data is very important for the power system operations.
  Three applications are introduced with the PQ monitoring data in Guangzhou grid,
  China. Firstly, the cumulative probability of PQ monitoring data is applied to certificate
  the PQ limits according to the national standards. Secondly, three types of voltage
  sags are counted by the PQ monitoring data to show the severity of voltage sags
  in local grid. Thirdly, the correlation analysis is applied to show the impact of
  PQ problem on the device malfunctions. The correlation coefficients between the
  PQ monitoring data and the device malfunction data can show the impacts of PQ problems
  on the devices directly. The malfunctions of capacitors/inductors are relevant to
  the voltage deviation and harmonic distortion obviously which is shown by the correlation
  coefficients. It is a good attempt to translate the PQ monitoring data into the
  useful information, which can help the operators decide.
6.year:
- 2018
7.type_publication:
- inproceedings
8.doi:
- 10.1109/POWERCON.2018.8601548
1.chave:
- 9617555
2.author:
- Rahmani, Dita
- Kusumasari, Tien
- Alam, Ekky
3.title:
- Addition of Process Decomposition in Open Source Tools-Based Cleansing Data Modules
4.keywords:
- Data integrity
- Data integration
- Companies
- Tools
- Data science
- Cleaning
- Open source software
- data cleansing
- pentaho data integration
- openrefine
5.abstract:
- In today's technological developments, data has an important role in supporting
  the achievement of goals for the company. The importance of data for companies is
  to fulfill the quality of supporting the company's business needs. The high quality
  that data has is of critical value to the company. However, there are many errors
  in the data that reduce the quality. Low-quality data, i.e., the data is inaccurate,
  incomplete, or out of date. There is a need for data quality management or Data
  Quality Management to manage data quality improvements to become consistent, accurate,
  complete, timely, and unique data. Regulating the quality of the data requires data
  cleansing. Data cleansing is a method to improve low-quality data by producing high-quality
  data. Therefore, this study will discuss the analysis and design of the decomposition
  of process packages for data cleansing in order to improve the quality of data that
  does not meet the company's needs. In the design carried out, the authors provide
  several solutions based on analysis to meet the needs of the company's data cleansing
  process flexibly with the decomposition of the process package that will be implemented
  using open-source tools, namely Pentaho Data Integration as the result of this research.
  There is a comparative evaluation using OpenRefine, which results in data cleansing
  using Pentaho Data Integration which is superior in the overall data cleaning process.
6.year:
- 2021
7.type_publication:
- inproceedings
8.doi:
- 10.1109/ICoDSA53588.2021.9617555
1.chave:
- 8978560
2.author:
- Mukwakungu, S.C.
- Motapane, T.I.
- Mbohwa, C.
3.title:
- The Assessment of Internal Service Quality Perception of System Administrators -
  Case of Services Provided by Data Centre Hosting to Local Bank in South Africa
4.keywords:
- SERVQUAL
- Quality Control
- Data Hosting Centers
- Service Level Objectives
5.abstract:
- This study assesses the perception of service quality provided by data hosting companies
  to system administrators at one of South Africa's five major banks in Johannesburg
  in terms of the difference between the expected quality level and the actual service
  quality received to suggest recommendations aimed at resolving the issues that would
  be exposed. The SERVQUAL model was adopted in the survey design to classify key
  service quality dimensions. With a 100% response rate, an analysis of the gap score
  was conducted on the data collected. The findings showed that such an exercise has
  never been conducted at the company over the past five years, and it indicated that
  the overall quality of service is at its lowest as perceived by system administrators
  which leads to a constantly decreasing level of customer satisfaction. From the
  evaluation of all the quality dimensions, none of them met customers satisfaction
  criteria, with all the dimensions presenting negative gap scores. The study recommends
  that data hosting centers should implement data center service quality (DCSQ) to
  successfully meet the quality service expectations of system administrators.
6.year:
- 2019
7.type_publication:
- inproceedings
8.doi:
- 10.1109/IEEM44572.2019.8978560
1.chave:
- 9421436
2.author:
- Wang, Xiaofeng
- Jiang, Yong
- Zhan, Gaofeng
- Zhao, Tong
3.title:
- Quality Analysis and Evaluation Method for Multisource Aggregation Data based on
  Structural Equation Model
4.keywords:
- Analytical models
- Adaptation models
- Numerical analysis
- Data integrity
- Computational modeling
- Urban areas
- Data aggregation
- Data Aggregation
- Data Analysis
- Quality Evaluation
- Structural Equation Model
5.abstract:
- In the era of big data, how to evaluate the data quality of multi-source aggregation
  data is very important. The reason is that uneven data quality will directly lead
  to inaccurate or ambiguous data in the database, and indirectly lead to the deviation
  of subsequent data mining and decision-making. In this paper, structural equation
  model(SEM) is introduced to explore the effectiveness of various data quality evaluation
  indicators in data aggregation and finding out internal relationship between them.
  A new quality evaluation method of multi-source aggregation data is proposed, based
  on the regression's significance analysis and factor loads of each observation index
  in the SEM model. The case analysis shows that the proposed method is feasible and
  can be used to evaluate the quality of multi-source aggregation data adaptively
  for a long time.
6.year:
- 2020
7.type_publication:
- inproceedings
8.doi:
- 10.1109/ICMCCE51767.2020.00280
1.chave:
- 7761250
2.author:
- Roarty, Hugh
- Palamara, Laura
- Kohut, Josh
- Glenn, Scott
3.title:
- Automated quality control of high frequency radar data II
4.keywords:
- Radar
- Quality control
- Sea measurements
- Sea surface
- Real-time systems
- Quality assurance
- radar
- remote sensing
- quality control
- MARACOOS
5.abstract:
- A paper was presented at OCEANS'12 by the authors on a similar topic. The original
  paper was more of a formulation of ideas for automated quality control of HF radar
  data with only a listing of potential tests. This paper lays out the framework for
  the quality assurance methods and quality control tests for the entire data processing
  chain. The paper also synthesizes a number of papers that have been presented recently
  on this topic of HF radar data quality control.
6.year:
- 2016
7.type_publication:
- inproceedings
8.doi:
- 10.1109/OCEANS.2016.7761250
1.chave:
- 9491766
2.author:
- Al, Hania
- Barham, Sawsan
- Qusef, Abdallah
3.title:
- Data Strategy and Its Impact on Open Government Data Quality
4.keywords:
- Technological innovation
- Data integrity
- Government
- Decision making
- Transforms
- Data models
- Information technology
- Open Data
- Open Government Data
- Quality Management
- Data Strategy
5.abstract:
- "Technology is playing a major role in decision-making and innovation through its\
  \ wide capabilities of data mining and analysis. Developed countries have recognized\
  \ data as an asset that can transform their services and economy to a better level\
  \ if used effectively. In this paper, the authors discuss how data strategy can\
  \ unleash the power of data and how such strategy affects open government data quality\
  \ positively and help countries to achieve better outcomes in terms of sharing and\
  \ reusing data. This paper analyzes UK & US data strategies and concludes the main\
  \ elements that data strategy should include. Also, Jordan\u2019s model in open\
  \ government data is discussed and the need to formulate a data strategy to enhance\
  \ data sharing and quality of open data is explained."
6.year:
- 2021
7.type_publication:
- inproceedings
8.doi:
- 10.1109/ICIT52682.2021.9491766
1.chave:
- 7840595
2.author:
- Haryadi, Adiska
- Hulstijn, Joris
- Wahyudi, Agung
- Voort, Haiko
- Janssen, Marijn
3.title:
- 'Antecedents of big data quality: An empirical examination in financial service
  organizations'
4.keywords:
- Big data
- Industries
- Insurance
- Companies
- Finance
- big data
- data quality
- big data quality
- antecedents
- finance
5.abstract:
- Big data has been acknowledged for its enormous potential. In contrast to the potential,
  in a recent survey more than half of financial service organizations reported that
  big data has not delivered the expected value. One of the main reasons for this
  is related to data quality. The objective of this research is to identify the antecedents
  of big data quality in financial institutions. This will help to understand how
  data quality from big data analysis can be improved. For this, a literature review
  was performed and data was collected using three case studies, followed by content
  analysis. The overall findings indicate that there are no fundamentally new data
  quality issues in big data projects. Nevertheless, the complexity of the issues
  is higher, which makes it harder to assess and attain data quality in big data projects
  compared to the traditional projects. Ten antecedents of big data quality were identified
  encompassing data, technology, people, process and procedure, organization, and
  external aspects.
6.year:
- 2016
7.type_publication:
- inproceedings
8.doi:
- 10.1109/BigData.2016.7840595
1.chave:
- 6118854
2.author:
- none
3.title:
- Towards Data Quality into the Data Warehouse Development
4.keywords:
- Data warehouses
- Data models
- Organizations
- Adaptation models
- Warehousing
- Interviews
- Data warehouse
- data quality
- data quality dimension
- data quality integration
5.abstract:
- 'Commonly, DW development methodologies, paying little attention to the problem
  of data quality and completeness. One of the common mistakes made during the planning
  of a data warehousing project is to assume that data quality will be addressed during
  testing. In addition to the data warehouse development methodologies, we will introduce
  in this paper a new approach to data warehouse development. This proposal will be
  based on integration data quality into the whole data warehouse development phase,
  denoted by: integrated requirement analysis for designing data warehouse (IRADAH).
  This paper shows that data quality is not only an integrated part of data warehouse
  project, but will remain a sustained and ongoing activity.'
6.year:
- 2011
7.type_publication:
- inproceedings
8.doi:
- 10.1109/DASC.2011.194
1.chave:
- 7870183
2.author:
- Clarke, Roger
3.title:
- Quality Assurance for Security Applications of Big Data
4.keywords:
- Big data
- Q-factor
- Sociology
- Statistics
- Security
- Reliability
- Quality assurance
- risk assessment
- risk management
- information quality
- data semantics
- data scrubbing
- decision quality
- transparency
5.abstract:
- The quality of inferences drawn from data, big or small, is heavily dependent on
  the quality of the data and the quality of the processes applied to it. Big data
  analytics is emerging from laboratories and being applied to intelligence and security
  needs. To achieve confidence in the outcomes of these applications, a quality assurance
  framework is needed. This paper outlines the challenges, and draws attention to
  the consequences of misconceived and misapplied projects. It presents key aspects
  of the necessary risk assessment and risk management approaches, and suggests opportunities
  for research.
6.year:
- 2016
7.type_publication:
- inproceedings
8.doi:
- 10.1109/EISIC.2016.010
1.chave:
- 4341093
2.author:
- Wang, Keqin
- Tong, Shurong
- Eynard, Benoit
- Roucoules, Lionel
- Matta, Nada
3.title:
- Application of Data Mining in Manufacturing Quality Data
4.keywords:
- Data mining
- Delta modulation
- Product design
- Manufacturing processes
- Knowledge management
- Data engineering
- Quality management
- Mechanical systems
- Systems engineering and theory
- Uncertainty
5.abstract:
- Knowledge on product quality is one of the most important knowledge sources throughout
  the product lifecycle for the efficiency and effectiveness of product design decisions.
  To provide quality related knowledge, this paper proposed one data mining based
  knowledge discovery approach. This approach can extract knowledge on product quality
  from large volume of quality related manufacturing data. The effectiveness of this
  approach is illustrated and validated by an example adapted from literature. Finally,
  some conclusions and future works are discussed.
6.year:
- 2007
7.type_publication:
- inproceedings
8.doi:
- 10.1109/WICOM.2007.1318
1.chave:
- 9458106
2.author:
- Meli, Matthew
- Gatt, Edward
- Casha, Owen
- Grech, Ivan
- Micallef, Joseph
3.title:
- A Low Cost LoRa-based IoT Big Data Capture and Analysis System for Indoor Air Quality
  Monitoring
4.keywords:
- Temperature measurement
- Temperature sensors
- Cloud computing
- Data visualization
- Big Data
- Logic gates
- Particle measurements
- IoT
- LPWAN
- Indoor Air Quality
- Big Data
- LoRa
5.abstract:
- This paper presents a low cost LoRa-based IoT big data capture and analysis system
  for indoor air quality monitoring. This system is presented as an alternative solution
  to expensive and bulky indoor air quality monitors. It enables multiple low cost
  nodes to be distributed within a building such that extensive location-based indoor
  air quality data is generated. This data is captured by a gateway and forwarded
  to a cloud-based LoRaWAN network which in turn publishes the received data via MQTT.
  A cloud-based data forwarding server is used to capture, format and store this big
  data on a cloud-based document-oriented database. Cloud-based services are used
  for data visualization and analysis. Periodic indoor air quality graphs along with
  air quality index and thermal comfort index heat maps are generated.
6.year:
- 2020
7.type_publication:
- inproceedings
8.doi:
- 10.1109/CSCI51800.2020.00070
1.chave:
- 7339050
2.author:
- Huang, Min
- Zhang, Tao
- Wang, Jingyang
- Zhu, Likun
3.title:
- A new air quality forecasting model using data mining and artificial neural network
4.keywords:
- Atmospheric modeling
- Predictive models
- Pollution
- Air quality
- Forecasting
- Meteorology
- Neural networks
- air quality forecasting
- data mining
- BP Neural Networks
5.abstract:
- 'In this paper, we have established a forecasting model of atmospheric pollution.
  The forecasting model which combines with data mining techniques and BP neural network
  algorithm is based on the monitoring data of air pollution obtained from Shijiazhuang
  air quality monitoring stations. Firstly this model uses the data mining technology
  to find the factors which affect air quality. Secondly it uses these factors data
  to train the neural network. Finally, the evaluation test of the forecasting model
  is evaluated. The results show that: The atmospheric quality forecasting model established
  in this paper can well meet the needs of practical application, because it has higher
  forecasting accuracy. The forecasting model improves the effectiveness and practicability,
  and can provide more reliable decision evidence for environmental protection departments.'
6.year:
- 2015
7.type_publication:
- inproceedings
8.doi:
- 10.1109/ICSESS.2015.7339050
1.chave:
- 9674175
2.author:
- Yujun, Yang
- Yimei, Yang
- Wang, Zhou
- Hongbo, Xiao
- Liyun, Li
3.title:
- Research On The Construction of Agricultural Product Quality Maintenance And Quality
  Traceability System Based On Big Data
4.keywords:
- Costs
- Information processing
- Big Data
- Media
- Maintenance engineering
- Wavelet analysis
- Agricultural products
- Quality traceability
- agricultural product quality
- big data
- agricultural product traceability
- data analysis
5.abstract:
- The quality and safety of agricultural products has been widely concerned by the
  whole society in recent years. Therefore, the traceability of agricultural products
  is a research hotspot of scholars. The quality and safety traceability system of
  agricultural products is an important method to monitor the quality and safety of
  agricultural products. The emergence and use of big data help to solve the problems
  of high cost, scattered information and incomplete industrial chain of quality and
  safety traceability of agricultural products and improve the efficiency and accuracy
  of the quality and safety traceability system of agricultural products. There are
  still some problems in the application of big data, such as weak pertinence. It
  is necessary to mine and use big data to realize the traceability of agricultural
  products.
6.year:
- 2021
7.type_publication:
- inproceedings
8.doi:
- 10.1109/ICCWAMTIP53232.2021.9674175
1.chave:
- 5477583
2.author:
- Zhao, Xiaofei
- Huang, Zhiqiu
3.title:
- A quality evaluation approach for OLAP metadata of multidimensional OLAP data
4.keywords:
- Multidimensional systems
- Object oriented modeling
- Computer science
- Data warehouses
- Aggregates
- Information science
- Information analysis
- Stability analysis
- Quality management
- Engineering management
- OnLine Analytical Processing(OLAP)
- Metadata Management
- Quality Evaluation
- Model-driven Architecture
- Formalization Mechanism
5.abstract:
- The quality of metadata in OnLine Analytical Processing(OLAP) process has remarkable
  influence on the stability and reliability of OLAP tools. Model-driven metadata
  integration approach introduces the metadata management concept based on the object
  oriented paradigm for modeling and querying OLAP metadata of multidimensional data.
  In this model, the basic concepts of the object oriented model including object,
  class, and relationship between objects are applied to describe objects of multidimensional
  data and the OLAP operations. Thus the metadata of multidimensional OLAP data is
  modeled as a fact and a set of elements that are organized into class hierarchy.
  However, the quality evaluation of OLAP metadata which describes multidimensional
  data is difficult because of the structural essential of the metadata. In this paper,
  we propose a quality evaluation approach for model-driven OLAP metadata. This approach
  depends on a static formalization mechanism, various reasoning procedures supported
  by the formalization can be used for the quality evaluation tasks.
6.year:
- 2010
7.type_publication:
- inproceedings
8.doi:
- 10.1109/ICIME.2010.5477583
1.chave:
- 9309805
2.author:
- Ntanzi, K.
- Lumbwe, A.
- Mukwakungu, S.
- Sukdeo, N.
3.title:
- The Relationship Between the Implementation of Quality Management Practices and
  Service Quality in the South African Financial Service Industry
4.keywords:
- Organizations
- Quality management
- Customer relationship management
- Industries
- Standards organizations
- Financial services
- Quality control
- Quality management
- service quality
- financial service industry
5.abstract:
- The purpose of this study is to evaluate the effect of quality management practices
  on customers, employees and service quality in the financial service industry. The
  sample of 30 customers and 30 employees were selected based on stratified and snowball
  sampling procedures respectively. This research used a mixed approach to collect
  data. The study was conducted by engaging with customers (business owners, students
  and the working class) in the form of interviews and questionnaires, and by using
  secondary data. The main quality principles was mainly based on employee satisfaction,
  customer focus and continual improvement to establish how customers choose the bank
  they bank depending on their different classes. The results reveal that employee
  satisfaction had a direct relationship with the level of quality the organization
  produced. In addition, business owners and the working class were more likely to
  consider the service quality of the organization before they decide to bank with
  them. This research shows the significance of the implementing quality practices
  in the financial service industry in order to gain or retain customers.
6.year:
- 2020
7.type_publication:
- inproceedings
8.doi:
- 10.1109/IEEM45057.2020.9309805
1.chave:
- 8258543
2.author:
- Liu, Lixin
- Chen, Jun
3.title:
- The influences of deep-sea vision data quality on observational analysis
4.keywords:
- Scattering
- Optical sensors
- Optical imaging
- Fish
- Backscatter
- deep-sea observation
- vision data quality
- automatic judging
5.abstract:
- Deep-sea study by optical observation method is an interdisciplinary subject and
  faces plenty of difficulties. To find out the influences of vision data quality,
  characteristic of vision data for deep-sea observation is analyzed, and a deep-sea
  landing experiment has been implemented. Data quality analyzing based on real deep-sea
  vision data that collected in the in-situ observation platform is actualized. It
  is expected that the research on influence mechanism of deep-sea vision quality
  is beneficial to the detection of region of interest, the judging of animal existence,
  the classification of species, and the trajectories labeling. Further analyzing
  on unsupervised deep-sea vision data quality control is necessary.
6.year:
- 2017
7.type_publication:
- inproceedings
8.doi:
- 10.1109/BigData.2017.8258543
1.chave:
- 9268064
2.author:
- Zhao, Yuxi
- Gong, Xiaowen
- Chen, Xu
3.title:
- Privacy-Preserving Incentive Mechanisms for Truthful Data Quality in Data Crowdsourcing
4.keywords:
- Crowdsourcing
- Task analysis
- Data integrity
- Data privacy
- Resource management
- Mobile computing
- Inference algorithms
- Data crowdsourcing
- differentially privacy
- incentive mechanism
- data quality elicitation
5.abstract:
- "Data crowdsourcing is a promising paradigm that leverages the \u201Cwisdom\u201D\
  \ of a potentially large crowd of \u201Cworkers\u201D in many application domains.\
  \ Quality-aware crowdsourcing is beneficial as it makes use of workers\u2019 data\
  \ quality to perform task allocation and data aggregation. However, a worker\u2019\
  s quality and data can be her private information that she may have incentive to\
  \ misreport to the crowdsourcing requester. Moreover, a worker\u2019s quality and\
  \ data can depend on her sensitive information (e.g., location), which can be inferred\
  \ from the outcomes of task allocation and data aggregation by an adversary. In\
  \ this paper, we devise Privacy-preserving crowdsourcing mechanisms for truthful\
  \ Data Quality Elicitation (PDQE). In these mechanisms, we design differentially\
  \ private task allocation and data aggregation algorithms to prevent the inference\
  \ of a worker\u2019s quality and data from the outcomes of these algorithms. In\
  \ the meantime, the mechanisms also incentivize workers to truthfully report their\
  \ quality and data and make desired efforts. We first focus on the mechanisms for\
  \ a single task (S-PDQE) and then extend it to the case of multiple tasks (M-PDQE).\
  \ We further show that both the mechanisms achieve a bounded performance gap compared\
  \ to the optimal strategy. We evaluate the proposed mechanisms using simulations\
  \ based on real-world data, which corroborate their highly-desired properties on\
  \ truthful data quality elicitation, data accuracy and privacy protection."
6.year:
- 2022
7.type_publication:
- article
8.doi:
- 10.1109/TMC.2020.3040138
1.chave:
- 8725744
2.author:
- Xiang, Zheng
- Xiaofang, Liu
- Weigang, Gao
3.title:
- Analysis of the Application of Military Big Data in Equipment Quality Information
  Management
4.keywords:
- Big Data
- Military equipment
- Information management
- Distributed databases
- Data analysis
- Maintenance engineering
- military big data
- equipment quality information
- management
5.abstract:
- This At present, big data has risen to the national strategy. Big data is fully
  integrated into the military field, becoming the driving force of military scientific
  research, the core element of construction management, and an important resource
  for war success. This paper mainly expounds the basic connotation of big data technology
  and military big data, and analyzes the application of military big data in equipment
  quality information management, and proposes information collection, storage, analysis,
  processing, exchange and feedback on equipment quality information management. The
  countermeasures provide methods and basis for military big data in equipment information
  management.
6.year:
- 2019
7.type_publication:
- inproceedings
8.doi:
- 10.1109/ICCCBDA.2019.8725744
1.chave:
- 8477366
2.author:
- Bouhamidi, Moulay
- Amar, Amine
3.title:
- Data Quality Analysis for a Smart Solar Resource Assessment
4.keywords:
- Sensors
- Data integrity
- Cleaning
- Maintenance engineering
- Solar radiation
- Calibration
- Wind speed
- component
- Data quality
- Irradiations
- Meterological data
- Bankable data
5.abstract:
- The development of solar-based projects stimulates the demand's increase in the
  concern of reliable data, especially for the life cycle cost analysis of solar technologies.
  Reliable data requires several important aspects that should be taken into consideration.
  We can evoke for example, the instrument performance specifications, methods of
  installation, frequency of calibrations and the regularity of maintenance and data
  analysis. However, when analyzing solar radiation dataset, many questionable values
  are still identified. Thus, the crucial step is to develop more accurate automatic
  procedures for performing quality control (QC) of solar data, in addition to the
  maintenance and human monitoring. The objective of the automatic procedures is to
  identify aberrant observations and to reduce the manual workload of human validators,
  in order to guarantee reliability and allow comparison analysis between different
  observational datasets. In the same context, we present and we analyze an implemented
  procedure, using several tests, for 3 stations of Moroccan Agency for Solar Energy
  (masen). The analysis shows that the adopted procedure is very efficient for quality
  assessment. However, the procedure should be integrated in a global quality process,
  to ensure the procurement of bankable data.
6.year:
- 2017
7.type_publication:
- inproceedings
8.doi:
- 10.1109/IRSEC.2017.8477366
1.chave:
- 9477756
2.author:
- Li, Yongquan
- Wang, Yanqing
- Song, Xiankun
- Cheng, Xinyu
3.title:
- Research on Course Quality Evaluation System Based on Artificial Intelligence
4.keywords:
- Visualization
- Data analysis
- Conferences
- Neural networks
- Education
- Data acquisition
- Quality assessment
- artificial intelligence
- data analysis
- course quality
- system design
5.abstract:
- 'In view of the complexity and inefficiency of the course quality assessment process,
  this paper conducts an in depth study of behavior analysis technology and designs
  an automated and intelligent curriculum quality assessment system. The system uses
  various artificial intelligence algorithms to analyze the collected images and text
  data related to classroom quality and produce objective visual results. The course
  quality assessment system is divided into four functional modules: curriculum quality
  index management, data acquisition management, data analysis management and system
  of rights management, which can quickly generate visual course quality assessment
  results based on data analysis results. By comparison, the results of intelligent
  evaluation and existing artificial evaluation have high consistency. The system
  is fully functional and intelligent at the touch of a button, which can greatly
  improve the efficiency of curriculum quality assessment and reduce the complexity
  of work.'
6.year:
- 2021
7.type_publication:
- inproceedings
8.doi:
- 10.1109/CSEI51395.2021.9477756
1.chave:
- 5380432
2.author:
- Liang, Siyuan
- Wu, Kening
3.title:
- Design of Data Management System for Agricultural Land Quality Dynamic Monitoring
4.keywords:
- Quality management
- Computerized monitoring
- Multimedia databases
- Technology management
- Memory
- Data analysis
- Conference management
- Engineering management
- Geology
- Agriculture
- dynamic monitoring of the quality of agricultural land
- data management
- system design
5.abstract:
- 'The design of a dynamic monitoring data management system for agricultural land
  is discussed in this paper. Advances in computer technology have simplified the
  process for dynamic monitoring of agricultural land quality. The main points discussed
  are: analysis of the data management system''s functional requirements for dynamic
  monitoring; analysis and design for management system data flow and data storage
  structure; introduction on software module design; discussion and additional explanation
  of the system.'
6.year:
- 2009
7.type_publication:
- inproceedings
8.doi:
- 10.1109/ICCEE.2009.186
1.chave:
- 8343025
2.author:
- Cai, Hong-xia
- Wei, Zhuang-yu
3.title:
- Analysis of civil aircraft quality data under the support of big data
4.keywords:
- Aircraft
- Aircraft manufacture
- Production
- Data mining
- Big Data
- Itemsets
- quality oriented
- aircraft
- data analysis
- Apriori algorithm
- big data Splunk platform
5.abstract:
- In the production assembly manufacturing process, a large amount of quality data
  has been generated by civil aircraft equipment system. With the passage of time
  and the accumulation of data, these massive data cannot be dealt with effectively
  using traditional statistical analysis of discrete manufacturing industry. To solve
  this problem, the method of quality data analysis for unsupervised learning presented
  in this paper was developed, after evaluating the generating characteristics of
  the civil aircraft quality data and the problems associated with the processing
  of the traditional quality data analysis. On this basis, in this paper, according
  to the disorder association, complex structure and large amount of data of the civil
  aircraft quality data, the data mining association analysis Apriori algorithm and
  the big data Splunk platform are introduced to effectively reduce the complexity
  of the quality data analysis through the complementary advantages of both, and put
  the data in an orderly, coherent state. The results show that the developed method
  is effective with high efficiency value.
6.year:
- 2017
7.type_publication:
- inproceedings
8.doi:
- 10.1109/ICSESS.2017.8343025
1.chave:
- 4592824
2.author:
- Yao, Yiyong
- Dai, Gang
- Zhao, Liping
3.title:
- Embedded quality analysis platform based on HPI technology
4.keywords:
- Data acquisition
- Digital signal processing
- Real time systems
- Quality control
- Driver circuits
- Manufacturing
- Hardware
- Field quality analysis
- HPI driver
- chain table based on priority
- Event-driven
5.abstract:
- The embedded quality data acquisition and analysis platform which meets the demand
  of field real-time quality control against more and more complicated quality control
  in inter-enterprise is presented in this paper. With HPI technology and regionalization
  of HPI memory mapping strategy first being adopted in this platform and the accomplishment
  of HPI driver under Windows CE.net, seamless connection between ARM and DSP is well
  implemented. Based on event-driven strategy for field quality data real-time acquisition
  and chain table priority strategy for multi-task scheduling, real-time mixture quality
  data acquisition and analysis on DSP and the friendly human-interface for real-time
  display and network data exchange on ARM are realized through HPI technology. System
  platform for field data dynamic processing and analysis is implemented and improved
  accordingly.
6.year:
- 2008
7.type_publication:
- inproceedings
8.doi:
- 10.1109/WCICA.2008.4592824
1.chave:
- 8931867
2.author:
- Bicevskis, Janis
- Nikiforova, Anastasija
- Bicevska, Zane
- Oditis, Ivo
- Karnitis, Girts
3.title:
- A Step Towards a Data Quality Theory
4.keywords:
- Data integrity
- Data models
- Data mining
- Analytical models
- Social networking (online)
- data quality
- data object
- domain-specific language
- executable model
5.abstract:
- "Data quality issues have been topical for many decades. However, a unified data\
  \ quality theory has not been proposed yet, since many concepts associated with\
  \ the term \u201Cdata quality\u201D are not straightforward enough. The paper proposes\
  \ a user-oriented data quality theory based on clearly defined concepts. The concepts\
  \ are defined by using three groups of domain-specific languages (DSLs): (1) the\
  \ first group uses the concept of a data object to describe the data to be analysed,\
  \ (2) the second group describes the data quality requirements, and (3) the third\
  \ group describes the process of data quality evaluation. The proposed idea proved\
  \ to be simple enough, but at the same time very effective in identifying data defects,\
  \ despite the different structures of data sets and the complexity of data. Approbation\
  \ of the approach demonstrated several advantages: (a) a graphical data quality\
  \ model allows defining of data quality even by non-IT and non-data quality professionals,\
  \ (b) data quality model is not related to the information system that has accumulated\
  \ data, i.e., this approach lets users analyse the \u201Cthird-party\u201D data,\
  \ and (c) data quality can be described at least at two levels of abstraction -\
  \ informally, using natural language, or formally, including executable program\
  \ routines or SQL statements."
6.year:
- 2019
7.type_publication:
- inproceedings
8.doi:
- 10.1109/SNAMS.2019.8931867
1.chave:
- 5331803
2.author:
- Lu, Jinsuo
- Huang, Tinglin
3.title:
- Data Mining on Forecast Raw Water Quality from Online Monitoring Station Based on
  Decision-Making Tree
4.keywords:
- Data mining
- Monitoring
- Decision making
- Algae
- Predictive models
- Water resources
- Purification
- Beverage industry
- Safety
- Testing
- data mining
- water quality
- forcasting
- decision-making tree
5.abstract:
- The excessive propagation of algae caused by eutrophication of aquatic environment
  in the urban source water supply is the main issues of concern to drinking water
  purification industry in recent years. The prediction of algae in raw water can
  offer time guarantee for operation of contingency caused by excessive propagation
  of algae which can ensure the safety of water supply. In the study, we collected
  115 daily measured data about indirect monitoring of raw water quality of algae
  and solar irradiance data from online monitoring and direct-line artificial monitoring
  of chlorophyll content of raw water. We select decision-making tree which is very
  visible and easy realized as data mining tools, and set up decision-making tree
  model which is used to predict the level of chlorophyll in raw water in next day.
  To enable online monitoring data and artificial monitoring data with the same dimension,
  combined with the algal growth dynamics, we transform several on-line monitoring
  data of dissolved oxygen and solar irradiance data in one day into one data per
  day, that is mean calculating the average standard deviation and average. The former
  100 sets of data are used to train and set up decision-making tree model which is
  to predict the level of chlorophyll in next day. The rest 15 sets of data are used
  to test data. The results of simulation show that the prediction accuracy can reach
  80%.
6.year:
- 2009
7.type_publication:
- inproceedings
8.doi:
- 10.1109/NCM.2009.261
1.chave:
- 9732098
2.author:
- Wrembel, Robert
3.title:
- 'Still Open Problems in Data Warehouse and Data Lake Research: extended abstract'
4.keywords:
- Social networking (online)
- Soft sensors
- Transforms
- Data warehouses
- Big Data applications
- Data models
- Security
- data integration
- data warehouse
- data lake
- big data
- extract transform load
- data processing workflow
- data processing pipeline
- data quality
- ETL optimization
- data source evolution
- metadata
5.abstract:
- 'During recent years, we observe a widespread of new data sources, especially all
  types of social media and IoT devices, which produce huge data volumes, whose content
  ranges from fully structured to totally unstructured. All these types of data are
  commonly referred to as big data. They are typically described by the three most
  important characteristics, called 3V [1], namely: an extremely large volume, a variety
  of data models and structures (data representations), as well as a high velocity
  at which data are generated. We argue that out of these three Vs, the most challenging
  is variety [2]. Such data need to be integrated and transformed into a common representation,
  which is suitable for analysis, in a similar manner as traditional (mainly table-like)
  data.'
6.year:
- 2021
7.type_publication:
- inproceedings
8.doi:
- 10.1109/SNAMS53716.2021.9732098
1.chave:
- 9642916
2.author:
- Kunakov, Egor
- Gulov, Alexei
- Lontsikh, Natalia
- Rodionov, Nikita
- Golovina, Elena
3.title:
- Applicability of Quality Metrics and New Approaches to the PDCA Cycle in Improving
  Quality Control and Management Systems Applied in Aircraft Manufacturing Processes
4.keywords:
- Measurement
- Productivity
- Digital control
- Quality assurance
- Manufacturing processes
- Process control
- Information security
- Information technology
- engineering technology
- information systems
- user interface development
- software product
- quality management system
- risk
- process approach
- continuous improvement
- Deming cycle
5.abstract:
- Digital control and information security technologies are used in modern technological
  processes. Integrated control tools designed to make correct decisions based on
  the electronic models should be used in the development of technological systems.
  The problems of interface quality assurance are due to complex metrics measurement
  and analysis processes. An increase in the number and duration of management time
  delays and mismanagement of large volumes of data increase the number of errors
  and test time and deteriorate the productivity of software tools used for determining
  the applicability of quality metrics. The article describes the process of improvement
  of technological parameters and characteristics by improving the Deming PDCA quality
  management cycle and software interfaces. It describes the role of risk accounting
  in the elimination of inconsistencies in the systems and processes.
6.year:
- 2021
7.type_publication:
- inproceedings
8.doi:
- 10.1109/ITQMIS53292.2021.9642916
1.chave:
- 8290181
2.author:
- Xiu, Q.
- Muro, K.
3.title:
- Robust inference traceability technology for product quality enhancement
4.keywords:
- Quality assessment
- Product design
- Manufacturing processes
- Estimation
- Correlation
- data preparation
- production time
- quality control
- traceability
5.abstract:
- In manufacturing industry, it is important to analyze defect manufacturing data
  for product quality enhancement. Manufacturing data for defect analysis are retrieved
  by individual ID, which is unique identifier assigned to each individual product.
  However, in production lines existing machining processes such as sintering process
  and cutting process, individual ID can't be attached. Hence, there arises a problem
  that manufacturing data for defect analysis can't be prepared. Therefore, we developed
  Robust Inference Traceability (RIT) technology which can accurately estimate individual
  ID in various manufacturing conditions such as processing time variation, parallel
  process and reversal of production order. RIT manages it by absorbing lead time
  variation and estimating production time. The result of applying RIT to actual manufacturing
  data shows that the technology obtained an average inference accuracy of 92.9%.
  As a result, it can be estimated that product quality can be enhanced by defect
  analysis such as root cause analysis.
6.year:
- 2017
7.type_publication:
- inproceedings
8.doi:
- 10.1109/IEEM.2017.8290181
1.chave:
- 9609770
2.author:
- none
3.title:
- Extract Transform Loading (ETL) Based Data Quality for Data Warehouse Development
4.keywords:
- Computer science
- Data integrity
- Loading
- Decision making
- Transforms
- Data warehouses
- Data models
- data quality
- data quality incorporation
- ETL
- style
- data warehouse
5.abstract:
- "Extract Transform Loading (ETL) plays a decisive role in data warehouse (DW) construction.\
  \ It involves retrieval informations from multiple sources to improve information\
  \ quality in DW for decision making process. A DW development relies on the development\
  \ of ETL. Therefore, ETL conceptual model not only represents an overview of overall\
  \ process, but also as a mapping amongst data sources, DW targets and required transformation\
  \ to make sure that data quality (DQ) dimensions are incorporated in order to meet\
  \ the requirements. In this paper, an ETL framework is proposed which incorporates\
  \ data quality to improve information processes in data warehouse development through\
  \ \u2018the story\u2019 of process whilst others framework more to technical approach.\
  \ In order to be useful, the proposed framework compared with other framework in\
  \ case of advantages and disadvantages for future improvement."
6.year:
- 2021
7.type_publication:
- inproceedings
8.doi:
- 10.1109/ICCSAI53272.2021.9609770
1.chave:
- 6815642
2.author:
- Ye, Sijing
- Zhu, Dehai
- Yao, Xiaochuang
- Zhang, Nan
- Fang, Shuai
- Li, Lin
3.title:
- Development of a Highly Flexible Mobile GIS-Based System for Collecting Arable Land
  Quality Data
4.keywords:
- Indexes
- Spatial databases
- Data collection
- Mobile communication
- Data models
- Vectors
- Geographic information systems
- Agricultural data collection
- Android
- arable land quality monitoring
- mobile geographic information system (GIS)
- Agricultural data collection
- Android
- arable land quality monitoring
- mobile geographic information system (GIS)
5.abstract:
- "In recent years, well-designed terminal-based methods for collecting index data\
  \ have gradually replaced traditional pen-and-paper methods and have been extensively\
  \ used in numerous studies. These new approaches offer users increased accuracy,\
  \ efficiency, consumption, and data compatibility compared to traditional methods.\
  \ In general, we find that spatial data content and quality index systems vary widely\
  \ across different arable land regions. Thus, a system for the investigation of\
  \ arable land quality indices that has the flexibility to utilize various types\
  \ of spatial data and quality indices without requiring program modification is\
  \ needed. This paper presents the framework, the module partition, and the structure\
  \ of the data exchange interface for a highly flexible mobile GIS-based system,\
  \ which we call the \u201Carable land quality index data collection system\u201D\
  \ (ALQIDCS). This system incorporates a series of self-adaptive methods, a data\
  \ table-driven model and two types of formulas for flexible data collection and\
  \ processing. We tested our prototype system by investigating arable land quality\
  \ in the Da Xing District, Beijing and in the Te Da La Qi District, Inner Mongolia,\
  \ China. The results indicate that the ALQIDCS can effectively adapt to variations\
  \ in spatial data and quality index systems and meet different objectives. The limitations\
  \ of the ALQIDCS and suggestions for future work are also presented."
6.year:
- 2014
7.type_publication:
- article
8.doi:
- 10.1109/JSTARS.2014.2320635
1.chave:
- 9701096
2.author:
- Deng, Siyang
- Liu, Zhu
- Huang, Lvchao
- Wang, Yonggui
- Chen, Kaiming
3.title:
- Resampled data splicing method based on continuous single-cycle used in power quality
  analysis
4.keywords:
- Interpolation
- Splicing
- Simulation
- Conferences
- Power quality
- Signal sampling
- Data processing
- continuous single cycle
- data splicing
- linear interpolation
- power quality
- resampling
5.abstract:
- When analyzing the spectrum of sampling data received from analog-to-digital converter,
  it usually could not satisfy the constraint that the length of the signal should
  be an integer power of 2 for FFT calculation. The common approach is to increase
  the sampling time or to fill the zeros after the signal, but these methods would
  cause the spectrum leakage or increase the time consuming for analysis. Therefore,
  this paper proposes an improved power quality data processing algorithm based on
  signal interpolation resample for continuous single cycle, and compares it with
  the common methods. The simulation results show that the improved algorithm can
  accurately capture the real frequency characteristics of the signal without increasing
  the signal sampling load or the amount of calculation.
6.year:
- 2021
7.type_publication:
- inproceedings
8.doi:
- 10.1109/ICEI52466.2021.00015
1.chave:
- 6199204
2.author:
- Zhang, Chaogui
- Zheng, Zhiyong
- Zhang, Fuqiang
- Ren, Jiangtao
3.title:
- Multidimensional traffic GPS data quality analysis using data cube model
4.keywords:
- Global Positioning System
- Roads
- Reliability
- Vehicles
- Probes
- Parameter estimation
- Data mining
- traffic GPS data
- data quality
- reliability
- data cube
- roll-up
- OLAP
5.abstract:
- In recent years, transport agencies collect more and more GPS data of probe vehicle,
  data mining on these immense amounts of traffic GPS data is necessary. However,
  since the GPS data are non-uniform and discontinuous in the network, the quality
  of these collected GPS data is unreliable and will be worse when there exists lots
  of noisy data. What's more, if the researchers lack another type of traffic data
  such as loop sensors' data for verification, the result of data mining will become
  unreliable. Therefore, we present an approach for multidimensional traffic GPS data
  quality analysis using data cube model. We propose data valid density, data ideality
  and an overall indicator to describe the quality of GPS data. The experiment results
  show that our approach can describe the data quality status of the network and help
  evaluate the reliability of traffic parameter estimations.
6.year:
- 2011
7.type_publication:
- inproceedings
8.doi:
- 10.1109/TMEE.2011.6199204
1.chave:
- 1640149
2.author:
- Paternostro, C.
- Pruessner, A.
- Semkiw, R.
3.title:
- Designing a quality oceanographic data processing environment
4.keywords:
- Data processing
- Data analysis
- Data visualization
- Software quality
- Quality control
- Software maintenance
- Oceans
- Control system analysis
- Algorithm design and analysis
- Collaborative software
5.abstract:
- Oceanographic data are increasing by data types and volume making present methods
  of processing and determining quality a cumbersome task. The National Oceanic and
  Atmospheric Administration's (NOAA) Center for Operational Oceanographic Products
  and Services (CO-OPS) is developing an end-to-end, state-of-the-art data management
  system to ingest, quality control, analyze, and disseminate water velocity and related
  data. The benefits include streamlining preliminary analysis, thus allowing time
  and resources for more in-depth investigations of the physical phenomena, increasing
  consistency of results between users, and improving overall data quality. The design
  of the system architecture follows a planned structured methodology improving the
  quality of the software developed. Designing a Web-based modular system will allow
  flexibility so the system can accommodate new analyses, reports and plots as well
  as allow for future data types. Well-defined algorithms will be implemented determining
  the quality of both the data and the analyses. This data management system will
  provide oceanographers the means to study water velocity data using a wide suite
  of mathematical and graphical tools. This will allow users to focus on the analysis
  results rather than the process.
6.year:
- 2005
7.type_publication:
- inproceedings
8.doi:
- 10.1109/OCEANS.2005.1640149
1.chave:
- 8054906
2.author:
- Diop, Mouhamed
- Camara, Mamadou
- Fall, Ibrahima
- Bah, Alassane
3.title:
- A methodology for prior management of temporal data quality in a data mining process
4.keywords:
- Data mining
- Data models
- Software
- Databases
- Complexity theory
- Software engineering
- Data Mining
- Data Quality
- Temporal Data
- CRISP-DM
- Software Engineering
- Data warehousing
5.abstract:
- In Data Mining (DM) projects, more specifically in the Data Understanding and the
  Data Preparation phases, several techniques found in the literature are used to
  detect and handle data quality problems such as missing data, outliers, inconsistent
  data or time-variant data. However, the main limitation in the application of these
  techniques is the complexity caused by a lack of anticipation in the detection and
  resolution of data quality problems. Then, a DM process model designed for the prior
  management of data quality was recently proposed. It has the distinctive feature
  of having linked the DM process and the Software Engineering (SE) one by combining
  them in parallel. However, authors of that work [1] have just specified what should
  be done, not how it should be. The present research work is an improvement of that
  DM process model. It adds to it a methodology that indicates in a concrete way a
  guideline on how to combine the SE process and the DM one to anticipate and manage
  data quality problems that can be found during the mining process. This work will
  specifically address the case of temporal data. The main contribution of this methodology
  is the definition, in concrete terms, of how to anticipate and automate all activities
  necessary to remove temporal data quality problems in a mining process.
6.year:
- 2017
7.type_publication:
- inproceedings
8.doi:
- 10.1109/ISACV.2017.8054906
1.chave:
- 9488490
2.author:
- Karakostas, Anastasios
- Poler, Raul
- Fraile, Francisco
- Vrochidis, Stefanos
3.title:
- "Industrial Data Services for Quality Control in Smart Manufacturing \u2013 the\
  \ i4Q Framework"
4.keywords:
- Quality assurance
- Quality control
- Tools
- Reliability engineering
- Production facilities
- Telecommunication computing
- Sensors
- Product Quality
- Process Quality
- Data Quality
- Data Reliability
- Blockchain
- Virtual Sensors
- Digital Twins
- Process Simulation
- Process Optimization
- Zero-defect Manufacturing
5.abstract:
- 'This paper presents a new innovative framework to support smart manufacturing quality
  assurance. More specifically, the i4Q framework provides an IoT-based Reliable Industrial
  Data Services (RIDS), a complete suite consisting of 22 innovative Solutions, able
  to manage the huge amount of industrial data coming from cheap cost-effective, smart,
  and small size interconnected factory devices for supporting manufacturing online
  monitoring and control. The i4Q Framework guarantees data reliability with functions
  grouped into five basic capabilities around the data cycle: sensing, communication,
  computing infrastructure, storage, and analysis-optimization. i4Q RIDS includes
  simulation and optimization tools for manufacturing line continuous process qualification,
  quality diagnosis, reconfiguration and certification for ensuring high manufacturing
  efficiency, leading to an integrated approach to zero-defect manufacturing. This
  paper presents the main principles of the i4Q framework and the relevant industrial
  case studies on which it will be evaluated.'
6.year:
- 2021
7.type_publication:
- inproceedings
8.doi:
- 10.1109/MetroInd4.0IoT51437.2021.9488490
1.chave:
- 4054999
2.author:
- Asheibi, Ali
- Stirling, David
- Robinson, Duane
3.title:
- Identification of Load Power Quality Characteristics using Data Mining
4.keywords:
- Power quality
- Data mining
- Large-scale systems
- Condition monitoring
- Clustering algorithms
- Computer networks
- Distributed computing
- Availability
- Computerized monitoring
- Supervised learning
- power quality
- harmonics
- data mining
5.abstract:
- The rapid increase in computer technology and the availability of large scale power
  quality monitoring data should now motivate distribution network service providers
  to attempt to extract information that may otherwise remain hidden within the recorded
  data. Such information may be critical for identification and diagnoses of power
  quality disturbance problems, prediction of system abnormalities or failure, and
  alarming of critical system situations. Data mining tools are an obvious candidate
  for assisting in such analysis of large scale power quality monitoring data. This
  paper describes a method of applying unsupervised and supervised learning strategies
  of data mining in power quality data analysis. Firstly underlying classes in harmonic
  data from medium and low voltage (MV/LV) distribution systems were identified using
  clustering. Secondly the link analysis is used to merge the obtained clusters into
  supergroups. The characteristics of these super-groups are discovered using various
  algorithms for classification techniques. Finally the a priori algorithm of association
  rules is used to find the correlation between the harmonic currents and voltages
  at different sites (substation, residential, commercial and industrial) for the
  interconnected supergroups
6.year:
- 2006
7.type_publication:
- inproceedings
8.doi:
- 10.1109/CCECE.2006.277720
1.chave:
- 8257913
2.author:
- Benbernou, Salima
- Ouziri, Mourad
3.title:
- Enhancing data quality by cleaning inconsistent big RDF data
4.keywords:
- Resource description framework
- Ontologies
- Sparks
- Cleaning
- Inference algorithms
- Automobiles
- Jacobian matrices
- Big data
- Data quality
- Inconsistency
- Logical inference
- RDF data cleaning
- Apache Spark ecosystems
5.abstract:
- We address the problem of dealing with inconsistencies in fusion of big data sources
  using Resource Description Framework (RDF) and ontologies. We propose a scalable
  approach ensuring data quality for query answering over big RDF data in a distributed
  way on a Spark ecosystem. In so doing, the cleaning inconsistent big RDF data approach
  is built on the following steps (1) modeling consistency rules to detect the inconsistency
  triples even if it is implicitly hidden including inference and inconsistent rules
  (2) detecting inconsistency through rule evaluation based on Apache Spark framework
  to discover the minimally sub-set of inconsistent triples (3) cleaning the inconsistency
  through finding the best repair for consistent query answering.
6.year:
- 2017
7.type_publication:
- inproceedings
8.doi:
- 10.1109/BigData.2017.8257913
1.chave:
- 8786289
2.author:
- Fereira, Marcos
- Silva, Leandro
3.title:
- Data Quality Measurement Framework
4.keywords:
- Tools
- Data integrity
- Loading
- Guidelines
- Indexes
- Data mining
- Software
- Data Quality, Data Profiling, Dat Mining, Data Governance, preprocessing
5.abstract:
- Data Quality evaluation is a key fundamental in Knowledge Data Discovery projects.
  There are some project frameworks, like CRISP-DM and DAMA DMBOK, that recommend
  the preparation of the Data Quality Report, as a tool to describe the found problems
  during the data exploration phase and to describe an approach to fix those problems.
  However, those frameworks are very generic in their guidelines and neither tell
  what exactly should be measured nor how to associate any measure to the data quality.
  Data Profiling tools and some ETL(Extraction, Transformation and Loading) tools
  as well, implement some basic Statistical Description tooling, but they do not propose
  any general methodology to evaluate quantitatively the quality of a set of data,
  except, perhaps, in the IBM Watson Analytics tool. This article proposes a quantitative
  measure for data quality evaluation, based on Statistical Description tools.
6.year:
- 2018
7.type_publication:
- inproceedings
8.doi:
- 10.1109/CLEI.2018.00061
1.chave:
- 5707184
2.author:
- Comerio, Marco
- Truong, Hong-Linh
- Batini, Carlo
- Dustdar, Schahram
3.title:
- Service-oriented data quality engineering and data publishing in the cloud
4.keywords:
- Cloud computing
- Publishing
- Data models
- Computational modeling
- Computer architecture
- Business
- Databases
- Data Quality Engineering
- Cloud Computing
- Software-as-a-Service
- Data-as-a-Service
- Crowdsourcing
5.abstract:
- Traditional data quality engineering techniques, often used and deployed within
  a single enterprise environment, are inadequate to cope with the rapid change of
  data, with a multitude of quality degrees, to be used in contemporary business models.
  The emerging cloud computing paradigm could potentially offer high-quality, composable
  data and techniques, under the Software-as-a-Service (SaaS), Data-asa-Service (DaaS)
  and crowdsourcing models, for data quality engineering and data publishing. However,
  so far how to utilize the potential of cloud computing models for data quality engineering
  has not been discussed. In this paper, we analyze requirements of data quality engineering
  and quality-aware data publishing processes in the cloud and we provide a conceptual
  architecture utilizing and supporting the SaaS, DaaS and crowdsourcing models for
  the realization of such processes.
6.year:
- 2010
7.type_publication:
- inproceedings
8.doi:
- 10.1109/SOCA.2010.5707184
1.chave:
- 8843451
2.author:
- Doku, Ronald
- Rawat, Danda
- Liu, Chunmei
3.title:
- Towards Federated Learning Approach to Determine Data Relevance in Big Data
4.keywords:
- Data models
- Blockchain
- Machine learning
- Mobile handsets
- Data privacy
- Cryptography
- Big Data
- Federated Learning Approach, Data Relevance, Big Data Analytics, Machine Learning
5.abstract:
- In the past few years, data has proliferated to astronomical proportions; as a result,
  big data has become the driving force behind the growth of many machine learning
  innovations. However, the incessant generation of data in the information age poses
  a needle in the haystack problem, where it has become challenging to determine useful
  data from a heap of irrelevant ones. This has resulted in a quality over quantity
  issue in data science where a lot of data is being generated, but the majority of
  it is irrelevant. Furthermore, most of the data and the resources needed to effectively
  train machine learning models are owned by major tech companies, resulting in a
  centralization problem. As such, federated learning seeks to transform how machine
  learning models are trained by adopting a distributed machine learning approach.
  Another promising technology is the blockchain, whose immutable nature ensures data
  integrity. By combining the blockchain's trust mechanism and federated learning's
  ability to disrupt data centralization, we propose an approach that determines relevant
  data and stores the data in a decentralized manner.
6.year:
- 2019
7.type_publication:
- inproceedings
8.doi:
- 10.1109/IRI.2019.00039
1.chave:
- 9148143
2.author:
- Lu, Xinghua
- Liu, Peihao
- Nie, Weidong
- Zhang, Hao
3.title:
- Whole Process Tracing Model of Pigeon Quality in Block Chain Environment
4.keywords:
- Data models
- Data mining
- Production
- Data integration
- Feature extraction
- Process control
- block chain
- Pigeons
- Quality
- Tracing the whole process
5.abstract:
- 'In order to improve the whole process traceability of meat pigeon quality, a whole
  process traceability model of meat pigeon quality based on block chain data fusion
  is proposed. The method comprises the following steps of: constructing a statistical
  information distribution model for tracing the whole process of meat pigeon quality;
  reorganizing the structure of information sources for tracing the whole process
  of meat pigeon quality by adopting a data structure reorganization method; establishing
  an information source characteristic distribution model for tracing the whole process
  of meat pigeon quality; carrying out tracking identification and large data mining
  of meat pigeon quality information by adopting an association rule mining method
  under a block chain mode; constructing a meat pigeon quality whole process tracing
  model; and combining information extraction and optimal scheduling of meat pigeon
  quality. The quantitative feature distribution set of meat pigeon quality is extracted,
  and the statistical feature analysis of the whole process traceability of meat pigeon
  quality is realized by combining the information detection and feature positioning
  methods. The dynamic evaluation of meat pigeon quality information is realized by
  using the meat pigeon quality statistical large data analysis method. The optimization
  design of the whole process traceability model of meat pigeon quality is realized
  by combining the block chain data fusion and the knowledge map feature analysis
  method. The simulation results show that the method has good real-time performance,
  strong dynamic tracing ability and good information positioning ability for the
  quality of meat pigeons.'
6.year:
- 2020
7.type_publication:
- inproceedings
8.doi:
- 10.1109/CIBDA50819.2020.00104
1.chave:
- 8258218
2.author:
- Colborne, Adrienne
- Smit, Michael
3.title:
- Identifying and mitigating risks to the quality of open data in the post-truth era
4.keywords:
- Big Data
- Meteorology
- Portals
- Voting
- open data
- post-truth
- fake news
- risk identification
- risk mitigation
- data quality assurance
5.abstract:
- 'Big Data analysis often relies on open data, integrating it with large private
  data sets, using it as ground truth information, or providing it as part of the
  input to large simulations. Data can be released openly by governments to achieve
  various objectives: transparency, informing citizen engagement, or supporting private
  enterprise, to name a few. To the latter objective, Big Data analytics algorithms
  rely on high-quality, timely access to various data sources, including open data.
  Examples include retail analytics drawing on open demographic data and weather forecast
  systems drawing on open weather and climate data. In this paper, we describe the
  rise of post-truth in society, and the risks this poses to the quality, integrity,
  and authenticity of open data. We also discuss approaches to identifying, assessing,
  and mitigating these risks, and suggest future steps to manage this data quality
  concern.'
6.year:
- 2017
7.type_publication:
- inproceedings
8.doi:
- 10.1109/BigData.2017.8258218
1.chave:
- 5521667
2.author:
- Masseroli, Marco
- Ghisalberti, Giorgio
- Tettamanti, Luca
3.title:
- Detection of Errors and Inconsistencies in Biomolecular Databases through Integrative
  Approaches and Quality Controls
4.keywords:
- Error correction
- Databases
- Quality control
- Ontologies
- Bioinformatics
- Intrusion detection
- Genomics
- Data warehouses
- Scattering
- Automatic testing
- Data quality
- Biological data warehousing
5.abstract:
- Most of the available biomolecular data are scattered in many databases, are computationally
  derived and include errors and inconsistencies. Here we show an integrative approach
  and a set of automatic procedures to test the quality of genomic and proteomic data
  from several different biomolecular databases integrated in our GFINDer data warehouse
  (http://www.bioinformatics.polimi.it/GFINDer/).
6.year:
- 2010
7.type_publication:
- inproceedings
8.doi:
- 10.1109/BIBE.2010.60
1.chave:
- 9274628
2.author:
- Sulistyo, Haidar
- Kusumasari, Tien
- Alam, Ekky
3.title:
- Implementation of Data Cleansing Pattern Module for Data Quality Management Application
  using Open Source Tools
4.keywords:
- Tools
- Data integrity
- Government
- Drugs
- Standards organizations
- Open source software
- Informatics
- data cleansing
- data quality
- data quality management
- open-source tools
- pattern
5.abstract:
- In today's digital world, maintaining the quality of the data is emerging as the
  keystone of business to obtain high quality data to make sure that the organizations
  create accurate and wise decision-making. Data were taken from various applications
  and mostly still raw and to transform the data into valuable and valid information,
  we need the data quality management in an organization is required to maintain its
  quality. The insight of the data is an essential tool for decision-makers. Good
  quality data exists when the data has been processed with data quality management.
  Good quality data has several beneficial impacts on organizations, and the ever-increasing
  amount of data in the rapidly expanding technological world of today makes analyzing
  it much more exciting. Therefore, this research will discuss one of the methods
  in data quality management that are data cleansing. We propose the design and implementation
  of pattern modules in data cleansing processing using open source tools to make
  the process of data cleansing of food and drug distribution permits in Indonesian
  government agencies more flexible and also create uniformity of the pattern data
  to obtain more insight to explore.
6.year:
- 2020
7.type_publication:
- inproceedings
8.doi:
- 10.1109/IC2IE50715.2020.9274628
1.chave:
- 9565819
2.author:
- Liu, Yongnan
- Guan, Xin
- Peng, Yu
- Chen, Hongyang
- Ohtsuki, Tomoaki
- Han, Zhu
3.title:
- Blockchain-Based Task Offloading for Edge Computing on Low-Quality Data via Distributed
  Learning in the Internet of Energy
4.keywords:
- Task analysis
- Blockchains
- Data integrity
- Edge computing
- Organizations
- Data privacy
- Reinforcement learning
- Internet of energy
- data quality
- consortium blockchain
- distributed learning
- task offloading
5.abstract:
- 'With the development of the Internet of energy, more and more participants share
  data by different types of edge devices. However, such multi-source heterogenous
  data typically contain low-quality data, e.g., missing values, which may result
  in potential risks. Besides, resource-constrained devices incur large latency in
  edge computing networks. To alleviate such latency, distributed task offloading
  schemes are designed to share the computation burden between edge nodes and nearby
  servers. However, there are three main drawbacks of such schemes. First, low-quality
  data are not carefully evaluated by constraints under scenarios, which may result
  in slow convergence in distributed computation. Second, multi-source data including
  sensitive information are computed and shared among edge nodes without privacy protection.
  Third, distributed tasks on low-quality data may result in low-quality results even
  with an optimal offloading scheme. To address the problems above, a task offloading
  framework for edge computing based on consortium blockchain and distributed reinforcement
  learning is proposed in this paper, which can provide high-quality task offloading
  policies with data privacy protected. This framework consists of three key components:
  data quality evaluation (DQ) with multiple data quality dimensions, data repairing
  (DR) with a repairing algorithm based on a novel repairing consensus mechanism and
  distributed reinforcement learning for task arrangement (DELTA) with a distributed
  reinforcement learning algorithm based on a novel low-quality data distributing
  strategy. Numeric results are presented to illustrate the effectiveness and efficiency
  of the proposed task offloading framework for edge computing on low-quality data
  in the IoE.'
6.year:
- 2022
7.type_publication:
- article
8.doi:
- 10.1109/JSAC.2021.3118417
1.chave:
- 9544855
2.author:
- Philip, Stephin
- Vashisth, Pawan
- Chaturvedi, Anant
- Gupta, Neha
3.title:
- Imputation of Missing Values using Improved K-Means Clustering Algorithm to Attain
  Data Quality
4.keywords:
- Measurement
- Data integrity
- Loading
- Data integration
- Clustering algorithms
- Euclidean distance
- Transforms
- Data Warehouse
- Data Quality
- Extract Transform and Load
- Data Purgation
- Missing Data
- Data Extraction
- Data Transformation
- Data Loading
5.abstract:
- A data warehouse aids in the management of large amounts of data that may be stored
  in order to handle user input during the computer process. The major issue with
  a data warehouse is to maintain the data that the user stores in good quality. Some
  traditional techniques can improve data quality while also increasing efficiency.
  Each unit of data has a unique feature that has been researched by many researchers
  and has an influence on data quality. This research article has enhanced the K-Means
  method by utilizing the Euclidean Distance metric to detect missing values from
  the gathered sources and replace them with closest values while maintaining the
  data's consistency, exactness, and quality. yThe improved data will assist developers
  in analysing data quality prior to data integration by allowing them to make informed
  decisions quickly in accordance with business requirements. Improved K-Means achieves
  better accuracy and requires less computational time for clustering data objects
  when compared to other related approaches.
6.year:
- 2021
7.type_publication:
- inproceedings
8.doi:
- 10.1109/ICIRCA51532.2021.9544855
1.chave:
- 9458716
2.author:
- Ding, Xiaoou
- Wang, Hongzhi
- Su, Jiaxuan
- Wang, Muxian
- Li, Jianzhong
- Gao, Hong
3.title:
- Leveraging Currency for Repairing Inconsistent and Incomplete Data (Extended Abstract)
4.keywords:
- Data integrity
- Conferences
- Maintenance engineering
- Data engineering
- Cognition
- Cleaning
- Currencies
- Data cleaning
- data quality management
- currency determining
- temporal data repairing
5.abstract:
- With the growth of data from various sources, data quality is faced with multiple
  problems. In this paper, we study the multiple data cleaning on incompleteness and
  inconsistency with currency reasoning and determination. We introduce a 4-step method,
  named Imp3C, for error detection and repair in incomplete and inconsistent data
  without timestamps. We propose an integrated currency determining approach to compute
  currency order among tuples, thus, the dirty data can be repaired effectively considering
  the temporal impact. Experiments on three real-life datasets verify that Imp3C improves
  data repairing performance with multiple quality problems, especially in datasets
  with complex currency orders.
6.year:
- 2021
7.type_publication:
- inproceedings
8.doi:
- 10.1109/ICDE51399.2021.00243
1.chave:
- 9403739
2.author:
- Wei, Li
- Dawei, Wang
- Lixia, Wang
3.title:
- Research on data Traceability Method Based on blockchain Technology
4.keywords:
- Industries
- Technological innovation
- Distributed databases
- Blockchain
- Data models
- Internet
- Safety
- blockchain
- data traceability
- data quality
- data security
- data governance
- energy Internet
- huge data
5.abstract:
- Energy Internet is a major innovation to deal with the environmental crisis and
  efficient energy management and use in the current society. The important condition
  to achieve this goal is to summarize, integrate, process and apply the data of various
  industries in the energy field, and then support the relevant management and decision-making.
  In this process, how to ensure the authenticity and credibility of data is one of
  the keys in the construction of energy Internet. Therefore, this paper will study
  the application scenarios of blockchain technology in data traceability. With the
  help of the natural characteristics of blockchain, such as decentralized, distributed
  storage, tamper proof, open and transparent, combined with relevant national standards
  and international theoretical models, based on the needs of energy Internet data
  integration and management, this paper will develop a data traceability method suitable
  for the energy industry, and build a covering energy Data life cycle model of Internet.
  Through the research of this paper, we can help all localities to establish data
  traceability mechanism in the energy Internet, to help users to accurately grasp
  where the data is created, what systems have been transferred, which users have
  carried out query and modification, and so on, so as to realize the monitoring and
  control of the whole process of data flow, which helps to improve the credibility
  of data, and also helps to ensure the safety and quality of data and promote the
  construction of energy Internet huge data application.
6.year:
- 2020
7.type_publication:
- inproceedings
8.doi:
- 10.1109/ICBASE51474.2020.00017
1.chave:
- 8843473
2.author:
- Homayouni, Hajar
- Ghosh, Sudipto
- Ray, Indrakshi
3.title:
- 'ADQuaTe: An Automated Data Quality Test Approach for Constraint Discovery and Fault
  Detection'
4.keywords:
- Drugs
- Data integrity
- Databases
- Data warehouses
- Data models
- Semantics
- Self-organizing feature maps
- Data quality tests
- Database
- Data warehouse
- Explainable learning
- Machine learning
- Unsupervised learning
5.abstract:
- Data quality tests validate the data stored in databases and data warehouses to
  detect violations of syntactic and semantic constraints. Domain experts grapple
  with the issues related to the capturing of all the important constraints and checking
  that they are satisfied. Domain experts often define the constraints in an ad hoc
  manner based on their knowledge of the application domain and needs of the stakeholders.
  We propose ADQuaTe, which is an automated data quality test approach that uses an
  unsupervised machine learning technique to discover constraints that may have been
  missed by experts. ADQuaTe marks records that violate the constraints as suspicious
  and explains the violations. We evaluate ADQuaTe on real-world applications using
  a health data warehouse and a plant diagnosis database to demonstrate that the approach
  can uncover previously detected as well as new faults in the data.
6.year:
- 2019
7.type_publication:
- inproceedings
8.doi:
- 10.1109/IRI.2019.00023
1.chave:
- 9640529
2.author:
- Zhao, Yuxi
- Gong, Xiaowen
- Lin, Fuhong
- Chen, Xu
3.title:
- Data Poisoning Attacks and Defenses in Dynamic Crowdsourcing with Online Data Quality
  Learning
4.keywords:
- Crowdsourcing
- Task analysis
- Data integrity
- Sensors
- Data aggregation
- Mobile computing
- Estimation
- Data crowdsourcing
- data poisoning attack
- online quality learning
- defense
5.abstract:
- Crowdsourcing has found a variety of applications. To improve data accuracy and
  cost-effectiveness, workers' data quality can be learned from their data in an online
  manner. However, crowdsourcing is vulnerable to data poisoning attacks, where the
  attacker reports malicious data to reduce aggregated data accuracy. In this paper,
  we study malicious data attacks on dynamic crowdsourcing where tasks are assigned
  and performed sequentially, and we explore online quality learning as a defense
  mechanism against the attack. We first focus on the asymptotic setting where workers'
  quality is accurately learned, based on which we turn to the non-asymptotic setting
  where the quality is estimated online with errors. For each setting, we characterize
  the conditions under which the attack strategy is effective. Our results show that
  the malicious noise variance needs to be within a certain range for the attack to
  be effective. Then we analyze the harm of effective attack. It reveals that the
  regret of the online quality learning algorithm can be substantially increased due
  to effective attacks. To further mitigate the attack, we also study median and MIE-based
  data aggregation as defense mechanisms. We evaluate the proposed attacks and defenses
  via simulation results based on real-world data.
6.year:
- 2021
7.type_publication:
- article
8.doi:
- 10.1109/TMC.2021.3133365
1.chave:
- 8407886
2.author:
- Tong, Pengfei
- Lu, Junguo
- Yun, Kihyeon
3.title:
- Fault detection for semiconductor quality control based on Spark using data mining
  technology
4.keywords:
- Data models
- Data mining
- Feature extraction
- Analytical models
- Quality control
- Semiconductor device modeling
- Manufacturing processes
- Quality control
- Data mining
- Statistical learning
- Feature extraction
- Parallel computing
- Classification
5.abstract:
- The aim of this paper is to discuss how to apply data mining technology to semiconductor
  manufacturing process quality control. The significance of this paper is that it
  solves a practical engineering problem and is not limited to theoretical analysis.
  This paper proposes and completes a complete semiconductor quality control program,
  including the problem analysis, the field semiconductor data collection, the data
  preprocessing, the feature selection, the classification model selection, the model
  building, the model testing, the model contrast, and the model improvement. The
  data preprocessing includes data cleaning, data standardization, data formatting,
  and so on. And the paper uses the Fisher criterion algorithm to select useful features.
  In addition, this paper selects two data mining algorithms (SVM and Random Forest)
  based on the distributed computing platform (Spark) and establishes the corresponding
  models for analysis. After analyzing and comparing the models, it is found that
  Random Forest has stronger anti-overfitting ability and is more suitable for the
  semiconductor quality control.
6.year:
- 2018
7.type_publication:
- inproceedings
8.doi:
- 10.1109/CCDC.2018.8407886
