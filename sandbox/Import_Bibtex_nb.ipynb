{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# IMPORTANDO ARQUIVOS BIBTEX e EXPORTANDO EM FORMATAÇÃO YAML\n",
    "- Arquivo texto em formato BIBTEX\n",
    "- Objetivo: Data quality de arquivos BIBtex\n",
    "- Extensão: .bib\n",
    "- Biblioteca: pybtex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pybtex.database.input import bibtex\n",
    "from pybtex.database import BibliographyData, Entry\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# #################################################################\n",
    "## Inicia importanção do arquivo\n",
    "# #################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_path = \"C:\\\\Users\\\\victo\\\\PycharmProjects\\\\BibtexDataQuality\\\\source\\\\\"\n",
    "output_path = \"C:\\\\Users\\\\victo\\\\PycharmProjects\\\\BibtexDataQuality\\\\output\\\\\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def arquivos(path, fendwith = ''):\n",
    "    if fendwith not in '':\n",
    "        caminhos = {nome : os.path.join(path, nome) for nome in os.listdir(path) if nome.endswith(fendwith)}\n",
    "    else:\n",
    "        caminhos = {nome : os.path.join(path, nome) for nome in os.listdir(path)}\n",
    "    return caminhos\n",
    "\n",
    "def author_names(author):\n",
    "    try:\n",
    "        return author.persons['author'][0:]\n",
    "    except:\n",
    "        return {'author': [(u'none, none')]}\n",
    "\n",
    "def join_names(person):\n",
    "    try:\n",
    "        return person.last_names[0] + ', ' + person.first_names[0]\n",
    "    except:\n",
    "        return []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inicia tratamento dos dados BIBTEX\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Article: Um artigo de um periódico ou revista.\n",
    "######    Campos obrigatórios: author, title, journal, year\n",
    "######    Campos opcionais: volume, number, pages, month, note, key\n",
    "#### Inproceedings: A entrada Inproceedings é usada para referenciar um artigo em um proceedings de uma conferência.\n",
    "###### Campos obrigatórios: author, title, booktitle, year \n",
    "###### Campos opcionais: editor, volume/number, series, pages, address, month, organization, publisher, note, key\n",
    "#### Incollection: A entrada Incollection é usada para referenciar uma parte (em geral um captulo) de uma coletânea. \n",
    "###### Campos obrigatórios: author, title, booktitle, publisher, year \n",
    "###### Campos opcionais: editor, volume/number, series, type, chapter, pages, address, edition, month, note, key\n",
    "#### inbook: O tipo de entrada em livro destina-se a ser usado para seções ou capítulos em um livro de autoria ou edição.\n",
    "###### Campos obrigatórios: author/editor, title, chapter/pages, publisher, year \n",
    "###### Campos opcionais: volume/number, series, type, address, edition, month, note, key\n",
    "http://paginapessoal.utfpr.edu.br/jamhour/publicacoes/arquivos/00_Compilado_JabRef_dez2015.pdf\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst_files = arquivos(source_path, '.bib')\n",
    "parser = bibtex.Parser()\n",
    "# file = parser.parse_file(lst_files['ACM.bib'])\n",
    "dict_file_fields = {}\n",
    "\n",
    "#verifica a estrutura de campos para cada tipo de publicação\n",
    "#cria um dicionario usando como chave a fonte e o tipo de publicação\n",
    "for f in lst_files:\n",
    "    parser = bibtex.Parser()\n",
    "    file = parser.parse_file(lst_files[f])\n",
    "    f_name = f.replace('.bib','')\n",
    "    for i in file.entries.values():\n",
    "        \n",
    "        for a in sorted(list(i.fields.keys())):\n",
    "\n",
    "            # if f_name + '-' + i.type not in dict_file_fields.keys():\n",
    "            #     dict_file_fields[f_name + '-' + i.type] = []\n",
    "            # else:\n",
    "            #     if a not in dict_file_fields[f_name + '-' + i.type]:\n",
    "            #         dict_file_fields[f_name + '-' +i.type].append(a)\n",
    "\n",
    "            if i.type not in dict_file_fields.keys():\n",
    "                dict_file_fields[i.type] = []\n",
    "            else:\n",
    "                if a not in dict_file_fields[i.type]:\n",
    "                    dict_file_fields[i.type].append(a)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'address': 'New York, NY, USA', 'booktitle': 'Proceedings of the 2019 International Conference on Big Data Engineering', 'doi': '10.1145/3341620.3341629', 'isbn': '9781450360913', 'keywords': ['Sentiment analysis, Big data, Big data quality metrics'], 'location': 'Hong Kong, Hong Kong', 'numpages': '8', 'pages': '36–43', 'publisher': 'Association for Computing Machinery', 'series': 'BDE 2019', 'title': 'Big Data Quality Metrics for Sentiment Analysis Approaches', 'url': 'https://doi.org/10.1145/3341620.3341629', 'year': '2019', 'abstract': 'In a world increasingly connected, and in which information flows quickly and affects a very large number of people, sentiment analysis has seen a spectacular development over the past ten years. This is due to the fact that the explosion of social networks has allowed anyone with internet access to publicly express his opinion. Moreover, the emergence of big data has brought enormous opportunities and powerful storage and analytics tools to the field of sentiment analysis. However, big data introduces new variables and constraints that could radically affect the traditional models of sentiment analysis. Therefore, new concerns, such as big data quality, have to be addressed to get the most out of big data. To the best of our knowledge, no contributions have been published so far which address big data quality in SA throughout its different processes. In this paper, we first highlight the most important big data quality metrics to consider in any big data project. Then, we show how these metrics could be specifically considered in SA approaches and this for each phase in the big data value chain.', 'articleno': '', 'ISSN': '', 'month': '', 'number': '', 'volume': '', 'publicationtype': 'inproceedings', 'author': ['El, Imane', 'Gahi, Youssef', 'Messoussi, Rochdi']}\n"
     ]
    }
   ],
   "source": [
    "lst_files = arquivos(source_path)\n",
    "parser = bibtex.Parser()\n",
    "# file = parser.parse_file(lst_files['IEEE.bib'])\n",
    "dict = {}\n",
    "\n",
    "# print(file)\n",
    "for f in lst_files:\n",
    "    if f == 'IEEE.bib':\n",
    "        parser = bibtex.Parser()\n",
    "        file = parser.parse_file(lst_files[f])\n",
    "\n",
    "        for i in file.entries.values():\n",
    "            key = i.key\n",
    "            dict[key] = {i.fields.get('fields', fields) : i.fields.get(fields, '')\\\n",
    "                        for fields in dict_file_fields[i.type]}\n",
    "            \n",
    "            dict[key]['publicationtype'] = i.type\n",
    "            dict[key]['author'] = [join_names(person) for person in i.persons.get('author', '')]\n",
    "\n",
    "            dict[key].update({'keywords' : dict[key]['keywords'].split(';')})\n",
    "\n",
    "    if f == 'ACM.bib':\n",
    "        parser = bibtex.Parser()\n",
    "        file = parser.parse_file(lst_files[f])\n",
    "\n",
    "        for i in file.entries.values():\n",
    "            key = i.key[-7:]\n",
    "            dict[key] = {i.fields.get('fields', fields) : i.fields.get(fields, '')\\\n",
    "                        for fields in dict_file_fields[i.type]}\n",
    "            \n",
    "            dict[key]['publicationtype'] = i.type\n",
    "            dict[key]['author'] = [join_names(person) for person in i.persons.get('author', '')]\n",
    "\n",
    "            dict[key].update({'keywords' : dict[key]['keywords'].split(';')})\n",
    "           \n",
    "print(dict['3341629'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inicia tratamento dos dados CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(lst)\n",
    "\n",
    "# #################################################################\n",
    "# #Verificar se os dados estão corretos em cada coluna\n",
    "# #################################################################\n",
    "\n",
    "# lst_field = ['chave', 'author', 'title', 'keywords', 'abstract', 'year', 'type_publication', 'doi']\n",
    "# for f in lst_field:\n",
    "#     for q in lst[0:]:\n",
    "#         print(q[f])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeEncodeError",
     "evalue": "'charmap' codec can't encode character '\\u2010' in position 663: character maps to <undefined>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeEncodeError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [478], line 27\u001b[0m\n\u001b[0;32m     25\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(output_path\u001b[39m+\u001b[39mfile, \u001b[39m'\u001b[39m\u001b[39mw\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mas\u001b[39;00m nfile:\n\u001b[0;32m     26\u001b[0m         writer \u001b[39m=\u001b[39m csv\u001b[39m.\u001b[39mwriter(nfile)\n\u001b[1;32m---> 27\u001b[0m         writer\u001b[39m.\u001b[39;49mwriterow(data)\n\u001b[0;32m     29\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mopen\u001b[39m(output_path\u001b[39m+\u001b[39mfile)\u001b[39m.\u001b[39mread())\n",
      "File \u001b[1;32mc:\\Users\\victo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\encodings\\cp1252.py:19\u001b[0m, in \u001b[0;36mIncrementalEncoder.encode\u001b[1;34m(self, input, final)\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mencode\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m, final\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[1;32m---> 19\u001b[0m     \u001b[39mreturn\u001b[39;00m codecs\u001b[39m.\u001b[39;49mcharmap_encode(\u001b[39minput\u001b[39;49m,\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49merrors,encoding_table)[\u001b[39m0\u001b[39m]\n",
      "\u001b[1;31mUnicodeEncodeError\u001b[0m: 'charmap' codec can't encode character '\\u2010' in position 663: character maps to <undefined>"
     ]
    }
   ],
   "source": [
    "# #################################################################\n",
    "# #Exporta arquivo YAML\n",
    "# #################################################################\n",
    "\n",
    "import yaml\n",
    "import json\n",
    "import csv\n",
    "\n",
    "t_out = 'json'\n",
    "\n",
    "file = 'output.' + t_out\n",
    "lst_field = ['author', 'title', 'keywords', 'abstract', 'year', 'type_publication', 'doi']\n",
    "\n",
    "if t_out == 'yaml':\n",
    "    with open(output_path+file, 'w') as nfile:\n",
    "        for data in dict.items():\n",
    "            yaml.dump(data, nfile)  # insere os dados na configuração YAML\n",
    "\n",
    "if t_out == 'json':\n",
    "    with open(output_path+file, 'w') as nfile:\n",
    "        for data in dict.items():\n",
    "            json.dump(data, nfile)  # insere os dados na configuração YAML\n",
    "\n",
    "if t_out == 'csv':\n",
    "    with open(output_path+file, 'w') as nfile:\n",
    "        writer = csv.writer(nfile)\n",
    "        writer.writerow(data)\n",
    "\n",
    "print(open(output_path+file).read())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "253cd15c32ffb283b0558be93b097dc27093fe6b891853f454fa08d2f15e4a97"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
