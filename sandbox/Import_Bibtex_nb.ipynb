{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# DATA QUALITY (Bibtex e csv)\n",
    "- Arquivo texto em formato BIBTEX\n",
    "- Objetivo: Data quality de input diversos(csv, bibtex e APIs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import hashlib\n",
    "import yaml\n",
    "import json\n",
    "import csv\n",
    "import requests\n",
    "from numpy import nan\n",
    "from pybtex.database.input import bibtex\n",
    "from pybtex.database import BibliographyData, Entry\n",
    "from unicodedata import normalize\n",
    "\n",
    "from flask import jsonify, Flask, render_template, request\n",
    "import werkzeug\n",
    "\n",
    "pd.set_option(\"display.max_columns\", 50)\n",
    "pd.set_option(\"display.max_rows\", 100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Criação de funções"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################## LISTAGEM DE ARQUIVOS\n",
    "def arquivos(path, fendwith = ''):\n",
    "    if fendwith not in '':\n",
    "        caminhos = {nome : os.path.join(path, nome) for nome in os.listdir(path) if nome.endswith(fendwith)}\n",
    "    else:\n",
    "        caminhos = {nome : os.path.join(path, nome) for nome in os.listdir(path)}\n",
    "    return caminhos\n",
    "\n",
    "######################## LISTAGEM DE AUTORES\n",
    "def author_names(author):\n",
    "    try:\n",
    "        return author.persons['author'][0:]\n",
    "    except:\n",
    "        return {'author': [(u'none, none')]}\n",
    "\n",
    "######################## CONCATENA AUTORES\n",
    "def join_names(person):\n",
    "    try:\n",
    "        return person.last_names[0] + ', ' + person.first_names[0]\n",
    "    except:\n",
    "        return []\n",
    "\n",
    "######################## OUTPUT POR TIPO DE EXTENSAO(JSON, YAML E CSV)\n",
    "def type_output_file(o_data, f_type, filename, path):\n",
    "\n",
    "    path_complete = path + filename + '.' + f_type\n",
    "\n",
    "    if f_type == 'yaml':\n",
    "        with open(path_complete, 'w') as nfile:\n",
    "            yaml.dump(o_data.to_dict(orient='records'), nfile)\n",
    "\n",
    "    if f_type == 'json':\n",
    "        with open(path_complete, 'w') as nfile:\n",
    "            result = o_data.to_json(orient=\"records\")\n",
    "            parsed = json.loads(result)\n",
    "            json.dump(parsed, nfile, indent=4)  \n",
    "\n",
    "    if f_type == 'csv':\n",
    "        o_data.to_csv(path_complete, sep='§')\n",
    "\n",
    "######################## CARGA DO ARQUIVO DE CONFIGURACAO\n",
    "def load_config(n_file):\n",
    "    with open(n_file, 'r') as config_file:\n",
    "        return yaml.load(config_file, Loader=yaml.SafeLoader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inicia importanção dos arquivos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BIBTEX\n",
    "- ref bibtex doc: http://paginapessoal.utfpr.edu.br/jamhour/publicacoes/arquivos/00_Compilado_JabRef_dez2015.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_path = \"C:\\\\Users\\\\victo\\\\PycharmProjects\\\\BibtexDataQuality\\\\source\\\\\"\n",
    "\n",
    "\n",
    "lst_files = arquivos(source_path, '.bib')\n",
    "parser = bibtex.Parser()\n",
    "# file = parser.parse_file(lst_files['ACM.bib'])\n",
    "dict_file_fields = {}\n",
    "list_file_fields = []\n",
    "\n",
    "#verifica a estrutura de campos para cada tipo de publicação\n",
    "#cria um dicionario usando como chave a fonte e o tipo de publicação\n",
    "for f in lst_files:\n",
    "    parser = bibtex.Parser()\n",
    "    file = parser.parse_file(lst_files[f])\n",
    "    f_name = f.replace('.bib','')\n",
    "    for i in file.entries.values():\n",
    "        \n",
    "        for a in sorted(i.fields.keys()):\n",
    "           \n",
    "            if a not in list_file_fields:\n",
    "                list_file_fields.append(a)\n",
    "list_file_fields.remove('ISSN')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### importanção e tratamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst_files = arquivos(source_path)\n",
    "parser = bibtex.Parser()\n",
    "# file = parser.parse_file(lst_files['IEEE.bib'])\n",
    "dict_bib = {}\n",
    "\n",
    "for f in lst_files:\n",
    "    \n",
    "    ############# IEEE\n",
    "    if 'IEEE' in f :\n",
    "        parser = bibtex.Parser()\n",
    "        file = parser.parse_file(lst_files[f])\n",
    "\n",
    "        for i in file.entries.values():\n",
    "            key = i.key\n",
    "            dict_bib[key] = {i.fields.get('fields', fields) : i.fields.get(fields, '')\\\n",
    "                            for fields in list_file_fields}\n",
    "            \n",
    "            dict_bib[key]['type_publication'] = i.type\n",
    "            dict_bib[key]['author'] = [join_names(person) for person in i.persons.get('author', '')]\n",
    "\n",
    "            dict_bib[key].update({'keywords' : dict_bib[key]['keywords'].split(';')})\n",
    "    \n",
    "    ############## ACM\n",
    "    if 'acm' in f:\n",
    "        parser = bibtex.Parser()\n",
    "        file = parser.parse_file(lst_files[f])\n",
    "\n",
    "        for i in file.entries.values():\n",
    "            key = i.key\n",
    "            dict_bib[key] = {i.fields.get('fields', fields) : i.fields.get(fields, '')\\\n",
    "                            for fields in list_file_fields}\n",
    "            \n",
    "            dict_bib[key]['type_publication'] = i.type\n",
    "            dict_bib[key]['author'] = [join_names(person) for person in i.persons.get('author', '')]\n",
    "\n",
    "            dict_bib[key].update({'keywords' : dict_bib[key].get('keywords', '').split(';')})\n",
    "\n",
    "            if i.type == 'inbook':\n",
    "                dict_bib[key].update({'doi' : dict_bib[key].get('url', '').replace('https://doi.org/', '')})\n",
    "\n",
    "    ############## Science Direct\n",
    "    if 'ScienceDirect' in f:\n",
    "        parser = bibtex.Parser()\n",
    "        file = parser.parse_file(lst_files[f])\n",
    "\n",
    "        for i in file.entries.values():\n",
    "            key = i.key\n",
    "            dict_bib[key] = {i.fields.get('fields', fields) : i.fields.get(fields, '')\\\n",
    "                        for fields in list_file_fields}\n",
    "            \n",
    "            dict_bib[key]['type_publication'] = i.type\n",
    "            dict_bib[key]['author'] = [join_names(person) for person in i.persons.get('author', '')]\n",
    "            dict_bib[key].update({'keywords' : dict_bib[key]['keywords'].split(';')})\n",
    "            \n",
    "            dict_bib[key].update({'doi' : i.fields['doi'].replace('https://doi.org/', '')})\n",
    "\n",
    "df_bib = pd.DataFrame(data=dict_bib.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'abstract': 'Cyber-physical systems (CPS) have been developed in many industrial sectors and application domains in which the quality requirements of data acquired are a common factor. Data quality in CPS can deteriorate because of several factors such as sensor faults and failures due to operating in harsh and uncertain environments. How can software engineering and artificial intelligence (AI) help manage and tame data quality issues in CPS? This is the question we aimed to investigate in the SEA4DQ workshop. Emerging trends in software engineering need to take data quality management seriously as CPS are increasingly datacentric in their approach to acquiring and processing data along the edge-fog-cloud continuum. This workshop provided researchers and practitioners a forum for exchanging ideas, experiences, understanding of the problems, visions for the future, and promising solutions to the problems in data quality in CPS. Examples of topics include software/hardware architectures and frameworks for data quality management in CPS; software engineering and AI to detect anomalies in CPS data or to repair erroneous CPS data. SEA4DQ 2021, which took place on August 24th, 2021 was a satellite event of the ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering (ESEC / FSE) 2021. The workshop attracted 35 international participants and was exciting with a great keynote, six excellent presentations, and concluded on a high note with a panel discussion. SEA4DQ was motivated by the common research interests from the EU projects for Zero-Defects Manufacturing such as InterQ and Dat4.Zero.', 'address': 'New York, NY, USA', 'doi': '10.1145/3502771.3502781', 'issn': '0163-5948', 'issue_date': 'January 2022', 'journal': 'SIGSOFT Softw. Eng. Notes', 'month': 'jan', 'number': '1', 'numpages': '4', 'pages': '26–29', 'publisher': 'Association for Computing Machinery', 'title': \"Software Engineering and AI for Data Quality in Cyber- Physical Systems - SEA4DQ'21 Workshop Report\", 'url': 'https://doi.org/10.1145/3502771.3502781', 'volume': '47', 'year': '2022', 'booktitle': '', 'isbn': '', 'keywords': [''], 'location': '', 'series': '', 'articleno': '', 'note': '', 'edition': '', 'type_publication': 'article', 'author': ['Nguyen, Phu', 'Sen, Sagar', 'Jourdan, Nicolas', 'Cassoli, Beatriz', 'Myrseth, Per', 'Armendia, Mikel', 'Myklebust, Odd']}\n"
     ]
    }
   ],
   "source": [
    "print(dict_bib['10.1145/3502771.3502781'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Output BIBTEX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = load_config('config.yaml')        \n",
    "\n",
    "output_path = config['output_path'][0]\n",
    "f_name = config['file_name'][0]\n",
    "file_ext = config['output_ext'][0]\n",
    "\n",
    "type_output_file(df_bib, file_ext, f_name, output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CSV SCIMAGO | JSR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Criando Funções"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_excel = \"C:\\\\Users\\\\victo\\\\PycharmProjects\\\\BibtexDataQuality\\\\source\\\\excel\"\n",
    "lst_excel = arquivos(source_excel)\n",
    "\n",
    "######################## REMOVE CARACTERES ESPECIAIS\n",
    "def rmscaract(text):\n",
    "    result = [re.sub(r\"[^a-zA-Z0-9]\",\"\", normalize('NFKD', words).encode('ASCII','ignore').decode('ASCII').lower()) for words in text.fillna('')]\n",
    "    return result\n",
    "\n",
    "######################## CRIA UM HASH A PARTIR DE UMA STRING\n",
    "def stringhash(instr):\n",
    "    hashnum = [hashlib.md5(ikeys.encode()).hexdigest() for ikeys in instr]\n",
    "    return hashnum\n",
    "\n",
    "######################## CRIA HASH JÁ COM STRING TRATADA\n",
    "def hashkey(strkeys):\n",
    "    stringadjs = rmscaract(strkeys)\n",
    "    hashresult = [hashlib.md5(ikeys.encode()).hexdigest() for ikeys in stringadjs]\n",
    "    return hashresult\n",
    "\n",
    "######################## FUNÇÃO PARA FILTRAGEM\n",
    "def filter_exp(df):\n",
    "    a = config['filter_field'][0] + config['search_operator'][0] + \"'\" + config['search_value'][0] + \"'\"\n",
    "    return df.query(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### importanção e tratamento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### JCS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_jcs = pd.read_csv(lst_excel['jcs_2020.csv'], delimiter=';').drop_duplicates(subset='Full Journal Title')\n",
    "cols = ['Rank', 'Full Journal Title','Total Cites', 'Journal Impact Factor', 'Eigenfactor Score']\n",
    "\n",
    "df_jcs = df_jcs[cols]\n",
    "df_jcs['hashid'] =  hashkey(df_jcs['Full Journal Title'])\n",
    "df_jcs = df_jcs.rename(columns={'Journal Impact Factor' : 'jcr_value'})\n",
    "\n",
    "# df_jcs['hashid'].count()\n",
    "# df_jcs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### SCIMAGO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\victo\\AppData\\Local\\Temp\\ipykernel_20644\\1289106711.py:1: DtypeWarning: Columns (5) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_scimago = pd.read_csv(lst_excel['scimagojr 2020.csv'], delimiter=';').drop_duplicates()\n"
     ]
    }
   ],
   "source": [
    "df_scimago = pd.read_csv(lst_excel['scimagojr 2020.csv'], delimiter=';').drop_duplicates()\n",
    "\n",
    "#cria coluna com hash do title para cruzamento com bibtex \n",
    "df_scimago['hashid'] =  hashkey(df_scimago['Title'])\n",
    "\n",
    "#tratamento coluna issn: splita o valor em outras linhas e dropa coluna antiga\n",
    "df_scimago['issnkey'] =  df_scimago.Issn.apply(lambda x : x.split(','))\n",
    "df_scimago = df_scimago.drop(columns=['Issn'])\n",
    "df_scimago = df_scimago.explode('issnkey')\n",
    "\n",
    "#renomendo coluna SJR para scimago_value\n",
    "df_scimago = df_scimago.rename(columns={'SJR' : 'scimago_value'})\n",
    "\n",
    "# df_scimago "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### JUNÇÃO JCS | SCIMAGO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full Journal Title    19525\n",
      "Title                 50224\n",
      "dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Sourceid                  48450\n",
       "Title                     50224\n",
       "Type                      48450\n",
       "scimago_value             47970\n",
       "SJR Best Quartile         48450\n",
       "H index                   48450\n",
       "Total Docs. (2020)        48450\n",
       "Total Docs. (3years)      48450\n",
       "Total Refs.               48450\n",
       "Total Cites (3years)      48450\n",
       "Citable Docs. (3years)    48450\n",
       "Cites / Doc. (2years)     48450\n",
       "Ref. / Doc.               48450\n",
       "Country                   48450\n",
       "Region                    48450\n",
       "Publisher                 41278\n",
       "Coverage                  48450\n",
       "Categories                48450\n",
       "hashid                    50224\n",
       "issnkey                   48450\n",
       "Total Cites               19525\n",
       "jcr_value                 19525\n",
       "Eigenfactor Score         19525\n",
       "dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Junção Scimago | JCS\n",
    "dfjournalrank = df_scimago.merge(df_jcs, left_on=['hashid'], right_on=['hashid'],how='outer')\n",
    "\n",
    "# Para os casos onde o campo Title está nulo preenche com o Full Journal Title\n",
    "dfjournalrank['Title'] = (dfjournalrank.Title.combine_first(dfjournalrank['Full Journal Title'])).str.lower()\n",
    "print(dfjournalrank[['Full Journal Title','Title']].count())\n",
    "\n",
    "# Dropa as colunas duplicadas\n",
    "dfjournalrank = dfjournalrank.drop(columns=['Rank_x', 'Rank_y', 'Full Journal Title'])\n",
    "dfjournalrank.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## JOIN BIBTEX | CSV SCIMAGO | CSV JCS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bib = pd.DataFrame(data=dict_bib.values())\n",
    "df_bib = df_bib.replace(r'^\\s*$', nan, regex=True)\n",
    "\n",
    "df_bib['issnkey'] = rmscaract(df_bib['issn'])\n",
    "df_bib['title'] = rmscaract(df_bib['title'])\n",
    "df_bib['hashid'] = hashkey(df_bib['title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['issnkey', 'isbn', 'journal', 'publisher', 'title', 'booktitle', 'doi', 'author', 'keywords', 'abstract', 'year', 'type_publication', 'jcr_value', 'scimago_value']\n",
    "\n",
    "# Junção BIBTEX | SCIMAGO | JSC e filtra as colunas principais\n",
    "df_join_bibtex_csv = df_bib.merge(dfjournalrank, left_on = 'issnkey', right_on = 'issnkey', how = 'left' )\n",
    "df_join_bibtex_csv = df_join_bibtex_csv[cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Carrega os parametros do arquivo de configuração\n",
    "config = load_config('config.yaml')        \n",
    "\n",
    "# Seta os paramtros nas variaveis\n",
    "output_path = config['output_path'][0]\n",
    "f_name = config['file_name'][0]\n",
    "file_ext = config['output_ext'][0]\n",
    "\n",
    "# Utiliza funcao de filtragem\n",
    "df_filter = filter_exp(df_join_bibtex_csv)\n",
    "\n",
    "# Utiliza funcao para o output do arquivo de acordo com os parametros do arquivo de configuração yaml\n",
    "type_output_file(df_filter, file_ext, f_name, output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(n_api, accesskey, c_filter = None):\n",
    "\n",
    "    # c_filter = c_filter.replace(' ', '%')\n",
    "\n",
    "    if 'IEEE' in n_api:\n",
    "        accesskey_IEEE = accesskey\n",
    "        request = requests.get(\"https://ieeexploreapi.ieee.org/api/v1/search/articles?query=(\" + c_filter + \")&apikey=\" + accesskey_IEEE)\n",
    "        dict_values = json.loads(request.content)['articles']\n",
    "    \n",
    "    if 'ScienceDirect' in n_api:\n",
    "        accesskey_SD = accesskey\n",
    "        request = requests.get(\"http://api.elsevier.com/content/search/scopus?query=(\" + str(c_filter) + \")&show=100&apiKey=\" + accesskey_SD)\n",
    "        # request = requests.get(\"http://api.elsevier.com/content/search/sciencedirect?query=(\" + c_filter + \")&apiKey=\" + accesskey_SD)\n",
    "        dict_values = dict(json.loads(request.content)['search-results'])['entry']\n",
    "\n",
    "    return pd.DataFrame(dict_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "ename": "JSONDecodeError",
     "evalue": "Expecting value: line 1 column 1 (char 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[74], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m sch_str \u001b[39m=\u001b[39m config[\u001b[39m'\u001b[39m\u001b[39msearch_string\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m0\u001b[39m]\n\u001b[0;32m      3\u001b[0m accesskey_IEEE \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mefugu53u622asc77hu7h6hbq\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m----> 4\u001b[0m df_ieee \u001b[39m=\u001b[39m get_data(\u001b[39m'\u001b[39;49m\u001b[39mIEEE\u001b[39;49m\u001b[39m'\u001b[39;49m, accesskey_IEEE, sch_str)\n\u001b[0;32m      6\u001b[0m a \u001b[39m=\u001b[39m {}\n\u001b[0;32m      7\u001b[0m cols \u001b[39m=\u001b[39m [\u001b[39m'\u001b[39m\u001b[39mdoi\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mtitle\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mpublisher\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mauthors\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mkeywords\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mcontent_type\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39missn\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39meissn\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39misbn\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mabstract\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mhtml_url\u001b[39m\u001b[39m'\u001b[39m,  \u001b[39m'\u001b[39m\u001b[39mpublication_title\u001b[39m\u001b[39m'\u001b[39m,  \u001b[39m'\u001b[39m\u001b[39mpublication_year\u001b[39m\u001b[39m'\u001b[39m]\n",
      "Cell \u001b[1;32mIn[71], line 8\u001b[0m, in \u001b[0;36mget_data\u001b[1;34m(n_api, accesskey, c_filter)\u001b[0m\n\u001b[0;32m      6\u001b[0m     accesskey_IEEE \u001b[39m=\u001b[39m accesskey\n\u001b[0;32m      7\u001b[0m     request \u001b[39m=\u001b[39m requests\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mhttps://ieeexploreapi.ieee.org/api/v1/search/articles?query=(\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m c_filter \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m)&apikey=\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m accesskey_IEEE)\n\u001b[1;32m----> 8\u001b[0m     dict_values \u001b[39m=\u001b[39m json\u001b[39m.\u001b[39;49mloads(request\u001b[39m.\u001b[39;49mcontent)[\u001b[39m'\u001b[39m\u001b[39marticles\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m     10\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mScienceDirect\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m n_api:\n\u001b[0;32m     11\u001b[0m     accesskey_SD \u001b[39m=\u001b[39m accesskey\n",
      "File \u001b[1;32mc:\\Users\\victo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\json\\__init__.py:346\u001b[0m, in \u001b[0;36mloads\u001b[1;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[0;32m    341\u001b[0m     s \u001b[39m=\u001b[39m s\u001b[39m.\u001b[39mdecode(detect_encoding(s), \u001b[39m'\u001b[39m\u001b[39msurrogatepass\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m    343\u001b[0m \u001b[39mif\u001b[39;00m (\u001b[39mcls\u001b[39m \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m object_hook \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m\n\u001b[0;32m    344\u001b[0m         parse_int \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m parse_float \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m\n\u001b[0;32m    345\u001b[0m         parse_constant \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m object_pairs_hook \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m kw):\n\u001b[1;32m--> 346\u001b[0m     \u001b[39mreturn\u001b[39;00m _default_decoder\u001b[39m.\u001b[39;49mdecode(s)\n\u001b[0;32m    347\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mcls\u001b[39m \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    348\u001b[0m     \u001b[39mcls\u001b[39m \u001b[39m=\u001b[39m JSONDecoder\n",
      "File \u001b[1;32mc:\\Users\\victo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\json\\decoder.py:337\u001b[0m, in \u001b[0;36mJSONDecoder.decode\u001b[1;34m(self, s, _w)\u001b[0m\n\u001b[0;32m    332\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecode\u001b[39m(\u001b[39mself\u001b[39m, s, _w\u001b[39m=\u001b[39mWHITESPACE\u001b[39m.\u001b[39mmatch):\n\u001b[0;32m    333\u001b[0m     \u001b[39m\"\"\"Return the Python representation of ``s`` (a ``str`` instance\u001b[39;00m\n\u001b[0;32m    334\u001b[0m \u001b[39m    containing a JSON document).\u001b[39;00m\n\u001b[0;32m    335\u001b[0m \n\u001b[0;32m    336\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 337\u001b[0m     obj, end \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mraw_decode(s, idx\u001b[39m=\u001b[39;49m_w(s, \u001b[39m0\u001b[39;49m)\u001b[39m.\u001b[39;49mend())\n\u001b[0;32m    338\u001b[0m     end \u001b[39m=\u001b[39m _w(s, end)\u001b[39m.\u001b[39mend()\n\u001b[0;32m    339\u001b[0m     \u001b[39mif\u001b[39;00m end \u001b[39m!=\u001b[39m \u001b[39mlen\u001b[39m(s):\n",
      "File \u001b[1;32mc:\\Users\\victo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\json\\decoder.py:355\u001b[0m, in \u001b[0;36mJSONDecoder.raw_decode\u001b[1;34m(self, s, idx)\u001b[0m\n\u001b[0;32m    353\u001b[0m     obj, end \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscan_once(s, idx)\n\u001b[0;32m    354\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[1;32m--> 355\u001b[0m     \u001b[39mraise\u001b[39;00m JSONDecodeError(\u001b[39m\"\u001b[39m\u001b[39mExpecting value\u001b[39m\u001b[39m\"\u001b[39m, s, err\u001b[39m.\u001b[39mvalue) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m    356\u001b[0m \u001b[39mreturn\u001b[39;00m obj, end\n",
      "\u001b[1;31mJSONDecodeError\u001b[0m: Expecting value: line 1 column 1 (char 0)"
     ]
    }
   ],
   "source": [
    "config = load_config('config.yaml')        \n",
    "sch_str = config['search_string'][0]\n",
    "accesskey_IEEE = 'efugu53u622asc77hu7h6hbq'\n",
    "df_ieee = get_data('IEEE', accesskey_IEEE, sch_str)\n",
    "\n",
    "a = {}\n",
    "cols = ['doi', 'title', 'publisher', 'authors','keywords', 'content_type', 'issn', 'eissn', 'isbn', 'abstract', 'html_url',  'publication_title',  'publication_year']\n",
    "\n",
    "# df_ieee = df_ieee[cols]\n",
    "df_ieee['isbn'] = df_ieee.isbn.replace('-','')\n",
    "df_ieee['issn'] = df_ieee.issn.replace('-','')\n",
    "\n",
    "# tratamento da coluna author dicionario para string\n",
    "df_ieee['authors'] = df_ieee['authors'].apply(lambda x : [a['full_name'] for a in x['authors']])\n",
    "df_ieee['authors'] = df_ieee['authors'].map(lambda x : ', '.join(x))\n",
    "\n",
    "# tratamento da coluna keywords dicionario para string (cria uma chave para as publicações sem o campo preenchido)\n",
    "df_ieee['keywords'] = df_ieee['index_terms'].map(lambda x : x.get('ieee_terms', {'terms': []})['terms'])\n",
    "df_ieee['keywords'] = df_ieee['keywords'].map(lambda x : ', '.join(x))\n",
    "df_ieee['eissn'] = ''\n",
    "\n",
    "df_ieee = df_ieee[cols]\n",
    "\n",
    "df_ieee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "accesskey_SD = 'ef8a7260c27897693d0fd7394a559726'\n",
    "sch_str = config['search_string'][0]\n",
    "df_api_sd = get_data('ScienceDirect', accesskey_SD, sch_str)\n",
    "\n",
    "df_api_sd['affiliation'] = df_api_sd['affiliation'].apply(lambda x : x[0]['affilname'])\n",
    "df_api_sd['prism:coverDate'] = pd.DatetimeIndex(df_api_sd['prism:coverDate']).year\n",
    "df_api_sd['keywords'] = ''\n",
    "df_api_sd['abstract'] = ''\n",
    "df_api_sd['isbn'] = ''\n",
    "\n",
    "cols = ['prism:doi', 'dc:title', 'affiliation', 'dc:creator', 'keywords', 'subtypeDescription', 'prism:issn', 'prism:eIssn', 'isbn', 'abstract', 'prism:url', 'prism:publicationName',  'prism:coverDate']\n",
    "cols_rename = ['doi',      'title',    'publisher',   'authors', 'keywords', 'content_type',       'issn',       'eissn', 'isbn', 'abstract', 'html_url',  'publication_title',      'publication_year']\n",
    "\n",
    "df_api_sd = df_api_sd[cols]\n",
    "df_api_sd.columns = cols_rename\n",
    "# df_api_sd.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Escrevendo os dados no banco de dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "#conexao com o banco\n",
    "\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "engine = create_engine('sqlite:///Bd_Publications.db', echo=False)\n",
    "engine = engine.execution_options(autocommit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sqlalchemy.engine.cursor.LegacyCursorResult at 0x1ef189add20>"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "engine.execute('''drop table tb_Publications''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sqlalchemy.engine.cursor.LegacyCursorResult at 0x1ef18b1e200>"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#criação da tabela\n",
    "engine.execute('''CREATE TABLE tb_Publications (\n",
    "        doi varchar(500) null,\n",
    "        title varchar(500) null,\n",
    "        publisher varchar(500) null,\n",
    "        authors varchar(500) null,\n",
    "        keywords varchar(500) null,\n",
    "        issn varchar(50) null,\n",
    "        eissn varchar(50) null,\n",
    "        isbn varchar(50) null,\n",
    "        abstract varchar(5000) null,\n",
    "        html_url varchar(500) null,\t\n",
    "        content_type varchar(500) null,\t\n",
    "        publication_title varchar(500) null,\t\n",
    "        publication_year int null\n",
    "    )''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setando a coluna que será o index\n",
    "def insert_values_db(df_input, indx_col, tb_name):\n",
    "    df=df_input.set_index(indx_col)\n",
    "\n",
    "    #inserção tabela\n",
    "    df.to_sql(tb_name, con=engine, if_exists='append')\n",
    "\n",
    "insert_values_db(df_api_sd, 'doi', 'tb_Publications')\n",
    "insert_values_db(df_ieee, 'doi', 'tb_Publications')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "insert_values_db(df_api_sd, 'doi', 'tb_Publications')\n",
    "insert_values_db(df_ieee, 'doi', 'tb_Publications')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"0\":{\"doi\":\"10.1109\\\\/CVPR.2016.90\",\"title\":\"Deep Residual Learning for Image Recognition\",\"publisher\":\"IEEE\",\"authors\":\"Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun\",\"keywords\":\"Training, Degradation, Complexity theory, Image recognition, Neural networks, Visualization, Image segmentation\",\"issn\":\"1063-6919\",\"eissn\":\"\",\"isbn\":\"978-1-4673-8852-8\",\"abstract\":\"Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers - 8\\\\u00d7 deeper than VGG nets [40] but still having lower complexity. An ensemble of these residual nets achieves 3.57% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC & COCO 2015 competitions1, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.\",\"html_url\":\"https:\\\\/\\\\/ieeexplore.ieee.org\\\\/document\\\\/7780459\\\\/\",\"content_type\":\"Conferences\",\"publication_title\":\"2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"publication_year\":2016},\"1\":{\"doi\":\"10.1109\\\\/CVPR.2016.91\",\"title\":\"You Only Look Once: Unified, Real-Time Object Detection\",\"publisher\":\"IEEE\",\"authors\":\"Joseph Redmon, Santosh Divvala, Ross Girshick, Ali Farhadi\",\"keywords\":\"Computer architecture, Microprocessors, Object detection, Training, Real-time systems, Neural networks, Pipelines\",\"issn\":\"1063-6919\",\"eissn\":\"\",\"isbn\":\"978-1-4673-8852-8\",\"abstract\":\"We present YOLO, a new approach to object detection. Prior work on object detection repurposes classifiers to perform detection. Instead, we frame object detection as a regression problem to spatially separated bounding boxes and associated class probabilities. A single neural network predicts bounding boxes and class probabilities directly from full images in one evaluation. Since the whole detection pipeline is a single network, it can be optimized end-to-end directly on detection performance. Our unified architecture is extremely fast. Our base YOLO model processes images in real-time at 45 frames per second. A smaller version of the network, Fast YOLO, processes an astounding 155 frames per second while still achieving double the mAP of other real-time detectors. Compared to state-of-the-art detection systems, YOLO makes more localization errors but is less likely to predict false positives on background. Finally, YOLO learns very general representations of objects. It outperforms other detection methods, including DPM and R-CNN, when generalizing from natural images to other domains like artwork.\",\"html_url\":\"https:\\\\/\\\\/ieeexplore.ieee.org\\\\/document\\\\/7780460\\\\/\",\"content_type\":\"Conferences\",\"publication_title\":\"2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"publication_year\":2016},\"2\":{\"doi\":\"10.1109\\\\/CVPR.2016.90\",\"title\":\"Deep Residual Learning for Image Recognition\",\"publisher\":\"IEEE\",\"authors\":\"Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun\",\"keywords\":\"Training, Degradation, Complexity theory, Image recognition, Neural networks, Visualization, Image segmentation\",\"issn\":\"1063-6919\",\"eissn\":\"\",\"isbn\":\"978-1-4673-8852-8\",\"abstract\":\"Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers - 8\\\\u00d7 deeper than VGG nets [40] but still having lower complexity. An ensemble of these residual nets achieves 3.57% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC & COCO 2015 competitions1, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.\",\"html_url\":\"https:\\\\/\\\\/ieeexplore.ieee.org\\\\/document\\\\/7780459\\\\/\",\"content_type\":\"Conferences\",\"publication_title\":\"2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"publication_year\":2016},\"3\":{\"doi\":\"10.1109\\\\/CVPR.2016.91\",\"title\":\"You Only Look Once: Unified, Real-Time Object Detection\",\"publisher\":\"IEEE\",\"authors\":\"Joseph Redmon, Santosh Divvala, Ross Girshick, Ali Farhadi\",\"keywords\":\"Computer architecture, Microprocessors, Object detection, Training, Real-time systems, Neural networks, Pipelines\",\"issn\":\"1063-6919\",\"eissn\":\"\",\"isbn\":\"978-1-4673-8852-8\",\"abstract\":\"We present YOLO, a new approach to object detection. Prior work on object detection repurposes classifiers to perform detection. Instead, we frame object detection as a regression problem to spatially separated bounding boxes and associated class probabilities. A single neural network predicts bounding boxes and class probabilities directly from full images in one evaluation. Since the whole detection pipeline is a single network, it can be optimized end-to-end directly on detection performance. Our unified architecture is extremely fast. Our base YOLO model processes images in real-time at 45 frames per second. A smaller version of the network, Fast YOLO, processes an astounding 155 frames per second while still achieving double the mAP of other real-time detectors. Compared to state-of-the-art detection systems, YOLO makes more localization errors but is less likely to predict false positives on background. Finally, YOLO learns very general representations of objects. It outperforms other detection methods, including DPM and R-CNN, when generalizing from natural images to other domains like artwork.\",\"html_url\":\"https:\\\\/\\\\/ieeexplore.ieee.org\\\\/document\\\\/7780460\\\\/\",\"content_type\":\"Conferences\",\"publication_title\":\"2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"publication_year\":2016},\"4\":{\"doi\":\"10.1109\\\\/CVPR.2016.90\",\"title\":\"Deep Residual Learning for Image Recognition\",\"publisher\":\"IEEE\",\"authors\":\"Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun\",\"keywords\":\"Training, Degradation, Complexity theory, Image recognition, Neural networks, Visualization, Image segmentation\",\"issn\":\"1063-6919\",\"eissn\":\"\",\"isbn\":\"978-1-4673-8852-8\",\"abstract\":\"Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers - 8\\\\u00d7 deeper than VGG nets [40] but still having lower complexity. An ensemble of these residual nets achieves 3.57% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC & COCO 2015 competitions1, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.\",\"html_url\":\"https:\\\\/\\\\/ieeexplore.ieee.org\\\\/document\\\\/7780459\\\\/\",\"content_type\":\"Conferences\",\"publication_title\":\"2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"publication_year\":2016},\"5\":{\"doi\":\"10.1109\\\\/CVPR.2016.91\",\"title\":\"You Only Look Once: Unified, Real-Time Object Detection\",\"publisher\":\"IEEE\",\"authors\":\"Joseph Redmon, Santosh Divvala, Ross Girshick, Ali Farhadi\",\"keywords\":\"Computer architecture, Microprocessors, Object detection, Training, Real-time systems, Neural networks, Pipelines\",\"issn\":\"1063-6919\",\"eissn\":\"\",\"isbn\":\"978-1-4673-8852-8\",\"abstract\":\"We present YOLO, a new approach to object detection. Prior work on object detection repurposes classifiers to perform detection. Instead, we frame object detection as a regression problem to spatially separated bounding boxes and associated class probabilities. A single neural network predicts bounding boxes and class probabilities directly from full images in one evaluation. Since the whole detection pipeline is a single network, it can be optimized end-to-end directly on detection performance. Our unified architecture is extremely fast. Our base YOLO model processes images in real-time at 45 frames per second. A smaller version of the network, Fast YOLO, processes an astounding 155 frames per second while still achieving double the mAP of other real-time detectors. Compared to state-of-the-art detection systems, YOLO makes more localization errors but is less likely to predict false positives on background. Finally, YOLO learns very general representations of objects. It outperforms other detection methods, including DPM and R-CNN, when generalizing from natural images to other domains like artwork.\",\"html_url\":\"https:\\\\/\\\\/ieeexplore.ieee.org\\\\/document\\\\/7780460\\\\/\",\"content_type\":\"Conferences\",\"publication_title\":\"2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"publication_year\":2016},\"6\":{\"doi\":\"10.1109\\\\/CVPR.2016.90\",\"title\":\"Deep Residual Learning for Image Recognition\",\"publisher\":\"IEEE\",\"authors\":\"Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun\",\"keywords\":\"Training, Degradation, Complexity theory, Image recognition, Neural networks, Visualization, Image segmentation\",\"issn\":\"1063-6919\",\"eissn\":\"\",\"isbn\":\"978-1-4673-8852-8\",\"abstract\":\"Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers - 8\\\\u00d7 deeper than VGG nets [40] but still having lower complexity. An ensemble of these residual nets achieves 3.57% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC & COCO 2015 competitions1, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.\",\"html_url\":\"https:\\\\/\\\\/ieeexplore.ieee.org\\\\/document\\\\/7780459\\\\/\",\"content_type\":\"Conferences\",\"publication_title\":\"2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"publication_year\":2016},\"7\":{\"doi\":\"10.1109\\\\/CVPR.2016.91\",\"title\":\"You Only Look Once: Unified, Real-Time Object Detection\",\"publisher\":\"IEEE\",\"authors\":\"Joseph Redmon, Santosh Divvala, Ross Girshick, Ali Farhadi\",\"keywords\":\"Computer architecture, Microprocessors, Object detection, Training, Real-time systems, Neural networks, Pipelines\",\"issn\":\"1063-6919\",\"eissn\":\"\",\"isbn\":\"978-1-4673-8852-8\",\"abstract\":\"We present YOLO, a new approach to object detection. Prior work on object detection repurposes classifiers to perform detection. Instead, we frame object detection as a regression problem to spatially separated bounding boxes and associated class probabilities. A single neural network predicts bounding boxes and class probabilities directly from full images in one evaluation. Since the whole detection pipeline is a single network, it can be optimized end-to-end directly on detection performance. Our unified architecture is extremely fast. Our base YOLO model processes images in real-time at 45 frames per second. A smaller version of the network, Fast YOLO, processes an astounding 155 frames per second while still achieving double the mAP of other real-time detectors. Compared to state-of-the-art detection systems, YOLO makes more localization errors but is less likely to predict false positives on background. Finally, YOLO learns very general representations of objects. It outperforms other detection methods, including DPM and R-CNN, when generalizing from natural images to other domains like artwork.\",\"html_url\":\"https:\\\\/\\\\/ieeexplore.ieee.org\\\\/document\\\\/7780460\\\\/\",\"content_type\":\"Conferences\",\"publication_title\":\"2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"publication_year\":2016},\"8\":{\"doi\":\"10.1109\\\\/CVPR.2016.90\",\"title\":\"Deep Residual Learning for Image Recognition\",\"publisher\":\"IEEE\",\"authors\":\"Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun\",\"keywords\":\"Training, Degradation, Complexity theory, Image recognition, Neural networks, Visualization, Image segmentation\",\"issn\":\"1063-6919\",\"eissn\":\"\",\"isbn\":\"978-1-4673-8852-8\",\"abstract\":\"Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers - 8\\\\u00d7 deeper than VGG nets [40] but still having lower complexity. An ensemble of these residual nets achieves 3.57% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC & COCO 2015 competitions1, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.\",\"html_url\":\"https:\\\\/\\\\/ieeexplore.ieee.org\\\\/document\\\\/7780459\\\\/\",\"content_type\":\"Conferences\",\"publication_title\":\"2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"publication_year\":2016},\"9\":{\"doi\":\"10.1109\\\\/CVPR.2016.91\",\"title\":\"You Only Look Once: Unified, Real-Time Object Detection\",\"publisher\":\"IEEE\",\"authors\":\"Joseph Redmon, Santosh Divvala, Ross Girshick, Ali Farhadi\",\"keywords\":\"Computer architecture, Microprocessors, Object detection, Training, Real-time systems, Neural networks, Pipelines\",\"issn\":\"1063-6919\",\"eissn\":\"\",\"isbn\":\"978-1-4673-8852-8\",\"abstract\":\"We present YOLO, a new approach to object detection. Prior work on object detection repurposes classifiers to perform detection. Instead, we frame object detection as a regression problem to spatially separated bounding boxes and associated class probabilities. A single neural network predicts bounding boxes and class probabilities directly from full images in one evaluation. Since the whole detection pipeline is a single network, it can be optimized end-to-end directly on detection performance. Our unified architecture is extremely fast. Our base YOLO model processes images in real-time at 45 frames per second. A smaller version of the network, Fast YOLO, processes an astounding 155 frames per second while still achieving double the mAP of other real-time detectors. Compared to state-of-the-art detection systems, YOLO makes more localization errors but is less likely to predict false positives on background. Finally, YOLO learns very general representations of objects. It outperforms other detection methods, including DPM and R-CNN, when generalizing from natural images to other domains like artwork.\",\"html_url\":\"https:\\\\/\\\\/ieeexplore.ieee.org\\\\/document\\\\/7780460\\\\/\",\"content_type\":\"Conferences\",\"publication_title\":\"2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"publication_year\":2016},\"10\":{\"doi\":\"10.1109\\\\/CVPR.2016.90\",\"title\":\"Deep Residual Learning for Image Recognition\",\"publisher\":\"IEEE\",\"authors\":\"Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun\",\"keywords\":\"Training, Degradation, Complexity theory, Image recognition, Neural networks, Visualization, Image segmentation\",\"issn\":\"1063-6919\",\"eissn\":\"\",\"isbn\":\"978-1-4673-8852-8\",\"abstract\":\"Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers - 8\\\\u00d7 deeper than VGG nets [40] but still having lower complexity. An ensemble of these residual nets achieves 3.57% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC & COCO 2015 competitions1, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.\",\"html_url\":\"https:\\\\/\\\\/ieeexplore.ieee.org\\\\/document\\\\/7780459\\\\/\",\"content_type\":\"Conferences\",\"publication_title\":\"2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"publication_year\":2016},\"11\":{\"doi\":\"10.1109\\\\/CVPR.2016.91\",\"title\":\"You Only Look Once: Unified, Real-Time Object Detection\",\"publisher\":\"IEEE\",\"authors\":\"Joseph Redmon, Santosh Divvala, Ross Girshick, Ali Farhadi\",\"keywords\":\"Computer architecture, Microprocessors, Object detection, Training, Real-time systems, Neural networks, Pipelines\",\"issn\":\"1063-6919\",\"eissn\":\"\",\"isbn\":\"978-1-4673-8852-8\",\"abstract\":\"We present YOLO, a new approach to object detection. Prior work on object detection repurposes classifiers to perform detection. Instead, we frame object detection as a regression problem to spatially separated bounding boxes and associated class probabilities. A single neural network predicts bounding boxes and class probabilities directly from full images in one evaluation. Since the whole detection pipeline is a single network, it can be optimized end-to-end directly on detection performance. Our unified architecture is extremely fast. Our base YOLO model processes images in real-time at 45 frames per second. A smaller version of the network, Fast YOLO, processes an astounding 155 frames per second while still achieving double the mAP of other real-time detectors. Compared to state-of-the-art detection systems, YOLO makes more localization errors but is less likely to predict false positives on background. Finally, YOLO learns very general representations of objects. It outperforms other detection methods, including DPM and R-CNN, when generalizing from natural images to other domains like artwork.\",\"html_url\":\"https:\\\\/\\\\/ieeexplore.ieee.org\\\\/document\\\\/7780460\\\\/\",\"content_type\":\"Conferences\",\"publication_title\":\"2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"publication_year\":2016},\"12\":{\"doi\":\"10.1109\\\\/CVPR.2016.90\",\"title\":\"Deep Residual Learning for Image Recognition\",\"publisher\":\"IEEE\",\"authors\":\"Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun\",\"keywords\":\"Training, Degradation, Complexity theory, Image recognition, Neural networks, Visualization, Image segmentation\",\"issn\":\"1063-6919\",\"eissn\":\"\",\"isbn\":\"978-1-4673-8852-8\",\"abstract\":\"Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers - 8\\\\u00d7 deeper than VGG nets [40] but still having lower complexity. An ensemble of these residual nets achieves 3.57% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC & COCO 2015 competitions1, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.\",\"html_url\":\"https:\\\\/\\\\/ieeexplore.ieee.org\\\\/document\\\\/7780459\\\\/\",\"content_type\":\"Conferences\",\"publication_title\":\"2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"publication_year\":2016},\"13\":{\"doi\":\"10.1109\\\\/CVPR.2016.91\",\"title\":\"You Only Look Once: Unified, Real-Time Object Detection\",\"publisher\":\"IEEE\",\"authors\":\"Joseph Redmon, Santosh Divvala, Ross Girshick, Ali Farhadi\",\"keywords\":\"Computer architecture, Microprocessors, Object detection, Training, Real-time systems, Neural networks, Pipelines\",\"issn\":\"1063-6919\",\"eissn\":\"\",\"isbn\":\"978-1-4673-8852-8\",\"abstract\":\"We present YOLO, a new approach to object detection. Prior work on object detection repurposes classifiers to perform detection. Instead, we frame object detection as a regression problem to spatially separated bounding boxes and associated class probabilities. A single neural network predicts bounding boxes and class probabilities directly from full images in one evaluation. Since the whole detection pipeline is a single network, it can be optimized end-to-end directly on detection performance. Our unified architecture is extremely fast. Our base YOLO model processes images in real-time at 45 frames per second. A smaller version of the network, Fast YOLO, processes an astounding 155 frames per second while still achieving double the mAP of other real-time detectors. Compared to state-of-the-art detection systems, YOLO makes more localization errors but is less likely to predict false positives on background. Finally, YOLO learns very general representations of objects. It outperforms other detection methods, including DPM and R-CNN, when generalizing from natural images to other domains like artwork.\",\"html_url\":\"https:\\\\/\\\\/ieeexplore.ieee.org\\\\/document\\\\/7780460\\\\/\",\"content_type\":\"Conferences\",\"publication_title\":\"2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"publication_year\":2016},\"14\":{\"doi\":\"10.1109\\\\/CVPR.2016.90\",\"title\":\"Deep Residual Learning for Image Recognition\",\"publisher\":\"IEEE\",\"authors\":\"Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun\",\"keywords\":\"Training, Degradation, Complexity theory, Image recognition, Neural networks, Visualization, Image segmentation\",\"issn\":\"1063-6919\",\"eissn\":\"\",\"isbn\":\"978-1-4673-8852-8\",\"abstract\":\"Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers - 8\\\\u00d7 deeper than VGG nets [40] but still having lower complexity. An ensemble of these residual nets achieves 3.57% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC & COCO 2015 competitions1, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.\",\"html_url\":\"https:\\\\/\\\\/ieeexplore.ieee.org\\\\/document\\\\/7780459\\\\/\",\"content_type\":\"Conferences\",\"publication_title\":\"2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"publication_year\":2016},\"15\":{\"doi\":\"10.1109\\\\/CVPR.2016.91\",\"title\":\"You Only Look Once: Unified, Real-Time Object Detection\",\"publisher\":\"IEEE\",\"authors\":\"Joseph Redmon, Santosh Divvala, Ross Girshick, Ali Farhadi\",\"keywords\":\"Computer architecture, Microprocessors, Object detection, Training, Real-time systems, Neural networks, Pipelines\",\"issn\":\"1063-6919\",\"eissn\":\"\",\"isbn\":\"978-1-4673-8852-8\",\"abstract\":\"We present YOLO, a new approach to object detection. Prior work on object detection repurposes classifiers to perform detection. Instead, we frame object detection as a regression problem to spatially separated bounding boxes and associated class probabilities. A single neural network predicts bounding boxes and class probabilities directly from full images in one evaluation. Since the whole detection pipeline is a single network, it can be optimized end-to-end directly on detection performance. Our unified architecture is extremely fast. Our base YOLO model processes images in real-time at 45 frames per second. A smaller version of the network, Fast YOLO, processes an astounding 155 frames per second while still achieving double the mAP of other real-time detectors. Compared to state-of-the-art detection systems, YOLO makes more localization errors but is less likely to predict false positives on background. Finally, YOLO learns very general representations of objects. It outperforms other detection methods, including DPM and R-CNN, when generalizing from natural images to other domains like artwork.\",\"html_url\":\"https:\\\\/\\\\/ieeexplore.ieee.org\\\\/document\\\\/7780460\\\\/\",\"content_type\":\"Conferences\",\"publication_title\":\"2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"publication_year\":2016},\"16\":{\"doi\":\"10.1109\\\\/CVPR.2016.90\",\"title\":\"Deep Residual Learning for Image Recognition\",\"publisher\":\"IEEE\",\"authors\":\"Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun\",\"keywords\":\"Training, Degradation, Complexity theory, Image recognition, Neural networks, Visualization, Image segmentation\",\"issn\":\"1063-6919\",\"eissn\":\"\",\"isbn\":\"978-1-4673-8852-8\",\"abstract\":\"Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers - 8\\\\u00d7 deeper than VGG nets [40] but still having lower complexity. An ensemble of these residual nets achieves 3.57% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC & COCO 2015 competitions1, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.\",\"html_url\":\"https:\\\\/\\\\/ieeexplore.ieee.org\\\\/document\\\\/7780459\\\\/\",\"content_type\":\"Conferences\",\"publication_title\":\"2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"publication_year\":2016},\"17\":{\"doi\":\"10.1109\\\\/CVPR.2016.91\",\"title\":\"You Only Look Once: Unified, Real-Time Object Detection\",\"publisher\":\"IEEE\",\"authors\":\"Joseph Redmon, Santosh Divvala, Ross Girshick, Ali Farhadi\",\"keywords\":\"Computer architecture, Microprocessors, Object detection, Training, Real-time systems, Neural networks, Pipelines\",\"issn\":\"1063-6919\",\"eissn\":\"\",\"isbn\":\"978-1-4673-8852-8\",\"abstract\":\"We present YOLO, a new approach to object detection. Prior work on object detection repurposes classifiers to perform detection. Instead, we frame object detection as a regression problem to spatially separated bounding boxes and associated class probabilities. A single neural network predicts bounding boxes and class probabilities directly from full images in one evaluation. Since the whole detection pipeline is a single network, it can be optimized end-to-end directly on detection performance. Our unified architecture is extremely fast. Our base YOLO model processes images in real-time at 45 frames per second. A smaller version of the network, Fast YOLO, processes an astounding 155 frames per second while still achieving double the mAP of other real-time detectors. Compared to state-of-the-art detection systems, YOLO makes more localization errors but is less likely to predict false positives on background. Finally, YOLO learns very general representations of objects. It outperforms other detection methods, including DPM and R-CNN, when generalizing from natural images to other domains like artwork.\",\"html_url\":\"https:\\\\/\\\\/ieeexplore.ieee.org\\\\/document\\\\/7780460\\\\/\",\"content_type\":\"Conferences\",\"publication_title\":\"2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"publication_year\":2016},\"18\":{\"doi\":\"10.1109\\\\/CVPR.2016.90\",\"title\":\"Deep Residual Learning for Image Recognition\",\"publisher\":\"IEEE\",\"authors\":\"Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun\",\"keywords\":\"Training, Degradation, Complexity theory, Image recognition, Neural networks, Visualization, Image segmentation\",\"issn\":\"1063-6919\",\"eissn\":\"\",\"isbn\":\"978-1-4673-8852-8\",\"abstract\":\"Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers - 8\\\\u00d7 deeper than VGG nets [40] but still having lower complexity. An ensemble of these residual nets achieves 3.57% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC & COCO 2015 competitions1, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.\",\"html_url\":\"https:\\\\/\\\\/ieeexplore.ieee.org\\\\/document\\\\/7780459\\\\/\",\"content_type\":\"Conferences\",\"publication_title\":\"2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"publication_year\":2016},\"19\":{\"doi\":\"10.1109\\\\/CVPR.2016.91\",\"title\":\"You Only Look Once: Unified, Real-Time Object Detection\",\"publisher\":\"IEEE\",\"authors\":\"Joseph Redmon, Santosh Divvala, Ross Girshick, Ali Farhadi\",\"keywords\":\"Computer architecture, Microprocessors, Object detection, Training, Real-time systems, Neural networks, Pipelines\",\"issn\":\"1063-6919\",\"eissn\":\"\",\"isbn\":\"978-1-4673-8852-8\",\"abstract\":\"We present YOLO, a new approach to object detection. Prior work on object detection repurposes classifiers to perform detection. Instead, we frame object detection as a regression problem to spatially separated bounding boxes and associated class probabilities. A single neural network predicts bounding boxes and class probabilities directly from full images in one evaluation. Since the whole detection pipeline is a single network, it can be optimized end-to-end directly on detection performance. Our unified architecture is extremely fast. Our base YOLO model processes images in real-time at 45 frames per second. A smaller version of the network, Fast YOLO, processes an astounding 155 frames per second while still achieving double the mAP of other real-time detectors. Compared to state-of-the-art detection systems, YOLO makes more localization errors but is less likely to predict false positives on background. Finally, YOLO learns very general representations of objects. It outperforms other detection methods, including DPM and R-CNN, when generalizing from natural images to other domains like artwork.\",\"html_url\":\"https:\\\\/\\\\/ieeexplore.ieee.org\\\\/document\\\\/7780460\\\\/\",\"content_type\":\"Conferences\",\"publication_title\":\"2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"publication_year\":2016}}'"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#leitura da tabela para validar se os dados foram escritos corretamente\n",
    "field = 'publication_year'\n",
    "value = '2016'\n",
    "\n",
    "query = \"SELECT * FROM tb_Publications where \" + field + '=' + value + \";\"\n",
    "df_table = pd.read_sql_query(query, engine)\n",
    "# df_table.to_json(orient=\"index\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "253cd15c32ffb283b0558be93b097dc27093fe6b891853f454fa08d2f15e4a97"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
