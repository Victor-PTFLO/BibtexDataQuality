{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# DATA QUALITY (Bibtex e csv)\n",
    "- Arquivo texto em formato BIBTEX\n",
    "- Objetivo: Data quality de input diversos(csv, bibtex e APIs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import hashlib\n",
    "import yaml\n",
    "import json\n",
    "import csv\n",
    "import requests\n",
    "from numpy import nan\n",
    "from pybtex.database.input import bibtex\n",
    "from pybtex.database import BibliographyData, Entry\n",
    "from unicodedata import normalize\n",
    "\n",
    "from flask import jsonify, Flask, render_template, request\n",
    "import werkzeug\n",
    "\n",
    "pd.set_option(\"display.max_columns\", 50)\n",
    "pd.set_option(\"display.max_rows\", 100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Criação de funções"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################## LISTAGEM DE ARQUIVOS\n",
    "def arquivos(path, fendwith = ''):\n",
    "    if fendwith not in '':\n",
    "        caminhos = {nome : os.path.join(path, nome) for nome in os.listdir(path) if nome.endswith(fendwith)}\n",
    "    else:\n",
    "        caminhos = {nome : os.path.join(path, nome) for nome in os.listdir(path)}\n",
    "    return caminhos\n",
    "\n",
    "######################## LISTAGEM DE AUTORES\n",
    "def author_names(author):\n",
    "    try:\n",
    "        return author.persons['author'][0:]\n",
    "    except:\n",
    "        return {'author': [(u'none, none')]}\n",
    "\n",
    "######################## CONCATENA AUTORES\n",
    "def join_names(person):\n",
    "    try:\n",
    "        return person.last_names[0] + ', ' + person.first_names[0]\n",
    "    except:\n",
    "        return []\n",
    "\n",
    "######################## OUTPUT POR TIPO DE EXTENSAO(JSON, YAML E CSV)\n",
    "def type_output_file(o_data, f_type, filename, path):\n",
    "\n",
    "    path_complete = path + filename + '.' + f_type\n",
    "\n",
    "    if f_type == 'yaml':\n",
    "        with open(path_complete, 'w') as nfile:\n",
    "            yaml.dump(o_data.to_dict(orient='records'), nfile)\n",
    "\n",
    "    if f_type == 'json':\n",
    "        with open(path_complete, 'w') as nfile:\n",
    "            result = o_data.to_json(orient=\"records\")\n",
    "            parsed = json.loads(result)\n",
    "            json.dump(parsed, nfile, indent=4)  \n",
    "\n",
    "    if f_type == 'csv':\n",
    "        o_data.to_csv(path_complete, sep='§')\n",
    "\n",
    "######################## CARGA DO ARQUIVO DE CONFIGURACAO\n",
    "def load_config(n_file):\n",
    "    with open(n_file, 'r') as config_file:\n",
    "        return yaml.load(config_file, Loader=yaml.SafeLoader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inicia importanção dos arquivos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BIBTEX\n",
    "- ref bibtex doc: http://paginapessoal.utfpr.edu.br/jamhour/publicacoes/arquivos/00_Compilado_JabRef_dez2015.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_path = \"C:\\\\Users\\\\victo\\\\PycharmProjects\\\\BibtexDataQuality\\\\source\\\\\"\n",
    "\n",
    "\n",
    "lst_files = arquivos(source_path, '.bib')\n",
    "parser = bibtex.Parser()\n",
    "# file = parser.parse_file(lst_files['ACM.bib'])\n",
    "dict_file_fields = {}\n",
    "list_file_fields = []\n",
    "\n",
    "#verifica a estrutura de campos para cada tipo de publicação\n",
    "#cria um dicionario usando como chave a fonte e o tipo de publicação\n",
    "for f in lst_files:\n",
    "    parser = bibtex.Parser()\n",
    "    file = parser.parse_file(lst_files[f])\n",
    "    f_name = f.replace('.bib','')\n",
    "    for i in file.entries.values():\n",
    "        \n",
    "        for a in sorted(i.fields.keys()):\n",
    "           \n",
    "            if a not in list_file_fields:\n",
    "                list_file_fields.append(a)\n",
    "list_file_fields.remove('ISSN')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### importanção e tratamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst_files = arquivos(source_path)\n",
    "parser = bibtex.Parser()\n",
    "# file = parser.parse_file(lst_files['IEEE.bib'])\n",
    "dict_bib = {}\n",
    "\n",
    "for f in lst_files:\n",
    "    \n",
    "    ############# IEEE\n",
    "    if 'IEEE' in f :\n",
    "        parser = bibtex.Parser()\n",
    "        file = parser.parse_file(lst_files[f])\n",
    "\n",
    "        for i in file.entries.values():\n",
    "            key = i.key\n",
    "            dict_bib[key] = {i.fields.get('fields', fields) : i.fields.get(fields, '')\\\n",
    "                            for fields in list_file_fields}\n",
    "            \n",
    "            dict_bib[key]['type_publication'] = i.type\n",
    "            dict_bib[key]['author'] = [join_names(person) for person in i.persons.get('author', '')]\n",
    "\n",
    "            dict_bib[key].update({'keywords' : dict_bib[key]['keywords'].split(';')})\n",
    "    \n",
    "    ############## ACM\n",
    "    if 'acm' in f:\n",
    "        parser = bibtex.Parser()\n",
    "        file = parser.parse_file(lst_files[f])\n",
    "\n",
    "        for i in file.entries.values():\n",
    "            key = i.key\n",
    "            dict_bib[key] = {i.fields.get('fields', fields) : i.fields.get(fields, '')\\\n",
    "                            for fields in list_file_fields}\n",
    "            \n",
    "            dict_bib[key]['type_publication'] = i.type\n",
    "            dict_bib[key]['author'] = [join_names(person) for person in i.persons.get('author', '')]\n",
    "\n",
    "            dict_bib[key].update({'keywords' : dict_bib[key].get('keywords', '').split(';')})\n",
    "\n",
    "            if i.type == 'inbook':\n",
    "                dict_bib[key].update({'doi' : dict_bib[key].get('url', '').replace('https://doi.org/', '')})\n",
    "\n",
    "    ############## Science Direct\n",
    "    if 'ScienceDirect' in f:\n",
    "        parser = bibtex.Parser()\n",
    "        file = parser.parse_file(lst_files[f])\n",
    "\n",
    "        for i in file.entries.values():\n",
    "            key = i.key\n",
    "            dict_bib[key] = {i.fields.get('fields', fields) : i.fields.get(fields, '')\\\n",
    "                        for fields in list_file_fields}\n",
    "            \n",
    "            dict_bib[key]['type_publication'] = i.type\n",
    "            dict_bib[key]['author'] = [join_names(person) for person in i.persons.get('author', '')]\n",
    "            dict_bib[key].update({'keywords' : dict_bib[key]['keywords'].split(';')})\n",
    "            \n",
    "            dict_bib[key].update({'doi' : i.fields['doi'].replace('https://doi.org/', '')})\n",
    "\n",
    "df_bib = pd.DataFrame(data=dict_bib.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'abstract': 'Cyber-physical systems (CPS) have been developed in many industrial sectors and application domains in which the quality requirements of data acquired are a common factor. Data quality in CPS can deteriorate because of several factors such as sensor faults and failures due to operating in harsh and uncertain environments. How can software engineering and artificial intelligence (AI) help manage and tame data quality issues in CPS? This is the question we aimed to investigate in the SEA4DQ workshop. Emerging trends in software engineering need to take data quality management seriously as CPS are increasingly datacentric in their approach to acquiring and processing data along the edge-fog-cloud continuum. This workshop provided researchers and practitioners a forum for exchanging ideas, experiences, understanding of the problems, visions for the future, and promising solutions to the problems in data quality in CPS. Examples of topics include software/hardware architectures and frameworks for data quality management in CPS; software engineering and AI to detect anomalies in CPS data or to repair erroneous CPS data. SEA4DQ 2021, which took place on August 24th, 2021 was a satellite event of the ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering (ESEC / FSE) 2021. The workshop attracted 35 international participants and was exciting with a great keynote, six excellent presentations, and concluded on a high note with a panel discussion. SEA4DQ was motivated by the common research interests from the EU projects for Zero-Defects Manufacturing such as InterQ and Dat4.Zero.', 'address': 'New York, NY, USA', 'doi': '10.1145/3502771.3502781', 'issn': '0163-5948', 'issue_date': 'January 2022', 'journal': 'SIGSOFT Softw. Eng. Notes', 'month': 'jan', 'number': '1', 'numpages': '4', 'pages': '26–29', 'publisher': 'Association for Computing Machinery', 'title': \"Software Engineering and AI for Data Quality in Cyber- Physical Systems - SEA4DQ'21 Workshop Report\", 'url': 'https://doi.org/10.1145/3502771.3502781', 'volume': '47', 'year': '2022', 'booktitle': '', 'isbn': '', 'keywords': [''], 'location': '', 'series': '', 'articleno': '', 'note': '', 'edition': '', 'type_publication': 'article', 'author': ['Nguyen, Phu', 'Sen, Sagar', 'Jourdan, Nicolas', 'Cassoli, Beatriz', 'Myrseth, Per', 'Armendia, Mikel', 'Myklebust, Odd']}\n"
     ]
    }
   ],
   "source": [
    "print(dict_bib['10.1145/3502771.3502781'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Output BIBTEX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = load_config('config.yaml')        \n",
    "\n",
    "output_path = config['output_path'][0]\n",
    "f_name = config['file_name'][0]\n",
    "file_ext = config['output_ext'][0]\n",
    "\n",
    "type_output_file(df_bib, file_ext, f_name, output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CSV SCIMAGO | JSR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Criando Funções"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_excel = \"C:\\\\Users\\\\victo\\\\PycharmProjects\\\\BibtexDataQuality\\\\source\\\\excel\"\n",
    "lst_excel = arquivos(source_excel)\n",
    "\n",
    "######################## REMOVE CARACTERES ESPECIAIS\n",
    "def rmscaract(text):\n",
    "    result = [re.sub(r\"[^a-zA-Z0-9]\",\"\", normalize('NFKD', words).encode('ASCII','ignore').decode('ASCII').lower()) for words in text.fillna('')]\n",
    "    return result\n",
    "\n",
    "######################## CRIA UM HASH A PARTIR DE UMA STRING\n",
    "def stringhash(instr):\n",
    "    hashnum = [hashlib.md5(ikeys.encode()).hexdigest() for ikeys in instr]\n",
    "    return hashnum\n",
    "\n",
    "######################## CRIA HASH JÁ COM STRING TRATADA\n",
    "def hashkey(strkeys):\n",
    "    stringadjs = rmscaract(strkeys)\n",
    "    hashresult = [hashlib.md5(ikeys.encode()).hexdigest() for ikeys in stringadjs]\n",
    "    return hashresult\n",
    "\n",
    "######################## FUNÇÃO PARA FILTRAGEM\n",
    "def filter_exp(df):\n",
    "    a = config['filter_field'][0] + config['search_operator'][0] + \"'\" + config['search_value'][0] + \"'\"\n",
    "    return df.query(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### importanção e tratamento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### JCS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_jcs = pd.read_csv(lst_excel['jcs_2020.csv'], delimiter=';').drop_duplicates(subset='Full Journal Title')\n",
    "cols = ['Rank', 'Full Journal Title','Total Cites', 'Journal Impact Factor', 'Eigenfactor Score']\n",
    "\n",
    "df_jcs = df_jcs[cols]\n",
    "df_jcs['hashid'] =  hashkey(df_jcs['Full Journal Title'])\n",
    "df_jcs = df_jcs.rename(columns={'Journal Impact Factor' : 'jcr_value'})\n",
    "\n",
    "# df_jcs['hashid'].count()\n",
    "# df_jcs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### SCIMAGO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\victo\\AppData\\Local\\Temp\\ipykernel_16736\\1289106711.py:1: DtypeWarning: Columns (5) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_scimago = pd.read_csv(lst_excel['scimagojr 2020.csv'], delimiter=';').drop_duplicates()\n"
     ]
    }
   ],
   "source": [
    "df_scimago = pd.read_csv(lst_excel['scimagojr 2020.csv'], delimiter=';').drop_duplicates()\n",
    "\n",
    "#cria coluna com hash do title para cruzamento com bibtex \n",
    "df_scimago['hashid'] =  hashkey(df_scimago['Title'])\n",
    "\n",
    "#tratamento coluna issn: splita o valor em outras linhas e dropa coluna antiga\n",
    "df_scimago['issnkey'] =  df_scimago.Issn.apply(lambda x : x.split(','))\n",
    "df_scimago = df_scimago.drop(columns=['Issn'])\n",
    "df_scimago = df_scimago.explode('issnkey')\n",
    "\n",
    "#renomendo coluna SJR para scimago_value\n",
    "df_scimago = df_scimago.rename(columns={'SJR' : 'scimago_value'})\n",
    "\n",
    "# df_scimago "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### JUNÇÃO JCS | SCIMAGO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full Journal Title    19525\n",
      "Title                 50224\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Junção Scimago | JCS\n",
    "dfjournalrank = df_scimago.merge(df_jcs, left_on=['hashid'], right_on=['hashid'],how='outer')\n",
    "\n",
    "# Para os casos onde o campo Title está nulo preenche com o Full Journal Title\n",
    "dfjournalrank['Title'] = (dfjournalrank.Title.combine_first(dfjournalrank['Full Journal Title'])).str.lower()\n",
    "print(dfjournalrank[['Full Journal Title','Title']].count())\n",
    "\n",
    "# Dropa as colunas duplicadas\n",
    "dfjournalrank = dfjournalrank.drop(columns=['Rank_x', 'Rank_y', 'Full Journal Title'])\n",
    "dfjournalrank = dfjournalrank.rename(columns={'Title' : 'JournalTitle'})\n",
    "# dfjournalrank.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## JOIN BIBTEX | CSV SCIMAGO | CSV JCS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bib = pd.DataFrame(data=dict_bib.values())\n",
    "df_bib = df_bib.replace(r'^\\s*$', nan, regex=True)\n",
    "\n",
    "df_bib['issnkey'] = rmscaract(df_bib['issn'])\n",
    "df_bib['title'] = rmscaract(df_bib['title'])\n",
    "df_bib['hashid'] = hashkey(df_bib['title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['issnkey', 'isbn', 'journal', 'publisher', 'title', 'booktitle', 'doi', 'author', 'keywords', 'abstract', 'year', 'type_publication', 'jcr_value', 'scimago_value']\n",
    "\n",
    "# Junção BIBTEX | SCIMAGO | JSC e filtra as colunas principais\n",
    "df_join_bibtex_csv = df_bib.merge(dfjournalrank, left_on = 'issnkey', right_on = 'issnkey', how = 'left' )\n",
    "df_join_bibtex_csv = df_join_bibtex_csv[cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Carrega os parametros do arquivo de configuração\n",
    "config = load_config('config.yaml')        \n",
    "\n",
    "# Seta os paramtros nas variaveis\n",
    "output_path = config['output_path'][0]\n",
    "f_name = config['file_name'][0]\n",
    "file_ext = config['output_ext'][0]\n",
    "\n",
    "# Utiliza funcao de filtragem\n",
    "df_filter = filter_exp(df_join_bibtex_csv)\n",
    "\n",
    "# Utiliza funcao para o output do arquivo de acordo com os parametros do arquivo de configuração yaml\n",
    "type_output_file(df_filter, file_ext, f_name, output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(n_api, accesskey, max_pages = 1, c_filter = None):\n",
    "\n",
    "    lst = []\n",
    "\n",
    "    if 'IEEE' in n_api:\n",
    "        accesskey_IEEE = accesskey\n",
    "        for req in range(1, max_pages, 1):\n",
    "            request = requests.get(\"https://ieeexploreapi.ieee.org/api/v1/search/articles?query=(\" + c_filter + \")&max_records=200&apikey=\" + accesskey_IEEE)\n",
    "            df_tmp = pd.DataFrame(json.loads(request.content)['articles'])\n",
    "            lst.append(df_tmp)\n",
    "\n",
    "    if 'ScienceDirect' in n_api:\n",
    "        accesskey_SD = accesskey\n",
    "        for req in range(1, max_pages, 1):\n",
    "            request = requests.get(\"http://api.elsevier.com/content/search/scopus?query=(\" + str(c_filter) + \")&show=100&apiKey=\" + accesskey_SD)\n",
    "            df_tmp = pd.DataFrame(dict(json.loads(request.content)['search-results'])['entry'])\n",
    "            lst.append(df_tmp)\n",
    "\n",
    "    return pd.concat(lst, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = load_config('config.yaml')        \n",
    "sch_str = config['search_string'][0]\n",
    "accesskey_IEEE = 'efugu53u622asc77hu7h6hbq'\n",
    "max_page = 10\n",
    "df_ieee = get_data('IEEE', accesskey_IEEE, max_page ,sch_str)\n",
    "\n",
    "# tratando colunas de issn\n",
    "df_ieee['isbn'] = rmscaract(df_ieee.isbn)\n",
    "df_ieee['issn'] = rmscaract(df_ieee.issn)\n",
    "\n",
    "# tratamento da coluna author dicionario para string\n",
    "df_ieee['authors'] = df_ieee['authors'].apply(lambda x : [a['full_name'] for a in x['authors']])\n",
    "df_ieee['authors'] = df_ieee['authors'].map(lambda x : ', '.join(x))\n",
    "\n",
    "# tratamento da coluna keywords dicionario para string (cria uma chave para as publicações sem o campo preenchido)\n",
    "df_ieee['keywords'] = df_ieee['index_terms'].map(lambda x : x.get('ieee_terms', {'terms': []})['terms'])\n",
    "df_ieee['keywords'] = df_ieee['keywords'].map(lambda x : ', '.join(x))\n",
    "df_ieee['eissn'] = ''\n",
    "\n",
    "# filtrando colunas\n",
    "cols = ['doi', 'title', 'publisher', 'authors','keywords', 'content_type', 'issn', 'eissn', 'isbn', 'abstract', 'html_url',  'publication_title',  'publication_year']\n",
    "df_ieee = df_ieee[cols]\n",
    "\n",
    "# df_ieee.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "accesskey_SD = 'ef8a7260c27897693d0fd7394a559726'\n",
    "sch_str = config['search_string'][0]\n",
    "max_page = 20\n",
    "df_api_sd = get_data('ScienceDirect', accesskey_SD, max_page, sch_str)\n",
    "\n",
    "# tratamento das colunas\n",
    "df_api_sd['prism:issn'] = rmscaract(df_api_sd['prism:issn'])\n",
    "df_api_sd['affiliation'] = df_api_sd['affiliation'].apply(lambda x : x[0]['affilname'])\n",
    "df_api_sd['prism:coverDate'] = pd.DatetimeIndex(df_api_sd['prism:coverDate']).year\n",
    "df_api_sd['keywords'] = ''\n",
    "df_api_sd['abstract'] = ''\n",
    "df_api_sd['isbn'] = ''\n",
    "\n",
    "# filtrando colunas e renomeando colunas\n",
    "cols = ['prism:doi', 'dc:title', 'affiliation', 'dc:creator', 'keywords', 'subtypeDescription', 'prism:issn', 'prism:eIssn', 'isbn', 'abstract', 'prism:url', 'prism:publicationName',  'prism:coverDate']\n",
    "cols_rename = ['doi',      'title',    'publisher',   'authors', 'keywords', 'content_type',       'issn',       'eissn', 'isbn', 'abstract', 'html_url',  'publication_title',      'publication_year']\n",
    "\n",
    "df_api_sd = df_api_sd[cols]\n",
    "df_api_sd.columns = cols_rename\n",
    "# df_api_sd.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doi</th>\n",
       "      <th>title</th>\n",
       "      <th>publisher</th>\n",
       "      <th>authors</th>\n",
       "      <th>keywords</th>\n",
       "      <th>content_type</th>\n",
       "      <th>issn</th>\n",
       "      <th>eissn</th>\n",
       "      <th>isbn</th>\n",
       "      <th>abstract</th>\n",
       "      <th>html_url</th>\n",
       "      <th>publication_title</th>\n",
       "      <th>publication_year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10.1109/CVPR.2016.90</td>\n",
       "      <td>Deep Residual Learning for Image Recognition</td>\n",
       "      <td>IEEE</td>\n",
       "      <td>Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun</td>\n",
       "      <td>Training, Degradation, Complexity theory, Imag...</td>\n",
       "      <td>Conferences</td>\n",
       "      <td>10636919</td>\n",
       "      <td></td>\n",
       "      <td>9781467388528</td>\n",
       "      <td>Deeper neural networks are more difficult to t...</td>\n",
       "      <td>https://ieeexplore.ieee.org/document/7780459/</td>\n",
       "      <td>2016 IEEE Conference on Computer Vision and Pa...</td>\n",
       "      <td>2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10.1109/TAC.1974.1100705</td>\n",
       "      <td>A new look at the statistical model identifica...</td>\n",
       "      <td>IEEE</td>\n",
       "      <td>H. Akaike</td>\n",
       "      <td>Testing, Maximum likelihood estimation, Time s...</td>\n",
       "      <td>Journals</td>\n",
       "      <td>23343303</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>The history of the development of statistical ...</td>\n",
       "      <td>https://ieeexplore.ieee.org/document/1100705/</td>\n",
       "      <td>IEEE Transactions on Automatic Control</td>\n",
       "      <td>1974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10.1109/ICNN.1995.488968</td>\n",
       "      <td>Particle swarm optimization</td>\n",
       "      <td>IEEE</td>\n",
       "      <td>J. Kennedy, R. Eberhart</td>\n",
       "      <td>Particle swarm optimization, Birds, Educationa...</td>\n",
       "      <td>Conferences</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0780327683</td>\n",
       "      <td>A concept for the optimization of nonlinear fu...</td>\n",
       "      <td>https://ieeexplore.ieee.org/document/488968/</td>\n",
       "      <td>Proceedings of ICNN'95 - International Confere...</td>\n",
       "      <td>1995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10.1109/TIP.2003.819861</td>\n",
       "      <td>Image quality assessment: from error visibilit...</td>\n",
       "      <td>IEEE</td>\n",
       "      <td>Zhou Wang, A.C. Bovik, H.R. Sheikh, E.P. Simon...</td>\n",
       "      <td>Image quality, Humans, Transform coding, Visua...</td>\n",
       "      <td>Journals</td>\n",
       "      <td>19410042</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Objective methods for assessing perceptual ima...</td>\n",
       "      <td>https://ieeexplore.ieee.org/document/1284395/</td>\n",
       "      <td>IEEE Transactions on Image Processing</td>\n",
       "      <td>2004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10.1109/4235.996017</td>\n",
       "      <td>A fast and elitist multiobjective genetic algo...</td>\n",
       "      <td>IEEE</td>\n",
       "      <td>K. Deb, A. Pratap, S. Agarwal, T. Meyarivan</td>\n",
       "      <td>Genetic algorithms, Sorting, Computational com...</td>\n",
       "      <td>Journals</td>\n",
       "      <td>19410026</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Multi-objective evolutionary algorithms (MOEAs...</td>\n",
       "      <td>https://ieeexplore.ieee.org/document/996017/</td>\n",
       "      <td>IEEE Transactions on Evolutionary Computation</td>\n",
       "      <td>2002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>470</th>\n",
       "      <td>10.1016/j.omega.2022.102802</td>\n",
       "      <td>Store brand introduction in a dual-channel sup...</td>\n",
       "      <td>Nanjing University of Finance and EcoNomics</td>\n",
       "      <td>Xiao Y.</td>\n",
       "      <td></td>\n",
       "      <td>Article</td>\n",
       "      <td>03050483</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>https://api.elsevier.com/content/abstract/scop...</td>\n",
       "      <td>Omega (United Kingdom)</td>\n",
       "      <td>2023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>471</th>\n",
       "      <td>10.1016/j.fm.2022.104167</td>\n",
       "      <td>Ethanol-lactate transition of Lachancea thermo...</td>\n",
       "      <td>Amsterdam Institute of Molecular and Life Scie...</td>\n",
       "      <td>Battjes J.</td>\n",
       "      <td></td>\n",
       "      <td>Article</td>\n",
       "      <td>07400020</td>\n",
       "      <td>10959998</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>https://api.elsevier.com/content/abstract/scop...</td>\n",
       "      <td>Food Microbiology</td>\n",
       "      <td>2023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>472</th>\n",
       "      <td>10.1016/j.fm.2022.104162</td>\n",
       "      <td>Determination and quantification of microbial ...</td>\n",
       "      <td>Quadram Institute Bioscience</td>\n",
       "      <td>Bloomfield S.J.</td>\n",
       "      <td></td>\n",
       "      <td>Article</td>\n",
       "      <td>07400020</td>\n",
       "      <td>10959998</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>https://api.elsevier.com/content/abstract/scop...</td>\n",
       "      <td>Food Microbiology</td>\n",
       "      <td>2023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>473</th>\n",
       "      <td>10.1016/j.rcim.2022.102468</td>\n",
       "      <td>Multiple source partial knowledge transfer for...</td>\n",
       "      <td>Nanjing Tech University</td>\n",
       "      <td>Liu X.</td>\n",
       "      <td></td>\n",
       "      <td>Article</td>\n",
       "      <td>07365845</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>https://api.elsevier.com/content/abstract/scop...</td>\n",
       "      <td>Robotics and Computer-Integrated Manufacturing</td>\n",
       "      <td>2023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>474</th>\n",
       "      <td>10.1016/j.rcim.2022.102455</td>\n",
       "      <td>An effective MBSE approach for constructing in...</td>\n",
       "      <td>Huazhong University of Science and Technology</td>\n",
       "      <td>Zhang X.</td>\n",
       "      <td></td>\n",
       "      <td>Article</td>\n",
       "      <td>07365845</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>https://api.elsevier.com/content/abstract/scop...</td>\n",
       "      <td>Robotics and Computer-Integrated Manufacturing</td>\n",
       "      <td>2023</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2275 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                             doi  \\\n",
       "0           10.1109/CVPR.2016.90   \n",
       "1       10.1109/TAC.1974.1100705   \n",
       "2       10.1109/ICNN.1995.488968   \n",
       "3        10.1109/TIP.2003.819861   \n",
       "4            10.1109/4235.996017   \n",
       "..                           ...   \n",
       "470  10.1016/j.omega.2022.102802   \n",
       "471     10.1016/j.fm.2022.104167   \n",
       "472     10.1016/j.fm.2022.104162   \n",
       "473   10.1016/j.rcim.2022.102468   \n",
       "474   10.1016/j.rcim.2022.102455   \n",
       "\n",
       "                                                 title  \\\n",
       "0         Deep Residual Learning for Image Recognition   \n",
       "1    A new look at the statistical model identifica...   \n",
       "2                          Particle swarm optimization   \n",
       "3    Image quality assessment: from error visibilit...   \n",
       "4    A fast and elitist multiobjective genetic algo...   \n",
       "..                                                 ...   \n",
       "470  Store brand introduction in a dual-channel sup...   \n",
       "471  Ethanol-lactate transition of Lachancea thermo...   \n",
       "472  Determination and quantification of microbial ...   \n",
       "473  Multiple source partial knowledge transfer for...   \n",
       "474  An effective MBSE approach for constructing in...   \n",
       "\n",
       "                                             publisher  \\\n",
       "0                                                 IEEE   \n",
       "1                                                 IEEE   \n",
       "2                                                 IEEE   \n",
       "3                                                 IEEE   \n",
       "4                                                 IEEE   \n",
       "..                                                 ...   \n",
       "470        Nanjing University of Finance and EcoNomics   \n",
       "471  Amsterdam Institute of Molecular and Life Scie...   \n",
       "472                       Quadram Institute Bioscience   \n",
       "473                            Nanjing Tech University   \n",
       "474      Huazhong University of Science and Technology   \n",
       "\n",
       "                                               authors  \\\n",
       "0    Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun   \n",
       "1                                            H. Akaike   \n",
       "2                              J. Kennedy, R. Eberhart   \n",
       "3    Zhou Wang, A.C. Bovik, H.R. Sheikh, E.P. Simon...   \n",
       "4          K. Deb, A. Pratap, S. Agarwal, T. Meyarivan   \n",
       "..                                                 ...   \n",
       "470                                            Xiao Y.   \n",
       "471                                         Battjes J.   \n",
       "472                                    Bloomfield S.J.   \n",
       "473                                             Liu X.   \n",
       "474                                           Zhang X.   \n",
       "\n",
       "                                              keywords content_type      issn  \\\n",
       "0    Training, Degradation, Complexity theory, Imag...  Conferences  10636919   \n",
       "1    Testing, Maximum likelihood estimation, Time s...     Journals  23343303   \n",
       "2    Particle swarm optimization, Birds, Educationa...  Conferences             \n",
       "3    Image quality, Humans, Transform coding, Visua...     Journals  19410042   \n",
       "4    Genetic algorithms, Sorting, Computational com...     Journals  19410026   \n",
       "..                                                 ...          ...       ...   \n",
       "470                                                         Article  03050483   \n",
       "471                                                         Article  07400020   \n",
       "472                                                         Article  07400020   \n",
       "473                                                         Article  07365845   \n",
       "474                                                         Article  07365845   \n",
       "\n",
       "        eissn           isbn  \\\n",
       "0              9781467388528   \n",
       "1                              \n",
       "2                 0780327683   \n",
       "3                              \n",
       "4                              \n",
       "..        ...            ...   \n",
       "470       NaN                  \n",
       "471  10959998                  \n",
       "472  10959998                  \n",
       "473       NaN                  \n",
       "474       NaN                  \n",
       "\n",
       "                                              abstract  \\\n",
       "0    Deeper neural networks are more difficult to t...   \n",
       "1    The history of the development of statistical ...   \n",
       "2    A concept for the optimization of nonlinear fu...   \n",
       "3    Objective methods for assessing perceptual ima...   \n",
       "4    Multi-objective evolutionary algorithms (MOEAs...   \n",
       "..                                                 ...   \n",
       "470                                                      \n",
       "471                                                      \n",
       "472                                                      \n",
       "473                                                      \n",
       "474                                                      \n",
       "\n",
       "                                              html_url  \\\n",
       "0        https://ieeexplore.ieee.org/document/7780459/   \n",
       "1        https://ieeexplore.ieee.org/document/1100705/   \n",
       "2         https://ieeexplore.ieee.org/document/488968/   \n",
       "3        https://ieeexplore.ieee.org/document/1284395/   \n",
       "4         https://ieeexplore.ieee.org/document/996017/   \n",
       "..                                                 ...   \n",
       "470  https://api.elsevier.com/content/abstract/scop...   \n",
       "471  https://api.elsevier.com/content/abstract/scop...   \n",
       "472  https://api.elsevier.com/content/abstract/scop...   \n",
       "473  https://api.elsevier.com/content/abstract/scop...   \n",
       "474  https://api.elsevier.com/content/abstract/scop...   \n",
       "\n",
       "                                     publication_title  publication_year  \n",
       "0    2016 IEEE Conference on Computer Vision and Pa...              2016  \n",
       "1               IEEE Transactions on Automatic Control              1974  \n",
       "2    Proceedings of ICNN'95 - International Confere...              1995  \n",
       "3                IEEE Transactions on Image Processing              2004  \n",
       "4        IEEE Transactions on Evolutionary Computation              2002  \n",
       "..                                                 ...               ...  \n",
       "470                             Omega (United Kingdom)              2023  \n",
       "471                                  Food Microbiology              2023  \n",
       "472                                  Food Microbiology              2023  \n",
       "473     Robotics and Computer-Integrated Manufacturing              2023  \n",
       "474     Robotics and Computer-Integrated Manufacturing              2023  \n",
       "\n",
       "[2275 rows x 13 columns]"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_api_union = pd.concat([df_ieee ,df_api_sd])\n",
    "df_api_union\n",
    "# df_api_union.count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['issn', 'isbn', 'JournalTitle', 'publisher', 'title', 'doi', 'authors', 'keywords', 'abstract', 'publication_year', 'content_type', 'jcr_value', 'scimago_value']\n",
    "\n",
    "df_join_api_csv = df_api_union.merge(dfjournalrank, left_on = 'issn', right_on = 'issnkey', how = 'left' )\n",
    "df_join_api_csv = df_join_api_csv[cols]\n",
    "# df_join_api_csv.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Escrevendo os dados no banco de dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "#conexao com o banco\n",
    "\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "engine = create_engine('sqlite:///Bd_Publications.db', echo=False)\n",
    "engine = engine.execution_options(autocommit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sqlalchemy.engine.cursor.LegacyCursorResult at 0x179f01023e0>"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "engine.execute('''drop table tb_Publications''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sqlalchemy.engine.cursor.LegacyCursorResult at 0x179ef3f7f10>"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#criação da tabela\n",
    "engine.execute('''CREATE TABLE tb_Publications (\n",
    "        doi varchar(500) null,\n",
    "        title varchar(500) null,\n",
    "        JournalTitle varchar(500) null,\n",
    "        publisher varchar(500) null,\n",
    "        authors varchar(500) null,\n",
    "        keywords varchar(500) null,\n",
    "        issn varchar(50) null,\n",
    "        eissn varchar(50) null,\n",
    "        isbn varchar(50) null,\n",
    "        abstract varchar(5000) null,\n",
    "        html_url varchar(500) null,\t\n",
    "        content_type varchar(500) null,\t\n",
    "        publication_title varchar(500) null,\t\n",
    "        publication_year int null,\n",
    "        jcr_value varchar(500) null,\n",
    "        scimago_value varchar(500) null\n",
    "    )''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setando a coluna que será o index\n",
    "def insert_values_db(df_input, indx_col, tb_name):\n",
    "    df=df_input.set_index(indx_col)\n",
    "\n",
    "    #inserção tabela\n",
    "    df.to_sql(tb_name, con=engine, if_exists='append')\n",
    "\n",
    "insert_values_db(df_join_api_csv, 'doi', 'tb_Publications')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"0\":{\"doi\":\"10.1109\\\\/CVPR.2016.90\",\"title\":\"Deep Residual Learning for Image Recognition\",\"JournalTitle\":\"proceedings of the ieee computer society conference on computer vision and pattern recognition\",\"publisher\":\"IEEE\",\"authors\":\"Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun\",\"keywords\":\"Training, Degradation, Complexity theory, Image recognition, Neural networks, Visualization, Image segmentation\",\"issn\":\"10636919\",\"eissn\":null,\"isbn\":\"9781467388528\",\"abstract\":\"Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers - 8\\\\u00d7 deeper than VGG nets [40] but still having lower complexity. An ensemble of these residual nets achieves 3.57% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC & COCO 2015 competitions1, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.\",\"html_url\":null,\"content_type\":\"Conferences\",\"publication_title\":null,\"publication_year\":2016,\"jcr_value\":null,\"scimago_value\":\"4,658\"},\"1\":{\"doi\":\"10.1109\\\\/CVPR.2016.91\",\"title\":\"You Only Look Once: Unified, Real-Time Object Detection\",\"JournalTitle\":\"proceedings of the ieee computer society conference on computer vision and pattern recognition\",\"publisher\":\"IEEE\",\"authors\":\"Joseph Redmon, Santosh Divvala, Ross Girshick, Ali Farhadi\",\"keywords\":\"Computer architecture, Microprocessors, Object detection, Training, Real-time systems, Neural networks, Pipelines\",\"issn\":\"10636919\",\"eissn\":null,\"isbn\":\"9781467388528\",\"abstract\":\"We present YOLO, a new approach to object detection. Prior work on object detection repurposes classifiers to perform detection. Instead, we frame object detection as a regression problem to spatially separated bounding boxes and associated class probabilities. A single neural network predicts bounding boxes and class probabilities directly from full images in one evaluation. Since the whole detection pipeline is a single network, it can be optimized end-to-end directly on detection performance. Our unified architecture is extremely fast. Our base YOLO model processes images in real-time at 45 frames per second. A smaller version of the network, Fast YOLO, processes an astounding 155 frames per second while still achieving double the mAP of other real-time detectors. Compared to state-of-the-art detection systems, YOLO makes more localization errors but is less likely to predict false positives on background. Finally, YOLO learns very general representations of objects. It outperforms other detection methods, including DPM and R-CNN, when generalizing from natural images to other domains like artwork.\",\"html_url\":null,\"content_type\":\"Conferences\",\"publication_title\":null,\"publication_year\":2016,\"jcr_value\":null,\"scimago_value\":\"4,658\"},\"2\":{\"doi\":\"10.1109\\\\/CVPR.2016.308\",\"title\":\"Rethinking the Inception Architecture for Computer Vision\",\"JournalTitle\":\"proceedings of the ieee computer society conference on computer vision and pattern recognition\",\"publisher\":\"IEEE\",\"authors\":\"Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, Zbigniew Wojna\",\"keywords\":\"Convolution, Computer architecture, Training, Computational efficiency, Computer vision, Benchmark testing, Computational modeling\",\"issn\":\"10636919\",\"eissn\":null,\"isbn\":\"9781467388528\",\"abstract\":\"Convolutional networks are at the core of most state of-the-art computer vision solutions for a wide variety of tasks. Since 2014 very deep convolutional networks started to become mainstream, yielding substantial gains in various benchmarks. Although increased model size and computational cost tend to translate to immediate quality gains for most tasks (as long as enough labeled data is provided for training), computational efficiency and low parameter count are still enabling factors for various use cases such as mobile vision and big-data scenarios. Here we are exploring ways to scale up networks in ways that aim at utilizing the added computation as efficiently as possible by suitably factorized convolutions and aggressive regularization. We benchmark our methods on the ILSVRC 2012 classification challenge validation set demonstrate substantial gains over the state of the art: 21:2% top-1 and 5:6% top-5 error for single frame evaluation using a network with a computational cost of 5 billion multiply-adds per inference and with using less than 25 million parameters. With an ensemble of 4 models and multi-crop evaluation, we report 3:5% top-5 error and 17:3% top-1 error on the validation set and 3:6% top-5 error on the official test set.\",\"html_url\":null,\"content_type\":\"Conferences\",\"publication_title\":null,\"publication_year\":2016,\"jcr_value\":null,\"scimago_value\":\"4,658\"},\"3\":{\"doi\":\"10.1109\\\\/TPAMI.2015.2439281\",\"title\":\"Image Super-Resolution Using Deep Convolutional Networks\",\"JournalTitle\":null,\"publisher\":\"IEEE\",\"authors\":\"Chao Dong, Chen Change Loy, Kaiming He, Xiaoou Tang\",\"keywords\":\"Image resolution, Neural networks, Image reconstruction, Convolutional codes, Feature extraction, Training\",\"issn\":\"19393539\",\"eissn\":null,\"isbn\":\"\",\"abstract\":\"We propose a deep learning method for single image super-resolution (SR). Our method directly learns an end-to-end mapping between the low\\\\/high-resolution images. The mapping is represented as a deep convolutional neural network (CNN) that takes the low-resolution image as the input and outputs the high-resolution one. We further show that traditional sparse-coding-based SR methods can also be viewed as a deep convolutional network. But unlike traditional methods that handle each component separately, our method jointly optimizes all layers. Our deep CNN has a lightweight structure, yet demonstrates state-of-the-art restoration quality, and achieves fast speed for practical on-line usage. We explore different network structures and parameter settings to achieve trade-offs between performance and speed. Moreover, we extend our network to cope with three color channels simultaneously, and show better overall reconstruction quality.\",\"html_url\":null,\"content_type\":\"Journals\",\"publication_title\":null,\"publication_year\":2016,\"jcr_value\":null,\"scimago_value\":null},\"4\":{\"doi\":\"10.1109\\\\/CVPR.2016.350\",\"title\":\"The Cityscapes Dataset for Semantic Urban Scene Understanding\",\"JournalTitle\":\"proceedings of the ieee computer society conference on computer vision and pattern recognition\",\"publisher\":\"IEEE\",\"authors\":\"Marius Cordts, Mohamed Omran, Sebastian Ramos, Timo Rehfeld, Markus Enzweiler, Rodrigo Benenson, Uwe Franke, Stefan Roth, Bernt Schiele\",\"keywords\":\"Urban areas, Semantics, Visualization, Benchmark testing, Vehicles, Training, Complexity theory\",\"issn\":\"10636919\",\"eissn\":null,\"isbn\":\"9781467388528\",\"abstract\":\"Visual understanding of complex urban street scenes is an enabling factor for a wide range of applications. Object detection has benefited enormously from large-scale datasets, especially in the context of deep learning. For semantic urban scene understanding, however, no current dataset adequately captures the complexity of real-world urban scenes. To address this, we introduce Cityscapes, a benchmark suite and large-scale dataset to train and test approaches for pixel-level and instance-level semantic labeling. Cityscapes is comprised of a large, diverse set of stereo video sequences recorded in streets from 50 different cities. 5000 of these images have high quality pixel-level annotations, 20 000 additional images have coarse annotations to enable methods that leverage large volumes of weakly-labeled data. Crucially, our effort exceeds previous attempts in terms of dataset size, annotation richness, scene variability, and complexity. Our accompanying empirical study provides an in-depth analysis of the dataset characteristics, as well as a performance evaluation of several state-of-the-art approaches based on our benchmark.\",\"html_url\":null,\"content_type\":\"Conferences\",\"publication_title\":null,\"publication_year\":2016,\"jcr_value\":null,\"scimago_value\":\"4,658\"},\"5\":{\"doi\":\"10.1109\\\\/JIOT.2016.2579198\",\"title\":\"Edge Computing: Vision and Challenges\",\"JournalTitle\":null,\"publisher\":\"IEEE\",\"authors\":\"Weisong Shi, Jie Cao, Quan Zhang, Youhuizi Li, Lanyu Xu\",\"keywords\":\"Cloud computing, Internet of things, Bandwidth, Time factors, Mobile handsets, Data privacy, Smart homes\",\"issn\":\"23722541\",\"eissn\":null,\"isbn\":\"\",\"abstract\":\"The proliferation of Internet of Things (IoT) and the success of rich cloud services have pushed the horizon of a new computing paradigm, edge computing, which calls for processing the data at the edge of the network. Edge computing has the potential to address the concerns of response time requirement, battery life constraint, bandwidth cost saving, as well as data safety and privacy. In this paper, we introduce the definition of edge computing, followed by several case studies, ranging from cloud offloading to smart home and city, as well as collaborative edge to materialize the concept of edge computing. Finally, we present several challenges and opportunities in the field of edge computing, and hope this paper will gain attention from the community and inspire more research in this direction.\",\"html_url\":null,\"content_type\":\"Journals\",\"publication_title\":null,\"publication_year\":2016,\"jcr_value\":null,\"scimago_value\":null},\"6\":{\"doi\":\"10.1109\\\\/CVPR.2016.90\",\"title\":\"Deep Residual Learning for Image Recognition\",\"JournalTitle\":\"proceedings of the ieee computer society conference on computer vision and pattern recognition\",\"publisher\":\"IEEE\",\"authors\":\"Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun\",\"keywords\":\"Training, Degradation, Complexity theory, Image recognition, Neural networks, Visualization, Image segmentation\",\"issn\":\"10636919\",\"eissn\":null,\"isbn\":\"9781467388528\",\"abstract\":\"Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers - 8\\\\u00d7 deeper than VGG nets [40] but still having lower complexity. An ensemble of these residual nets achieves 3.57% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC & COCO 2015 competitions1, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.\",\"html_url\":null,\"content_type\":\"Conferences\",\"publication_title\":null,\"publication_year\":2016,\"jcr_value\":null,\"scimago_value\":\"4,658\"},\"7\":{\"doi\":\"10.1109\\\\/CVPR.2016.91\",\"title\":\"You Only Look Once: Unified, Real-Time Object Detection\",\"JournalTitle\":\"proceedings of the ieee computer society conference on computer vision and pattern recognition\",\"publisher\":\"IEEE\",\"authors\":\"Joseph Redmon, Santosh Divvala, Ross Girshick, Ali Farhadi\",\"keywords\":\"Computer architecture, Microprocessors, Object detection, Training, Real-time systems, Neural networks, Pipelines\",\"issn\":\"10636919\",\"eissn\":null,\"isbn\":\"9781467388528\",\"abstract\":\"We present YOLO, a new approach to object detection. Prior work on object detection repurposes classifiers to perform detection. Instead, we frame object detection as a regression problem to spatially separated bounding boxes and associated class probabilities. A single neural network predicts bounding boxes and class probabilities directly from full images in one evaluation. Since the whole detection pipeline is a single network, it can be optimized end-to-end directly on detection performance. Our unified architecture is extremely fast. Our base YOLO model processes images in real-time at 45 frames per second. A smaller version of the network, Fast YOLO, processes an astounding 155 frames per second while still achieving double the mAP of other real-time detectors. Compared to state-of-the-art detection systems, YOLO makes more localization errors but is less likely to predict false positives on background. Finally, YOLO learns very general representations of objects. It outperforms other detection methods, including DPM and R-CNN, when generalizing from natural images to other domains like artwork.\",\"html_url\":null,\"content_type\":\"Conferences\",\"publication_title\":null,\"publication_year\":2016,\"jcr_value\":null,\"scimago_value\":\"4,658\"},\"8\":{\"doi\":\"10.1109\\\\/CVPR.2016.308\",\"title\":\"Rethinking the Inception Architecture for Computer Vision\",\"JournalTitle\":\"proceedings of the ieee computer society conference on computer vision and pattern recognition\",\"publisher\":\"IEEE\",\"authors\":\"Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, Zbigniew Wojna\",\"keywords\":\"Convolution, Computer architecture, Training, Computational efficiency, Computer vision, Benchmark testing, Computational modeling\",\"issn\":\"10636919\",\"eissn\":null,\"isbn\":\"9781467388528\",\"abstract\":\"Convolutional networks are at the core of most state of-the-art computer vision solutions for a wide variety of tasks. Since 2014 very deep convolutional networks started to become mainstream, yielding substantial gains in various benchmarks. Although increased model size and computational cost tend to translate to immediate quality gains for most tasks (as long as enough labeled data is provided for training), computational efficiency and low parameter count are still enabling factors for various use cases such as mobile vision and big-data scenarios. Here we are exploring ways to scale up networks in ways that aim at utilizing the added computation as efficiently as possible by suitably factorized convolutions and aggressive regularization. We benchmark our methods on the ILSVRC 2012 classification challenge validation set demonstrate substantial gains over the state of the art: 21:2% top-1 and 5:6% top-5 error for single frame evaluation using a network with a computational cost of 5 billion multiply-adds per inference and with using less than 25 million parameters. With an ensemble of 4 models and multi-crop evaluation, we report 3:5% top-5 error and 17:3% top-1 error on the validation set and 3:6% top-5 error on the official test set.\",\"html_url\":null,\"content_type\":\"Conferences\",\"publication_title\":null,\"publication_year\":2016,\"jcr_value\":null,\"scimago_value\":\"4,658\"},\"9\":{\"doi\":\"10.1109\\\\/TPAMI.2015.2439281\",\"title\":\"Image Super-Resolution Using Deep Convolutional Networks\",\"JournalTitle\":null,\"publisher\":\"IEEE\",\"authors\":\"Chao Dong, Chen Change Loy, Kaiming He, Xiaoou Tang\",\"keywords\":\"Image resolution, Neural networks, Image reconstruction, Convolutional codes, Feature extraction, Training\",\"issn\":\"19393539\",\"eissn\":null,\"isbn\":\"\",\"abstract\":\"We propose a deep learning method for single image super-resolution (SR). Our method directly learns an end-to-end mapping between the low\\\\/high-resolution images. The mapping is represented as a deep convolutional neural network (CNN) that takes the low-resolution image as the input and outputs the high-resolution one. We further show that traditional sparse-coding-based SR methods can also be viewed as a deep convolutional network. But unlike traditional methods that handle each component separately, our method jointly optimizes all layers. Our deep CNN has a lightweight structure, yet demonstrates state-of-the-art restoration quality, and achieves fast speed for practical on-line usage. We explore different network structures and parameter settings to achieve trade-offs between performance and speed. Moreover, we extend our network to cope with three color channels simultaneously, and show better overall reconstruction quality.\",\"html_url\":null,\"content_type\":\"Journals\",\"publication_title\":null,\"publication_year\":2016,\"jcr_value\":null,\"scimago_value\":null},\"10\":{\"doi\":\"10.1109\\\\/CVPR.2016.350\",\"title\":\"The Cityscapes Dataset for Semantic Urban Scene Understanding\",\"JournalTitle\":\"proceedings of the ieee computer society conference on computer vision and pattern recognition\",\"publisher\":\"IEEE\",\"authors\":\"Marius Cordts, Mohamed Omran, Sebastian Ramos, Timo Rehfeld, Markus Enzweiler, Rodrigo Benenson, Uwe Franke, Stefan Roth, Bernt Schiele\",\"keywords\":\"Urban areas, Semantics, Visualization, Benchmark testing, Vehicles, Training, Complexity theory\",\"issn\":\"10636919\",\"eissn\":null,\"isbn\":\"9781467388528\",\"abstract\":\"Visual understanding of complex urban street scenes is an enabling factor for a wide range of applications. Object detection has benefited enormously from large-scale datasets, especially in the context of deep learning. For semantic urban scene understanding, however, no current dataset adequately captures the complexity of real-world urban scenes. To address this, we introduce Cityscapes, a benchmark suite and large-scale dataset to train and test approaches for pixel-level and instance-level semantic labeling. Cityscapes is comprised of a large, diverse set of stereo video sequences recorded in streets from 50 different cities. 5000 of these images have high quality pixel-level annotations, 20 000 additional images have coarse annotations to enable methods that leverage large volumes of weakly-labeled data. Crucially, our effort exceeds previous attempts in terms of dataset size, annotation richness, scene variability, and complexity. Our accompanying empirical study provides an in-depth analysis of the dataset characteristics, as well as a performance evaluation of several state-of-the-art approaches based on our benchmark.\",\"html_url\":null,\"content_type\":\"Conferences\",\"publication_title\":null,\"publication_year\":2016,\"jcr_value\":null,\"scimago_value\":\"4,658\"},\"11\":{\"doi\":\"10.1109\\\\/JIOT.2016.2579198\",\"title\":\"Edge Computing: Vision and Challenges\",\"JournalTitle\":null,\"publisher\":\"IEEE\",\"authors\":\"Weisong Shi, Jie Cao, Quan Zhang, Youhuizi Li, Lanyu Xu\",\"keywords\":\"Cloud computing, Internet of things, Bandwidth, Time factors, Mobile handsets, Data privacy, Smart homes\",\"issn\":\"23722541\",\"eissn\":null,\"isbn\":\"\",\"abstract\":\"The proliferation of Internet of Things (IoT) and the success of rich cloud services have pushed the horizon of a new computing paradigm, edge computing, which calls for processing the data at the edge of the network. Edge computing has the potential to address the concerns of response time requirement, battery life constraint, bandwidth cost saving, as well as data safety and privacy. In this paper, we introduce the definition of edge computing, followed by several case studies, ranging from cloud offloading to smart home and city, as well as collaborative edge to materialize the concept of edge computing. Finally, we present several challenges and opportunities in the field of edge computing, and hope this paper will gain attention from the community and inspire more research in this direction.\",\"html_url\":null,\"content_type\":\"Journals\",\"publication_title\":null,\"publication_year\":2016,\"jcr_value\":null,\"scimago_value\":null},\"12\":{\"doi\":\"10.1109\\\\/CVPR.2016.90\",\"title\":\"Deep Residual Learning for Image Recognition\",\"JournalTitle\":\"proceedings of the ieee computer society conference on computer vision and pattern recognition\",\"publisher\":\"IEEE\",\"authors\":\"Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun\",\"keywords\":\"Training, Degradation, Complexity theory, Image recognition, Neural networks, Visualization, Image segmentation\",\"issn\":\"10636919\",\"eissn\":null,\"isbn\":\"9781467388528\",\"abstract\":\"Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers - 8\\\\u00d7 deeper than VGG nets [40] but still having lower complexity. An ensemble of these residual nets achieves 3.57% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC & COCO 2015 competitions1, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.\",\"html_url\":null,\"content_type\":\"Conferences\",\"publication_title\":null,\"publication_year\":2016,\"jcr_value\":null,\"scimago_value\":\"4,658\"},\"13\":{\"doi\":\"10.1109\\\\/CVPR.2016.91\",\"title\":\"You Only Look Once: Unified, Real-Time Object Detection\",\"JournalTitle\":\"proceedings of the ieee computer society conference on computer vision and pattern recognition\",\"publisher\":\"IEEE\",\"authors\":\"Joseph Redmon, Santosh Divvala, Ross Girshick, Ali Farhadi\",\"keywords\":\"Computer architecture, Microprocessors, Object detection, Training, Real-time systems, Neural networks, Pipelines\",\"issn\":\"10636919\",\"eissn\":null,\"isbn\":\"9781467388528\",\"abstract\":\"We present YOLO, a new approach to object detection. Prior work on object detection repurposes classifiers to perform detection. Instead, we frame object detection as a regression problem to spatially separated bounding boxes and associated class probabilities. A single neural network predicts bounding boxes and class probabilities directly from full images in one evaluation. Since the whole detection pipeline is a single network, it can be optimized end-to-end directly on detection performance. Our unified architecture is extremely fast. Our base YOLO model processes images in real-time at 45 frames per second. A smaller version of the network, Fast YOLO, processes an astounding 155 frames per second while still achieving double the mAP of other real-time detectors. Compared to state-of-the-art detection systems, YOLO makes more localization errors but is less likely to predict false positives on background. Finally, YOLO learns very general representations of objects. It outperforms other detection methods, including DPM and R-CNN, when generalizing from natural images to other domains like artwork.\",\"html_url\":null,\"content_type\":\"Conferences\",\"publication_title\":null,\"publication_year\":2016,\"jcr_value\":null,\"scimago_value\":\"4,658\"},\"14\":{\"doi\":\"10.1109\\\\/CVPR.2016.308\",\"title\":\"Rethinking the Inception Architecture for Computer Vision\",\"JournalTitle\":\"proceedings of the ieee computer society conference on computer vision and pattern recognition\",\"publisher\":\"IEEE\",\"authors\":\"Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, Zbigniew Wojna\",\"keywords\":\"Convolution, Computer architecture, Training, Computational efficiency, Computer vision, Benchmark testing, Computational modeling\",\"issn\":\"10636919\",\"eissn\":null,\"isbn\":\"9781467388528\",\"abstract\":\"Convolutional networks are at the core of most state of-the-art computer vision solutions for a wide variety of tasks. Since 2014 very deep convolutional networks started to become mainstream, yielding substantial gains in various benchmarks. Although increased model size and computational cost tend to translate to immediate quality gains for most tasks (as long as enough labeled data is provided for training), computational efficiency and low parameter count are still enabling factors for various use cases such as mobile vision and big-data scenarios. Here we are exploring ways to scale up networks in ways that aim at utilizing the added computation as efficiently as possible by suitably factorized convolutions and aggressive regularization. We benchmark our methods on the ILSVRC 2012 classification challenge validation set demonstrate substantial gains over the state of the art: 21:2% top-1 and 5:6% top-5 error for single frame evaluation using a network with a computational cost of 5 billion multiply-adds per inference and with using less than 25 million parameters. With an ensemble of 4 models and multi-crop evaluation, we report 3:5% top-5 error and 17:3% top-1 error on the validation set and 3:6% top-5 error on the official test set.\",\"html_url\":null,\"content_type\":\"Conferences\",\"publication_title\":null,\"publication_year\":2016,\"jcr_value\":null,\"scimago_value\":\"4,658\"},\"15\":{\"doi\":\"10.1109\\\\/TPAMI.2015.2439281\",\"title\":\"Image Super-Resolution Using Deep Convolutional Networks\",\"JournalTitle\":null,\"publisher\":\"IEEE\",\"authors\":\"Chao Dong, Chen Change Loy, Kaiming He, Xiaoou Tang\",\"keywords\":\"Image resolution, Neural networks, Image reconstruction, Convolutional codes, Feature extraction, Training\",\"issn\":\"19393539\",\"eissn\":null,\"isbn\":\"\",\"abstract\":\"We propose a deep learning method for single image super-resolution (SR). Our method directly learns an end-to-end mapping between the low\\\\/high-resolution images. The mapping is represented as a deep convolutional neural network (CNN) that takes the low-resolution image as the input and outputs the high-resolution one. We further show that traditional sparse-coding-based SR methods can also be viewed as a deep convolutional network. But unlike traditional methods that handle each component separately, our method jointly optimizes all layers. Our deep CNN has a lightweight structure, yet demonstrates state-of-the-art restoration quality, and achieves fast speed for practical on-line usage. We explore different network structures and parameter settings to achieve trade-offs between performance and speed. Moreover, we extend our network to cope with three color channels simultaneously, and show better overall reconstruction quality.\",\"html_url\":null,\"content_type\":\"Journals\",\"publication_title\":null,\"publication_year\":2016,\"jcr_value\":null,\"scimago_value\":null},\"16\":{\"doi\":\"10.1109\\\\/CVPR.2016.350\",\"title\":\"The Cityscapes Dataset for Semantic Urban Scene Understanding\",\"JournalTitle\":\"proceedings of the ieee computer society conference on computer vision and pattern recognition\",\"publisher\":\"IEEE\",\"authors\":\"Marius Cordts, Mohamed Omran, Sebastian Ramos, Timo Rehfeld, Markus Enzweiler, Rodrigo Benenson, Uwe Franke, Stefan Roth, Bernt Schiele\",\"keywords\":\"Urban areas, Semantics, Visualization, Benchmark testing, Vehicles, Training, Complexity theory\",\"issn\":\"10636919\",\"eissn\":null,\"isbn\":\"9781467388528\",\"abstract\":\"Visual understanding of complex urban street scenes is an enabling factor for a wide range of applications. Object detection has benefited enormously from large-scale datasets, especially in the context of deep learning. For semantic urban scene understanding, however, no current dataset adequately captures the complexity of real-world urban scenes. To address this, we introduce Cityscapes, a benchmark suite and large-scale dataset to train and test approaches for pixel-level and instance-level semantic labeling. Cityscapes is comprised of a large, diverse set of stereo video sequences recorded in streets from 50 different cities. 5000 of these images have high quality pixel-level annotations, 20 000 additional images have coarse annotations to enable methods that leverage large volumes of weakly-labeled data. Crucially, our effort exceeds previous attempts in terms of dataset size, annotation richness, scene variability, and complexity. Our accompanying empirical study provides an in-depth analysis of the dataset characteristics, as well as a performance evaluation of several state-of-the-art approaches based on our benchmark.\",\"html_url\":null,\"content_type\":\"Conferences\",\"publication_title\":null,\"publication_year\":2016,\"jcr_value\":null,\"scimago_value\":\"4,658\"},\"17\":{\"doi\":\"10.1109\\\\/JIOT.2016.2579198\",\"title\":\"Edge Computing: Vision and Challenges\",\"JournalTitle\":null,\"publisher\":\"IEEE\",\"authors\":\"Weisong Shi, Jie Cao, Quan Zhang, Youhuizi Li, Lanyu Xu\",\"keywords\":\"Cloud computing, Internet of things, Bandwidth, Time factors, Mobile handsets, Data privacy, Smart homes\",\"issn\":\"23722541\",\"eissn\":null,\"isbn\":\"\",\"abstract\":\"The proliferation of Internet of Things (IoT) and the success of rich cloud services have pushed the horizon of a new computing paradigm, edge computing, which calls for processing the data at the edge of the network. Edge computing has the potential to address the concerns of response time requirement, battery life constraint, bandwidth cost saving, as well as data safety and privacy. In this paper, we introduce the definition of edge computing, followed by several case studies, ranging from cloud offloading to smart home and city, as well as collaborative edge to materialize the concept of edge computing. Finally, we present several challenges and opportunities in the field of edge computing, and hope this paper will gain attention from the community and inspire more research in this direction.\",\"html_url\":null,\"content_type\":\"Journals\",\"publication_title\":null,\"publication_year\":2016,\"jcr_value\":null,\"scimago_value\":null},\"18\":{\"doi\":\"10.1109\\\\/CVPR.2016.90\",\"title\":\"Deep Residual Learning for Image Recognition\",\"JournalTitle\":\"proceedings of the ieee computer society conference on computer vision and pattern recognition\",\"publisher\":\"IEEE\",\"authors\":\"Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun\",\"keywords\":\"Training, Degradation, Complexity theory, Image recognition, Neural networks, Visualization, Image segmentation\",\"issn\":\"10636919\",\"eissn\":null,\"isbn\":\"9781467388528\",\"abstract\":\"Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers - 8\\\\u00d7 deeper than VGG nets [40] but still having lower complexity. An ensemble of these residual nets achieves 3.57% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC & COCO 2015 competitions1, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.\",\"html_url\":null,\"content_type\":\"Conferences\",\"publication_title\":null,\"publication_year\":2016,\"jcr_value\":null,\"scimago_value\":\"4,658\"},\"19\":{\"doi\":\"10.1109\\\\/CVPR.2016.91\",\"title\":\"You Only Look Once: Unified, Real-Time Object Detection\",\"JournalTitle\":\"proceedings of the ieee computer society conference on computer vision and pattern recognition\",\"publisher\":\"IEEE\",\"authors\":\"Joseph Redmon, Santosh Divvala, Ross Girshick, Ali Farhadi\",\"keywords\":\"Computer architecture, Microprocessors, Object detection, Training, Real-time systems, Neural networks, Pipelines\",\"issn\":\"10636919\",\"eissn\":null,\"isbn\":\"9781467388528\",\"abstract\":\"We present YOLO, a new approach to object detection. Prior work on object detection repurposes classifiers to perform detection. Instead, we frame object detection as a regression problem to spatially separated bounding boxes and associated class probabilities. A single neural network predicts bounding boxes and class probabilities directly from full images in one evaluation. Since the whole detection pipeline is a single network, it can be optimized end-to-end directly on detection performance. Our unified architecture is extremely fast. Our base YOLO model processes images in real-time at 45 frames per second. A smaller version of the network, Fast YOLO, processes an astounding 155 frames per second while still achieving double the mAP of other real-time detectors. Compared to state-of-the-art detection systems, YOLO makes more localization errors but is less likely to predict false positives on background. Finally, YOLO learns very general representations of objects. It outperforms other detection methods, including DPM and R-CNN, when generalizing from natural images to other domains like artwork.\",\"html_url\":null,\"content_type\":\"Conferences\",\"publication_title\":null,\"publication_year\":2016,\"jcr_value\":null,\"scimago_value\":\"4,658\"},\"20\":{\"doi\":\"10.1109\\\\/CVPR.2016.308\",\"title\":\"Rethinking the Inception Architecture for Computer Vision\",\"JournalTitle\":\"proceedings of the ieee computer society conference on computer vision and pattern recognition\",\"publisher\":\"IEEE\",\"authors\":\"Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, Zbigniew Wojna\",\"keywords\":\"Convolution, Computer architecture, Training, Computational efficiency, Computer vision, Benchmark testing, Computational modeling\",\"issn\":\"10636919\",\"eissn\":null,\"isbn\":\"9781467388528\",\"abstract\":\"Convolutional networks are at the core of most state of-the-art computer vision solutions for a wide variety of tasks. Since 2014 very deep convolutional networks started to become mainstream, yielding substantial gains in various benchmarks. Although increased model size and computational cost tend to translate to immediate quality gains for most tasks (as long as enough labeled data is provided for training), computational efficiency and low parameter count are still enabling factors for various use cases such as mobile vision and big-data scenarios. Here we are exploring ways to scale up networks in ways that aim at utilizing the added computation as efficiently as possible by suitably factorized convolutions and aggressive regularization. We benchmark our methods on the ILSVRC 2012 classification challenge validation set demonstrate substantial gains over the state of the art: 21:2% top-1 and 5:6% top-5 error for single frame evaluation using a network with a computational cost of 5 billion multiply-adds per inference and with using less than 25 million parameters. With an ensemble of 4 models and multi-crop evaluation, we report 3:5% top-5 error and 17:3% top-1 error on the validation set and 3:6% top-5 error on the official test set.\",\"html_url\":null,\"content_type\":\"Conferences\",\"publication_title\":null,\"publication_year\":2016,\"jcr_value\":null,\"scimago_value\":\"4,658\"},\"21\":{\"doi\":\"10.1109\\\\/TPAMI.2015.2439281\",\"title\":\"Image Super-Resolution Using Deep Convolutional Networks\",\"JournalTitle\":null,\"publisher\":\"IEEE\",\"authors\":\"Chao Dong, Chen Change Loy, Kaiming He, Xiaoou Tang\",\"keywords\":\"Image resolution, Neural networks, Image reconstruction, Convolutional codes, Feature extraction, Training\",\"issn\":\"19393539\",\"eissn\":null,\"isbn\":\"\",\"abstract\":\"We propose a deep learning method for single image super-resolution (SR). Our method directly learns an end-to-end mapping between the low\\\\/high-resolution images. The mapping is represented as a deep convolutional neural network (CNN) that takes the low-resolution image as the input and outputs the high-resolution one. We further show that traditional sparse-coding-based SR methods can also be viewed as a deep convolutional network. But unlike traditional methods that handle each component separately, our method jointly optimizes all layers. Our deep CNN has a lightweight structure, yet demonstrates state-of-the-art restoration quality, and achieves fast speed for practical on-line usage. We explore different network structures and parameter settings to achieve trade-offs between performance and speed. Moreover, we extend our network to cope with three color channels simultaneously, and show better overall reconstruction quality.\",\"html_url\":null,\"content_type\":\"Journals\",\"publication_title\":null,\"publication_year\":2016,\"jcr_value\":null,\"scimago_value\":null},\"22\":{\"doi\":\"10.1109\\\\/CVPR.2016.350\",\"title\":\"The Cityscapes Dataset for Semantic Urban Scene Understanding\",\"JournalTitle\":\"proceedings of the ieee computer society conference on computer vision and pattern recognition\",\"publisher\":\"IEEE\",\"authors\":\"Marius Cordts, Mohamed Omran, Sebastian Ramos, Timo Rehfeld, Markus Enzweiler, Rodrigo Benenson, Uwe Franke, Stefan Roth, Bernt Schiele\",\"keywords\":\"Urban areas, Semantics, Visualization, Benchmark testing, Vehicles, Training, Complexity theory\",\"issn\":\"10636919\",\"eissn\":null,\"isbn\":\"9781467388528\",\"abstract\":\"Visual understanding of complex urban street scenes is an enabling factor for a wide range of applications. Object detection has benefited enormously from large-scale datasets, especially in the context of deep learning. For semantic urban scene understanding, however, no current dataset adequately captures the complexity of real-world urban scenes. To address this, we introduce Cityscapes, a benchmark suite and large-scale dataset to train and test approaches for pixel-level and instance-level semantic labeling. Cityscapes is comprised of a large, diverse set of stereo video sequences recorded in streets from 50 different cities. 5000 of these images have high quality pixel-level annotations, 20 000 additional images have coarse annotations to enable methods that leverage large volumes of weakly-labeled data. Crucially, our effort exceeds previous attempts in terms of dataset size, annotation richness, scene variability, and complexity. Our accompanying empirical study provides an in-depth analysis of the dataset characteristics, as well as a performance evaluation of several state-of-the-art approaches based on our benchmark.\",\"html_url\":null,\"content_type\":\"Conferences\",\"publication_title\":null,\"publication_year\":2016,\"jcr_value\":null,\"scimago_value\":\"4,658\"},\"23\":{\"doi\":\"10.1109\\\\/JIOT.2016.2579198\",\"title\":\"Edge Computing: Vision and Challenges\",\"JournalTitle\":null,\"publisher\":\"IEEE\",\"authors\":\"Weisong Shi, Jie Cao, Quan Zhang, Youhuizi Li, Lanyu Xu\",\"keywords\":\"Cloud computing, Internet of things, Bandwidth, Time factors, Mobile handsets, Data privacy, Smart homes\",\"issn\":\"23722541\",\"eissn\":null,\"isbn\":\"\",\"abstract\":\"The proliferation of Internet of Things (IoT) and the success of rich cloud services have pushed the horizon of a new computing paradigm, edge computing, which calls for processing the data at the edge of the network. Edge computing has the potential to address the concerns of response time requirement, battery life constraint, bandwidth cost saving, as well as data safety and privacy. In this paper, we introduce the definition of edge computing, followed by several case studies, ranging from cloud offloading to smart home and city, as well as collaborative edge to materialize the concept of edge computing. Finally, we present several challenges and opportunities in the field of edge computing, and hope this paper will gain attention from the community and inspire more research in this direction.\",\"html_url\":null,\"content_type\":\"Journals\",\"publication_title\":null,\"publication_year\":2016,\"jcr_value\":null,\"scimago_value\":null},\"24\":{\"doi\":\"10.1109\\\\/CVPR.2016.90\",\"title\":\"Deep Residual Learning for Image Recognition\",\"JournalTitle\":\"proceedings of the ieee computer society conference on computer vision and pattern recognition\",\"publisher\":\"IEEE\",\"authors\":\"Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun\",\"keywords\":\"Training, Degradation, Complexity theory, Image recognition, Neural networks, Visualization, Image segmentation\",\"issn\":\"10636919\",\"eissn\":null,\"isbn\":\"9781467388528\",\"abstract\":\"Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers - 8\\\\u00d7 deeper than VGG nets [40] but still having lower complexity. An ensemble of these residual nets achieves 3.57% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC & COCO 2015 competitions1, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.\",\"html_url\":null,\"content_type\":\"Conferences\",\"publication_title\":null,\"publication_year\":2016,\"jcr_value\":null,\"scimago_value\":\"4,658\"},\"25\":{\"doi\":\"10.1109\\\\/CVPR.2016.91\",\"title\":\"You Only Look Once: Unified, Real-Time Object Detection\",\"JournalTitle\":\"proceedings of the ieee computer society conference on computer vision and pattern recognition\",\"publisher\":\"IEEE\",\"authors\":\"Joseph Redmon, Santosh Divvala, Ross Girshick, Ali Farhadi\",\"keywords\":\"Computer architecture, Microprocessors, Object detection, Training, Real-time systems, Neural networks, Pipelines\",\"issn\":\"10636919\",\"eissn\":null,\"isbn\":\"9781467388528\",\"abstract\":\"We present YOLO, a new approach to object detection. Prior work on object detection repurposes classifiers to perform detection. Instead, we frame object detection as a regression problem to spatially separated bounding boxes and associated class probabilities. A single neural network predicts bounding boxes and class probabilities directly from full images in one evaluation. Since the whole detection pipeline is a single network, it can be optimized end-to-end directly on detection performance. Our unified architecture is extremely fast. Our base YOLO model processes images in real-time at 45 frames per second. A smaller version of the network, Fast YOLO, processes an astounding 155 frames per second while still achieving double the mAP of other real-time detectors. Compared to state-of-the-art detection systems, YOLO makes more localization errors but is less likely to predict false positives on background. Finally, YOLO learns very general representations of objects. It outperforms other detection methods, including DPM and R-CNN, when generalizing from natural images to other domains like artwork.\",\"html_url\":null,\"content_type\":\"Conferences\",\"publication_title\":null,\"publication_year\":2016,\"jcr_value\":null,\"scimago_value\":\"4,658\"},\"26\":{\"doi\":\"10.1109\\\\/CVPR.2016.308\",\"title\":\"Rethinking the Inception Architecture for Computer Vision\",\"JournalTitle\":\"proceedings of the ieee computer society conference on computer vision and pattern recognition\",\"publisher\":\"IEEE\",\"authors\":\"Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, Zbigniew Wojna\",\"keywords\":\"Convolution, Computer architecture, Training, Computational efficiency, Computer vision, Benchmark testing, Computational modeling\",\"issn\":\"10636919\",\"eissn\":null,\"isbn\":\"9781467388528\",\"abstract\":\"Convolutional networks are at the core of most state of-the-art computer vision solutions for a wide variety of tasks. Since 2014 very deep convolutional networks started to become mainstream, yielding substantial gains in various benchmarks. Although increased model size and computational cost tend to translate to immediate quality gains for most tasks (as long as enough labeled data is provided for training), computational efficiency and low parameter count are still enabling factors for various use cases such as mobile vision and big-data scenarios. Here we are exploring ways to scale up networks in ways that aim at utilizing the added computation as efficiently as possible by suitably factorized convolutions and aggressive regularization. We benchmark our methods on the ILSVRC 2012 classification challenge validation set demonstrate substantial gains over the state of the art: 21:2% top-1 and 5:6% top-5 error for single frame evaluation using a network with a computational cost of 5 billion multiply-adds per inference and with using less than 25 million parameters. With an ensemble of 4 models and multi-crop evaluation, we report 3:5% top-5 error and 17:3% top-1 error on the validation set and 3:6% top-5 error on the official test set.\",\"html_url\":null,\"content_type\":\"Conferences\",\"publication_title\":null,\"publication_year\":2016,\"jcr_value\":null,\"scimago_value\":\"4,658\"},\"27\":{\"doi\":\"10.1109\\\\/TPAMI.2015.2439281\",\"title\":\"Image Super-Resolution Using Deep Convolutional Networks\",\"JournalTitle\":null,\"publisher\":\"IEEE\",\"authors\":\"Chao Dong, Chen Change Loy, Kaiming He, Xiaoou Tang\",\"keywords\":\"Image resolution, Neural networks, Image reconstruction, Convolutional codes, Feature extraction, Training\",\"issn\":\"19393539\",\"eissn\":null,\"isbn\":\"\",\"abstract\":\"We propose a deep learning method for single image super-resolution (SR). Our method directly learns an end-to-end mapping between the low\\\\/high-resolution images. The mapping is represented as a deep convolutional neural network (CNN) that takes the low-resolution image as the input and outputs the high-resolution one. We further show that traditional sparse-coding-based SR methods can also be viewed as a deep convolutional network. But unlike traditional methods that handle each component separately, our method jointly optimizes all layers. Our deep CNN has a lightweight structure, yet demonstrates state-of-the-art restoration quality, and achieves fast speed for practical on-line usage. We explore different network structures and parameter settings to achieve trade-offs between performance and speed. Moreover, we extend our network to cope with three color channels simultaneously, and show better overall reconstruction quality.\",\"html_url\":null,\"content_type\":\"Journals\",\"publication_title\":null,\"publication_year\":2016,\"jcr_value\":null,\"scimago_value\":null},\"28\":{\"doi\":\"10.1109\\\\/CVPR.2016.350\",\"title\":\"The Cityscapes Dataset for Semantic Urban Scene Understanding\",\"JournalTitle\":\"proceedings of the ieee computer society conference on computer vision and pattern recognition\",\"publisher\":\"IEEE\",\"authors\":\"Marius Cordts, Mohamed Omran, Sebastian Ramos, Timo Rehfeld, Markus Enzweiler, Rodrigo Benenson, Uwe Franke, Stefan Roth, Bernt Schiele\",\"keywords\":\"Urban areas, Semantics, Visualization, Benchmark testing, Vehicles, Training, Complexity theory\",\"issn\":\"10636919\",\"eissn\":null,\"isbn\":\"9781467388528\",\"abstract\":\"Visual understanding of complex urban street scenes is an enabling factor for a wide range of applications. Object detection has benefited enormously from large-scale datasets, especially in the context of deep learning. For semantic urban scene understanding, however, no current dataset adequately captures the complexity of real-world urban scenes. To address this, we introduce Cityscapes, a benchmark suite and large-scale dataset to train and test approaches for pixel-level and instance-level semantic labeling. Cityscapes is comprised of a large, diverse set of stereo video sequences recorded in streets from 50 different cities. 5000 of these images have high quality pixel-level annotations, 20 000 additional images have coarse annotations to enable methods that leverage large volumes of weakly-labeled data. Crucially, our effort exceeds previous attempts in terms of dataset size, annotation richness, scene variability, and complexity. Our accompanying empirical study provides an in-depth analysis of the dataset characteristics, as well as a performance evaluation of several state-of-the-art approaches based on our benchmark.\",\"html_url\":null,\"content_type\":\"Conferences\",\"publication_title\":null,\"publication_year\":2016,\"jcr_value\":null,\"scimago_value\":\"4,658\"},\"29\":{\"doi\":\"10.1109\\\\/JIOT.2016.2579198\",\"title\":\"Edge Computing: Vision and Challenges\",\"JournalTitle\":null,\"publisher\":\"IEEE\",\"authors\":\"Weisong Shi, Jie Cao, Quan Zhang, Youhuizi Li, Lanyu Xu\",\"keywords\":\"Cloud computing, Internet of things, Bandwidth, Time factors, Mobile handsets, Data privacy, Smart homes\",\"issn\":\"23722541\",\"eissn\":null,\"isbn\":\"\",\"abstract\":\"The proliferation of Internet of Things (IoT) and the success of rich cloud services have pushed the horizon of a new computing paradigm, edge computing, which calls for processing the data at the edge of the network. Edge computing has the potential to address the concerns of response time requirement, battery life constraint, bandwidth cost saving, as well as data safety and privacy. In this paper, we introduce the definition of edge computing, followed by several case studies, ranging from cloud offloading to smart home and city, as well as collaborative edge to materialize the concept of edge computing. Finally, we present several challenges and opportunities in the field of edge computing, and hope this paper will gain attention from the community and inspire more research in this direction.\",\"html_url\":null,\"content_type\":\"Journals\",\"publication_title\":null,\"publication_year\":2016,\"jcr_value\":null,\"scimago_value\":null},\"30\":{\"doi\":\"10.1109\\\\/CVPR.2016.90\",\"title\":\"Deep Residual Learning for Image Recognition\",\"JournalTitle\":\"proceedings of the ieee computer society conference on computer vision and pattern recognition\",\"publisher\":\"IEEE\",\"authors\":\"Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun\",\"keywords\":\"Training, Degradation, Complexity theory, Image recognition, Neural networks, Visualization, Image segmentation\",\"issn\":\"10636919\",\"eissn\":null,\"isbn\":\"9781467388528\",\"abstract\":\"Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers - 8\\\\u00d7 deeper than VGG nets [40] but still having lower complexity. An ensemble of these residual nets achieves 3.57% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC & COCO 2015 competitions1, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.\",\"html_url\":null,\"content_type\":\"Conferences\",\"publication_title\":null,\"publication_year\":2016,\"jcr_value\":null,\"scimago_value\":\"4,658\"},\"31\":{\"doi\":\"10.1109\\\\/CVPR.2016.91\",\"title\":\"You Only Look Once: Unified, Real-Time Object Detection\",\"JournalTitle\":\"proceedings of the ieee computer society conference on computer vision and pattern recognition\",\"publisher\":\"IEEE\",\"authors\":\"Joseph Redmon, Santosh Divvala, Ross Girshick, Ali Farhadi\",\"keywords\":\"Computer architecture, Microprocessors, Object detection, Training, Real-time systems, Neural networks, Pipelines\",\"issn\":\"10636919\",\"eissn\":null,\"isbn\":\"9781467388528\",\"abstract\":\"We present YOLO, a new approach to object detection. Prior work on object detection repurposes classifiers to perform detection. Instead, we frame object detection as a regression problem to spatially separated bounding boxes and associated class probabilities. A single neural network predicts bounding boxes and class probabilities directly from full images in one evaluation. Since the whole detection pipeline is a single network, it can be optimized end-to-end directly on detection performance. Our unified architecture is extremely fast. Our base YOLO model processes images in real-time at 45 frames per second. A smaller version of the network, Fast YOLO, processes an astounding 155 frames per second while still achieving double the mAP of other real-time detectors. Compared to state-of-the-art detection systems, YOLO makes more localization errors but is less likely to predict false positives on background. Finally, YOLO learns very general representations of objects. It outperforms other detection methods, including DPM and R-CNN, when generalizing from natural images to other domains like artwork.\",\"html_url\":null,\"content_type\":\"Conferences\",\"publication_title\":null,\"publication_year\":2016,\"jcr_value\":null,\"scimago_value\":\"4,658\"},\"32\":{\"doi\":\"10.1109\\\\/CVPR.2016.308\",\"title\":\"Rethinking the Inception Architecture for Computer Vision\",\"JournalTitle\":\"proceedings of the ieee computer society conference on computer vision and pattern recognition\",\"publisher\":\"IEEE\",\"authors\":\"Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, Zbigniew Wojna\",\"keywords\":\"Convolution, Computer architecture, Training, Computational efficiency, Computer vision, Benchmark testing, Computational modeling\",\"issn\":\"10636919\",\"eissn\":null,\"isbn\":\"9781467388528\",\"abstract\":\"Convolutional networks are at the core of most state of-the-art computer vision solutions for a wide variety of tasks. Since 2014 very deep convolutional networks started to become mainstream, yielding substantial gains in various benchmarks. Although increased model size and computational cost tend to translate to immediate quality gains for most tasks (as long as enough labeled data is provided for training), computational efficiency and low parameter count are still enabling factors for various use cases such as mobile vision and big-data scenarios. Here we are exploring ways to scale up networks in ways that aim at utilizing the added computation as efficiently as possible by suitably factorized convolutions and aggressive regularization. We benchmark our methods on the ILSVRC 2012 classification challenge validation set demonstrate substantial gains over the state of the art: 21:2% top-1 and 5:6% top-5 error for single frame evaluation using a network with a computational cost of 5 billion multiply-adds per inference and with using less than 25 million parameters. With an ensemble of 4 models and multi-crop evaluation, we report 3:5% top-5 error and 17:3% top-1 error on the validation set and 3:6% top-5 error on the official test set.\",\"html_url\":null,\"content_type\":\"Conferences\",\"publication_title\":null,\"publication_year\":2016,\"jcr_value\":null,\"scimago_value\":\"4,658\"},\"33\":{\"doi\":\"10.1109\\\\/TPAMI.2015.2439281\",\"title\":\"Image Super-Resolution Using Deep Convolutional Networks\",\"JournalTitle\":null,\"publisher\":\"IEEE\",\"authors\":\"Chao Dong, Chen Change Loy, Kaiming He, Xiaoou Tang\",\"keywords\":\"Image resolution, Neural networks, Image reconstruction, Convolutional codes, Feature extraction, Training\",\"issn\":\"19393539\",\"eissn\":null,\"isbn\":\"\",\"abstract\":\"We propose a deep learning method for single image super-resolution (SR). Our method directly learns an end-to-end mapping between the low\\\\/high-resolution images. The mapping is represented as a deep convolutional neural network (CNN) that takes the low-resolution image as the input and outputs the high-resolution one. We further show that traditional sparse-coding-based SR methods can also be viewed as a deep convolutional network. But unlike traditional methods that handle each component separately, our method jointly optimizes all layers. Our deep CNN has a lightweight structure, yet demonstrates state-of-the-art restoration quality, and achieves fast speed for practical on-line usage. We explore different network structures and parameter settings to achieve trade-offs between performance and speed. Moreover, we extend our network to cope with three color channels simultaneously, and show better overall reconstruction quality.\",\"html_url\":null,\"content_type\":\"Journals\",\"publication_title\":null,\"publication_year\":2016,\"jcr_value\":null,\"scimago_value\":null},\"34\":{\"doi\":\"10.1109\\\\/CVPR.2016.350\",\"title\":\"The Cityscapes Dataset for Semantic Urban Scene Understanding\",\"JournalTitle\":\"proceedings of the ieee computer society conference on computer vision and pattern recognition\",\"publisher\":\"IEEE\",\"authors\":\"Marius Cordts, Mohamed Omran, Sebastian Ramos, Timo Rehfeld, Markus Enzweiler, Rodrigo Benenson, Uwe Franke, Stefan Roth, Bernt Schiele\",\"keywords\":\"Urban areas, Semantics, Visualization, Benchmark testing, Vehicles, Training, Complexity theory\",\"issn\":\"10636919\",\"eissn\":null,\"isbn\":\"9781467388528\",\"abstract\":\"Visual understanding of complex urban street scenes is an enabling factor for a wide range of applications. Object detection has benefited enormously from large-scale datasets, especially in the context of deep learning. For semantic urban scene understanding, however, no current dataset adequately captures the complexity of real-world urban scenes. To address this, we introduce Cityscapes, a benchmark suite and large-scale dataset to train and test approaches for pixel-level and instance-level semantic labeling. Cityscapes is comprised of a large, diverse set of stereo video sequences recorded in streets from 50 different cities. 5000 of these images have high quality pixel-level annotations, 20 000 additional images have coarse annotations to enable methods that leverage large volumes of weakly-labeled data. Crucially, our effort exceeds previous attempts in terms of dataset size, annotation richness, scene variability, and complexity. Our accompanying empirical study provides an in-depth analysis of the dataset characteristics, as well as a performance evaluation of several state-of-the-art approaches based on our benchmark.\",\"html_url\":null,\"content_type\":\"Conferences\",\"publication_title\":null,\"publication_year\":2016,\"jcr_value\":null,\"scimago_value\":\"4,658\"},\"35\":{\"doi\":\"10.1109\\\\/JIOT.2016.2579198\",\"title\":\"Edge Computing: Vision and Challenges\",\"JournalTitle\":null,\"publisher\":\"IEEE\",\"authors\":\"Weisong Shi, Jie Cao, Quan Zhang, Youhuizi Li, Lanyu Xu\",\"keywords\":\"Cloud computing, Internet of things, Bandwidth, Time factors, Mobile handsets, Data privacy, Smart homes\",\"issn\":\"23722541\",\"eissn\":null,\"isbn\":\"\",\"abstract\":\"The proliferation of Internet of Things (IoT) and the success of rich cloud services have pushed the horizon of a new computing paradigm, edge computing, which calls for processing the data at the edge of the network. Edge computing has the potential to address the concerns of response time requirement, battery life constraint, bandwidth cost saving, as well as data safety and privacy. In this paper, we introduce the definition of edge computing, followed by several case studies, ranging from cloud offloading to smart home and city, as well as collaborative edge to materialize the concept of edge computing. Finally, we present several challenges and opportunities in the field of edge computing, and hope this paper will gain attention from the community and inspire more research in this direction.\",\"html_url\":null,\"content_type\":\"Journals\",\"publication_title\":null,\"publication_year\":2016,\"jcr_value\":null,\"scimago_value\":null},\"36\":{\"doi\":\"10.1109\\\\/CVPR.2016.90\",\"title\":\"Deep Residual Learning for Image Recognition\",\"JournalTitle\":\"proceedings of the ieee computer society conference on computer vision and pattern recognition\",\"publisher\":\"IEEE\",\"authors\":\"Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun\",\"keywords\":\"Training, Degradation, Complexity theory, Image recognition, Neural networks, Visualization, Image segmentation\",\"issn\":\"10636919\",\"eissn\":null,\"isbn\":\"9781467388528\",\"abstract\":\"Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers - 8\\\\u00d7 deeper than VGG nets [40] but still having lower complexity. An ensemble of these residual nets achieves 3.57% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC & COCO 2015 competitions1, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.\",\"html_url\":null,\"content_type\":\"Conferences\",\"publication_title\":null,\"publication_year\":2016,\"jcr_value\":null,\"scimago_value\":\"4,658\"},\"37\":{\"doi\":\"10.1109\\\\/CVPR.2016.91\",\"title\":\"You Only Look Once: Unified, Real-Time Object Detection\",\"JournalTitle\":\"proceedings of the ieee computer society conference on computer vision and pattern recognition\",\"publisher\":\"IEEE\",\"authors\":\"Joseph Redmon, Santosh Divvala, Ross Girshick, Ali Farhadi\",\"keywords\":\"Computer architecture, Microprocessors, Object detection, Training, Real-time systems, Neural networks, Pipelines\",\"issn\":\"10636919\",\"eissn\":null,\"isbn\":\"9781467388528\",\"abstract\":\"We present YOLO, a new approach to object detection. Prior work on object detection repurposes classifiers to perform detection. Instead, we frame object detection as a regression problem to spatially separated bounding boxes and associated class probabilities. A single neural network predicts bounding boxes and class probabilities directly from full images in one evaluation. Since the whole detection pipeline is a single network, it can be optimized end-to-end directly on detection performance. Our unified architecture is extremely fast. Our base YOLO model processes images in real-time at 45 frames per second. A smaller version of the network, Fast YOLO, processes an astounding 155 frames per second while still achieving double the mAP of other real-time detectors. Compared to state-of-the-art detection systems, YOLO makes more localization errors but is less likely to predict false positives on background. Finally, YOLO learns very general representations of objects. It outperforms other detection methods, including DPM and R-CNN, when generalizing from natural images to other domains like artwork.\",\"html_url\":null,\"content_type\":\"Conferences\",\"publication_title\":null,\"publication_year\":2016,\"jcr_value\":null,\"scimago_value\":\"4,658\"},\"38\":{\"doi\":\"10.1109\\\\/CVPR.2016.308\",\"title\":\"Rethinking the Inception Architecture for Computer Vision\",\"JournalTitle\":\"proceedings of the ieee computer society conference on computer vision and pattern recognition\",\"publisher\":\"IEEE\",\"authors\":\"Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, Zbigniew Wojna\",\"keywords\":\"Convolution, Computer architecture, Training, Computational efficiency, Computer vision, Benchmark testing, Computational modeling\",\"issn\":\"10636919\",\"eissn\":null,\"isbn\":\"9781467388528\",\"abstract\":\"Convolutional networks are at the core of most state of-the-art computer vision solutions for a wide variety of tasks. Since 2014 very deep convolutional networks started to become mainstream, yielding substantial gains in various benchmarks. Although increased model size and computational cost tend to translate to immediate quality gains for most tasks (as long as enough labeled data is provided for training), computational efficiency and low parameter count are still enabling factors for various use cases such as mobile vision and big-data scenarios. Here we are exploring ways to scale up networks in ways that aim at utilizing the added computation as efficiently as possible by suitably factorized convolutions and aggressive regularization. We benchmark our methods on the ILSVRC 2012 classification challenge validation set demonstrate substantial gains over the state of the art: 21:2% top-1 and 5:6% top-5 error for single frame evaluation using a network with a computational cost of 5 billion multiply-adds per inference and with using less than 25 million parameters. With an ensemble of 4 models and multi-crop evaluation, we report 3:5% top-5 error and 17:3% top-1 error on the validation set and 3:6% top-5 error on the official test set.\",\"html_url\":null,\"content_type\":\"Conferences\",\"publication_title\":null,\"publication_year\":2016,\"jcr_value\":null,\"scimago_value\":\"4,658\"},\"39\":{\"doi\":\"10.1109\\\\/TPAMI.2015.2439281\",\"title\":\"Image Super-Resolution Using Deep Convolutional Networks\",\"JournalTitle\":null,\"publisher\":\"IEEE\",\"authors\":\"Chao Dong, Chen Change Loy, Kaiming He, Xiaoou Tang\",\"keywords\":\"Image resolution, Neural networks, Image reconstruction, Convolutional codes, Feature extraction, Training\",\"issn\":\"19393539\",\"eissn\":null,\"isbn\":\"\",\"abstract\":\"We propose a deep learning method for single image super-resolution (SR). Our method directly learns an end-to-end mapping between the low\\\\/high-resolution images. The mapping is represented as a deep convolutional neural network (CNN) that takes the low-resolution image as the input and outputs the high-resolution one. We further show that traditional sparse-coding-based SR methods can also be viewed as a deep convolutional network. But unlike traditional methods that handle each component separately, our method jointly optimizes all layers. Our deep CNN has a lightweight structure, yet demonstrates state-of-the-art restoration quality, and achieves fast speed for practical on-line usage. We explore different network structures and parameter settings to achieve trade-offs between performance and speed. Moreover, we extend our network to cope with three color channels simultaneously, and show better overall reconstruction quality.\",\"html_url\":null,\"content_type\":\"Journals\",\"publication_title\":null,\"publication_year\":2016,\"jcr_value\":null,\"scimago_value\":null},\"40\":{\"doi\":\"10.1109\\\\/CVPR.2016.350\",\"title\":\"The Cityscapes Dataset for Semantic Urban Scene Understanding\",\"JournalTitle\":\"proceedings of the ieee computer society conference on computer vision and pattern recognition\",\"publisher\":\"IEEE\",\"authors\":\"Marius Cordts, Mohamed Omran, Sebastian Ramos, Timo Rehfeld, Markus Enzweiler, Rodrigo Benenson, Uwe Franke, Stefan Roth, Bernt Schiele\",\"keywords\":\"Urban areas, Semantics, Visualization, Benchmark testing, Vehicles, Training, Complexity theory\",\"issn\":\"10636919\",\"eissn\":null,\"isbn\":\"9781467388528\",\"abstract\":\"Visual understanding of complex urban street scenes is an enabling factor for a wide range of applications. Object detection has benefited enormously from large-scale datasets, especially in the context of deep learning. For semantic urban scene understanding, however, no current dataset adequately captures the complexity of real-world urban scenes. To address this, we introduce Cityscapes, a benchmark suite and large-scale dataset to train and test approaches for pixel-level and instance-level semantic labeling. Cityscapes is comprised of a large, diverse set of stereo video sequences recorded in streets from 50 different cities. 5000 of these images have high quality pixel-level annotations, 20 000 additional images have coarse annotations to enable methods that leverage large volumes of weakly-labeled data. Crucially, our effort exceeds previous attempts in terms of dataset size, annotation richness, scene variability, and complexity. Our accompanying empirical study provides an in-depth analysis of the dataset characteristics, as well as a performance evaluation of several state-of-the-art approaches based on our benchmark.\",\"html_url\":null,\"content_type\":\"Conferences\",\"publication_title\":null,\"publication_year\":2016,\"jcr_value\":null,\"scimago_value\":\"4,658\"},\"41\":{\"doi\":\"10.1109\\\\/JIOT.2016.2579198\",\"title\":\"Edge Computing: Vision and Challenges\",\"JournalTitle\":null,\"publisher\":\"IEEE\",\"authors\":\"Weisong Shi, Jie Cao, Quan Zhang, Youhuizi Li, Lanyu Xu\",\"keywords\":\"Cloud computing, Internet of things, Bandwidth, Time factors, Mobile handsets, Data privacy, Smart homes\",\"issn\":\"23722541\",\"eissn\":null,\"isbn\":\"\",\"abstract\":\"The proliferation of Internet of Things (IoT) and the success of rich cloud services have pushed the horizon of a new computing paradigm, edge computing, which calls for processing the data at the edge of the network. Edge computing has the potential to address the concerns of response time requirement, battery life constraint, bandwidth cost saving, as well as data safety and privacy. In this paper, we introduce the definition of edge computing, followed by several case studies, ranging from cloud offloading to smart home and city, as well as collaborative edge to materialize the concept of edge computing. Finally, we present several challenges and opportunities in the field of edge computing, and hope this paper will gain attention from the community and inspire more research in this direction.\",\"html_url\":null,\"content_type\":\"Journals\",\"publication_title\":null,\"publication_year\":2016,\"jcr_value\":null,\"scimago_value\":null},\"42\":{\"doi\":\"10.1109\\\\/CVPR.2016.90\",\"title\":\"Deep Residual Learning for Image Recognition\",\"JournalTitle\":\"proceedings of the ieee computer society conference on computer vision and pattern recognition\",\"publisher\":\"IEEE\",\"authors\":\"Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun\",\"keywords\":\"Training, Degradation, Complexity theory, Image recognition, Neural networks, Visualization, Image segmentation\",\"issn\":\"10636919\",\"eissn\":null,\"isbn\":\"9781467388528\",\"abstract\":\"Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers - 8\\\\u00d7 deeper than VGG nets [40] but still having lower complexity. An ensemble of these residual nets achieves 3.57% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC & COCO 2015 competitions1, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.\",\"html_url\":null,\"content_type\":\"Conferences\",\"publication_title\":null,\"publication_year\":2016,\"jcr_value\":null,\"scimago_value\":\"4,658\"},\"43\":{\"doi\":\"10.1109\\\\/CVPR.2016.91\",\"title\":\"You Only Look Once: Unified, Real-Time Object Detection\",\"JournalTitle\":\"proceedings of the ieee computer society conference on computer vision and pattern recognition\",\"publisher\":\"IEEE\",\"authors\":\"Joseph Redmon, Santosh Divvala, Ross Girshick, Ali Farhadi\",\"keywords\":\"Computer architecture, Microprocessors, Object detection, Training, Real-time systems, Neural networks, Pipelines\",\"issn\":\"10636919\",\"eissn\":null,\"isbn\":\"9781467388528\",\"abstract\":\"We present YOLO, a new approach to object detection. Prior work on object detection repurposes classifiers to perform detection. Instead, we frame object detection as a regression problem to spatially separated bounding boxes and associated class probabilities. A single neural network predicts bounding boxes and class probabilities directly from full images in one evaluation. Since the whole detection pipeline is a single network, it can be optimized end-to-end directly on detection performance. Our unified architecture is extremely fast. Our base YOLO model processes images in real-time at 45 frames per second. A smaller version of the network, Fast YOLO, processes an astounding 155 frames per second while still achieving double the mAP of other real-time detectors. Compared to state-of-the-art detection systems, YOLO makes more localization errors but is less likely to predict false positives on background. Finally, YOLO learns very general representations of objects. It outperforms other detection methods, including DPM and R-CNN, when generalizing from natural images to other domains like artwork.\",\"html_url\":null,\"content_type\":\"Conferences\",\"publication_title\":null,\"publication_year\":2016,\"jcr_value\":null,\"scimago_value\":\"4,658\"},\"44\":{\"doi\":\"10.1109\\\\/CVPR.2016.308\",\"title\":\"Rethinking the Inception Architecture for Computer Vision\",\"JournalTitle\":\"proceedings of the ieee computer society conference on computer vision and pattern recognition\",\"publisher\":\"IEEE\",\"authors\":\"Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, Zbigniew Wojna\",\"keywords\":\"Convolution, Computer architecture, Training, Computational efficiency, Computer vision, Benchmark testing, Computational modeling\",\"issn\":\"10636919\",\"eissn\":null,\"isbn\":\"9781467388528\",\"abstract\":\"Convolutional networks are at the core of most state of-the-art computer vision solutions for a wide variety of tasks. Since 2014 very deep convolutional networks started to become mainstream, yielding substantial gains in various benchmarks. Although increased model size and computational cost tend to translate to immediate quality gains for most tasks (as long as enough labeled data is provided for training), computational efficiency and low parameter count are still enabling factors for various use cases such as mobile vision and big-data scenarios. Here we are exploring ways to scale up networks in ways that aim at utilizing the added computation as efficiently as possible by suitably factorized convolutions and aggressive regularization. We benchmark our methods on the ILSVRC 2012 classification challenge validation set demonstrate substantial gains over the state of the art: 21:2% top-1 and 5:6% top-5 error for single frame evaluation using a network with a computational cost of 5 billion multiply-adds per inference and with using less than 25 million parameters. With an ensemble of 4 models and multi-crop evaluation, we report 3:5% top-5 error and 17:3% top-1 error on the validation set and 3:6% top-5 error on the official test set.\",\"html_url\":null,\"content_type\":\"Conferences\",\"publication_title\":null,\"publication_year\":2016,\"jcr_value\":null,\"scimago_value\":\"4,658\"},\"45\":{\"doi\":\"10.1109\\\\/TPAMI.2015.2439281\",\"title\":\"Image Super-Resolution Using Deep Convolutional Networks\",\"JournalTitle\":null,\"publisher\":\"IEEE\",\"authors\":\"Chao Dong, Chen Change Loy, Kaiming He, Xiaoou Tang\",\"keywords\":\"Image resolution, Neural networks, Image reconstruction, Convolutional codes, Feature extraction, Training\",\"issn\":\"19393539\",\"eissn\":null,\"isbn\":\"\",\"abstract\":\"We propose a deep learning method for single image super-resolution (SR). Our method directly learns an end-to-end mapping between the low\\\\/high-resolution images. The mapping is represented as a deep convolutional neural network (CNN) that takes the low-resolution image as the input and outputs the high-resolution one. We further show that traditional sparse-coding-based SR methods can also be viewed as a deep convolutional network. But unlike traditional methods that handle each component separately, our method jointly optimizes all layers. Our deep CNN has a lightweight structure, yet demonstrates state-of-the-art restoration quality, and achieves fast speed for practical on-line usage. We explore different network structures and parameter settings to achieve trade-offs between performance and speed. Moreover, we extend our network to cope with three color channels simultaneously, and show better overall reconstruction quality.\",\"html_url\":null,\"content_type\":\"Journals\",\"publication_title\":null,\"publication_year\":2016,\"jcr_value\":null,\"scimago_value\":null},\"46\":{\"doi\":\"10.1109\\\\/CVPR.2016.350\",\"title\":\"The Cityscapes Dataset for Semantic Urban Scene Understanding\",\"JournalTitle\":\"proceedings of the ieee computer society conference on computer vision and pattern recognition\",\"publisher\":\"IEEE\",\"authors\":\"Marius Cordts, Mohamed Omran, Sebastian Ramos, Timo Rehfeld, Markus Enzweiler, Rodrigo Benenson, Uwe Franke, Stefan Roth, Bernt Schiele\",\"keywords\":\"Urban areas, Semantics, Visualization, Benchmark testing, Vehicles, Training, Complexity theory\",\"issn\":\"10636919\",\"eissn\":null,\"isbn\":\"9781467388528\",\"abstract\":\"Visual understanding of complex urban street scenes is an enabling factor for a wide range of applications. Object detection has benefited enormously from large-scale datasets, especially in the context of deep learning. For semantic urban scene understanding, however, no current dataset adequately captures the complexity of real-world urban scenes. To address this, we introduce Cityscapes, a benchmark suite and large-scale dataset to train and test approaches for pixel-level and instance-level semantic labeling. Cityscapes is comprised of a large, diverse set of stereo video sequences recorded in streets from 50 different cities. 5000 of these images have high quality pixel-level annotations, 20 000 additional images have coarse annotations to enable methods that leverage large volumes of weakly-labeled data. Crucially, our effort exceeds previous attempts in terms of dataset size, annotation richness, scene variability, and complexity. Our accompanying empirical study provides an in-depth analysis of the dataset characteristics, as well as a performance evaluation of several state-of-the-art approaches based on our benchmark.\",\"html_url\":null,\"content_type\":\"Conferences\",\"publication_title\":null,\"publication_year\":2016,\"jcr_value\":null,\"scimago_value\":\"4,658\"},\"47\":{\"doi\":\"10.1109\\\\/JIOT.2016.2579198\",\"title\":\"Edge Computing: Vision and Challenges\",\"JournalTitle\":null,\"publisher\":\"IEEE\",\"authors\":\"Weisong Shi, Jie Cao, Quan Zhang, Youhuizi Li, Lanyu Xu\",\"keywords\":\"Cloud computing, Internet of things, Bandwidth, Time factors, Mobile handsets, Data privacy, Smart homes\",\"issn\":\"23722541\",\"eissn\":null,\"isbn\":\"\",\"abstract\":\"The proliferation of Internet of Things (IoT) and the success of rich cloud services have pushed the horizon of a new computing paradigm, edge computing, which calls for processing the data at the edge of the network. Edge computing has the potential to address the concerns of response time requirement, battery life constraint, bandwidth cost saving, as well as data safety and privacy. In this paper, we introduce the definition of edge computing, followed by several case studies, ranging from cloud offloading to smart home and city, as well as collaborative edge to materialize the concept of edge computing. Finally, we present several challenges and opportunities in the field of edge computing, and hope this paper will gain attention from the community and inspire more research in this direction.\",\"html_url\":null,\"content_type\":\"Journals\",\"publication_title\":null,\"publication_year\":2016,\"jcr_value\":null,\"scimago_value\":null},\"48\":{\"doi\":\"10.1109\\\\/CVPR.2016.90\",\"title\":\"Deep Residual Learning for Image Recognition\",\"JournalTitle\":\"proceedings of the ieee computer society conference on computer vision and pattern recognition\",\"publisher\":\"IEEE\",\"authors\":\"Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun\",\"keywords\":\"Training, Degradation, Complexity theory, Image recognition, Neural networks, Visualization, Image segmentation\",\"issn\":\"10636919\",\"eissn\":null,\"isbn\":\"9781467388528\",\"abstract\":\"Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers - 8\\\\u00d7 deeper than VGG nets [40] but still having lower complexity. An ensemble of these residual nets achieves 3.57% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC & COCO 2015 competitions1, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.\",\"html_url\":null,\"content_type\":\"Conferences\",\"publication_title\":null,\"publication_year\":2016,\"jcr_value\":null,\"scimago_value\":\"4,658\"},\"49\":{\"doi\":\"10.1109\\\\/CVPR.2016.91\",\"title\":\"You Only Look Once: Unified, Real-Time Object Detection\",\"JournalTitle\":\"proceedings of the ieee computer society conference on computer vision and pattern recognition\",\"publisher\":\"IEEE\",\"authors\":\"Joseph Redmon, Santosh Divvala, Ross Girshick, Ali Farhadi\",\"keywords\":\"Computer architecture, Microprocessors, Object detection, Training, Real-time systems, Neural networks, Pipelines\",\"issn\":\"10636919\",\"eissn\":null,\"isbn\":\"9781467388528\",\"abstract\":\"We present YOLO, a new approach to object detection. Prior work on object detection repurposes classifiers to perform detection. Instead, we frame object detection as a regression problem to spatially separated bounding boxes and associated class probabilities. A single neural network predicts bounding boxes and class probabilities directly from full images in one evaluation. Since the whole detection pipeline is a single network, it can be optimized end-to-end directly on detection performance. Our unified architecture is extremely fast. Our base YOLO model processes images in real-time at 45 frames per second. A smaller version of the network, Fast YOLO, processes an astounding 155 frames per second while still achieving double the mAP of other real-time detectors. Compared to state-of-the-art detection systems, YOLO makes more localization errors but is less likely to predict false positives on background. Finally, YOLO learns very general representations of objects. It outperforms other detection methods, including DPM and R-CNN, when generalizing from natural images to other domains like artwork.\",\"html_url\":null,\"content_type\":\"Conferences\",\"publication_title\":null,\"publication_year\":2016,\"jcr_value\":null,\"scimago_value\":\"4,658\"},\"50\":{\"doi\":\"10.1109\\\\/CVPR.2016.308\",\"title\":\"Rethinking the Inception Architecture for Computer Vision\",\"JournalTitle\":\"proceedings of the ieee computer society conference on computer vision and pattern recognition\",\"publisher\":\"IEEE\",\"authors\":\"Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, Zbigniew Wojna\",\"keywords\":\"Convolution, Computer architecture, Training, Computational efficiency, Computer vision, Benchmark testing, Computational modeling\",\"issn\":\"10636919\",\"eissn\":null,\"isbn\":\"9781467388528\",\"abstract\":\"Convolutional networks are at the core of most state of-the-art computer vision solutions for a wide variety of tasks. Since 2014 very deep convolutional networks started to become mainstream, yielding substantial gains in various benchmarks. Although increased model size and computational cost tend to translate to immediate quality gains for most tasks (as long as enough labeled data is provided for training), computational efficiency and low parameter count are still enabling factors for various use cases such as mobile vision and big-data scenarios. Here we are exploring ways to scale up networks in ways that aim at utilizing the added computation as efficiently as possible by suitably factorized convolutions and aggressive regularization. We benchmark our methods on the ILSVRC 2012 classification challenge validation set demonstrate substantial gains over the state of the art: 21:2% top-1 and 5:6% top-5 error for single frame evaluation using a network with a computational cost of 5 billion multiply-adds per inference and with using less than 25 million parameters. With an ensemble of 4 models and multi-crop evaluation, we report 3:5% top-5 error and 17:3% top-1 error on the validation set and 3:6% top-5 error on the official test set.\",\"html_url\":null,\"content_type\":\"Conferences\",\"publication_title\":null,\"publication_year\":2016,\"jcr_value\":null,\"scimago_value\":\"4,658\"},\"51\":{\"doi\":\"10.1109\\\\/TPAMI.2015.2439281\",\"title\":\"Image Super-Resolution Using Deep Convolutional Networks\",\"JournalTitle\":null,\"publisher\":\"IEEE\",\"authors\":\"Chao Dong, Chen Change Loy, Kaiming He, Xiaoou Tang\",\"keywords\":\"Image resolution, Neural networks, Image reconstruction, Convolutional codes, Feature extraction, Training\",\"issn\":\"19393539\",\"eissn\":null,\"isbn\":\"\",\"abstract\":\"We propose a deep learning method for single image super-resolution (SR). Our method directly learns an end-to-end mapping between the low\\\\/high-resolution images. The mapping is represented as a deep convolutional neural network (CNN) that takes the low-resolution image as the input and outputs the high-resolution one. We further show that traditional sparse-coding-based SR methods can also be viewed as a deep convolutional network. But unlike traditional methods that handle each component separately, our method jointly optimizes all layers. Our deep CNN has a lightweight structure, yet demonstrates state-of-the-art restoration quality, and achieves fast speed for practical on-line usage. We explore different network structures and parameter settings to achieve trade-offs between performance and speed. Moreover, we extend our network to cope with three color channels simultaneously, and show better overall reconstruction quality.\",\"html_url\":null,\"content_type\":\"Journals\",\"publication_title\":null,\"publication_year\":2016,\"jcr_value\":null,\"scimago_value\":null},\"52\":{\"doi\":\"10.1109\\\\/CVPR.2016.350\",\"title\":\"The Cityscapes Dataset for Semantic Urban Scene Understanding\",\"JournalTitle\":\"proceedings of the ieee computer society conference on computer vision and pattern recognition\",\"publisher\":\"IEEE\",\"authors\":\"Marius Cordts, Mohamed Omran, Sebastian Ramos, Timo Rehfeld, Markus Enzweiler, Rodrigo Benenson, Uwe Franke, Stefan Roth, Bernt Schiele\",\"keywords\":\"Urban areas, Semantics, Visualization, Benchmark testing, Vehicles, Training, Complexity theory\",\"issn\":\"10636919\",\"eissn\":null,\"isbn\":\"9781467388528\",\"abstract\":\"Visual understanding of complex urban street scenes is an enabling factor for a wide range of applications. Object detection has benefited enormously from large-scale datasets, especially in the context of deep learning. For semantic urban scene understanding, however, no current dataset adequately captures the complexity of real-world urban scenes. To address this, we introduce Cityscapes, a benchmark suite and large-scale dataset to train and test approaches for pixel-level and instance-level semantic labeling. Cityscapes is comprised of a large, diverse set of stereo video sequences recorded in streets from 50 different cities. 5000 of these images have high quality pixel-level annotations, 20 000 additional images have coarse annotations to enable methods that leverage large volumes of weakly-labeled data. Crucially, our effort exceeds previous attempts in terms of dataset size, annotation richness, scene variability, and complexity. Our accompanying empirical study provides an in-depth analysis of the dataset characteristics, as well as a performance evaluation of several state-of-the-art approaches based on our benchmark.\",\"html_url\":null,\"content_type\":\"Conferences\",\"publication_title\":null,\"publication_year\":2016,\"jcr_value\":null,\"scimago_value\":\"4,658\"},\"53\":{\"doi\":\"10.1109\\\\/JIOT.2016.2579198\",\"title\":\"Edge Computing: Vision and Challenges\",\"JournalTitle\":null,\"publisher\":\"IEEE\",\"authors\":\"Weisong Shi, Jie Cao, Quan Zhang, Youhuizi Li, Lanyu Xu\",\"keywords\":\"Cloud computing, Internet of things, Bandwidth, Time factors, Mobile handsets, Data privacy, Smart homes\",\"issn\":\"23722541\",\"eissn\":null,\"isbn\":\"\",\"abstract\":\"The proliferation of Internet of Things (IoT) and the success of rich cloud services have pushed the horizon of a new computing paradigm, edge computing, which calls for processing the data at the edge of the network. Edge computing has the potential to address the concerns of response time requirement, battery life constraint, bandwidth cost saving, as well as data safety and privacy. In this paper, we introduce the definition of edge computing, followed by several case studies, ranging from cloud offloading to smart home and city, as well as collaborative edge to materialize the concept of edge computing. Finally, we present several challenges and opportunities in the field of edge computing, and hope this paper will gain attention from the community and inspire more research in this direction.\",\"html_url\":null,\"content_type\":\"Journals\",\"publication_title\":null,\"publication_year\":2016,\"jcr_value\":null,\"scimago_value\":null}}'"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#leitura da tabela para validar se os dados foram escritos corretamente\n",
    "field = 'publication_year'\n",
    "value = '2016'\n",
    "\n",
    "query = \"SELECT * FROM tb_Publications where \" + field + '=' + value + \";\"\n",
    "df_table = pd.read_sql_query(query, engine)\n",
    "df_table.to_json(orient=\"index\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7 (tags/v3.10.7:6cc6b13, Sep  5 2022, 14:08:36) [MSC v.1933 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "253cd15c32ffb283b0558be93b097dc27093fe6b891853f454fa08d2f15e4a97"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
