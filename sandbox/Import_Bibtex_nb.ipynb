{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# IMPORTANDO ARQUIVOS BIBTEX e EXPORTANDO EM FORMATAÇÃO YAML\n",
    "- Arquivo texto em formato BIBTEX\n",
    "- Objetivo: Data quality de arquivos BIBtex\n",
    "- Extensão: .bib\n",
    "- Biblioteca: pybtex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pybtex.database.input import bibtex\n",
    "from pybtex.database import BibliographyData, Entry\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# #################################################################\n",
    "## Inicia importanção do arquivo\n",
    "# #################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [],
   "source": [
    "def arquivos(path, fendwith = ''):\n",
    "    if fendwith not in '':\n",
    "        caminhos = {nome : os.path.join(path, nome) for nome in os.listdir(path) if nome.endswith(fendwith)}\n",
    "    else:\n",
    "        caminhos = {nome : os.path.join(path, nome) for nome in os.listdir(path)}\n",
    "    return caminhos\n",
    "\n",
    "def author_names(author):\n",
    "    try:\n",
    "        return author.persons['author'][0:]\n",
    "    except:\n",
    "        return {'author': [(u'none, none')]}\n",
    "\n",
    "def join_names(person):\n",
    "    try:\n",
    "        return person.last_names[0] + ', ' + person.first_names[0]\n",
    "    except:\n",
    "        return []\n",
    "\n",
    "def type_output_file(f_type, filename, path):\n",
    "    \n",
    "    import yaml\n",
    "    import json\n",
    "    import csv\n",
    "\n",
    "    path_complete = path + filename + '.' + f_type\n",
    "\n",
    "    if f_type == 'yaml':\n",
    "        with open(path_complete, 'w') as nfile:\n",
    "            for data in dict.items():\n",
    "                yaml.dump(data, nfile)  # insere os dados na configuração YAML\n",
    "\n",
    "    if f_type == 'json':\n",
    "        with open(path_complete, 'w') as nfile:\n",
    "            for data in dict.items():\n",
    "                json.dump(data, nfile)  # insere os dados na configuração YAML\n",
    "\n",
    "    if f_type == 'csv':\n",
    "        with open(path_complete, 'w', -1, \"utf-8\") as nfile:\n",
    "        #head\n",
    "            nfile.write('{0}\\n'.format('§ '.join(str(x) for x in list_file_fields)))\n",
    "            #row\n",
    "            for row in dict.keys():\n",
    "                # print(dict[row])\n",
    "                nfile.write('{0}\\n'.format('§ '.join(str(x) for x in dict[row].values())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inicia tratamento dos dados BIBTEX\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ref bibtex doc: http://paginapessoal.utfpr.edu.br/jamhour/publicacoes/arquivos/00_Compilado_JabRef_dez2015.pdf\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_path = \"C:\\\\Users\\\\victo\\\\PycharmProjects\\\\BibtexDataQuality\\\\source\\\\\"\n",
    "\n",
    "lst_files = arquivos(source_path, '.bib')\n",
    "parser = bibtex.Parser()\n",
    "# file = parser.parse_file(lst_files['ACM.bib'])\n",
    "dict_file_fields = {}\n",
    "list_file_fields = []\n",
    "\n",
    "#verifica a estrutura de campos para cada tipo de publicação\n",
    "#cria um dicionario usando como chave a fonte e o tipo de publicação\n",
    "for f in lst_files:\n",
    "    parser = bibtex.Parser()\n",
    "    file = parser.parse_file(lst_files[f])\n",
    "    f_name = f.replace('.bib','')\n",
    "    for i in file.entries.values():\n",
    "        \n",
    "        for a in sorted(list(i.fields.keys())):\n",
    "           \n",
    "            if a not in list_file_fields:\n",
    "                list_file_fields.append(a)\n",
    "list_file_fields.remove('ISSN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst_files = arquivos(source_path)\n",
    "parser = bibtex.Parser()\n",
    "# file = parser.parse_file(lst_files['IEEE.bib'])\n",
    "dict = {}\n",
    "\n",
    "for f in lst_files:\n",
    "    \n",
    "    ############# IEEE\n",
    "    if 'IEEE' in f :\n",
    "        parser = bibtex.Parser()\n",
    "        file = parser.parse_file(lst_files[f])\n",
    "\n",
    "        for i in file.entries.values():\n",
    "            key = i.key\n",
    "            dict[key] = {i.fields.get('fields', fields) : i.fields.get(fields, '')\\\n",
    "                        for fields in list_file_fields}\n",
    "            \n",
    "            dict[key]['type_publication'] = i.type\n",
    "            dict[key]['author'] = [join_names(person) for person in i.persons.get('author', '')]\n",
    "\n",
    "            dict[key].update({'keywords' : dict[key]['keywords'].split(';')})\n",
    "    \n",
    "    ############## ACM\n",
    "    if 'acm' in f:\n",
    "        parser = bibtex.Parser()\n",
    "        file = parser.parse_file(lst_files[f])\n",
    "\n",
    "        for i in file.entries.values():\n",
    "            key = i.key\n",
    "            dict[key] = {i.fields.get('fields', fields) : i.fields.get(fields, '')\\\n",
    "                        for fields in list_file_fields}\n",
    "            \n",
    "            dict[key]['type_publication'] = i.type\n",
    "            dict[key]['author'] = [join_names(person) for person in i.persons.get('author', '')]\n",
    "\n",
    "            dict[key].update({'keywords' : dict[key].get('keywords', '').split(';')})\n",
    "\n",
    "            if i.type == 'inbook':\n",
    "                dict[key].update({'doi' : dict[key].get('url', '').replace('https://doi.org/', '')})\n",
    "\n",
    "    ############## Science Direct\n",
    "    if 'ScienceDirect' in f:\n",
    "        parser = bibtex.Parser()\n",
    "        file = parser.parse_file(lst_files[f])\n",
    "\n",
    "        for i in file.entries.values():\n",
    "            key = i.key\n",
    "            dict[key] = {i.fields.get('fields', fields) : i.fields.get(fields, '')\\\n",
    "                        for fields in list_file_fields}\n",
    "            \n",
    "            dict[key]['type_publication'] = i.type\n",
    "            dict[key]['author'] = [join_names(person) for person in i.persons.get('author', '')]\n",
    "            dict[key].update({'keywords' : dict[key]['keywords'].split(';')})\n",
    "            dict[key].update({'doi' : i.fields['doi'].replace('https://doi.org/', '')})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'abstract': 'Cyber-physical systems (CPS) have been developed in many industrial sectors and application domains in which the quality requirements of data acquired are a common factor. Data quality in CPS can deteriorate because of several factors such as sensor faults and failures due to operating in harsh and uncertain environments. How can software engineering and artificial intelligence (AI) help manage and tame data quality issues in CPS? This is the question we aimed to investigate in the SEA4DQ workshop. Emerging trends in software engineering need to take data quality management seriously as CPS are increasingly datacentric in their approach to acquiring and processing data along the edge-fog-cloud continuum. This workshop provided researchers and practitioners a forum for exchanging ideas, experiences, understanding of the problems, visions for the future, and promising solutions to the problems in data quality in CPS. Examples of topics include software/hardware architectures and frameworks for data quality management in CPS; software engineering and AI to detect anomalies in CPS data or to repair erroneous CPS data. SEA4DQ 2021, which took place on August 24th, 2021 was a satellite event of the ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering (ESEC / FSE) 2021. The workshop attracted 35 international participants and was exciting with a great keynote, six excellent presentations, and concluded on a high note with a panel discussion. SEA4DQ was motivated by the common research interests from the EU projects for Zero-Defects Manufacturing such as InterQ and Dat4.Zero.', 'address': 'New York, NY, USA', 'doi': '10.1145/3502771.3502781', 'issn': '0163-5948', 'issue_date': 'January 2022', 'journal': 'SIGSOFT Softw. Eng. Notes', 'month': 'jan', 'number': '1', 'numpages': '4', 'pages': '26–29', 'publisher': 'Association for Computing Machinery', 'title': \"Software Engineering and AI for Data Quality in Cyber- Physical Systems - SEA4DQ'21 Workshop Report\", 'url': 'https://doi.org/10.1145/3502771.3502781', 'volume': '47', 'year': '2022', 'booktitle': '', 'isbn': '', 'keywords': [''], 'location': '', 'series': '', 'articleno': '', 'note': '', 'edition': '', 'type_publication': 'article', 'author': ['Nguyen, Phu', 'Sen, Sagar', 'Jourdan, Nicolas', 'Cassoli, Beatriz', 'Myrseth, Per', 'Armendia, Mikel', 'Myklebust, Odd']}\n"
     ]
    }
   ],
   "source": [
    "print(dict['10.1145/3502771.3502781'])\n",
    "# print({key : dict['10.1145/3411764.3445130'][key] for key in ['author', 'title', 'keywords', 'year', 'type_publication', 'doi']})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "output_path = \"C:\\\\Users\\\\victo\\\\PycharmProjects\\\\BibtexDataQuality\\\\output\\\\\"\n",
    "\n",
    "type_output_file('csv', 'output', output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inicia tratamento dos dados CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(lst)\n",
    "\n",
    "# #################################################################\n",
    "# #Verificar se os dados estão corretos em cada coluna\n",
    "# #################################################################\n",
    "\n",
    "# lst_field = ['chave', 'author', 'title', 'keywords', 'abstract', 'year', 'type_publication', 'doi']\n",
    "# for f in lst_field:\n",
    "#     for q in lst[0:]:\n",
    "#         print(q[f])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "253cd15c32ffb283b0558be93b097dc27093fe6b891853f454fa08d2f15e4a97"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
