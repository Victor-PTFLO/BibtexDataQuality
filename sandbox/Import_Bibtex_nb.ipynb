{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# DATA QUALITY (Bibtex e csv)\n",
    "- Arquivo texto em formato BIBTEX\n",
    "- Objetivo: Data quality de input diversos(csv, bibtex e APIs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import hashlib\n",
    "import yaml\n",
    "import json\n",
    "import csv\n",
    "from numpy import nan\n",
    "from pybtex.database.input import bibtex\n",
    "from pybtex.database import BibliographyData, Entry\n",
    "from unicodedata import normalize\n",
    "\n",
    "pd.set_option(\"display.max_columns\", 50)\n",
    "pd.set_option(\"display.max_rows\", 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Criação de funções"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################## LISTAGEM DE ARQUIVOS\n",
    "def arquivos(path, fendwith = ''):\n",
    "    if fendwith not in '':\n",
    "        caminhos = {nome : os.path.join(path, nome) for nome in os.listdir(path) if nome.endswith(fendwith)}\n",
    "    else:\n",
    "        caminhos = {nome : os.path.join(path, nome) for nome in os.listdir(path)}\n",
    "    return caminhos\n",
    "\n",
    "######################## LISTAGEM DE AUTORES\n",
    "def author_names(author):\n",
    "    try:\n",
    "        return author.persons['author'][0:]\n",
    "    except:\n",
    "        return {'author': [(u'none, none')]}\n",
    "\n",
    "######################## CONCATENA AUTORES\n",
    "def join_names(person):\n",
    "    try:\n",
    "        return person.last_names[0] + ', ' + person.first_names[0]\n",
    "    except:\n",
    "        return []\n",
    "\n",
    "######################## OUTPUT POR TIPO DE EXTENSAO(JSON, YAML E CSV)\n",
    "def type_output_file(o_data, f_type, filename, path):\n",
    "\n",
    "    path_complete = path + filename + '.' + f_type\n",
    "\n",
    "    if f_type == 'yaml':\n",
    "        with open(path_complete, 'w') as nfile:\n",
    "            yaml.dump(o_data.to_dict(orient='records'), nfile)\n",
    "\n",
    "    if f_type == 'json':\n",
    "        with open(path_complete, 'w') as nfile:\n",
    "            result = o_data.to_json(orient=\"records\")\n",
    "            parsed = json.loads(result)\n",
    "            json.dump(parsed, nfile, indent=4)  \n",
    "\n",
    "    if f_type == 'csv':\n",
    "        o_data.to_csv(path_complete, sep='§')\n",
    "\n",
    "######################## CARGA DO ARQUIVO DE CONFIGURACAO\n",
    "def load_config(n_file):\n",
    "    with open(n_file, 'r') as config_file:\n",
    "        return yaml.load(config_file, Loader=yaml.SafeLoader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inicia importanção dos arquivos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BIBTEX\n",
    "- ref bibtex doc: http://paginapessoal.utfpr.edu.br/jamhour/publicacoes/arquivos/00_Compilado_JabRef_dez2015.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_path = \"C:\\\\Users\\\\victo\\\\PycharmProjects\\\\BibtexDataQuality\\\\source\\\\\"\n",
    "\n",
    "lst_files = arquivos(source_path, '.bib')\n",
    "parser = bibtex.Parser()\n",
    "# file = parser.parse_file(lst_files['ACM.bib'])\n",
    "dict_file_fields = {}\n",
    "list_file_fields = []\n",
    "\n",
    "#verifica a estrutura de campos para cada tipo de publicação\n",
    "#cria um dicionario usando como chave a fonte e o tipo de publicação\n",
    "for f in lst_files:\n",
    "    parser = bibtex.Parser()\n",
    "    file = parser.parse_file(lst_files[f])\n",
    "    f_name = f.replace('.bib','')\n",
    "    for i in file.entries.values():\n",
    "        \n",
    "        for a in sorted(i.fields.keys()):\n",
    "           \n",
    "            if a not in list_file_fields:\n",
    "                list_file_fields.append(a)\n",
    "list_file_fields.remove('ISSN')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### importanção e tratamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst_files = arquivos(source_path)\n",
    "parser = bibtex.Parser()\n",
    "# file = parser.parse_file(lst_files['IEEE.bib'])\n",
    "dict_bib = {}\n",
    "\n",
    "for f in lst_files:\n",
    "    \n",
    "    ############# IEEE\n",
    "    if 'IEEE' in f :\n",
    "        parser = bibtex.Parser()\n",
    "        file = parser.parse_file(lst_files[f])\n",
    "\n",
    "        for i in file.entries.values():\n",
    "            key = i.key\n",
    "            dict_bib[key] = {i.fields.get('fields', fields) : i.fields.get(fields, '')\\\n",
    "                            for fields in list_file_fields}\n",
    "            \n",
    "            dict_bib[key]['type_publication'] = i.type\n",
    "            dict_bib[key]['author'] = [join_names(person) for person in i.persons.get('author', '')]\n",
    "\n",
    "            dict_bib[key].update({'keywords' : dict_bib[key]['keywords'].split(';')})\n",
    "    \n",
    "    ############## ACM\n",
    "    if 'acm' in f:\n",
    "        parser = bibtex.Parser()\n",
    "        file = parser.parse_file(lst_files[f])\n",
    "\n",
    "        for i in file.entries.values():\n",
    "            key = i.key\n",
    "            dict_bib[key] = {i.fields.get('fields', fields) : i.fields.get(fields, '')\\\n",
    "                            for fields in list_file_fields}\n",
    "            \n",
    "            dict_bib[key]['type_publication'] = i.type\n",
    "            dict_bib[key]['author'] = [join_names(person) for person in i.persons.get('author', '')]\n",
    "\n",
    "            dict_bib[key].update({'keywords' : dict_bib[key].get('keywords', '').split(';')})\n",
    "\n",
    "            if i.type == 'inbook':\n",
    "                dict_bib[key].update({'doi' : dict_bib[key].get('url', '').replace('https://doi.org/', '')})\n",
    "\n",
    "    ############## Science Direct\n",
    "    if 'ScienceDirect' in f:\n",
    "        parser = bibtex.Parser()\n",
    "        file = parser.parse_file(lst_files[f])\n",
    "\n",
    "        for i in file.entries.values():\n",
    "            key = i.key\n",
    "            dict_bib[key] = {i.fields.get('fields', fields) : i.fields.get(fields, '')\\\n",
    "                        for fields in list_file_fields}\n",
    "            \n",
    "            dict_bib[key]['type_publication'] = i.type\n",
    "            dict_bib[key]['author'] = [join_names(person) for person in i.persons.get('author', '')]\n",
    "            dict_bib[key].update({'keywords' : dict_bib[key]['keywords'].split(';')})\n",
    "            \n",
    "            dict_bib[key].update({'doi' : i.fields['doi'].replace('https://doi.org/', '')})\n",
    "\n",
    "df_bib = pd.DataFrame(data=dict_bib.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'abstract': 'Cyber-physical systems (CPS) have been developed in many industrial sectors and application domains in which the quality requirements of data acquired are a common factor. Data quality in CPS can deteriorate because of several factors such as sensor faults and failures due to operating in harsh and uncertain environments. How can software engineering and artificial intelligence (AI) help manage and tame data quality issues in CPS? This is the question we aimed to investigate in the SEA4DQ workshop. Emerging trends in software engineering need to take data quality management seriously as CPS are increasingly datacentric in their approach to acquiring and processing data along the edge-fog-cloud continuum. This workshop provided researchers and practitioners a forum for exchanging ideas, experiences, understanding of the problems, visions for the future, and promising solutions to the problems in data quality in CPS. Examples of topics include software/hardware architectures and frameworks for data quality management in CPS; software engineering and AI to detect anomalies in CPS data or to repair erroneous CPS data. SEA4DQ 2021, which took place on August 24th, 2021 was a satellite event of the ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering (ESEC / FSE) 2021. The workshop attracted 35 international participants and was exciting with a great keynote, six excellent presentations, and concluded on a high note with a panel discussion. SEA4DQ was motivated by the common research interests from the EU projects for Zero-Defects Manufacturing such as InterQ and Dat4.Zero.', 'address': 'New York, NY, USA', 'doi': '10.1145/3502771.3502781', 'issn': '0163-5948', 'issue_date': 'January 2022', 'journal': 'SIGSOFT Softw. Eng. Notes', 'month': 'jan', 'number': '1', 'numpages': '4', 'pages': '26–29', 'publisher': 'Association for Computing Machinery', 'title': \"Software Engineering and AI for Data Quality in Cyber- Physical Systems - SEA4DQ'21 Workshop Report\", 'url': 'https://doi.org/10.1145/3502771.3502781', 'volume': '47', 'year': '2022', 'booktitle': '', 'isbn': '', 'keywords': [''], 'location': '', 'series': '', 'articleno': '', 'note': '', 'edition': '', 'type_publication': 'article', 'author': ['Nguyen, Phu', 'Sen, Sagar', 'Jourdan, Nicolas', 'Cassoli, Beatriz', 'Myrseth, Per', 'Armendia, Mikel', 'Myklebust, Odd']}\n"
     ]
    }
   ],
   "source": [
    "print(dict_bib['10.1145/3502771.3502781'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Output BIBTEX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = load_config('config.yaml')        \n",
    "\n",
    "output_path = config['output_path'][0]\n",
    "f_name = config['file_name'][0]\n",
    "file_ext = config['output_ext'][0]\n",
    "\n",
    "type_output_file(df_bib, file_ext, f_name, output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CSV SCIMAGO | JSR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Criando Funções"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_excel = \"C:\\\\Users\\\\victo\\\\PycharmProjects\\\\BibtexDataQuality\\\\source\\\\excel\\\\\"\n",
    "lst_excel = arquivos(source_excel)\n",
    "\n",
    "######################## REMOVE CARACTERES ESPECIAIS\n",
    "def rmscaract(text):\n",
    "    result = [re.sub(r\"[^a-zA-Z0-9]\",\"\", normalize('NFKD', words).encode('ASCII','ignore').decode('ASCII').lower()) for words in text.fillna('')]\n",
    "    return result\n",
    "\n",
    "######################## CRIA UM HASH A PARTIR DE UMA STRING\n",
    "def stringhash(instr):\n",
    "    hashnum = [hashlib.md5(ikeys.encode()).hexdigest() for ikeys in instr]\n",
    "    return hashnum\n",
    "\n",
    "######################## CRIA HASH JÁ COM STRING TRATADA\n",
    "def hashkey(strkeys):\n",
    "    stringadjs = rmscaract(strkeys)\n",
    "    hashresult = [hashlib.md5(ikeys.encode()).hexdigest() for ikeys in stringadjs]\n",
    "    return hashresult\n",
    "\n",
    "######################## FUNÇÃO PARA FILTRAGEM\n",
    "def filter_exp(df):\n",
    "    a = config['filter_field'][0] + config['search_operator'][0] + \"'\" + config['search_value'][0] + \"'\"\n",
    "    return df.query(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### importanção e tratamento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### JCS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12312"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_jcs = pd.read_csv(lst_excel['jcs_2020.csv'], delimiter=';').drop_duplicates(subset='Full Journal Title')\n",
    "cols = ['Rank', 'Full Journal Title','Total Cites', 'Journal Impact Factor', 'Eigenfactor Score']\n",
    "\n",
    "df_jcs = df_jcs[cols]\n",
    "df_jcs['hashid'] =  hashkey(df_jcs['Full Journal Title'])\n",
    "df_jcs = df_jcs.rename(columns={'Journal Impact Factor' : 'jcr_value'})\n",
    "\n",
    "df_jcs['hashid'].count()\n",
    "# dfjcs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### SCIMAGO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\victo\\AppData\\Local\\Temp\\ipykernel_9608\\4210968985.py:1: DtypeWarning: Columns (5) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_scimago = pd.read_csv(lst_excel['scimagojr 2020.csv'], delimiter=';').drop_duplicates()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "32952"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_scimago = pd.read_csv(lst_excel['scimagojr 2020.csv'], delimiter=';').drop_duplicates()\n",
    "\n",
    "df_scimago['hashid'] =  hashkey(df_scimago['Title'])\n",
    "df_scimago['issnkey'] =  df_scimago.Issn.str.slice(stop=8)\n",
    "df_scimago = df_scimago.rename(columns={'SJR' : 'scimago_value'})\n",
    "\n",
    "df_scimago['hashid'].count()\n",
    "# dfscimago"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### JUNÇÃO JCS | SCIMAGO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full Journal Title    12325\n",
      "Title                 34727\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "dfjournalrank = df_scimago.merge(df_jcs, left_on=['hashid'], right_on=['hashid'],how='outer')\n",
    "\n",
    "dfjournalrank['Title'] = (dfjournalrank.Title.combine_first(dfjournalrank['Full Journal Title'])).str.lower()\n",
    "print(dfjournalrank[['Full Journal Title','Title']].count())\n",
    "\n",
    "dfjournalrank = dfjournalrank.drop(columns=['Rank_x', 'Rank_y', 'Full Journal Title'])\n",
    "# dfjournalrank"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## JOIN BIBTEX | CSV SCIMAGO | CSV JCS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bib = pd.DataFrame(data=dict_bib.values())\n",
    "df_bib = df_bib.replace(r'^\\s*$', nan, regex=True)\n",
    "\n",
    "df_bib['issnkey'] = rmscaract(df_bib['issn'])\n",
    "df_bib['title'] = rmscaract(df_bib['title'])\n",
    "df_bib['hashid'] = hashkey(df_bib['title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['issnkey', 'isbn', 'journal', 'publisher', 'title', 'booktitle', 'doi', 'author', 'keywords', 'abstract', 'year', 'type_publication', 'jcr_value', 'scimago_value']\n",
    "\n",
    "df_join_bibtex_csv = df_bib.merge(dfjournalrank, left_on = 'issnkey', right_on = 'issnkey', how = 'left' )\n",
    "df_join_bibtex_csv = df_join_bibtex_csv[cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_exp(df):\n",
    "    a = config['filter_field'][0] + config['search_operator'][0] + \"'\" + config['search_value'][0] + \"'\"\n",
    "    return df.query(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = load_config('config.yaml')        \n",
    "\n",
    "output_path = config['output_path'][0]\n",
    "f_name = config['file_name'][0]\n",
    "file_ext = config['output_ext'][0]\n",
    "\n",
    "df_filter = filter_exp(df_join_bibtex_csv)\n",
    "\n",
    "type_output_file(df_filter, file_ext, f_name, output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n",
      "<class 'dict'>\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "accesskey_IEEE = 'efugu53u622asc77hu7h6hbq'\n",
    "accesskey_SD = 'ef8a7260c27897693d0fd7394a559726'\n",
    "\n",
    "# def buscar_dados():\n",
    "request = requests.get(\"https://ieeexploreapi.ieee.org/api/v1/search/articles?parameter&apikey=\" + accesskey_IEEE)\n",
    "dict_IEEE = json.loads(request.content)\n",
    "print(type(dict_IEEE))\n",
    "\n",
    "request = requests.get(\"http://api.elsevier.com/content/search/scopus?query=heart&apiKey=\" + accesskey_SD)\n",
    "dict_SD = json.loads(request.content)\n",
    "print(type(dict_SD))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "253cd15c32ffb283b0558be93b097dc27093fe6b891853f454fa08d2f15e4a97"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
