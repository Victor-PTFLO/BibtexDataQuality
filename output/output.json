["3341629", {"address": "New York, NY, USA", "booktitle": "Proceedings of the 2019 International Conference on Big Data Engineering", "doi": "10.1145/3341620.3341629", "isbn": "9781450360913", "keywords": ["Sentiment analysis, Big data, Big data quality metrics"], "location": "Hong Kong, Hong Kong", "numpages": "8", "pages": "36\u201343", "publisher": "Association for Computing Machinery", "series": "BDE 2019", "title": "Big Data Quality Metrics for Sentiment Analysis Approaches", "url": "https://doi.org/10.1145/3341620.3341629", "year": "2019", "abstract": "In a world increasingly connected, and in which information flows quickly and affects a very large number of people, sentiment analysis has seen a spectacular development over the past ten years. This is due to the fact that the explosion of social networks has allowed anyone with internet access to publicly express his opinion. Moreover, the emergence of big data has brought enormous opportunities and powerful storage and analytics tools to the field of sentiment analysis. However, big data introduces new variables and constraints that could radically affect the traditional models of sentiment analysis. Therefore, new concerns, such as big data quality, have to be addressed to get the most out of big data. To the best of our knowledge, no contributions have been published so far which address big data quality in SA throughout its different processes. In this paper, we first highlight the most important big data quality metrics to consider in any big data project. Then, we show how these metrics could be specifically considered in SA approaches and this for each phase in the big data value chain.", "articleno": "", "ISSN": "", "month": "", "number": "", "volume": "", "publicationtype": "inproceedings", "author": ["El, Imane", "Gahi, Youssef", "Messoussi, Rochdi"]}]["3419803", {"address": "New York, NY, USA", "booktitle": "Proceedings of the 13th International Conference on Intelligent Systems: Theories and Applications", "doi": "10.1145/3419604.3419803", "isbn": "9781450377331", "keywords": ["Data Quality, Quality Models, Data Quality evaluation, Big Data"], "location": "Rabat, Morocco", "numpages": "6", "pages": "", "publisher": "Association for Computing Machinery", "series": "SITA'20", "title": "Towards a Data Quality Assessment in Big Data", "url": "https://doi.org/10.1145/3419604.3419803", "year": "2020", "abstract": "In recent years, as more and more data sources have become available and the volumes of data potentially accessible have increased, the assessment of data quality has taken a central role whether at the academic, professional or any other sector. Given that users are often concerned with the need to filter a large amount of data to better satisfy their requirements and needs, and that data analysis can be based on inaccurate, incomplete, ambiguous, duplicated and of poor quality, it makes everyone wonder what the results of these analyses will really be like. However, there is a very complex process involved in the identification of new, valid, potentially useful and meaningful data from a large data collection and various information systems, and is critically dependent on a number of measures to be developed to ensure data quality. To this end, the main objective of this paper is to introduce a general study on data quality related with big data, by providing what other researchers came up with on that subject. The paper will be finalized by a comparative study between the different existing data quality models.", "articleno": "16", "ISSN": "", "month": "", "number": "", "volume": "", "publicationtype": "inproceedings", "author": ["Reda, Oumaima", "Sassi, Imad", "Zellou, Ahmed", "Anter, Samir"]}]["3010090", {"address": "New York, NY, USA", "booktitle": "Proceedings of the International Conference on Big Data and Advanced Wireless Technologies", "doi": "10.1145/3010089.3010090", "isbn": "9781450347792", "keywords": ["Data Quality, Data Quality Dimensions, Big Data characteristics, Big Data"], "location": "Blagoevgrad, Bulgaria", "numpages": "6", "pages": "", "publisher": "Association for Computing Machinery", "series": "BDAW '16", "title": "Defining Big Data", "url": "https://doi.org/10.1145/3010089.3010090", "year": "2016", "abstract": "As Big Data becomes better understood, there is a need for a comprehensive definition of Big Data to support work in fields such as data quality for Big Data. Existing definitions of Big Data define Big Data by comparison with existing, usually relational, definitions, or define Big Data in terms of data characteristics or use an approach which combines data characteristics with the Big Data environment. In this paper we examine existing definitions of Big Data and discuss the strengths and limitations of the different approaches, with particular reference to issues related to data quality in Big Data. We identify the issues presented by incomplete or inconsistent definitions. We propose an alternative definition and relate this definition to our work on quality in Big Data.", "articleno": "5", "ISSN": "", "month": "", "number": "", "volume": "", "publicationtype": "inproceedings", "author": ["Emmanuel, Isitor", "Stanier, Clare"]}]["3281026", {"address": "New York, NY, USA", "booktitle": "Proceedings of the 1st ACM SIGSOFT International Workshop on Ensemble-Based Software Engineering", "doi": "10.1145/3281022.3281026", "isbn": "9781450360548", "keywords": ["Big Data, Data Quality, Smart Data"], "location": "Lake Buena Vista, FL, USA", "numpages": "6", "pages": "19\u201324", "publisher": "Association for Computing Machinery", "series": "EnSEmble 2018", "title": "From Big Data to Smart Data: A Data Quality Perspective", "url": "https://doi.org/10.1145/3281022.3281026", "year": "2018", "abstract": "Big Data (BD) solutions are designed to better support decision-making processes in order to optimize organizational performance. These BD solutions use company\u2019s core business data, using typically large datasets. However, data that doesn\u2019t meet adequate quality levels will lead to BD solutions that will not produce useful results, and consequently may not be used to make adequate business decisions. For a long time, companies have collected and stored large amounts of data without being able to exploit the advantage of exploring it. Nowadays, and thanks to the Big Data explosion, organizations have begun to recognize the need for estimating the value of their data and, vice-versa, managing data accordingly to their value. This need of managing the Value of data has led to the concept of Smart Data. It not only involves the datasets, but also the set of technologies, tools, processes and methodologies that enable all the Values from the data to the End-users (Business, data scientist, BI\u2026). Consequently, Smart data is data actionable. We discovered that data quality is one of the most important issues when it comes to \u201csmartizing\u201d data. In this paper, we introduce a methodology to make data smarter, taking as a reference point, the quality level of the data itself.", "articleno": "", "ISSN": "", "month": "", "number": "", "volume": "", "publicationtype": "inproceedings", "author": ["Baldassarre, Maria", "Caballero, Ismael", "Caivano, Danilo", "Rivas, Bibiano", "Piattini, Mario"]}]["3141139", {"address": "New York, NY, USA", "booktitle": "Proceedings of the 2017 International Conference on Cloud and Big Data Computing", "doi": "10.1145/3141128.3141139", "isbn": "9781450353434", "keywords": ["databases, NoSQL, MapReduce, NewSQL, data warehouse, big data"], "location": "London, United Kingdom", "numpages": "5", "pages": "6\u201310", "publisher": "Association for Computing Machinery", "series": "ICCBDC 2017", "title": "Big Data and New Data Warehousing Approaches", "url": "https://doi.org/10.1145/3141128.3141139", "year": "2017", "abstract": "Big data are a data trend present around us mainly through Internet -- social networks and smart devices and meters -- mostly without us being aware of them. Also they are a fact that both industry and scientific research needs to deal with. They are interesting from analytical point of view, for they contain knowledge that cannot be ignored and left unused. Traditional system that supports the advanced analytics and knowledge extraction -- data warehouse -- is not able to cope with large amounts of fast incoming various and unstructured data, and may be facing a paradigm shift in terms of utilized concepts, technologies and methodologies, which have become a very active research area in the last few years. This paper provides an overview of research trends important for the big data warehousing, concepts and technologies used for data storage and (ETL) processing, and research approaches done in attempts to empower traditional data warehouses for handling big data.", "articleno": "", "ISSN": "", "month": "", "number": "", "volume": "", "publicationtype": "inproceedings", "author": ["Pti\\v{c}ek, Marina", "Vrdoljak, Boris"]}]["2527071", {"address": "New York, NY, USA", "booktitle": "Proceedings of the 17th International Database Engineering &amp; Applications Symposium", "doi": "10.1145/2513591.2527071", "isbn": "9781450320252", "keywords": ["big data posting, OLAP over big data, privacy of big data, big data"], "location": "Barcelona, Spain", "numpages": "6", "pages": "198\u2013203", "publisher": "Association for Computing Machinery", "series": "IDEAS '13", "title": "Big Data: A Research Agenda", "url": "https://doi.org/10.1145/2513591.2527071", "year": "2013", "abstract": "Recently, a great deal of interest for Big Data has risen, mainly driven from a widespread number of research problems strongly related to real-life applications and systems, such as representing, modeling, processing, querying and mining massive, distributed, large-scale repositories (mostly being of unstructured nature). Inspired by this main trend, in this paper we discuss three important aspects of Big Data research, namely OLAP over Big Data, Big Data Posting, and Privacy of Big Data. We also depict future research directions, hence implicitly defining a research agenda aiming at leading future challenges in this research field.", "articleno": "", "ISSN": "", "month": "", "number": "", "volume": "", "publicationtype": "inproceedings", "author": ["Cuzzocrea, Alfredo", "Sacc\\`{a}, Domenico", "Ullman, Jeffrey"]}]["2658845", {"address": "New York, NY, USA", "booktitle": "Proceedings of the First International Workshop on Bringing the Value of \"Big Data\" to Users (Data4U 2014)", "doi": "10.1145/2658840.2658845", "isbn": "9781450331869", "keywords": ["Big data, data integration, diverse data sources"], "location": "Hangzhou, China", "numpages": "4", "pages": "25\u201328", "publisher": "Association for Computing Machinery", "series": "Data4U '14", "title": "Taming Big Data: Integrating Diverse Public Data Sources for Economic Competitiveness Analytics", "url": "https://doi.org/10.1145/2658840.2658845", "year": "2014", "abstract": "In an era where Big Data can greatly impact a broad population, many novel opportunities arise, chief among them the ability to integrate data from diverse sources and \"wrangle\" it to extract novel insights. Conceived as a tool that can help both expert and non-expert users better understand public data, MATTERS was collaboratively developed by the Massachusetts High Tech Council, WPI and other institutions as an analytic platform offering dynamic modeling capabilities. MATTERS is an integrative data source on high fidelity cost and talent competitiveness metrics. Its goal is to extract, integrate and model rich economic, financial, educational and technological information from renowned heterogeneous web data sources ranging from The US Census Bureau, The Bureau of Labor Statistics to the Institute of Education Sciences, all known to be critical factors influencing economic competitiveness of states. This demonstration of MATTERS illustrates how we tackle challenges of data acquisition, cleaning, integration and wrangling into appropriate representations, visualization and story-telling with data in the context of state competitiveness in the high-tech sector.", "articleno": "", "ISSN": "", "month": "", "number": "", "volume": "", "publicationtype": "inproceedings", "author": ["Neamtu, Rodica", "Ahsan, Ramoza", "Stokes, Jeff", "Hoxha, Armend", "Bao, Jialiang", "Gvozdenovic, Stefan", "Meyer, Ted", "Patel, Nilesh", "Rangan, Raghu", "Wang, Yumou", "Zhang, Dongyun", "Rundensteiner, Elke"]}]["3538951", {"address": "New York, NY, USA", "booktitle": "Proceedings of the 4th International Conference on Big Data Engineering", "doi": "10.1145/3538950.3538951", "isbn": "9781450395632", "keywords": ["Traceability Verification, Drug Traceability, Big Data"], "location": "Beijing, China", "numpages": "7", "pages": "1\u20137", "publisher": "Association for Computing Machinery", "series": "BDE '22", "title": "A Drug Safety Traceability Model Based on Big Data", "url": "https://doi.org/10.1145/3538950.3538951", "year": "2022", "abstract": "In order to solve the storage and management problems of heterogeneous drug data, this paper uses big data technology to complete the cleaning and distributed storage of drug data, and improve the function of data sharing and traceability. At the same time, in order to improve the drug traceability function, ensure the reliable storage of traceability information, and make the traceability process more reliable. This paper will put forward a drug traceability system model based on big data on the basis of existing research. Secondly, an evidence chain framework is proposed to verify evidence files. At last, the simulation experiment is carried out to test and illustrate the credibility of the traceability verification model.", "articleno": "", "ISSN": "", "month": "", "number": "", "volume": "", "publicationtype": "inproceedings", "author": ["Zhang, Lin", "Jiang, Rong", "Wang, Meng", "Yang, Yue", "Wang, Chenguang"]}]["3206166", {"address": "New York, NY, USA", "booktitle": "Proceedings of the 2018 International Conference on Big Data and Education", "doi": "10.1145/3206157.3206166", "isbn": "9781450363587", "keywords": ["Big Data, Data Management"], "location": "Honolulu, HI, USA", "numpages": "5", "pages": "52\u201356", "publisher": "Association for Computing Machinery", "series": "ICBDE '18", "title": "The 10 Vs, Issues and Challenges of Big Data", "url": "https://doi.org/10.1145/3206157.3206166", "year": "2018", "abstract": "In this emerging computing and digital globe, information and Knowledge are created and then collected with a rapid approach by wide range of applications through scientific computing and commercial workloads. Over 3.8 billion people out of 7.6 billion population of the world are connected to the internet. Out of 13.4 billion devices, 8.06 billion devices have a mobile connection. In 2020, 38.5 billion devices will be connected and globally internet traffic will be 92 times greater than it was in 2005. The use of such devices and internet not only increase the data volume but the velocity of market brings in fast-track and accelerates as information is transferred and shared with light speed on optic fiber and wireless networks. This fast generation of huge data creates numerous challenges. The existing approaches addressing issues such as, Volume, Variety, Velocity and Value in big data research perspective. The objectives of the paper are to investigate and analyze the current status of Big Data and furthermore a comprehensive overview of various aspects has discussed, and additionally has been described all 10 Vs' (Issues) of Big Data.", "articleno": "", "ISSN": "", "month": "", "number": "", "volume": "", "publicationtype": "inproceedings", "author": ["Khan, Nawsher", "Alsaqer, Mohammed", "Shah, Habib", "Badsha, Gran", "Abbasi, Aftab", "Salehian, Soulmaz"]}]["2644168", {"address": "New York, NY, USA", "booktitle": "Proceedings of the 2014 International Conference on Big Data Science and Computing", "doi": "10.1145/2640087.2644168", "isbn": "9781450328913", "keywords": ["Big data, R, Mining, Hadoop, Visualization"], "location": "Beijing, China", "numpages": "6", "pages": "", "publisher": "Association for Computing Machinery", "series": "BigDataScience '14", "title": "Big Data Analysis with Interactive Visualization Using R Packages", "url": "https://doi.org/10.1145/2640087.2644168", "year": "2014", "abstract": "Compared to the traditional data storing, processing, analyzing and visualization which have been performed, Big data requires evolutionary technologies of massive data processing on distributed and parallel systems, such as Hadoop system. Big data analytic systems, thus, have been popular to derive important decision making in various areas. However, visualization on analytic system faces various limitation due to the huge amount of data. This brings the necessity of interactive visualization techniques beyond the traditional static visualization. R has been used and improved for a big data analysis and mining tool. Also, R is supported with various and abundant packages for different targets with visualization. However interactive visualization packages are not easily found in the market. This paper compares and analyzes interactive web packages with visualization packages for R. This paper also proposes interactive web visualized analysis environment for big data with a combination of interactive web packages and visualization packages. In particular, Big data analysis techniques with sensed data are presented as the result by reflecting the decision view on sensing field.", "articleno": "18", "ISSN": "", "month": "", "number": "", "volume": "", "publicationtype": "inproceedings", "author": ["Cho, Wonhee", "Lim, Yoojin", "Lee, Hwangro", "Varma, Mohan", "Lee, Moonsoo", "Choi, Eunmi"]}]["3456390", {"address": "New York, NY, USA", "booktitle": "2021 Workshop on Algorithm and Big Data", "doi": "10.1145/3456389.3456390", "isbn": "9781450389945", "keywords": ["Marketing, Big data, Personalized service"], "location": "Fuzhou, China", "numpages": "4", "pages": "79\u201382", "publisher": "Association for Computing Machinery", "series": "WABD 2021", "title": "Opportunities and Challenges of Marketing in the Context of Big Data", "url": "https://doi.org/10.1145/3456389.3456390", "year": "2021", "abstract": "In the era of big data, under the conditions of rapid economic development in our country, various enterprises have also vigorously carried out marketing. In the context of big data, marketing research should be strengthened to effectively improve market. Market issues ensure that marketing has improved its status in the era of big data. This article has conducted a research and analysis on marketing in the context of big data. And then the opportunities and challenges of marketing in the context of big data has been explained, which gradually optimize the marketing implementation effect. The challenges faced by marketing has been understood which ensures that the big data model plays its best role in it.", "articleno": "", "ISSN": "", "month": "", "number": "", "volume": "", "publicationtype": "inproceedings", "author": ["Cao, Shuangshuang"]}]["3404694", {"address": "New York, NY, USA", "booktitle": "Proceedings of the 5th International Conference on Big Data and Computing", "doi": "10.1145/3404687.3404694", "isbn": "9781450375474", "keywords": ["MapReduce, Big Data Technology, Apache Hadoop, HDFS"], "location": "Chengdu, China", "numpages": "9", "pages": "23\u201331", "publisher": "Association for Computing Machinery", "series": "ICBDC '20", "title": "A Comprehensive Overview of BIG DATA Technologies: A Survey", "url": "https://doi.org/10.1145/3404687.3404694", "year": "2020", "abstract": "In as much as the approaches of the new revolution, machines including transmission media like social media sites, nowadays quantity of data swell hastily. So, size is the core and only facet that leaps the mention of BIG DATA. In this article, an effort to touch a comprehensive view of big data technologies, because of the swift evolution of data by an industry trying the academic press to catch up. This paper also offers a unified explanation of big data as well as the analytics methods. A practical discriminate characteristic of this paper is core analytics associated with unstructured data which is more than 90% of big data. To deal with complicated Big Data problems, great work has been done. This paper analyzes contemporary Big Data technologies. Therein article further strengthens the necessity to formulate new tools for analytics. It bestows not sole an intercontinental overview of big data techniques even though the valuation according to big data Hadoop Ecosystem. It classifies and debates the main technologies feature, challenges, and usage as well.", "articleno": "", "ISSN": "", "month": "", "number": "", "volume": "", "publicationtype": "inproceedings", "author": ["Raza, Muhammad", "XuJian, Zhao"]}]["2896837", {"address": "New York, NY, USA", "booktitle": "Proceedings of the 2nd International Workshop on BIG Data Software Engineering", "doi": "10.1145/2896825.2896837", "isbn": "9781450341523", "keywords": ["big data, architecture landscape, value discovery, value engineering, energy industry, innovation, ecosystem"], "location": "Austin, Texas", "numpages": "7", "pages": "44\u201350", "publisher": "Association for Computing Machinery", "series": "BIGDSE '16", "title": "Toward Big Data Value Engineering for Innovation", "url": "https://doi.org/10.1145/2896825.2896837", "year": "2016", "abstract": "This article articulates the requirements for an effective big data value engineering method. It then presents a value discovery method, called Eco-ARCH (Eco-ARCHitecture), tightly integrated with the BDD (Big Data Design) method for addressing these requirements, filling a methodological void. Eco-ARCH promotes a fundamental shift in design thinking for big data system design -- from \"bounded rationality\" for problem solving to \"expandable rationality\" for design for innovation. The Eco-ARCH approach is most suitable for big data value engineering when system boundaries are fluid, requirements are ill-defined, many stakeholders are unknown and design goals are not provided, where no architecture pre-exists, where system behavior is non-deterministic and continuously evolving, and where co-creation with consumers and prosumers is essential to achieving innovation goals. The method was augmented and empirically validated in collaboration with an IT service company in the energy industry to generate a new business model that we call \"eBay in the Grid\".", "articleno": "", "ISSN": "", "month": "", "number": "", "volume": "", "publicationtype": "inproceedings", "author": ["Chen, Hong-Mei", "Kazman, Rick", "Garbajosa, Juan", "Gonzalez, Eloy"]}]["2819302", {"address": "", "booktitle": "Proceedings of the First International Workshop on BIG Data Software Engineering", "doi": "", "isbn": "", "keywords": ["software architecture, collaborative practice research, embedded case study methodology, big data, data system design methods, system engineering"], "location": "Florence, Italy", "numpages": "7", "pages": "44\u201350", "publisher": "IEEE Press", "series": "BIGDSE '15", "title": "Big Data System Development: An Embedded Case Study with a Global Outsourcing Firm", "url": "", "year": "2015", "abstract": "Big data system development is dramatically different from small (traditional, structured) data system development. At the end of 2014, big data deployment is still scarce and failures abound. Outsourcing has become a main strategy for many enterprises. We therefore selected an outsourcing company who has successfully deployed big data projects for our study. Our research results from analyzing 10 outsourced big data projects provide a glimpse into early adopters of big data, illuminates the challenges for system development that stem from the 5Vs of big data and crystallizes the importance of architecture design choices and technology selection. We followed a collaborative practice research (CPR) method to develop and validate a new method, called BDD. BDD is the first attempt to systematically combine architecture design with data modeling approaches to address big data system development challenges. The use of reference architectures and a technology catalog are advancements to architecture design methods and are proving to be well-suited for big data system architecture design and system development.", "articleno": "", "ISSN": "", "month": "", "number": "", "volume": "", "publicationtype": "inproceedings", "author": ["Chen, Hong-Mei", "Kazman, Rick", "Haziyev, Serge", "Hrytsay, Olha"]}]["3335545", {"address": "New York, NY, USA", "booktitle": "Proceedings of the 4th International Conference on Big Data and Computing", "doi": "10.1145/3335484.3335545", "isbn": "9781450362788", "keywords": ["effectiveness, big data, large-scale complex systems, evaluation"], "location": "Guangzhou, China", "numpages": "5", "pages": "72\u201376", "publisher": "Association for Computing Machinery", "series": "ICBDC '19", "title": "Evaluation of Large-Scale Complex Systems Effectiveness Based on Big Data", "url": "https://doi.org/10.1145/3335484.3335545", "year": "2019", "abstract": "With the advent of the information age, big data technology came into being. The wide application of big data brings new opportunities and challenges to the construction of national defense and military information. Under the background of information-based joint operations characterized by large complex systems, how to scientifically and rationally plan the construction of large complex systems, and maximize the effectiveness of the complex system has become a key concern for system construction decision makers and researchers. This paper combines the application of big data in the construction of large complex systems, and focuses on the evaluation of the effectiveness of large complex systems based on big data, which can be used for reference by relevant researchers.", "articleno": "", "ISSN": "", "month": "", "number": "", "volume": "", "publicationtype": "inproceedings", "author": ["Zhi-peng, Sun", "Gui-ming, Chen", "Hui, Zhang"]}]["3408314", {"address": "New York, NY, USA", "articleno": "110", "doi": "10.1145/3408314", "issn": "0360-0300", "issue_date": "September 2021", "journal": "ACM Comput. Surv.", "keywords": ["software reference architecture, Big Data, Big Data systems, requirements engineering, software engineering, quality assurance"], "month": "sep", "number": "5", "numpages": "39", "publisher": "Association for Computing Machinery", "title": "Big Data Systems: A Software Engineering Perspective", "url": "https://doi.org/10.1145/3408314", "volume": "53", "year": "2020", "abstract": "Big Data Systems (BDSs) are an emerging class of scalable software technologies whereby massive amounts of heterogeneous data are gathered from multiple sources, managed, analyzed (in batch, stream or hybrid fashion), and served to end-users and external applications. Such systems pose specific challenges in all phases of software development lifecycle and might become very complex by evolving data, technologies, and target value over time. Consequently, many organizations and enterprises have found it difficult to adopt BDSs. In this article, we provide insight into three major activities of software engineering in the context of BDSs as well as the choices made to tackle them regarding state-of-the-art research and industry efforts. These activities include the engineering of requirements, designing and constructing software to meet the specified requirements, and software/data quality assurance. We also disclose some open challenges of developing effective BDSs, which need attention from both researchers and practitioners.", "pages": "", "note": "", "ISSN": "0360-0300", "publicationtype": "article", "author": ["Davoudian, Ali", "Liu, Mengchi"]}]["2811235", {"address": "New York, NY, USA", "booktitle": "Proceedings of the ACM Eighteenth International Workshop on Data Warehousing and OLAP", "doi": "10.1145/2811222.2811235", "isbn": "9781450337854", "keywords": ["database design, big data, nosql"], "location": "Melbourne, Australia", "numpages": "4", "pages": "35\u201338", "publisher": "Association for Computing Machinery", "series": "DOLAP '15", "title": "Big Data Design", "url": "https://doi.org/10.1145/2811222.2811235", "year": "2015", "abstract": "It is widely accepted today that Relational databases are not appropriate in highly distributed shared-nothing architectures of commodity hardware, that need to handle poorly structured heterogeneous data. This has brought the blooming of NoSQL systems with the purpose of mitigating such problem, specially in the presence of analytical workloads. Thus, the change in the data model and the new analytical needs beyond OLAP take us to rethink methods and models to design and manage these newborn repositories. In this paper, we will analyze state of the art and future research directions.", "articleno": "", "ISSN": "", "month": "", "number": "", "volume": "", "publicationtype": "inproceedings", "author": ["Abell\\'{o}, Alberto"]}]["3366121", {"address": "New York, NY, USA", "booktitle": "Proceedings of the 21st International Conference on Information Integration and Web-Based Applications &amp; Services", "doi": "10.1145/3366030.3366121", "isbn": "9781450371797", "keywords": ["Big Data Source Selection, Source reliability, Big Data integration, Data quality"], "location": "Munich, Germany", "numpages": "6", "pages": "611\u2013616", "publisher": "Association for Computing Machinery", "series": "iiWAS2019", "title": "Data Source Selection in Big Data Context", "url": "https://doi.org/10.1145/3366030.3366121", "year": "2020", "abstract": "Big Data presents promising technological and economical opportunities. In fact, it has become the raw material of production for many organizations. Data is available in large quantities, and it continues generating abundantly. However, not all the data will have valuable knowledge. Unreliable sources provide misleading and biased information, and even reliable sources could suffer from low data quality.In this paper, we propose a novel methodology for the selectability of data sources, by both considering the presence and the absence of users' preferences. The proposed model integrates multiple factors that affect the reliability of data sources, including their quality, gain, cost and coverage. Experimental results on real world data-sets, show its capability to find the subset of relevant and reliable sources with the lowest cost.", "articleno": "", "ISSN": "", "month": "", "number": "", "volume": "", "publicationtype": "inproceedings", "author": ["Safhi, Hicham", "Frikh, Bouchra", "Ouhbi, Brahim"]}]["3555969", {"address": "New York, NY, USA", "booktitle": "Proceedings of the 2022 6th International Conference on Cloud and Big Data Computing", "doi": "10.1145/3555962.3555969", "isbn": "9781450396578", "keywords": ["Applications, Big Data, Structured data, Management, Unstructured data, Accounting, Analytics"], "location": "Birmingham, United Kingdom", "numpages": "5", "pages": "37\u201341", "publisher": "Association for Computing Machinery", "series": "ICCBDC '22", "title": "Unstructured Over Structured, Big Data Analytics and Applications In Accounting and Management", "url": "https://doi.org/10.1145/3555962.3555969", "year": "2022", "abstract": "Generating value from big data is a task that requires models\u2019 preparation and use of advanced technologies but which, above all, is based on the ability to extract, manage and analyse data. These processes\u2019 effectiveness depends on the data's quality and their structured or unstructured nature. We are witnessing a growing number of applications based on unstructured data mining in the accounting and management fields. This research aims to demonstrating that despite the traditional association between accounting and quantitative analyses (expected to be based mainly on structured financial data). The findings show that several useful applications now rely on unstructured data in this field. A basic analysis of the cybersecurity risks is also presented, along with mitigating strategies to allow companies to comply with current regulations such as the GDPR. The result might appear surprising from the business perspective, but it is not from a data science perspective. In conclusion the growing number of unsctructured data business applications should orientate a better understanding of their potential and target better training of finance specialist on data processing skills.", "articleno": "", "ISSN": "", "month": "", "number": "", "volume": "", "publicationtype": "inproceedings", "author": ["Faccia, Alessio", "Cavaliere, Luigi", "Petratos, Pythagoras", "Mosteanu, Narcisa"]}]["2331058", {"address": "New York, NY, USA", "articleno": "", "doi": "10.1145/2331042.2331058", "issn": "1528-4972", "issue_date": "Fall 2012", "journal": "XRDS", "keywords": [""], "month": "sep", "number": "1", "numpages": "5", "publisher": "Association for Computing Machinery", "title": "Interactive Analysis of Big Data", "url": "https://doi.org/10.1145/2331042.2331058", "volume": "19", "year": "2012", "abstract": "New user interfaces can transform how we work with big data, and raise exciting research problems that span human-computer interaction, machine learning, and distributed systems.", "pages": "50\u201354", "note": "", "ISSN": "1528-4972", "publicationtype": "article", "author": ["Heer, Jeffrey", "Kandel, Sean"]}]["2656358", {"address": "New York, NY, USA", "booktitle": "Proceedings of the Fourth ACM International Symposium on Development and Analysis of Intelligent Vehicular Networks and Applications", "doi": "10.1145/2656346.2656358", "isbn": "9781450330282", "keywords": ["cloud computing, big data"], "location": "Montreal, QC, Canada", "numpages": "6", "pages": "139\u2013144", "publisher": "Association for Computing Machinery", "series": "DIVANet '14", "title": "Big Data Architecture Evolution: 2014 and Beyond", "url": "https://doi.org/10.1145/2656346.2656358", "year": "2014", "abstract": "This paper aims at developing the Big Data Architecture, and its relation with Analytics, Cloud Services as well as Business Intelligence. The chief aim from all mentioned is to enable the Enterprise Architecture and the Vision of an Organizational target to utilize all the data they are ingesting and regressing data for their short-term or long-terms analytical needs, while making sure that they are addressing during the design phase of such data architecture for both directly and indirectly related stakeholder. Since all stakeholders have their relative interests to utilize the transformed data-sets. This paper also identifies most of the Big Data Architecture, threat analysis within a Big Data System and Big Data Analytic Roadmaps, in terms of smaller components by conducting a gap-analysis that has significant importance as Baseline Big Data Architecture, targeting the end resultant Architectures, once the distillation process of main Big Data Architecture is completed by the Data Architects.", "articleno": "", "ISSN": "", "month": "", "number": "", "volume": "", "publicationtype": "inproceedings", "author": ["Mohammad, Atif", "Mcheick, Hamid", "Grant, Emanuel"]}]["2699136", {"address": "New York, NY, USA", "booktitle": "Proceedings of the 5th ACM Conference on Data and Application Security and Privacy", "doi": "10.1145/2699026.2699136", "isbn": "9781450331913", "keywords": ["privacy, big data, security"], "location": "San Antonio, Texas, USA", "numpages": "2", "pages": "279\u2013280", "publisher": "Association for Computing Machinery", "series": "CODASPY '15", "title": "Big Data Security and Privacy", "url": "https://doi.org/10.1145/2699026.2699136", "year": "2015", "abstract": "This paper describes the issues surrounding big data security and privacy and provides a summary of the National Science Foundation sponsored workshop on this topic held in Dallas, Texas on September 16-17, 2014. Our goal is to build a community in big data security and privacy to explore the challenging research problems.", "articleno": "", "ISSN": "", "month": "", "number": "", "volume": "", "publicationtype": "inproceedings", "author": ["Thuraisingham, Bhavani"]}]["2351318", {"address": "New York, NY, USA", "booktitle": "Proceedings of the 1st International Workshop on Big Data, Streams and Heterogeneous Source Mining: Algorithms, Systems, Programming Models and Applications", "doi": "10.1145/2351316.2351318", "isbn": "9781450315470", "keywords": [""], "location": "Beijing, China", "numpages": "5", "pages": "7\u201311", "publisher": "Association for Computing Machinery", "series": "BigMine '12", "title": "Big Data, Big Business: Bridging the Gap", "url": "https://doi.org/10.1145/2351316.2351318", "year": "2012", "abstract": "Business analytics, occupying the intersection of the worlds of management science, computer science and statistical science, is a potent force for innovation in both the private and public sectors. The successes of business analytics in strategy, process optimization and competitive advantage has led to data being increasingly recognized as a valuable asset in many organizations. In recent years, thanks to a dramatic increase in the volume, variety and velocity of data, the loosely defined concept of \"Big Data\" has emerged as a topic of discussion in its own right -- with different viewpoints in both the business and technical worlds. From our perspective, it is important for discussions of \"Big Data\" to start from a well-defined business goal, and remain moored to fundamental principles of both cost/benefit analysis as well as core statistical science. This note discusses some business case considerations for analytics projects involving \"Big Data\", and proposes key questions that businesses should ask. With practical lessons from Big Data deployments in business, we also pose a number of research challenges that may be addressed to enable the business analytics community bring best data analytic practices when confronted with massive data sets.", "articleno": "", "ISSN": "", "month": "", "number": "", "volume": "", "publicationtype": "inproceedings", "author": ["Gopalkrishnan, Vivekanand", "Steier, David", "Lewis, Harvey", "Guszcza, James"]}]["3175687", {"address": "New York, NY, USA", "booktitle": "Proceedings of the International Conference on Big Data and Internet of Thing", "doi": "10.1145/3175684.3175687", "isbn": "9781450354301", "keywords": ["Manufacturing, Industrial Big Data, Framework, Project Selection, Project Prioritization"], "location": "London, United Kingdom", "numpages": "5", "pages": "6\u201310", "publisher": "Association for Computing Machinery", "series": "BDIOT2017", "title": "A Data-Based Method for Industrial Big Data Project Prioritization", "url": "https://doi.org/10.1145/3175684.3175687", "year": "2017", "abstract": "The application of Big Data Techniques (BDT) in discrete manufacturing appears to be very promising, considering lighthouse projects in this area. In general, the goal is to collect all data from manufacturing systems comprehensively, in order to enable new findings and decision support by means of appropriate Industrial Big Data (IBD) analysis procedures. However, due to limited human and economic resources, potential IBD projects need to get prioritized -- in the best case according to their cost-benefit ratio. Available methods for this purpose are insufficient, due to their limited ability to be operationalized, error-proneness, and lack of scientific evidence. In this paper, we discuss how cost-benefit-analysis frameworks can be applied to the preliminary selection of production use cases for the implementation of BDT in larger production systems. It supports the use case selection process from information about production needs, available BDT, and given condition(s) per use case. This concept paper attempts to consolidate the hitherto fragmented discourse on how to prioritize IBD projects, evaluates the challenges of prioritization in this field, and presents a prioritization concept to overcome these challenges.", "articleno": "", "ISSN": "", "month": "", "number": "", "volume": "", "publicationtype": "inproceedings", "author": ["Kuschicke, Felix", "Thiele, Thomas", "Meisen, Tobias", "Jeschke, Sabina"]}]["2014.10", {"address": "USA", "booktitle": "Proceedings of the 2014 IEEE/ACM International Symposium on Big Data Computing", "doi": "10.1109/BDC.2014.10", "isbn": "9781479918973", "keywords": ["Ensemble learning, Hadoop, Big Data, Distributed computing, Bayesian network, Kepler, Scientific workflow"], "location": "", "numpages": "10", "pages": "16\u201325", "publisher": "IEEE Computer Society", "series": "BDC '14", "title": "A Scalable Data Science Workflow Approach for Big Data Bayesian Network Learning", "url": "https://doi.org/10.1109/BDC.2014.10", "year": "2014", "abstract": "In the Big Data era, machine learning has more potential to discover valuable insights from the data. As an important machine learning technique, Bayesian Network (BN) has been widely used to model probabilistic relationships among variables. To deal with the challenges of Big Data PN learning, we apply the techniques in distributed data-parallelism (DDP) and scientific workflow to the BN learning process. We first propose an intelligent Big Data pre-processing approach and a data quality score to measure and ensure the data quality and data faithfulness. Then, a new weight based ensemble algorithm is proposed to learn a BN structure from an ensemble of local results. To easily integrate the algorithm with DDP engines, such as Hadoop, we employ Kepler scientific workflow to build the whole learning process. We demonstrate how Kepler can facilitate building and running our Big Data BN learning application. Our experiments show good scalability and learning accuracy when running the application in real distributed environments.", "articleno": "", "ISSN": "", "month": "", "number": "", "volume": "", "publicationtype": "inproceedings", "author": ["Wang, Jianwu", "Tang, Yan", "Nguyen, Mai", "Altintas, Ilkay"]}]["3216124", {"address": "New York, NY, USA", "booktitle": "Proceedings of the 22nd International Database Engineering &amp; Applications Symposium", "doi": "10.1145/3216122.3216124", "isbn": "9781450365277", "keywords": ["Big Data, Data Quality Assessment, Veracity"], "location": "Villa San Giovanni, Italy", "numpages": "8", "pages": "37\u201344", "publisher": "Association for Computing Machinery", "series": "IDEAS '18", "title": "Quality Awareness for a Successful Big Data Exploitation", "url": "https://doi.org/10.1145/3216122.3216124", "year": "2018", "abstract": "The combination of data and technology is having a high impact on the way we live. The world is getting smarter thanks to the quantity of collected and analyzed data. However, it is necessary to consider that such amount of data is continuously increasing and it is necessary to deal with novel requirements related to variety, volume, velocity, and veracity issues. In this paper we focus on veracity that is related to the presence of uncertain or imprecise data: errors, missing or invalid data can compromise the usefulness of the collected values. In such a scenario, new methods and techniques able to evaluate the quality of the available data are needed. In fact, the literature provides many data quality assessment and improvement techniques, especially for structured data, but in the Big Data era new algorithms have to be designed. We aim to provide an overview of the issues and challenges related to Data Quality assessment in the Big Data scenario. We also propose a possible solution developed by considering a smart city case study and we describe the lessons learned in the design and implementation phases.", "articleno": "", "ISSN": "", "month": "", "number": "", "volume": "", "publicationtype": "inproceedings", "author": ["Cappiello, Cinzia", "Sam\\'{a}, Walter", "Vitali, Monica"]}]["3006311", {"address": "New York, NY, USA", "booktitle": "Proceedings of the 3rd IEEE/ACM International Conference on Big Data Computing, Applications and Technologies", "doi": "10.1145/3006299.3006311", "isbn": "9781450346177", "keywords": ["data complexity, data lifecycle, data management, big data, data organization, vs challenges"], "location": "Shanghai, China", "numpages": "7", "pages": "100\u2013106", "publisher": "Association for Computing Machinery", "series": "BDCAT '16", "title": "Towards a Comprehensive Data Lifecycle Model for Big Data Environments", "url": "https://doi.org/10.1145/3006299.3006311", "year": "2016", "abstract": "A huge amount of data is constantly being produced in the world. Data coming from the IoT, from scientific simulations, or from any other field of the eScience, are accumulated over historical data sets and set up the seed for future Big Data processing, with the final goal to generate added value and discover knowledge. In such computing processes, data are the main resource; however, organizing and managing data during their entire life cycle becomes a complex research topic. As part of this, Data LifeCycle (DLC) models have been proposed to efficiently organize large and complex data sets, from creation to consumption, in any field, and any scale, for an effective data usage and big data exploitation.Several DLC frameworks can be found in the literature, each one defined for specific environments and scenarios. However, we realized that there is no global and comprehensive DLC model to be easily adapted to different scientific areas. For this reason, in this paper we describe the Comprehensive Scenario Agnostic Data LifeCycle (COSA-DLC) model, a DLC model which: i) is proved to be comprehensive as it addresses the 6Vs challenges (namely Value, Volume, Variety, Velocity, Variability and Veracity; and ii), it can be easily adapted to any particular scenario and, therefore, fit the requirements of a specific scientific field. In this paper we also include two use cases to illustrate the ease of the adaptation in different scenarios. We conclude that the comprehensive scenario agnostic DLC model provides several advantages, such as facilitating global data management, organization and integration, easing the adaptation to any kind of scenario, guaranteeing good data quality levels and, therefore, saving design time and efforts for the scientific and industrial communities.", "articleno": "", "ISSN": "", "month": "", "number": "", "volume": "", "publicationtype": "inproceedings", "author": ["Sinaeepourfard, Amir", "Garcia, Jordi", "Masip-Bruin, Xavier", "Mar\\'{\\i}n-Torder, Eva"]}]["2593889", {"address": "New York, NY, USA", "booktitle": "Future of Software Engineering Proceedings", "doi": "10.1145/2593882.2593889", "isbn": "9781450328654", "keywords": ["Data Science, Statistics, Operational Data, Analytics, Data Quality, Game Theory, Data Engineering"], "location": "Hyderabad, India", "numpages": "15", "pages": "85\u201399", "publisher": "Association for Computing Machinery", "series": "FOSE 2014", "title": "Engineering Big Data Solutions", "url": "https://doi.org/10.1145/2593882.2593889", "year": "2014", "abstract": "Structured and unstructured data in operational support tools have long been prevalent in software engineering. Similar data is now becoming widely available in other domains. Software systems that utilize such operational data (OD) to help with software design and maintenance activities are increasingly being built despite the difficulties of drawing valid conclusions from disparate and low-quality data and the continuing evolution of operational support tools. This paper proposes systematizing approaches to the engineering of OD-based systems. To prioritize and structure research areas we consider historic developments, such as big data hype; synthesize defining features of OD, such as confounded measures and unobserved context; and discuss emerging new applications, such as diverse and large OD collections and extremely short development intervals. To sustain the credibility of OD-based systems more research will be needed to investigate effective existing approaches and to synthesize novel, OD-specific engineering principles.", "articleno": "", "ISSN": "", "month": "", "number": "", "volume": "", "publicationtype": "inproceedings", "author": ["Mockus, Audris"]}]["3524442", {"address": "New York, NY, USA", "booktitle": "Proceedings of the 5th International Conference on Big Data and Education", "doi": "10.1145/3524383.3524442", "isbn": "9781450395793", "keywords": ["public opinion system, university public opinion, data analysis, Big data"], "location": "Shanghai, China", "numpages": "6", "pages": "121\u2013126", "publisher": "Association for Computing Machinery", "series": "ICBDE '22", "title": "Research on the Application of Big Data in University's Public Opinion Monitoring and Processing", "url": "https://doi.org/10.1145/3524383.3524442", "year": "2022", "abstract": "Higher educational institutions rely on reputation to attract and earn the legitimacy of the public. However, the emerging trend of public misinformation due to the chunk of information available on the internet affects public opinion formation (POF) about universities. Therefore, narrowing down on the fallouts in POF can provide evidence for managerial and policy interventions. This paper explores five-layer POF and its application with big data in university public opinion monitoring and processing. It reveals that big data technology can be actively employed to safeguard the image of university public opinion formation, but this can be inhibited by the lack of commitment to integrating multiple stakeholders on a common platform, privacy, and security concerns. The paper recommends collaboration between universities and the government to increase momentum on big data with requisite actions for mutual benefits.", "articleno": "", "ISSN": "", "month": "", "number": "", "volume": "", "publicationtype": "inproceedings", "author": ["Cai, Mingjun", "Sam, Francis", "Asante, Evans"]}]["2627561", {"address": "New York, NY, USA", "articleno": "", "doi": "10.1145/2627534.2627561", "issn": "0163-5999", "issue_date": "March 2014", "journal": "SIGMETRICS Perform. Eval. Rev.", "keywords": ["algorithms, cloud computing, analytics, tactical environment, big data"], "month": "apr", "number": "4", "numpages": "4", "publisher": "Association for Computing Machinery", "title": "Tactical Big Data Analytics: Challenges, Use Cases, and Solutions", "url": "https://doi.org/10.1145/2627534.2627561", "volume": "41", "year": "2014", "abstract": "We discuss tactical challenges of the Big Data analytics regarding the underlying data, application space, and com- puting environment, and present a comprehensive solution framework motivated by the relevant tactical use cases. First, we summarize the unique characteristics of the Big Data problem in the Department of Defense (DoD) context and underline the main differences from the commercial Big Data problems. Then, we introduce two use cases, (i) Big Data analytics with multi-intelligence (multi-INT) sensor data and (ii) man-machine crowdsourcing using MapReduce framework. For these two use cases, we introduce Big Data analytics and cloud computing solutions in a coherent frame- work that supports tactical data, application, and computing needs.", "pages": "86\u201389", "note": "", "ISSN": "0163-5999", "publicationtype": "article", "author": ["Savas, Onur", "Sagduyu, Yalin", "Deng, Julia", "Li, Jason"]}]["8.00100", {"address": "", "booktitle": "Proceedings of the 18th IEEE/ACM International Symposium on Cluster, Cloud and Grid Computing", "doi": "10.1109/CCGRID.2018.00100", "isbn": "9781538658154", "keywords": [""], "location": "Washington, District of Columbia", "numpages": "7", "pages": "675\u2013681", "publisher": "IEEE Press", "series": "CCGrid '18", "title": "Pedigree-Ing Your Big Data: Data-Driven Big Data Privacy in Distributed Environments", "url": "https://doi.org/10.1109/CCGRID.2018.00100", "year": "2018", "abstract": "This paper introduces a general framework for supporting data-driven privacy-preserving big data management in distributed environments, such as emerging Cloud settings. The proposed framework can be viewed as an alternative to classical approaches where the privacy of big data is ensured via security-inspired protocols that check several (protocol) layers in order to achieve the desired privacy. Unfortunately, this injects considerable computational overheads in the overall process, thus introducing relevant challenges to be considered. Our approach instead tries to recognize the \"pedigree\" of suitable summary data representatives computed on top of the target big data repositories, hence avoiding computational overheads due to protocol checking. We also provide a relevant realization of the framework above, the so-called Data-dRIven aggregate-PROvenance privacy-preserving big Multidimensional data (DRIPROM) framework, which specifically considers multidimensional data as the case of interest.", "articleno": "", "ISSN": "", "month": "", "number": "", "volume": "", "publicationtype": "inproceedings", "author": ["Cuzzocrea, Alfredo", "Damiani, Ernesto"]}]["3404693", {"address": "New York, NY, USA", "booktitle": "Proceedings of the 5th International Conference on Big Data and Computing", "doi": "10.1145/3404687.3404693", "isbn": "9781450375474", "keywords": ["Application platform, Locomotive system, Key technology, Railway, Big data"], "location": "Chengdu, China", "numpages": "7", "pages": "6\u201312", "publisher": "Association for Computing Machinery", "series": "ICBDC '20", "title": "Research on the Big Data Platform and Its Key Technologies for the Railway Locomotive System", "url": "https://doi.org/10.1145/3404687.3404693", "year": "2020", "abstract": "In order to improve the efficiency of locomotive organization and the quality of locomotive operation, this paper analyzes and discusses the big data platform and some key technologies suitable for the big data application of the railway locomotive system. Firstly, the definition of big data of the railway locomotive system is proposed, and the current data characteristics of the railway locomotive system are summarized, then the status quo and demands of big data application of the railway locomotive system are analyzed. Secondly, the overall architecture of the big data platform for the railway locomotive system is proposed. Furthermore, seven application scenarios available for the big data platform are analyzed, including locomotive running organization, high-speed railway, repair, maintenance and other fields. Finally, some key technologies, which consist of data collection system of front-line operations, locomotive equipment portrait analysis, staff portrait analysis, transmission and analysis of locomotive video, intelligent auxiliary driving system, are provided to increase efficiency of the locomotive organization and capability of safety management. The obtained results can play a positive role in the construction and application of big data of the railway locomotive system.", "articleno": "", "ISSN": "", "month": "", "number": "", "volume": "", "publicationtype": "inproceedings", "author": ["Xin, Li", "Tianyun, Shi", "Xiaoning, Ma"]}]["3418697", {"address": "New York, NY, USA", "booktitle": "2020 the 3rd International Conference on Computing and Big Data", "doi": "10.1145/3418688.3418697", "isbn": "9781450387866", "keywords": ["Digital transformation, Big data, Multi-stage assembly, Industrie 4.0"], "location": "Taichung, Taiwan", "numpages": "7", "pages": "48\u201354", "publisher": "Association for Computing Machinery", "series": "ICCBD '20", "title": "The Effect of Big Data Platforms on Multi-Stage Production System in Industrie 4.0", "url": "https://doi.org/10.1145/3418688.3418697", "year": "2020", "abstract": "The aim of this paper is to analyze how Industrie 4.0 triggers changes in the business models of manufacturing SMEs (small and medium-sized enterprises) by big data platforms in selected casting manufacturer in Taiwan. A generalized model is presented to determine the optimal production run time, production rate, the advertising effort and demand with observation features that minimize the total cost per unit time. Advances in science and technology such as IoT technology, big data platform to investigate information asymmetry between manufacturer and customers. Numerical examples and sensitivity analysis are then provided by the collecting real data from Taiwan. Finally, concluding remarks are offered.", "articleno": "", "ISSN": "", "month": "", "number": "", "volume": "", "publicationtype": "inproceedings", "author": ["Liou, Teau-Jiuan", "Weng, Ming-Wei", "Lee, Liza"]}]["2479730", {"address": "New York, NY, USA", "booktitle": "Proceedings of the 14th Annual International Conference on Digital Government Research", "doi": "10.1145/2479724.2479730", "isbn": "9781450320573", "keywords": ["big data, open government"], "location": "Quebec, Canada", "numpages": "10", "pages": "1\u201310", "publisher": "Association for Computing Machinery", "series": "dg.o '13", "title": "Big Data and E-Government: Issues, Policies, and Recommendations", "url": "https://doi.org/10.1145/2479724.2479730", "year": "2013", "abstract": "The promises and potential of Big Data in transforming digital government services, governments, and the interaction between governments, citizens, and the business sector, are substantial. From \"smart\" government to transformational government, Big Data can foster collaboration; create real-time solutions to challenges in agriculture, health, transportation, and more; and usher in a new era of policy- and decision-making. There are, however, a range of policy challenges to address regarding Big Data, including access and dissemination; digital asset management, archiving and preservation; privacy; and security. This paper selectively reviews and analyzes the U.S. policy context regarding Big Data and offers recommendations aimed at facilitating Big Data initiatives.", "articleno": "", "ISSN": "", "month": "", "number": "", "volume": "", "publicationtype": "inproceedings", "author": ["Bertot, John", "Choi, Heeyoon"]}]["3150226", {"address": "New York, NY, USA", "articleno": "10", "doi": "10.1145/3150226", "issn": "0360-0300", "issue_date": "January 2019", "journal": "ACM Comput. Surv.", "keywords": ["retrieval, survey, multimedia analysis, multimedia databases, 5V challenges, Big data analytics, machine learning, data mining, mobile multimedia, indexing"], "month": "jan", "number": "1", "numpages": "34", "publisher": "Association for Computing Machinery", "title": "Multimedia Big Data Analytics: A Survey", "url": "https://doi.org/10.1145/3150226", "volume": "51", "year": "2018", "abstract": "With the proliferation of online services and mobile technologies, the world has stepped into a multimedia big data era. A vast amount of research work has been done in the multimedia area, targeting different aspects of big data analytics, such as the capture, storage, indexing, mining, and retrieval of multimedia big data. However, very few research work provides a complete survey of the whole pine-line of the multimedia big data analytics, including the management and analysis of the large amount of data, the challenges and opportunities, and the promising research directions. To serve this purpose, we present this survey, which conducts a comprehensive overview of the state-of-the-art research work on multimedia big data analytics. It also aims to bridge the gap between multimedia challenges and big data solutions by providing the current big data frameworks, their applications in multimedia analyses, the strengths and limitations of the existing methods, and the potential future directions in multimedia big data analytics. To the best of our knowledge, this is the first survey that targets the most recent multimedia management techniques for very large-scale data and also provides the research studies and technologies advancing the multimedia analyses in this big data era.", "pages": "", "note": "", "ISSN": "0360-0300", "publicationtype": "article", "author": ["Pouyanfar, Samira", "Yang, Yimin", "Chen, Shu-Ching", "Shyu, Mei-Ling", "Iyengar, S."]}]["3545802", {"address": "New York, NY, USA", "booktitle": "Proceedings of the 7th International Conference on Big Data and Computing", "doi": "10.1145/3545801.3545802", "isbn": "9781450396097", "keywords": ["multi-head attention, Electric power data, data quality evaluation, multi-dimensional indicators"], "location": "Shenzhen, China", "numpages": "5", "pages": "1\u20135", "publisher": "Association for Computing Machinery", "series": "ICBDC '22", "title": "A Multi-Head Attention Mechanism Base Multi-Dimensional Data Quality Evaluation Model", "url": "https://doi.org/10.1145/3545801.3545802", "year": "2022", "abstract": "High-quality power data is the basis for reliable operation of power systems, efficient data processing, and effective mining of the potential value of power data. How to use big data, artificial intelligence and other technologies to evaluate the quality of power data is a hot research topic in the field of electric power. At present, most of the power data quality evaluation methods are simple and lack the research of general data quality evaluation model. Therefore, this paper proposes a multi-dimensional data quality evaluation model based on a multi-head attention mechanism. The model measures multiple indicators such as completeness, accuracy, smoothness, and correlation. The corresponding methods are used to quantify these indicators to form a data quality evaluation index system oriented to multi-dimensional indicators; then, an application feedback mechanism based on a multi-head attention network is used to correct the calculation weights and score outputs, so as to achieve the evaluation of power data quality. Finally, the validation analysis is carried out based on the electricity data of a region in China. The experimental results show that the proposed method can effectively evaluate the quality of electric power data.", "articleno": "", "ISSN": "", "month": "", "number": "", "volume": "", "publicationtype": "inproceedings", "author": ["Liu, Xiaobao", "Li, Qi", "Zhu, Shaosong", "Wang, Cong", "Meng, Lingzhen"]}]["3492546", {"address": "New York, NY, USA", "articleno": "", "doi": "10.1145/3492546", "issn": "1936-1955", "issue_date": "", "journal": "J. Data and Information Quality", "keywords": ["classification, data quality, big data, deep learning, machine learning, label noise, data streams"], "month": "apr", "number": "", "numpages": "", "publisher": "Association for Computing Machinery", "title": "A Survey on Classifying Big Data with Label Noise", "url": "https://doi.org/10.1145/3492546", "volume": "", "year": "2022", "abstract": "Class label noise is a critical component of data quality that directly inhibits the predictive performance of machine learning algorithms. While many data-level and algorithm-level methods exist for treating label noise, the challenges associated with big data call for new and improved methods. This survey addresses these concerns by providing an extensive literature review on treating label noise within big data. We begin with an introduction to the class label noise problem and traditional methods for treating label noise. Next, we present 30 methods for treating class label noise in a range of big data contexts, i.e. high volume, high variety, and high velocity problems. The surveyed works include distributed solutions capable of operating on data sets of arbitrary sizes, deep learning techniques for large-scale data sets with limited clean labels, and streaming techniques for detecting class noise in the presence of concept drift. Common trends and best practices are identified in each of these areas, implementation details are reviewed, empirical results are compared across studies when applicable, and references to 17 open-source projects and programming packages are provided. An emphasis on label noise challenges, solutions, and empirical results as they relate to big data distinguishes this work as a unique contribution that will inspire future research and guide machine learning practitioners.", "pages": "", "note": "Just Accepted", "ISSN": "1936-1955", "publicationtype": "article", "author": ["Johnson, Justin", "Khoshgoftaar, Taghi"]}]["2900335", {"address": "New York, NY, USA", "booktitle": "Proceedings of the International Conference on Internet of Things and Cloud Computing", "doi": "10.1145/2896387.2900335", "isbn": "9781450340632", "keywords": ["Warehousing Big Data, Protecting Big Data, Big Data Analytics, Big Data"], "location": "Cambridge, United Kingdom", "numpages": "7", "pages": "", "publisher": "Association for Computing Machinery", "series": "ICC '16", "title": "Warehousing and Protecting Big Data: State-Of-The-Art-Analysis, Methodologies, Future Challenges", "url": "https://doi.org/10.1145/2896387.2900335", "year": "2016", "abstract": "This paper proposes a comprehensive critical survey on the issues of warehousing and protecting big data, which are recognized as critical challenges of emerging big data research. Indeed, both are critical aspects to be considered in order to build truly, high-performance and highly-flexible big data management systems. We report on state-of-the-art approaches, methodologies and trends, and finally conclude by providing open problems and challenging research directions to be considered by future efforts.", "articleno": "14", "ISSN": "", "month": "", "number": "", "volume": "", "publicationtype": "inproceedings", "author": ["Cuzzocrea, Alfredo"]}]["3209372", {"address": "New York, NY, USA", "booktitle": "Proceedings of the 19th Annual International Conference on Digital Government Research: Governance in the Data Age", "doi": "10.1145/3209281.3209372", "isbn": "9781450365260", "keywords": ["census big data, electronic census, big data challenges, use, big data analytics, cross case analysis, public value creation"], "location": "Delft, The Netherlands", "numpages": "10", "pages": "", "publisher": "Association for Computing Machinery", "series": "dg.o '18", "title": "Census Big Data Analytics Use: International Cross Case Analysis", "url": "https://doi.org/10.1145/3209281.3209372", "year": "2018", "abstract": "Despite the growing practices in big data and big data analytics use, there is still the paucity of research on links between government big data analytics use and public value creation. This multi-case study of Australia, Ireland, Mexico, and U.S.A. examines the state of big data and big data analytics use in the national census context. The census agencies are at varying stages in digitally transforming their national census process, products and services through assimilating and using big data and big data analytics. The cross-case analysis of government websites and documents identified emerging agency challenges in creating public value in the national census context: (1) big data analytics capability development, (2) cross agency data access and data integration, and (3) data security, privacy &amp; trust. Based on the insights gained, a research model aims to postulate the possible links among challenges, big data/big data analytics use, and public value creation.", "articleno": "10", "ISSN": "", "month": "", "number": "", "volume": "", "publicationtype": "inproceedings", "author": ["Chatfield, Akemi", "Ojo, Adegboyega", "Puron-Cid, Gabriel", "Reddick, Christopher"]}]["3289108", {"address": "New York, NY, USA", "booktitle": "Proceedings of the 2nd International Conference on Smart Digital Environment", "doi": "10.1145/3289100.3289108", "isbn": "9781450365079", "keywords": ["MapReduce jobs, Hadoop, Intelligent processing, Multidimensional approach, Big Data, Data placing"], "location": "Rabat, Morocco", "numpages": "6", "pages": "42\u201347", "publisher": "Association for Computing Machinery", "series": "ICSDE'18", "title": "Towards Efficient Big Data: Hadoop Data Placing and Processing", "url": "https://doi.org/10.1145/3289100.3289108", "year": "2018", "abstract": "Currently, the generated data flow is growing at a high rate resulting to the problem of data obesity and abundance, but yet a lack of pertinent information. To handle this Big Data, Hadoop is a distributed framework that facilitates data storage and processing. Although Hadoop is designed to deal with demands of storage and analysis of ever-growing Data, its performance characteristics are still to improve. In this regard, many approaches have been proposed to enhance Hadoop capabilities. Nevertheless, an overview of these approaches shows that several aspects need to be improved in terms of performance and data relevancy. The main challenge is how to extract efficiently value from the big data sources. For this purpose, we propose in this paper to discuss Hadoop architecture and intelligent data discovery, and propose an effective on-demand Big Data contribution enabling to process relevant data in efficient and effective way according to the stakeholder's needs, and aiming to boost Data appointment by integrating multidimensional approach.", "articleno": "", "ISSN": "", "month": "", "number": "", "volume": "", "publicationtype": "inproceedings", "author": ["Bahadi, Jihane", "El, Bouchra", "Courtine, M\\'{e}lanie", "Rhanoui, Maryem", "Kergosien, Yannick"]}]["3297743", {"address": "New York, NY, USA", "booktitle": "Proceedings of the 2018 International Conference on Big Data Engineering and Technology", "doi": "10.1145/3297730.3297743", "isbn": "9781450365826", "keywords": ["platform architecture, big data platform, insurance industry, Financial technology, time and space data"], "location": "Chengdu, China", "numpages": "5", "pages": "31\u201335", "publisher": "Association for Computing Machinery", "series": "BDET 2018", "title": "Big Data Platform Architecture under The Background of Financial Technology: In The Insurance Industry As An Example", "url": "https://doi.org/10.1145/3297730.3297743", "year": "2018", "abstract": "With the rise of the concept of financial technology, financial and technology gradually in-depth integration, scientific and technological means to become financial product innovation, improve financial efficiency and reduce financial transaction costs an important driving force. In this context, the new technology platform is from the business philosophy, business model, technical means, sales, internal management and other dimensions to re-shape the financial industry. In this paper, the existing big data platform architecture technology innovation, adding space-time data elements, combined with the insurance industry for practical analysis, put forward a meaningful product circle and customer circle.", "articleno": "", "ISSN": "", "month": "", "number": "", "volume": "", "publicationtype": "inproceedings", "author": ["Liu, Yi", "Peng, Jiawen", "Yu, Zhihao"]}]["3297731", {"address": "New York, NY, USA", "booktitle": "Proceedings of the 2018 International Conference on Big Data Engineering and Technology", "doi": "10.1145/3297730.3297731", "isbn": "9781450365826", "keywords": ["fuzzy case-based reasoning, fuzzy decision tree, big data-streaming, Target data optimization"], "location": "Chengdu, China", "numpages": "5", "pages": "26\u201330", "publisher": "Association for Computing Machinery", "series": "BDET 2018", "title": "Target Data Optimization Based on Big Data-Streaming for Two-Stage Fuzzy Extraction System", "url": "https://doi.org/10.1145/3297730.3297731", "year": "2018", "abstract": "How to extract target data effectively and intelligently is key point in the big data-streaming operation. Data extractions need focus on priority of data selection to reduce impact of 4Vs because of requirement of real-time computation. Corresponding big data-streaming for three-stage extraction system is presented in terms of hierarchal base and fuzzy representations. The proposed approach is based on a combination of clustering, classification and relationships method with fuzzy weighted similarity under hierarchical feature-based model. Moreover, heuristic fuzzy CBR-FDT- algorithms are provided to explore the target data optimization. Successful case study and experiment with simulations demonstrated the performance of the proposed approach.", "articleno": "", "ISSN": "", "month": "", "number": "", "volume": "", "publicationtype": "inproceedings", "author": ["Chen, Rui-Yang"]}]["3331659", {"address": "New York, NY, USA", "articleno": "", "doi": "10.1145/3331651.3331659", "issn": "1931-0145", "issue_date": "June 2019", "journal": "SIGKDD Explor. Newsl.", "keywords": ["community engagement, education, co-design, big data"], "month": "may", "number": "1", "numpages": "4", "publisher": "Association for Computing Machinery", "title": "Co-Designing Learning Materials to Empower Laypersons to Better Understand Big Data and Big Data Methods", "url": "https://doi.org/10.1145/3331651.3331659", "volume": "21", "year": "2019", "abstract": "University of Colorado Anschutz Medical Campus' Data Science to Patient Value Program and 2040 Partners for Health sought to create open learning materials for engaged citizens and community leaders regarding big data and big data methods to support their collaboration in patient-centered and participatorybased community research and evaluation. 2040 is a local nonprofit organization that cultivates partnerships in Aurora, Colorado neighborhoods to tackle critical health needs. Our goal was to co-design and co-create a series of big data learning modules accessible to community laypeople, so they might better understand big data topics and be empowered more actively engage in health research and evaluation that uses big data methods.", "pages": "41\u201344", "note": "", "ISSN": "1931-0145", "publicationtype": "article", "author": ["Schilling, Lisa", "Pena-Jackson, Griselda", "Russell, Seth", "Corral, Janet", "Kwan, Bethany", "Ressalam, Julie"]}]["9.00019", {"address": "", "booktitle": "Proceedings of the 4th International Workshop on Metamorphic Testing", "doi": "10.1109/MET.2019.00019", "isbn": "", "keywords": ["quality assessment, data quality, metamorphic testing, big data, metamorphic data relations"], "location": "Montreal, Quebec, Canada", "numpages": "8", "pages": "76\u201383", "publisher": "IEEE Press", "series": "MET '19", "title": "Addressing Data Quality Problems with Metamorphic Data Relations", "url": "https://doi.org/10.1109/MET.2019.00019", "year": "2019", "abstract": "In the era of big data, cloud computing and the Internet of Things, the quality of data has tremendous impact on our everyday life. Moreover, the increasing velocity, volume and variety of data requires new approaches for quality assessment. In this paper, a new approach for quality assessment is presented that applies metamorphic testing to data quality. The exemplary application of the approach on a big data application shows promising results for the suitability of the approach.", "articleno": "", "ISSN": "", "month": "", "number": "", "volume": "", "publicationtype": "inproceedings", "author": ["Auer, Florian", "Felderer, Michael"]}]["3372478", {"address": "New York, NY, USA", "booktitle": "Proceedings of the 2019 3rd International Conference on Big Data Research", "doi": "10.1145/3372454.3372478", "isbn": "9781450372015", "keywords": ["IoT, Theory of evidence, Smart City, Data Reliability"], "location": "Cergy-Pontoise, France", "numpages": "6", "pages": "18\u201323", "publisher": "Association for Computing Machinery", "series": "ICBDR 2019", "title": "Assessing Reliability of Big Data Stream for Smart City", "url": "https://doi.org/10.1145/3372454.3372478", "year": "2020", "abstract": "Proliferation of IoT (Internet of Things) and sensor technology has expedited the realization of Smart City. To enable necessary functions, sensors distributed across the city generate a huge volume of stream data that are crucial for controlling Smart City devices. However, due to conditions such as wears and tears, battery drain, or malicious attacks, not all data are reliable even when they are accurately measured. These data could lead to invalid and devastating consequences (e.g., failed utility or transportation services). The assessment of data reliability is necessary and challenging especially for Smart City, as it has to keep up with velocity of big data stream to provide up-to-date results. Most research on data reliability has focused on data fusion and anomaly detection that lack a quantified measure of how much the data over a period of time are adequately reliable for decision-makings. This paper alleviates these issues and presents an online approach to assessing Big stream data reliability in a timely manner. By employing a well-studied evidence-based theory, our approach provides a computational framework that assesses data reliability in terms of belief likelihoods. The framework is lightweight and easy to scale, deeming fit for streaming data. We evaluate the approach using a real application of light sensing data of 1,323,298 instances. The preliminary results are consistent with logical rationales, confirming validity of the approach.", "articleno": "", "ISSN": "", "month": "", "number": "", "volume": "", "publicationtype": "inproceedings", "author": ["Puangpontip, Supadchaya", "Hewett, Rattikorn"]}]["3148238", {"address": "New York, NY, USA", "articleno": "12", "doi": "10.1145/3148238", "issn": "1936-1955", "issue_date": "June 2017", "journal": "J. Data and Information Quality", "keywords": ["requirements for metrics, data quality assessment, data quality metrics, Data quality"], "month": "jan", "number": "2", "numpages": "32", "publisher": "Association for Computing Machinery", "title": "Requirements for Data Quality Metrics", "url": "https://doi.org/10.1145/3148238", "volume": "9", "year": "2018", "abstract": "Data quality and especially the assessment of data quality have been intensively discussed in research and practice alike. To support an economically oriented management of data quality and decision making under uncertainty, it is essential to assess the data quality level by means of well-founded metrics. However, if not adequately defined, these metrics can lead to wrong decisions and economic losses. Therefore, based on a decision-oriented framework, we present a set of five requirements for data quality metrics. These requirements are relevant for a metric that aims to support an economically oriented management of data quality and decision making under uncertainty. We further demonstrate the applicability and efficacy of these requirements by evaluating five data quality metrics for different data quality dimensions. Moreover, we discuss practical implications when applying the presented requirements.", "pages": "", "note": "", "ISSN": "1936-1955", "publicationtype": "article", "author": ["Heinrich, Bernd", "Hristova, Diana", "Klier, Mathias", "Schiller, Alexander", "Szubartowicz, Michael"]}]["2742794", {"address": "New York, NY, USA", "booktitle": "Proceedings of the 2015 ACM SIGMOD International Conference on Management of Data", "doi": "10.1145/2723372.2742794", "isbn": "9781450327589", "keywords": ["telco churn prediction, customer retention, big data"], "location": "Melbourne, Victoria, Australia", "numpages": "12", "pages": "607\u2013618", "publisher": "Association for Computing Machinery", "series": "SIGMOD '15", "title": "Telco Churn Prediction with Big Data", "url": "https://doi.org/10.1145/2723372.2742794", "year": "2015", "abstract": "We show that telco big data can make churn prediction much more easier from the $3$V's perspectives: Volume, Variety, Velocity. Experimental results confirm that the prediction performance has been significantly improved by using a large volume of training data, a large variety of features from both business support systems (BSS) and operations support systems (OSS), and a high velocity of processing new coming data. We have deployed this churn prediction system in one of the biggest mobile operators in China. From millions of active customers, this system can provide a list of prepaid customers who are most likely to churn in the next month, having $0.96$ precision for the top $50000$ predicted churners in the list. Automatic matching retention campaigns with the targeted potential churners significantly boost their recharge rates, leading to a big business value.", "articleno": "", "ISSN": "", "month": "", "number": "", "volume": "", "publicationtype": "inproceedings", "author": ["Huang, Yiqing", "Zhu, Fangzhou", "Yuan, Mingxuan", "Deng, Ke", "Li, Yanhua", "Ni, Bing", "Dai, Wenyuan", "Yang, Qiang", "Zeng, Jia"]}]["3047377", {"address": "New York, NY, USA", "booktitle": "Proceedings of the 10th International Conference on Theory and Practice of Electronic Governance", "doi": "10.1145/3047273.3047377", "isbn": "9781450348256", "keywords": ["counterfactuals, ex-post policy evaluation, Big data, data linkage"], "location": "New Delhi AA, India", "numpages": "4", "pages": "228\u2013231", "publisher": "Association for Computing Machinery", "series": "ICEGOV '17", "title": "Exploiting Big Data for Evaluation Studies", "url": "https://doi.org/10.1145/3047273.3047377", "year": "2017", "abstract": "The collection and analysis of relevant data for evaluating public policies is not a straightforward task. An important type of such studies is the so-called ex-post evaluation. The main objective of ex-post evaluations is to determine to what extent a realized intervention is successful in tackling a societal challenge, e.g., youth unemployment. At a first glance an obvious method is to collect some baseline measurements for a set of relevant variables, apply the intervention for a while and collect the new measurement values for the same set of variables. Then, comparing the measurement values of the variables before and after the intervention provides an insight into the extent of successfulness of the intervention. This, however, is only true if the \"ceteris paribus\" condition holds. In practice it is infeasible to enforce this condition for societal challenges. Often, after having the baseline measurements, several phenomena emerge that may impact the new measurements without being taken into account. This makes it difficult to determine how much of the measured differences between the values of the variables before and after the intervention should be attributed to the emerging phenomena (or the so-called counterfactuals) and how much of the differences can be attributed to the applied intervention.This paper discusses how exploiting big data may contribute to the task of elucidating the influences of counterfactuals (and interventions) in ex-post evaluation studies. The paper proposes a framework to utilize big data for accounting for the impact of emerging phenomena in ex-post evaluation studies.", "articleno": "", "ISSN": "", "month": "", "number": "", "volume": "", "publicationtype": "inproceedings", "author": ["Netten, Niels", "Bargh, Mortaza", "Choenni, Sunil", "Meijer, Ronald"]}]["3345282", {"address": "New York, NY, USA", "booktitle": "Proceedings of the 20th International Conference on Computer Systems and Technologies", "doi": "10.1145/3345252.3345282", "isbn": "9781450371490", "keywords": ["Big Data, Big Data Value Chain, Smart City, GATE Platform, Emerging Architectures"], "location": "Ruse, Bulgaria", "numpages": "8", "pages": "261\u2013268", "publisher": "Association for Computing Machinery", "series": "CompSysTech '19", "title": "Conceptual Architecture of GATE Big Data Platform", "url": "https://doi.org/10.1145/3345252.3345282", "year": "2019", "abstract": "Today we experience a data-driven society. All human activities, industrial processes and research lead to data generation of unprecedented scale, spurring new products, services and businesses. Big Data and its application have been a target for European Commission -- with more than 100 FP7 and about 50 H2020 funded projects under Big Data domain. GATE project aims to establish and sustain in the long run a Centre of Excellence as collaborative environment for conducting Big Data research and innovation, facilitated by GATE platform and Innovation Labs. This paper proposes a conceptual architecture of GATE platform, that is holistic, symbiotic, open, evolving and data-integrated. It is also modular and with component-based design that allows to position a mix of products and tools from different providers. GATE platform will enable start-ups, SMEs and large enterprises, as well as other organizations in a wide range of sectors, to build advanced Data driven services and applications. The usability of the proposed architecture is proven through a development of a sample time series data visualization application. Its architecture follows the proposed one through implementation of required components using open technology stack.", "articleno": "", "ISSN": "", "month": "", "number": "", "volume": "", "publicationtype": "inproceedings", "author": ["Petrova-Antonova, Dessislava", "Krasteva, Iva", "Ilieva, Sylvia", "Pavlova, Irena"]}]["3328841", {"address": "New York, NY, USA", "booktitle": "Proceedings of the 2019 8th International Conference on Software and Information Engineering", "doi": "10.1145/3328833.3328841", "isbn": "9781450361057", "keywords": ["Challenges, Analytics, Benefits, Big Data"], "location": "Cairo, Egypt", "numpages": "4", "pages": "196\u2013199", "publisher": "Association for Computing Machinery", "series": "ICSIE '19", "title": "Big Data in Healthcare: Are We Getting Useful Insights from This Avalanche of Data?", "url": "https://doi.org/10.1145/3328833.3328841", "year": "2019", "abstract": "The benefits of deriving useful insights from avalanche of data available everywhere cannot be overemphasized. Big Data analytics can revolutionize the healthcare industry. It can also ensure functional productivity, help forecast and suggest feedbacks to disease outbreaks, enhance clinical practice, and optimize healthcare expenditure which cuts across all stakeholders in healthcare sectors. Notwithstanding these immense capabilities available in the general application of big data; studies on derivation of useful insights from healthcare data that can enhance medical practice have received little academic attention. Therefore, this study highlighted the possibility of making very insightful healthcare outcomes with big data through a simple classification problem which classifies the tendency of individuals towards specific drugs based on personality measures. Our model though trained with less than 2000 samples and with a simple neural network architecture achieved mean accuracies of 76.87% (sd=0.0097) and 75.86% (sd=0.0123) for the 0.15 and 0.05 validation sets respectively. The relatively acceptable performance recorded by our model despite the small dataset could largely be attributed to number of attributes in our dataset. It is essential to uncover some of the many complexities in our societies in relations to healthcare; and through many machine learning architectures like the neural networks these complex relationships can be discovered", "articleno": "", "ISSN": "", "month": "", "number": "", "volume": "", "publicationtype": "inproceedings", "author": ["Adenuga, Kayode", "Muniru, Idris", "Sadiq, Fatai", "Adenuga, Rahmat", "Solihudeen, Muhammad"]}]["9849793", {"address": "", "booktitle": "2022 4th International Conference on Advances in Computer Technology, Information Science and Communications (CTISC)", "doi": "10.1109/CTISC54888.2022.9849793", "isbn": "", "keywords": ["Information science", "Data integration", "Companies", "Reliability theory", "Big Data", "Product design", "Quality assessment", "quality data package", "big data", "intelligent manufacturing", "data fusion"], "location": "", "numpages": "", "pages": "1-6", "publisher": "", "series": "", "title": "Research on the Big Data-based Product Quality Data Package Construction and Application", "url": "", "year": "2022", "abstract": "In the new environment of intelligent manufacturing, enterprise quality data has increased exponentially. How to manage, utilize, mine and analyze quality data has become a key issue in modern quality management. This article expands the definition of the product quality data package in the intelligent manufacturing environment, and proposes a big data-based product quality data package construction and management solution, gives a quality data fusion method based on business decision, outlines the application of quality data package. Finally, a chip manufacturing company was used to verify the feasibility of the product quality data package construction and management plan.", "articleno": "", "ISSN": "", "month": "April", "number": "", "volume": "", "publicationtype": "inproceedings", "author": ["Chen, Bingquan", "Nie, Guojian", "Jiang, Shixin", "Hu, Ning"]}]["9245455", {"address": "", "booktitle": "2020 1st International Conference on Big Data Analytics and Practices (IBDAP)", "doi": "10.1109/IBDAP50342.2020.9245455", "isbn": "", "keywords": ["Data integrity", "Pipelines", "Data visualization", "Big Data", "Syntactics", "Software", "Python", "Data Quality Management", "Data Profiling", "Data Quality Auditing", "Python Package", "Data Pipeline"], "location": "", "numpages": "", "pages": "1-4", "publisher": "", "series": "", "title": "Sakdas: A Python Package for Data Profiling and Data Quality Auditing", "url": "", "year": "2020", "abstract": "Data Profiling and data quality management become a more significant part of data engineering, which an essential part of ensuring that the system delivers quality information to users. In the last decade, data quality was considered to need more managing. Especially in the big data era that the data comes from many sources, many data types, and an enormous amount. Thus it makes the managing of data quality is more difficult and complicated. The traditional system was unable to respond as needed. The data quality managing software for big data was developed but often found in a high-priced, difficult to customize as needed, and mostly provide as GUI, which is challenging to integrate with other systems. From this problem, we have developed an opensource package for data quality managing. By using Python programming language, Which is a programming language that is widely used in the scientific and engineering field today. Because it is a programming language that is easy to read syntax, small, and has many additional packages to integrate. The software developed here is called \u201cSakdas\u201d this package has been divided into three parts. The first part deals with data profiling provide a set of data analyses to generate a data profile, and this profile will help to define the data quality rules. The second part deals with data quality auditing that users can set their own data quality rules for data quality measurement. The final part deals with data visualizing that provides data profiling and data auditing report to improve the data quality. The results of the profiling and auditing services, the user can specify both the form of a report for self-review. Or in the form of JSON for use in post-process automation.", "articleno": "", "ISSN": "", "month": "Sep.", "number": "", "volume": "", "publicationtype": "inproceedings", "author": ["Loetpipatwanich, Sakda", "Vichitthamaros, Preecha"]}]["8432043", {"address": "", "booktitle": "2018 IEEE International Conference on Software Quality, Reliability and Security Companion (QRS-C)", "doi": "10.1109/QRS-C.2018.00115", "isbn": "", "keywords": ["Warranties", "Data mining", "Product design", "Quality assessment", "Databases", "Big Data", "Reliability engineering", "quality warranty data", "big data analysis", "association rules", "quality improvement", "PDCA"], "location": "", "numpages": "", "pages": "643-644", "publisher": "", "series": "", "title": "A Method of Quality Improvement Based on Big Quality Warranty Data Analysis", "url": "", "year": "2018", "abstract": "Quality warranty data includes big data of product use and customer services, which is foundation of product quality and reliability improvement. This paper presents a method of quality warranty data analysis, which is based on the big data analysis technology. By means of the method of association rules mining, it distinguishes the association rules of failure modes while feeding back the information to the process of product design, production, and usage. To achieve product fault location and fault disposal, the key factors such as fault type and fault cause are analyzed. Meanwhile, this paper adopted the principles of PDCA circulation to propose a procedure of product quality improvement. The quality improvement procedure based on quality warranty data analysis provides a comprehensive and systematic quality improvement for different stages and different types of products. Finally, a case study of household appliances in China is given to illustrate the method.", "articleno": "", "ISSN": "", "month": "July", "number": "", "volume": "", "publicationtype": "inproceedings", "author": ["Pan, Xing", "Zhang, Manli", "Chen, Xi"]}]["8725668", {"address": "", "booktitle": "2019 IEEE 4th International Conference on Cloud Computing and Big Data Analysis (ICCCBDA)", "doi": "10.1109/ICCCBDA.2019.8725668", "isbn": "", "keywords": ["Power quality", "Data integration", "Distributed databases", "Monitoring", "Power grids", "Computer architecture", "Data models", "Power Quality", "Wide Area Distribution", "Data Integration"], "location": "", "numpages": "", "pages": "185-188", "publisher": "", "series": "", "title": "Research on Wide-area Distributed Power Quality Data Fusion Technology of Power Grid", "url": "", "year": "2019", "abstract": "With the advancement of the \"big operation\" system construction, the online monitoring system for power quality has been integrated, and various power quality data have been incorporated into relevant organizations for unified management. Power quality management has a larger range of data, more types, and higher frequency. It needs to realize the unified storage management and efficient access of massive heterogeneous power quality data for the characteristics of data applications and the collection and aggregation of these effective data. This paper proposes a new type of grid wide-area distributed power quality data integration architecture, which is designed for multi-source, heterogeneous, distributed data integration technology and wide-area distributed data storage technology to solve the big data source problem and realize the sharing of power quality data information of the whole network.", "articleno": "", "ISSN": "", "month": "April", "number": "", "volume": "", "publicationtype": "inproceedings", "author": ["Jin, Li", "Haosong, Li", "Zhongping, Xu", "Ting, Wang", "Shuai, Wang", "Yutong, Wei", "Dongliang, Hu", "Chunting, Kang", "Jia, Wu", "Dan, Su"]}]["8605945", {"address": "", "booktitle": "2018 International Conference on Innovations in Information Technology (IIT)", "doi": "10.1109/INNOVATIONS.2018.8605945", "isbn": "", "keywords": ["Big Data", "Data integrity", "Data mining", "Feature extraction", "Data models", "Measurement", "Quality assessment", "Big Data", "Data Quality", "Unstructured Data", "Quality of Unstructured Big Data"], "location": "", "numpages": "", "pages": "69-74", "publisher": "", "series": "", "title": "Big Data Quality Assessment Model for Unstructured Data", "url": "", "year": "2018", "abstract": "Big Data has gained an enormous momentum the past few years because of the tremendous volume of generated and processed Data from diverse application domains. Nowadays, it is estimated that 80% of all the generated data is unstructured. Evaluating the quality of Big data has been identified to be essential to guarantee data quality dimensions including for example completeness, and accuracy. Current initiatives for unstructured data quality evaluation are still under investigations. In this paper, we propose a quality evaluation model to handle quality of Unstructured Big Data (UBD). The later captures and discover first key properties of unstructured big data and its characteristics, provides some comprehensive mechanisms to sample, profile the UBD dataset and extract features and characteristics from heterogeneous data types in different formats. A Data Quality repository manage relationships between Data quality dimensions, quality Metrics, features extraction methods, mining methodologies, data types and data domains. An analysis of the samples provides a data profile of UBD. This profile is extended to a quality profile that contains the quality mapping with selected features for quality assessment. We developed an UBD quality assessment model that handles all the processes from the UBD profiling exploration to the Quality report. The model provides an initial blueprint for quality estimation of unstructured Big data. It also, states a set of quality characteristics and indicators that can be used to outline an initial data quality schema of UBD.", "articleno": "", "ISSN": "2325-5498", "month": "Nov", "number": "", "volume": "", "publicationtype": "inproceedings", "author": ["Taleb, Ikbal", "Serhani, Mohamed", "Dssouli, Rachida"]}]["8029366", {"address": "", "booktitle": "2017 IEEE International Congress on Big Data (BigData Congress)", "doi": "10.1109/BigDataCongress.2017.73", "isbn": "", "keywords": ["Big Data", "Optimization", "Data models", "Quality assessment", "Big Data", "Data Quality Evaluation", "Data Quality Rules Discovery", "Big Data Pre-Processing"], "location": "", "numpages": "", "pages": "498-501", "publisher": "", "series": "", "title": "Big Data Pre-Processing: Closing the Data Quality Enforcement Loop", "url": "", "year": "2017", "abstract": "In the Big Data Era, data is the core for any governmental, institutional, and private organization. Efforts were geared towards extracting highly valuable insights that cannot happen if data is of poor quality. Therefore, data quality (DQ) is considered as a key element in Big data processing phase. In this stage, low quality data is not penetrated to the Big Data value chain. This paper, addresses the data quality rules discovery (DQR) after the evaluation of quality and prior to Big Data pre-processing. We propose a DQR discovery model to enhance and accurately target the pre-processing activities based on quality requirements. We defined, a set of pre-processing activities associated with data quality dimensions (DQD's) to automatize the DQR generation process. Rules optimization are applied on validated rules to avoid multi-passes pre-processing activities and eliminates duplicate rules. Conducted experiments showed an increased quality scores after applying the discovered and optimized DQR's on data.", "articleno": "", "ISSN": "", "month": "June", "number": "", "volume": "", "publicationtype": "inproceedings", "author": ["Taleb, Ikbal", "Serhani, Mohamed"]}]["8386521", {"address": "", "booktitle": "2018 IEEE 3rd International Conference on Cloud Computing and Big Data Analysis (ICCCBDA)", "doi": "10.1109/ICCCBDA.2018.8386521", "isbn": "", "keywords": ["Data integrity", "Power quality", "Monitoring", "Power measurement", "Redundancy", "Big Data", "Business", "power qualiy", "data quality", "big data", "data provenance", "data assessment"], "location": "", "numpages": "", "pages": "248-252", "publisher": "", "series": "", "title": "Data quality assessment for on-line monitoring and measuring system of power quality based on big data and data provenance theory", "url": "", "year": "2018", "abstract": "Currently, on-line monitoring and measuring system of power quality has accumulated a huge amount of data. In the age of big data, those data integrated from various systems will face big data application problems. This paper proposes a data quality assessment system method for on-line monitoring and measuring system of power quality based on big data and data provenance to assess integrity, redundancy, accuracy, timeliness, intelligence and consistency of data set and single data. Specific assessment rule which conforms to the situation of on-line monitoring and measuring system of power quality will be devised to found data quality problems. Thus it will provide strong data support for big data application of power quality.", "articleno": "", "ISSN": "", "month": "April", "number": "", "volume": "", "publicationtype": "inproceedings", "author": ["Hongxun, Tian", "Honggang, Wang", "Kun, Zhou", "Mingtai, Shi", "Haosong, Li", "Zhongping, Xu", "Taifeng, Kang", "Jin, Li", "Yaqi, Cai"]}]["9742286", {"address": "", "booktitle": "2021 2nd International Conference on Electronics, Communications and Information Technology (CECIT)", "doi": "10.1109/CECIT53797.2021.00045", "isbn": "", "keywords": ["Switches", "Data systems", "Data models", "Information and communication technology", "component", "quality data", "data system", "intelligent selection"], "location": "", "numpages": "", "pages": "218-223", "publisher": "", "series": "", "title": "An Intelligent Selection Method Based On Electronic Component Quality Data System", "url": "", "year": "2021", "abstract": "Aiming at the difficulty of electronic component quality data management and application, and the lack of data system and application methods required for data management in selection scenarios, this paper proposes an intelligent selection method based on electronic component quality data system, uses Bi-LSTM-ATT model for entity identification, and identifies data association based on entity relationship. By calculating the Tanimoto coefficient, the intelligent matching and push of similar products and substitute products are realized, and the intellectualization of component selection is fully supported. Finally, taking the scenario of fast switching diode selection as an example, the feasibility of the method proposed in this paper is verified, which provides a model for the intelligent application of quality data resources.", "articleno": "", "ISSN": "", "month": "Dec", "number": "", "volume": "", "publicationtype": "inproceedings", "author": ["Wu, Xiangwei", "Yang, Hongqi", "Liu, Yuke", "Nie, Guojia", "Yang, Lihao", "Yang, Yun"]}]["8078781", {"address": "", "booktitle": "2017 IEEE 2nd International Conference on Big Data Analysis (ICBDA)", "doi": "10.1109/ICBDA.2017.8078781", "isbn": "", "keywords": ["Filling", "Algorithm design and analysis", "Noise reduction", "Training", "Clustering algorithms", "Inspection", "Big Data", "Big data of quality inspection", "Stacked denoising auto-encoder", "Filling algorithm"], "location": "", "numpages": "", "pages": "84-88", "publisher": "", "series": "", "title": "Missing data of quality inspection imputation algorithm base on stacked denoising auto-encoder", "url": "", "year": "2017", "abstract": "Analyzing and processing big data of quality inspection is the key factor in ensuring product quality and People's property security. Big data of quality inspection collected by social network and E-commerce is missing in most cases. And the incompleteness of data brings huge challenge for analyzing and processing. Therefore, the algorithm of data filling based on stacked denoising auto-encoder is proposed in this text. As the experiment shows that the algorithm proposed in this text is effective in dealing with big data of quality inspection.", "articleno": "", "ISSN": "", "month": "March", "number": "", "volume": "", "publicationtype": "inproceedings", "author": ["Ning, Xiuli", "Xu, Yingcheng", "Gao, Xiaohong", "Li, Ying"]}]["8332632", {"address": "", "booktitle": "2017 14th Web Information Systems and Applications Conference (WISA)", "doi": "10.1109/WISA.2017.29", "isbn": "", "keywords": ["Big Data", "Data integrity", "Power grids", "History", "Real-time systems", "Sensors", "data quality", "electric power data", "data quality assessment", "big data", "framework"], "location": "", "numpages": "", "pages": "289-292", "publisher": "", "series": "", "title": "A Big Data Framework for Electric Power Data Quality Assessment", "url": "", "year": "2017", "abstract": "Since a low-quality data may influence the effectiveness and reliability of applications, data quality is required to be guaranteed. Data quality assessment is considered as the foundation of the promotion of data quality, so it is essential to access the data quality before any other data related activities. In the electric power industry, more and more electric power data is continuously accumulated, and many electric power applications have been developed based on these data. In China, the power grid has many special characteristic, traditional big data assessment frameworks cannot be directly applied. Therefore, a big data framework for electric power data quality assessment is proposed. Based on big data techniques, the framework can accumulate both the real-time data and the history data, provide an integrated computation environment for electric power big data assessment, and support the storage of different types of data.", "articleno": "", "ISSN": "", "month": "Nov", "number": "", "volume": "", "publicationtype": "inproceedings", "author": ["Liu, He", "Huang, Fupeng", "Li, Han", "Liu, Weiwei", "Wang, Tongxun"]}]["9148352", {"address": "", "booktitle": "2020 International Conference on Computer Information and Big Data Applications (CIBDA)", "doi": "10.1109/CIBDA50819.2020.00024", "isbn": "", "keywords": ["Data visualization", "Aquaculture", "Data mining", "Big Data", "Neural networks", "Data models", "Predictive models", "aquaculture", "big data", "water quality warning", "data visualization"], "location": "", "numpages": "", "pages": "70-74", "publisher": "", "series": "", "title": "Implementation of Water Quality Management Platform for Aquaculture Based on Big Data", "url": "", "year": "2020", "abstract": "In order to ensure the quality and quantity of aquaculture, aquaculture farmers need to grasp the water quality in time. However, most farmers have to collect water quality data manually at present, and cannot store and reuse that information rapidly. This paper aims to use SpringBoot framework and JPA framework to build a big data platform of acquisition automation and visualization, which realizes the data analysis and display of heterogeneous water quality and breeding information. The platform can make the water quality prediction and real-time warning. Meanwhile, it realizes the management of robots, users and breeding experts. The application of this platform will bring better social benefits to aquaculture farmers.", "articleno": "", "ISSN": "", "month": "April", "number": "", "volume": "", "publicationtype": "inproceedings", "author": ["Peng, Zhibin", "Chen, Yuefeng", "Zhang, Zehong", "Qiu, Queling", "Han, Xiaoqiang"]}]["9378148", {"address": "", "booktitle": "2020 IEEE International Conference on Big Data (Big Data)", "doi": "10.1109/BigData50022.2020.9378148", "isbn": "", "keywords": ["Java", "Quality assurance", "Semantics", "Big Data", "Tools", "Syntactics", "Cleaning", "Data Collection", "Data Analytics", "Model-Driven Development", "Historical Data", "Data Parsing", "Data Cleaning", "Ethics", "Data Assurance", "Data Quality"], "location": "", "numpages": "", "pages": "1914-1923", "publisher": "", "series": "", "title": "Towards Automatic Data Cleansing and Classification of Valid Historical Data An Incremental Approach Based on MDD", "url": "", "year": "2020", "abstract": "The project Death and Burial Data: Ireland 1864-1922 (DBDIrl) examines the relationship between historical death registration data and burial data to explore the history of power in Ireland from 1864 to 1922. Its core Big Data arises from historical records from a variety of heterogeneous sources, some aspects are pre-digitized and machine readable. A huge data set (over 4 million records in each source) and its slow manual enrichment (ca 7,000 records processed so far) pose issues of quality, scalability, and creates the need for a quality assurance technology that is accessible to non-programmers. An important goal for the researcher community is to produce a reusable, high-level quality assurance tool for the ingested data that is domain specific (historic data), highly portable across data sources, thus independent of storage technology.This paper outlines the step-wise design of the finer granular digital format, aimed for storage and digital archiving, and the design and test of two generations of the techniques, used in the first two data ingestion and cleaning phases.The first small scale phase was exploratory, based on metadata enrichment transcription to Excel, and conducted in parallel with the design of the final digital format and the discovery of all the domain-specific rules and constraints for the syntax and semantic validity of individual entries. Excel embedded quality checks or database-specific techniques are not adequate due to the technology independence requirement. This first phase produced a Java parser with an embedded data cleaning and evaluation classifier, continuously improved and refined as insights grew. The next, larger scale phase uses a bespoke Historian Web Application that embeds the Java validator from the parser, as well as a new Boolean classifier for valid and complete data assurance built using a Model-Driven Development technique that we also describe. This solution enforces property constraints directly at data capture time, removing the need for additional parsing and cleaning stages. The new classifier is built in an easy to use graphical technology, and the ADD-Lib tool it uses is a modern low-code development environment that auto-generates code in a large number of programming languages. It thus meets the technology independence requirement and historians are now able to produce new classifiers themselves without being able to program. We aim to infuse the project with computational and archival thinking in order to produce a robust data set that is FAIR compliant (Free Accessible Inter-operable and Re-useable).", "articleno": "", "ISSN": "", "month": "Dec", "number": "", "volume": "", "publicationtype": "inproceedings", "author": ["O\u2019Shea, Enda", "Khan, Rafflesia", "Breathnach, Ciara", "Margaria, Tiziana"]}]["9332030", {"address": "", "booktitle": "2020 3rd International Conference on Information and Communications Technology (ICOIACT)", "doi": "10.1109/ICOIACT50329.2020.9332030", "isbn": "", "keywords": ["Data integrity", "Government", "Decision making", "Data integration", "Information and communication technology", "Data cleansing", "Data quality", "Data quality management", "Null cleansing", "Pentaho data integration"], "location": "", "numpages": "", "pages": "12-16", "publisher": "", "series": "", "title": "Implementation of Data Cleansing Null Method for Data Quality Management Dashboard using Pentaho Data Integration", "url": "", "year": "2020", "abstract": "Data is a collection of facts or information collected from various sources that are dirty and will affect the quality of decision-making in an organization. Data cleansing ensures that the data is correct, useable, and consistent. Data may be incomplete, inaccurate, or has the wrong format and needs to be corrected or deleted. Data cleansing processing can improve the quality of the data significantly. The data cleansing processing requires to create useful quality data that provides significant benefits for the recipient. The availability of data is crucial in an organization to develop competent, valid, and trustworthy decisions. The null or blank field in data is one of many problems to maintain data quality management in an organization, especially in Indonesian government agencies. The brand registration number permits contain many blank fields, including the complete data needed for the next step processing. Therefore, to solve the amount of blank data, this research will discuss the design and implementation of the data cleansing null method using Pentaho Data Integration (PDI). The result will be implemented to the data quality management (DQM) dashboard using the laravel framework and MySQL as a DBMS.", "articleno": "", "ISSN": "", "month": "Nov", "number": "", "volume": "", "publicationtype": "inproceedings", "author": ["Sulistyo, Haidar", "Kusumasari, Tien", "Alam, Ekky"]}]["9322890", {"address": "", "booktitle": "2020 International Conference Quality Management, Transport and Information Security, Information Technologies (IT&QM&IS)", "doi": "10.1109/ITQMIS51053.2020.9322890", "isbn": "", "keywords": ["Quality management", "Process control", "Digital transformation", "Information technology", "Task analysis", "Information security", "Companies", "digital technologies", "quality", "quality management", "digital quality management system DQMS", "aerospace", "life cycle", "digital transformation", "Big data", "control of technological processes", "Internet of things"], "location": "", "numpages": "", "pages": "3-5", "publisher": "", "series": "", "title": "The Prospects for the Creation of a Digital Quality Management System DQMS", "url": "", "year": "2020", "abstract": "The development of digital technologies can give a new impetus to the development of quality management (QM). The development of new approaches based on the integration of quality management methods and digital technologies creates prerequisites for the digital transformation of the entire product lifecycle. The difficulty of creating an effective quality management system using digital technologies is not only in the absence of specialists in two areas of knowledge simultaneously, but also in the lack of integration of modern quality management methods with existing software products. In most ready-made solutions, quality management is limited to controlling process parameters and product quality. Automatic registration of process parameters with real-time data analysis should be additionally enabled in DQMS. This will allow you to organize monitoring and control of processes at each automated workplace. The accumulated analysis results will help you make decisions in difficult situations. A set of processes with digital control and analysis ensures quality assurance at all stages of the product lifecycle.", "articleno": "", "ISSN": "", "month": "Sep.", "number": "", "volume": "", "publicationtype": "inproceedings", "author": ["Vasiliev, Victor", "Aleksandrova, Svetlana"]}]["4424094", {"address": "", "booktitle": "2007 9th International Conference on Electrical Power Quality and Utilisation", "doi": "10.1109/EPQU.2007.4424094", "isbn": "", "keywords": ["Data systems", "Monitoring", "Power quality", "Substations", "Power measurement", "Electric variables measurement", "Data analysis", "Current measurement", "Frequency measurement", "Data acquisition", "power quality", "data acquisition", "monitoring", "data system", "big consumers"], "location": "", "numpages": "", "pages": "1-5", "publisher": "", "series": "", "title": "Data system for the monitoring of power quality in the transmission substations supplying big consumers", "url": "", "year": "2007", "abstract": "During 2006, at CN Transelectrica - OMEPA Branch request, ISPE - Power Systems Department has conceived the designing documentations (Feasibility Study and Tender Documents) for \u201cPower Quality Analyzing System at the big consumers\u201d. The present paper reports the purpose and technical endowment proposed by ISPE for \u201cPower Quality Monitoring and Analyzing System\u201d that will be developed at OMEPA.", "articleno": "", "ISSN": "2150-6655", "month": "Oct", "number": "", "volume": "", "publicationtype": "inproceedings", "author": ["Vatra, Fanica", "Poida, Ana", "Stanescu, Carmen"]}]["9422032", {"address": "", "booktitle": "2020 2nd International Conference on Information Technology and Computer Application (ITCA)", "doi": "10.1109/ITCA52113.2020.00070", "isbn": "", "keywords": ["Measurement", "Analytical models", "Standards organizations", "Data integration", "Systems architecture", "Financial management", "Information age", "data link", "OLAP", "multidimensional data model"], "location": "", "numpages": "", "pages": "298-302", "publisher": "", "series": "", "title": "Multi dimensional data distribution monitoring based on OLAP", "url": "", "year": "2020", "abstract": "With the rapid development of the Internet, society is gradually entering the information age, and various data in enterprises have become the most important strategic core resources of all enterprises. The operation and decision-making of enterprises all require a large amount of data analysis. Nowadays, many companies do not pay enough attention to the monitoring of data asset distribution. In addition, various internal systems such as financial management and ERP systems are relatively independent. Each system has its own data organization standard, which makes it difficult to conduct a unified management of data. This also directly leads to the one-sided and subjective problem of enterprise managers' distribution of data assets. With the construction of the data center of each enterprise, the data of each system is aggregated to the center through data integration technology. Therefore, all enterprises need to build a multi-dimensional data distribution monitoring model around data links to comprehensively monitor the status of various data distributions across the company's entire network, and improve data service capabilities and sharing capabilities as well as the company's operational capabilities. This article uses OLAP technology to construct a multi-dimensional data distribution monitoring model for the data link in the process of power enterprise data integration. This article first selects the dimensions and metrics that need to be monitored in the multidimensional data, and then constructs the conceptual model, logical model and physical model of the multidimensional data using on line analytical processing technology. Finally, an example analysis of OLAP system architecture based on B/S structure is realized. The overall data distribution of the enterprise can be grasped by analyzing the various dimensions of the data link, such as System type, location distribution, and time.", "articleno": "", "ISSN": "", "month": "Dec", "number": "", "volume": "", "publicationtype": "inproceedings", "author": ["Mao, Yifan", "Huang, Shasha", "Cui, Shuo", "Wang, HaiFeng", "Zhang, Junyan", "Ding, Wenhao"]}]["8109169", {"address": "", "booktitle": "2017 IEEE 13th International Conference on e-Science (e-Science)", "doi": "10.1109/eScience.2017.64", "isbn": "", "keywords": ["Quality control", "Automation", "Legged locomotion", "Carbon", "Soil", "Ecosystems", "data quality", "quality assurance", "quality control", "observational data", "research infrastructures"], "location": "", "numpages": "", "pages": "446-447", "publisher": "", "series": "", "title": "Hunting Data Rogues at Scale: Data Quality Control for Observational Data in Research Infrastructures", "url": "", "year": "2017", "abstract": "Data quality control is one of the most time consuming activities within Research Infrastructures (RIs), especially when involving observational data and multiple data providers. In this work we report on our ongoing development of data rogues, a scalable approach to manage data quality issues for observational data within RIs. The motivation for this work started with the creation of the FLUXNET2015 dataset, which includes carbon, water, and energy fluxes plus micrometeorological and ancillary data measured in over 200 sites around the world. To create an uniform dataset, including derived data products, extensive work on data quality control was needed. The unpredictable nature of observational data quality issues makes the automation of data quality control inherently difficult. Developed based on this experience, the data rogues methodology allows for increased automation of quality control activities by systematically identifying, cataloging, and documenting implementations of solutions to data issues. We believe this methodology can be extended and applied to others domains and types of data, making the automation of data quality control a more tractable problem.", "articleno": "", "ISSN": "", "month": "Oct", "number": "", "volume": "", "publicationtype": "inproceedings", "author": ["Pastorello, Gilberto", "Gunter, Dan", "Chu, Housen", "Christianson, Danielle", "Trotta, Carlo", "Canfora, Eleonora", "Faybishenko, Boris", "Cheah, You-Wei", "Beekwilder, Norm", "Chan, Stephen", "Dengel, Sigrid", "Keenan, Trevor", "O'Brien, Fianna", "Elbashandy, Abdelrahman", "Poindexter, Cristina", "Humphrey, Marty", "Papale, Dario", "Agarwal, Deb"]}]["8674323", {"address": "", "booktitle": "2018 6th International Conference on Cyber and IT Service Management (CITSM)", "doi": "10.1109/CITSM.2018.8674323", "isbn": "", "keywords": ["Data integrity", "Data models", "Organizations", "Standards organizations", "Capability maturity model", "Protocols", "data quality", "data quality management", "maturity model", "data quality maturity model"], "location": "", "numpages": "", "pages": "1-4", "publisher": "", "series": "", "title": "Data Quality Management Maturity Model: A Case Study in BPS-Statistics of Kaur Regency, Bengkulu Province, 2017", "url": "", "year": "2018", "abstract": "Data are widely used in an organization not only for operation but also for strategic level use. Poor data quality can have negative impact for an organization such as poor decision making and planning. Therefore, data quality management becomes an issue growing today not only to the academic but also professional communities. Based on this issue, this paper presents and analyzes a case study developed in a governmental agency, BPS-Statistics of Kaur Regency. For analysis, a data quality maturity model is used to measure the implementation of data quality management in the organization. The results show that for the dimension of `Data quality expectations' is at a maturity of 4.25. `Data quality protocol' is at a maturity of 3.50. `Policies' reaches a maturity of 3.67. `Data quality protocol' and `Data standard' are at a maturity of 4.42. `Data governance' is at a maturity of 3.00. `Technology' is at a maturity 3.17. `Performance management' is at a maturity of 3.33. However, this also implies that implementing these particular dimensions will lead to a direct increase in overall maturity.", "articleno": "", "ISSN": "", "month": "Aug", "number": "", "volume": "", "publicationtype": "inproceedings", "author": ["Sabtiana, Rela", "Yudhoatmojo, Satrio", "Hidayanto, Achmad"]}]["8078796", {"address": "", "booktitle": "2017 IEEE 2nd International Conference on Big Data Analysis (ICBDA)", "doi": "10.1109/ICBDA.2017.8078796", "isbn": "", "keywords": ["Data engineering", "Data analysis", "Data integration", "Big Data", "Distributed databases", "Data models", "Uncertainty", "army data engineering", "data management", "data integration", "data analysis", "representation of data analysis results", "data quality"], "location": "", "numpages": "", "pages": "149-152", "publisher": "", "series": "", "title": "Some key problems of data management in army data engineering based on big data", "url": "", "year": "2017", "abstract": "This paper analyzed the challenges of data management in army data engineering, such as big data volume, data heterogeneous, high rate of data generation and update, high time requirement of data processing, and widely separated data sources. We discussed the disadvantages of traditional data management technologies to deal with these problems. We also highlighted the key problems of data management in army data engineering including data integration, data analysis, representation of data analysis results, and evaluation of data quality.", "articleno": "", "ISSN": "", "month": "March", "number": "", "volume": "", "publicationtype": "inproceedings", "author": ["HongJu, Xiao", "Fei, Wang", "FenMei, Wang", "XiuZhen, Wang"]}]["9006358", {"address": "", "booktitle": "2019 IEEE International Conference on Big Data (Big Data)", "doi": "10.1109/BigData47090.2019.9006358", "isbn": "", "keywords": ["Data integrity", "Instruments", "Atmospheric measurements", "Dictionaries", "Big Data", "Portals", "scientific data workflows", "data quality", "provenance", "atmospheric science"], "location": "", "numpages": "", "pages": "3260-3266", "publisher": "", "series": "", "title": "Provenance\u2013aware workflow for data quality management and improvement for large continuous scientific data streams", "url": "", "year": "2019", "abstract": "Data quality assessment, management and improvement is an integral part of any big data intensive scientific research to ensure accurate, reliable, and reproducible scientific discoveries. The task of maintaining the quality of data, however, is non-trivial and poses a challenge for a program like the Department of Energy's Atmospheric Radiation Measurement (ARM) that collects data from hundreds of instruments across the world, and distributes thousands of streaming data products that are continuously produced in near-real-time for an archive 1.7 Petabyte in size and growing. In this paper, we present a computational data processing workflow to address the data quality issues via an easy and intuitive web-based portal that allows reporting of any quality issues for any site, facility or instruments at a granularity down to individual variables in the data files. This portal allows instrument specialists and scientists to provide corrective actions in the form of symbolic equations. A parallel processing framework applies the data improvement to a large volume of data in an efficient, parallel environment, while optimizing data transfer and file I/O operations; corrected files are then systematically versioned and archived. A provenance tracking module tracks and records any change made to the data during its entire life cycle which are communicated transparently to the scientific users. Developed in Python using open source technologies, this software architecture enables fast and efficient management and improvement of data in an operational data center environment.", "articleno": "", "ISSN": "", "month": "Dec", "number": "", "volume": "", "publicationtype": "inproceedings", "author": ["Kumar, Jitendra", "Crow, Michael", "Devarakonda, Ranjeet", "Giansiracusa, Michael", "Guntupally, Kavya", "Olatt, Joseph", "Price, Zach", "Shanafield, Harold", "Singh, Alka"]}]["5768635", {"address": "", "booktitle": "2011 International Conference on Consumer Electronics, Communications and Networks (CECNet)", "doi": "10.1109/CECNET.2011.5768635", "isbn": "", "keywords": ["Quality control", "Data models", "Unified modeling language", "Solid modeling", "Product design", "Design automation", "quality control", "data model", "structure mapping model"], "location": "", "numpages": "", "pages": "2082-2085", "publisher": "", "series": "", "title": "Data model for product life cycle quality control mapping with product structure geometrical model", "url": "", "year": "2011", "abstract": "This paper focus on the technique of learning the knowledge on the design quality and manufacture quality. The characteristic of quality control are illustrated, such as status, process, structure etc, and a lot of feature of the quality control information which consist of geometrical structure model, manufacture technique, detect, fault diagnose and data analysis were presented. Then an approach of the mapping between the product quality control information to the components geometrical model is put out, which can be implemented to optimal the product design, manufacture and assembly in quality control. Finally a prototype system was designed based on the data model.", "articleno": "", "ISSN": "", "month": "April", "number": "", "volume": "", "publicationtype": "inproceedings", "author": ["Jihong, Shan", "Yalang, Mao", "Yi, Sun"]}]["7584971", {"address": "", "booktitle": "2016 IEEE International Congress on Big Data (BigData Congress)", "doi": "10.1109/BigDataCongress.2016.65", "isbn": "", "keywords": ["Big data", "Metadata", "Measurement", "Quality assessment", "Quality of service", "Unified modeling language", "Big Data", "Quality assessment", "Metadata", "Quality metrics", "quality Metadata", "Quality of process", "Hybrid quality assessment"], "location": "", "numpages": "", "pages": "418-425", "publisher": "", "series": "", "title": "An Hybrid Approach to Quality Evaluation across Big Data Value Chain", "url": "", "year": "2016", "abstract": "While the potential benefits of Big Data adoption are significant, and some initial successes have already been realized, there remain many research and technical challenges that must be addressed to fully realize this potential. The Big Data processing, storage and analytics, of course, are major challenges that are most easily recognized. However, there are additional challenges related for instance to Big Data collection, integration, and quality enforcement. This paper proposes a hybrid approach to Big Data quality evaluation across the Big Data value chain. It consists of assessing first the quality of Big Data itself, which involve processes such as cleansing, filtering and approximation. Then, assessing the quality of process handling this Big Data, which involve for example processing and analytics process. We conduct a set of experiments to evaluate Quality of Data prior and after its pre-processing, and the Quality of the pre-processing and processing on a large dataset. Quality metrics have been measured to access three Big Data quality dimensions: accuracy, completeness, and consistency. The results proved that combination of data-driven and process-driven quality evaluation lead to improved quality enforcement across the Big Data value chain. Hence, we recorded high prediction accuracy and low processing time after we evaluate 6 well-known classification algorithms as part of processing and analytics phase of Big Data value chain.", "articleno": "", "ISSN": "", "month": "June", "number": "", "volume": "", "publicationtype": "inproceedings", "author": ["Serhani, Mohamed", "El, Hadeel", "Taleb, Ikbal", "Nujum, Alramzana"]}]["8532518", {"address": "", "booktitle": "2018 IEEE AUTOTESTCON", "doi": "10.1109/AUTEST.2018.8532518", "isbn": "", "keywords": ["Data integrity", "Manufacturing", "Measurement", "Decision making", "Production facilities", "Data models", "manufacturing test", "data quality", "test data quality", "cost of data quality"], "location": "", "numpages": "", "pages": "1-6", "publisher": "", "series": "", "title": "Measuring Manufacturing Test Data Analysis Quality", "url": "", "year": "2018", "abstract": "Manufacturing test data volumes are constantly increasing. While there has been extensive focus in the literature on big data processing, less focus has existed on data quality, and considerably less focus has been placed specifically on manufacturing test data quality. This paper presents a fully automated test data quality measurement developed by the authors to facilitate analysis of manufacturing test operations, resulting in a single number used to compare manufacturing test data quality across programs and factories, and focusing effort cost-effectively. The automation enables program and factory users to see, understand, and improve their test data quality directly. Immediate improvements in test data quality speed manufacturing test operation analysis, reducing elapsed time and overall spend in test operations. Data quality has significant financial impacts to businesses [1]. While manufacturing cost models are well understood, data quality cost models are less well understood (see Eppler & Helfert [2] who review manufacturing cost models and create a taxonomy for data quality costs). Kim & Choi [3] discuss measuring data quality costs, and a rudimentary data quality cost calculation is described in [4]. Haug et al. [5] describe a classification of costs for poor data quality, and while they do not provide a cost calculation, they do define optimality for data quality. Laranjeiro et al. [6] have a recent survey of poor data quality classification. Ge & Helfert [7] extend the work in [2], and provide an updated review of data quality costs. Test data is specifically addressed in the context of data processing in [8]. Big data quality efforts are reviewed in [9, 10]. Data quality metrics are discussed in [11], and requirements for data quality metrics are identified in [12]. Data inconsistencies are detailed in [13], while categorical data inconsistencies are explained in [14]. In the current work, manufacturing test data quality is directly correlated to the speed of manufacturing test operations analysis. A measurement for manufacturing test data quality indicates the speed at which analysis can be performed, and increases in the test data quality score have precipitated increases in the speed of analysis, described herein.", "articleno": "", "ISSN": "1558-4550", "month": "Sep.", "number": "", "volume": "", "publicationtype": "inproceedings", "author": ["Burkhardt, Andrew", "Berryman, Sheila", "Brio, Ashley", "Ferkau, Susan", "Hubner, Gloria", "Lynch, Kevin", "Mittman, Susan", "Sonderer, Kathy"]}]["7816918", {"address": "", "booktitle": "2016 Intl IEEE Conferences on Ubiquitous Intelligence & Computing, Advanced and Trusted Computing, Scalable Computing and Communications, Cloud and Big Data Computing, Internet of People, and Smart World Congress (UIC/ATC/ScalCom/CBDCom/IoP/SmartWorld)", "doi": "10.1109/UIC-ATC-ScalCom-CBDCom-IoP-SmartWorld.2016.0122", "isbn": "", "keywords": ["Big data", "Measurement", "Feature extraction", "Quality assessment", "Social network services", "Companies", "Context", "Big Data", "data quality dimensions", "data quality evaluation", "Big data sampling"], "location": "", "numpages": "", "pages": "759-765", "publisher": "", "series": "", "title": "Big Data Quality: A Quality Dimensions Evaluation", "url": "", "year": "2016", "abstract": "Data is the most valuable asset companies are proud of. When its quality degrades, the consequences are unpredictable, can lead to complete wrong insights. In Big Data context, evaluating the data quality is challenging, must be done prior to any Big data analytics by providing some data quality confidence. Given the huge data size, its fast generation, it requires mechanisms, strategies to evaluate, assess data quality in a fast, efficient way. However, checking the Quality of Big Data is a very costly process if it is applied on the entire data. In this paper, we propose an efficient data quality evaluation scheme by applying sampling strategies on Big data sets. The Sampling will reduce the data size to a representative population samples for fast quality evaluation. The evaluation targeted some data quality dimensions like completeness, consistency. The experimentations have been conducted on Sleep disorder's data set by applying Big data bootstrap sampling techniques. The results showed that the mean quality score of samples is representative for the original data, illustrate the importance of sampling to reduce computing costs when Big data quality evaluation is concerned. We applied the Quality results generated as quality proposals on the original data to increase its quality.", "articleno": "", "ISSN": "", "month": "July", "number": "", "volume": "", "publicationtype": "inproceedings", "author": ["Taleb, Ikbal", "Kassabi, Hadeel", "Serhani, Mohamed", "Dssouli, Rachida", "Bouhaddioui, Chafik"]}]["9323615", {"address": "", "booktitle": "IGARSS 2020 - 2020 IEEE International Geoscience and Remote Sensing Symposium", "doi": "10.1109/IGARSS39084.2020.9323615", "isbn": "", "keywords": ["Data integrity", "Machine learning", "Earth", "Satellites", "Big Data", "Process control", "Monitoring", "Big Data", "Machine Learning", "Earth Observation Data", "Data management", "Data Quality", "Random Forest"], "location": "", "numpages": "", "pages": "3101-3103", "publisher": "", "series": "", "title": "A Machine Learning Approach for Data Quality Control of Earth Observation Data Management System", "url": "", "year": "2020", "abstract": "In the big data era, innovative technologies like cloud computing, artificial intelligence, and machine learning are increasingly utilized in the large-scale data management systems of many industry sectors to make them more scalable and intelligent. Applying them to automate and optimize earth observation data management is a hot topic. To improve data quality control mechanisms, a machine learning method in combination with built-in quality rules is presented in this paper to evolve processes around data quality and enhance management of earth observation data. The rules of quality check are set up to detect the common issues, including data completeness, data latency, bad data, and data duplication, and the machine learning model is trained, tested, and deployed to address these quality issues automatically and reduce manual efforts.", "articleno": "", "ISSN": "2153-7003", "month": "Sep.", "number": "", "volume": "", "publicationtype": "inproceedings", "author": ["Han, Weiguo", "Jochum, Matthew"]}]["9599117", {"address": "", "booktitle": "2021 IEEE 22nd International Conference on Information Reuse and Integration for Data Science (IRI)", "doi": "10.1109/IRI51335.2021.00024", "isbn": "", "keywords": ["Analytical models", "Phase measurement", "Data integrity", "Data acquisition", "Production", "Data science", "Data models", "Data quality", "data analysis quality", "industrial data science", "quality management", "quality assurance", "data quality criteria"], "location": "", "numpages": "", "pages": "131-138", "publisher": "", "series": "", "title": "Towards integrated Data Analysis Quality: Criteria for the application of Industrial Data Science", "url": "", "year": "2021", "abstract": "The application of Industrial Data Science in context of connected Smart Products requires modeling and structuring data for its design, development and use. Especially for Smart Products, a comprehensive handling of data quality is mandatory, because of their interdisciplinary character and broad range of heterogeneous stakeholders covering the entire product lifecycle. The overall goal of data preparation is to provide high-quality data for application and evaluation by users. Established process models for industrial data analysis often treat the specification and assurance of data quality as a single-point activity with a defined conclusion. Providing end-to-end data quality has received little attention in the field of industrial data analytics. In this paper, we will (1) structure four distinct phases for ensuring end-to-end data quality along data analytics activities, (2) define a set of criteria and measures for meeting and quantifying data quality requirements based on established criteria, and (3) provide a step-by-step model for establishing and maintaining high Data Quality for Industrial Data Science applications. The quality criteria aim to identify pointwise and continuous actions during the data analysis process. Such criteria target a shared responsibility for maintaining data quality during analyses between analyst and user. The developed model provides an actionable approach for assessing and ensuring the requirements of Data Analysis Quality.", "articleno": "", "ISSN": "", "month": "Aug", "number": "", "volume": "", "publicationtype": "inproceedings", "author": ["West, Nikolai", "Gries, Jonas", "Brockmeier, Carina", "G\u00f6bel, Jens", "Deuse, Jochen"]}]["6842887", {"address": "", "booktitle": "2014 16th International Conference on Harmonics and Quality of Power (ICHQP)", "doi": "10.1109/ICHQP.2014.6842887", "isbn": "", "keywords": ["Quality of service", "Data mining", "Data visualization", "Standards", "Visualization", "Voltage fluctuations", "Joining processes", "QoS", "PQ", "data mining", "SQL", "QoS reporting", "compliance to compatibility", "network risk management", "interactive data visualization", "contextualization", "data dashboards"], "location": "", "numpages": "", "pages": "44-48", "publisher": "", "series": "", "title": "Quality of supply data mining", "url": "", "year": "2014", "abstract": "Extracting useful network management information from a large volume of QoS data obtained all over a network can be simplified by innovative data mining techniques. The need for QoS expertise is reduced as interactive visualization by brushing and linking of datasets reveals interrelation of parameters. Data contextualization by annotated data can aid the assessment on the global level of compatibility between supply and use conditions. Data dashboards can further simplify the analysis of QoS data by recognizing the network connectivity of different sites, seasonal effects and direction of voltage waveform events.", "articleno": "", "ISSN": "2164-0610", "month": "May", "number": "", "volume": "", "publicationtype": "inproceedings", "author": ["Stander, Tiaan", "Rens, Johan"]}]["5557442", {"address": "", "booktitle": "2010 Third International Symposium on Electronic Commerce and Security", "doi": "10.1109/ISECS.2010.15", "isbn": "", "keywords": ["Data models", "Databases", "Quality assessment", "Accuracy", "Dictionaries", "Quality control", "data", "data quality", "metadata", "rule base"], "location": "", "numpages": "", "pages": "29-32", "publisher": "", "series": "", "title": "A Noval Data Quality Controlling and Assessing Model Based on Rules", "url": "", "year": "2010", "abstract": "As a resource, data is the base for information construction and application. According to the principle of \u201cgarbage in and garbage out\u201d, it needs us to ensure data reliability, no errors and accurately reflect the real situation to support the right decisions. However, due to various reasons, it leads to poor quality of dirty data in existing system business, while the dirty data is an important factor which affects right decisions. For the above, in this paper, a metadata-based data quality rule base is created for improving traditional quality control model, a more practical application of the weighted assessment algorithm is proposed and a three-tier data quality assessment system model is constructed based on the study of definition and classification of quality, assessment algorithm, metadata and the control theory. This model is confirmed to achieve comprehensive quality of data management and control in oilfield practical applications.", "articleno": "", "ISSN": "", "month": "July", "number": "", "volume": "", "publicationtype": "inproceedings", "author": ["Man, Yuan", "Wei, Liu", "Gang, Huang", "Juntao, Gao"]}]["9671672", {"address": "", "booktitle": "2021 IEEE International Conference on Big Data (Big Data)", "doi": "10.1109/BigData52589.2021.9671672", "isbn": "", "keywords": ["Data integrity", "Decision making", "Process control", "Quality control", "Organizations", "Big Data", "Data models", "data quality control", "data quality assessment", "unsupervised learning", "anomaly detection", "automated data quality control"], "location": "", "numpages": "", "pages": "2327-2336", "publisher": "", "series": "", "title": "Unsupervised Anomaly Detection in Data Quality Control", "url": "", "year": "2021", "abstract": "Data is one of the most valuable assets of an organization and has a tremendous impact on its long-term success and decision-making processes. Typically, organizational data error and outlier detection processes perform manually and reactively, making them time-consuming and prone to human errors. Additionally, rich data types, unlabeled data, and increased volume have made such data more complex. Accordingly, an automated anomaly detection approach is required to improve data management and quality control processes. This study introduces an unsupervised anomaly detection approach based on models comparison, consensus learning, and a combination of rules of thumb with iterative hyper-parameter tuning to increase data quality. Furthermore, a domain expert is considered a human in the loop to evaluate and check the data quality and to judge the output of the unsupervised model. An experiment has been conducted to assess the proposed approach in the context of a case study. The experiment results confirm that the proposed approach can improve the quality of organizational data and facilitate anomaly detection processes.", "articleno": "", "ISSN": "", "month": "Dec", "number": "", "volume": "", "publicationtype": "inproceedings", "author": ["Poon, Lex", "Farshidi, Siamak", "Li, Na", "Zhao, Zhiming"]}]["6021598", {"address": "", "booktitle": "Proceedings of the 2011 International Conference on Electrical Engineering and Informatics", "doi": "10.1109/ICEEI.2011.6021598", "isbn": "", "keywords": ["ISO standards", "Quality management", "Data warehouses", "Organizations", "Planning", "Data models", "Standards organizations", "Data Quality", "Data Source", "Quality Management System", "Data Warehouse", "ISO 9001:2008"], "location": "", "numpages": "", "pages": "1-6", "publisher": "", "series": "", "title": "Managing Data Source quality for data warehouse in manufacturing services", "url": "", "year": "2011", "abstract": "Data quality and Data Source Management is one of the key success factors for data warehouse project. Many data warehouse projects fail due to poor quality of the data. It is believed that the problems can be fixed later and because of that, a lot of time will be spent to fix the error. If low quality data fed into the data warehouse system, the result will be not accurate if these data are used in the decision making. Many data warehouse and business intelligence projects failure are due to wrong or low quality data. Therefore this paper will underpins several aspects such as Total Data Quality Management (TDQM), ISO 9001:2008 and Quality Management System (QMS) in order to address data quality problems in the early stage and find out the best procedure to manage the data sources. To find a standard procedure in managing data source base on ISO 9001:2008 standard, process in managing data source is identified and a compared to the ISO 9001:2008 Quality Management System (QMS) requirements. As a result, this process is viewed as a kind of production process and relate to the concepts of quality management known from the manufacturing and service domain. More precisely, a high quality management system in managing data source is proposed. This system is based on ISO 9001:2008 standard and hopes it can help organizations in implementing and operating quality management system. By using ISO 9001:2008 framework to the process of managing data source, this approach will be similar to the manufacturing concept that has an added advantage when compared to traditional approaches in managing data source.", "articleno": "", "ISSN": "2155-6830", "month": "July", "number": "", "volume": "", "publicationtype": "inproceedings", "author": ["Idris, Norizam", "Ahmad, Kamsuriah"]}]["5540836", {"address": "", "booktitle": "2010 International Conference On Computer Design and Applications", "doi": "10.1109/ICCDA.2010.5540836", "isbn": "", "keywords": ["Multidimensional systems", "Data analysis", "Power quality", "Monitoring", "Voltage", "Power engineering computing", "Data mining", "Energy management", "Quality management", "Frequency", "Multidimensional Data Analysi", "Power Quality", "MS Analysis Services"], "location": "", "numpages": "", "pages": "V1-390-V1-393", "publisher": "", "series": "", "title": "Application and research of multidimensional data analysis in power quality", "url": "", "year": "2010", "abstract": "To extract potentially useful information from a large number of power quality monitoring data, this article carries out the analysis of power quality data by the use of multi-dimensional data analysis from multiple perspectives. Considering the data mining in data pretreatment, disturbance recognition related applications, then analyzes and builds a power quality cube. Through the cube analysis, the manager can have a comprehensive and detailed understanding of the trends of power quality data in order to operate in the power to make scientific decision-making.", "articleno": "", "ISSN": "", "month": "June", "number": "", "volume": "1", "publicationtype": "inproceedings", "author": ["Hongke, Han", "Linhai, Qi"]}]["8258221", {"address": "", "booktitle": "2017 IEEE International Conference on Big Data (Big Data)", "doi": "10.1109/BigData.2017.8258221", "isbn": "", "keywords": ["Data mining", "Medical diagnostic imaging", "Currencies", "Tools", "Medical services", "Measurement", "data quality", "decision quality", "healthcare", "clinical data", "EHRs", "data quality assurance", "stratified sampling", "bias"], "location": "", "numpages": "", "pages": "2612-2619", "publisher": "", "series": "", "title": "Is data quality enough for a clinical decision?: Apply machine learning and avoid bias", "url": "", "year": "2017", "abstract": "This paper provides a practical guideline for the assurance and (re-)usage of clinical data. It proposes a process which aims to provide a systematic data quality assurance even without involving a medical domain expert. Especially when (re-)using clinical data, data quality is an important topic because clinical data are not purposely collected. Therefore, data driven conclusions might be false, because a given dataset is not representative. These false data driven conclusions could even harm the life of patients. Thus, all researchers should adhere to some basic principles that can prevent false conclusions. Twelve empirical experiments were conducted in order to prove that my process is able to assure data quality with respect to the descriptive and predictive analysis. Descriptive results obtained by applying stratified sampling are conflicting in four out of nine population inputs. Sampling is carried based on the top ranked feature drawn by the Contextual Data Quality Assurance (CDQA). Between datasets these features are confirmed by the Mutual Data Quality Assurance (MDQA). Stratified sampled inputs improve predictive results compared to raw data. Both Area Under the Curve (AUC) scores and accuracy scores increase by three percent.", "articleno": "", "ISSN": "", "month": "Dec", "number": "", "volume": "", "publicationtype": "inproceedings", "author": ["Hee, Kim"]}]["8465410", {"address": "", "booktitle": "2018 International Conference on Advances in Big Data, Computing and Data Communication Systems (icABCD)", "doi": "10.1109/ICABCD.2018.8465410", "isbn": "", "keywords": ["Quality of service", "Big Data", "Business intelligence", "Structured Query Language", "Tools", "Sparks", "Service Quality Management", "In-Memory Big Data", "Business Intelligence", "Service Quality Index", "Over The Top Application (OTT)", "Data Traffic and ROI"], "location": "", "numpages": "", "pages": "1-8", "publisher": "", "series": "", "title": "Modeling of an Efficient Low Cost, Tree Based Data Service Quality Management for Mobile Operators Using in-Memory Big Data Processing and Business Intelligence use Cases", "url": "", "year": "2018", "abstract": "Network Operators are shifting their business interest towards Data services in a geometric progression manner, as Data services is becoming the major source of Telco revenue. The wide use of Data platforms; such as WhatsApp, Skype, Hangout and other Over the Top (OTT) voice applications over the traditional voice services is a clear indication that Network Operators need to adjust their business model and needs. And couple with the adoption of Smartphones usage which grows continuously year by year, this means more subscribers to manage, large amount of transactions generated, more network resources to be added and evidently more human technical expertise required to ensure good service quality. That has led to high investment on Robust Service Quality Management (SQM) and Customer Experience Management (CEM) to stay competitive in the market. The high investment is justified by the integration of Big Data Solutions, Machine Learning capabilities and good visualization of insight data. However, the Return on Investment (ROI) of the expensive systems are not as conspicuous as the provided functionalities and business rules. Therefore, in this paper an efficient model for low cost SQM system is presented, exploring the advantages of In-Memory Big Data processing and low cost business Intelligence tools to showcase how a good Service Quality Management can be implemented with no big investment.", "articleno": "", "ISSN": "", "month": "Aug", "number": "", "volume": "", "publicationtype": "inproceedings", "author": ["Ogudo, Kingsley", "Nestor, Dahj"]}]["7404600", {"address": "", "booktitle": "OCEANS 2015 - MTS/IEEE Washington", "doi": "10.23919/OCEANS.2015.7404600", "isbn": "", "keywords": ["Instruments", "Quality control", "Oceans", "Real-time systems", "Quality assurance", "Manuals", "Data models", "Data Quality Assurance", "Data Quality Control", "Automated data quality control", "Manual data quality control", "Cabled Ocean observatory", "data assessment annotations", "workflow"], "location": "", "numpages": "", "pages": "1-8", "publisher": "", "series": "", "title": "Data quality control and quality assurance practices for Ocean Networks Canada observatories", "url": "", "year": "2015", "abstract": "Cabled observatory installations permit the acquisition of large volumes of continuous, high-resolution data from in-situ instruments. This type of data acquisition presents new challenges and opportunities in the development of data quality assurance and quality control (QAQC) measures. Ocean Networks Canada (ONC) operates the world-leading NEPTUNE and VENUS cabled ocean observing systems in the NE Pacific, and a small seafloor observatory in the Canadian Arctic. ONC collects high-resolution, real-time data on physical, chemical, biological, and geological aspects of the ocean over long time periods, supporting research on complex earth and ocean processes with innovative methods. High quality research depends on high quality data, which in turn depends on robust data quality control practices. For the data to be useful to potential end users, they must be qualified under accepted international standards with additional metadata pertaining to methods of measurement, instrument calibrations, and subsequent data processing included. Ocean Network Canada's QAQC methodology presented here has been developed with the key objectives of maintaining consistency within a single data set and within a collection of data sets. The QAQC model also ensures that the end user has sufficient information on the quality and errors of the data to assess its suitability for their use. The data QAQC procedures and tools have the capability to associate distinctly different but related types of information with data to provide a systematic and timely examination of the measurements. Efforts have been taken to develop efficient and accurate data QAQC techniques and tools to ensure quality data delivery to the end users in a timely manner. The large volume of data coming from extremely complex, diverse, and unpredictable ocean environments has resulted in many challenges as well as opportunities to develop efficient and informative tools for data QAQC at ONC. This paper describes the current and future steps that ONC is undertaking to ensure that data delivered by the observatories are of high quality, easily accessible, and reliable.", "articleno": "", "ISSN": "", "month": "Oct", "number": "", "volume": "", "publicationtype": "inproceedings", "author": ["Abeysirigunawardena, Dilumie", "Jeffries, Marlene", "Morley, Michael", "Bui, Alice", "Hoeberechts, Maia"]}]["9821441", {"abstract": "This chapter examines quality in terms of these information quality (InfoQ) components: quality of the analysis goal, data quality, analysis quality and quality of utility. Although the quality of each of the individual components affects InfoQ, it is the combination of the four that determines the level of InfoQ. The chapter aims to help the reader understand the difference between the quality of a single component and that of InfoQ. In a sense, we build here on the seminal work of Jeffreys, de Finetti, Ramsey, Savage Luce, Raiffa, Shlaifer, Fishburn, and many others who developed decision theory over 75 years ago, integrating data\u2010driven inference and modeling with decision making. Our updated approach to InfoQ is predicated on modern technologies that provide online access to structured and unstructured data with advanced computational and visualization capabilities. In this context, InfoQ needs to be viewed in a wide context where data is transformed into insights that drive focused and adapted responses. This chapter provides the foundation for an expanded approach to InfoQ.", "booktitle": "Information Quality: The Potential of Data and Analytics to Generate Knowledge", "doi": "10.1002/9781118890622.ch2", "isbn": "9781118890646", "keywords": ["Data integrity", "Time measurement", "Data analysis", "Cleaning", "Task analysis", "Stakeholders", "Medical services"], "number": "", "pages": "18-30", "publisher": "Wiley", "title": "Quality of goal, data quality, and analysis quality", "url": "https://ieeexplore.ieee.org/document/9821441", "volume": "", "year": "2017", "publicationtype": "inbook", "author": ["Kenett, Ron", "Shmueli, Galit"]}]["7364060", {"address": "", "booktitle": "2015 IEEE International Conference on Big Data (Big Data)", "doi": "10.1109/BigData.2015.7364060", "isbn": "", "keywords": ["Documentation", "Data warehouses", "Medical services", "Big data", "Databases", "Cleaning", "Medical diagnostic imaging", "electronic health records", "data quality", "big data", "multiple data vendors", "metrics"], "location": "", "numpages": "", "pages": "2612-2620", "publisher": "", "series": "", "title": "Evaluation of data quality of multisite electronic health record data for secondary analysis", "url": "", "year": "2015", "abstract": "Currently, a large amount of data is amassed in electronic health records (EHRs). However, EHR systems are largely information silos, that is, uses of these systems are often confined to management of patient information and analytics specific to a clinician's practice. A growing trend in healthcare is combining multiple databases to support epidemiological research. The College Health Surveillance Network is the first national data warehouse containing EHR data from 31 different student health centers. Each member university contributes to the data warehouse by uploading select EHR data including patient demographics, diagnoses, and procedures to a common server on a monthly basis. In this paper, we focus on the data quality dimensions from a subsample of the data comprised of over 5.7 million patient visits for approximately 980,000 patients with 4,465 unique diagnoses from 23 of those universities. We examine the data for measures of completeness, consistency, and availability for secondary use for epidemiological research. Additionally, clinical documentation practices and EHR vendor were evaluated as potential contributors to data quality. We found that overall about 70% of the data in the data warehouse is available for secondary use, and identified clinical documentation practices that are correlated to a reduction in data quality. This suggests that automated quality control and proactive clinical documentation support could reduce ad-hoc data cleaning needs resulting in greater data availability for secondary use.", "articleno": "", "ISSN": "", "month": "Oct", "number": "", "volume": "", "publicationtype": "inproceedings", "author": ["Nobles, Alicia", "Vilankar, Ketki", "Wu, Hao", "Barnes, Laura"]}]["8457745", {"address": "", "booktitle": "2018 IEEE International Congress on Big Data (BigData Congress)", "doi": "10.1109/BigDataCongress.2018.00029", "isbn": "", "keywords": ["Big Data", "Data integrity", "Quality management", "Data visualization", "Databases", "Sensors", "Social network services", "Big Data, Data Quality, Quality Management framework, Quality of Big Data"], "location": "", "numpages": "", "pages": "166-173", "publisher": "", "series": "", "title": "Big Data Quality: A Survey", "url": "", "year": "2018", "abstract": "With the advances in communication technologies and the high amount of data generated, collected, and stored, it becomes crucial to manage the quality of this data deluge in an efficient and cost-effective way. The storage, processing, privacy and analytics are the main keys challenging aspects of Big Data that require quality evaluation and monitoring. Quality has been recognized by the Big Data community as an essential facet of its maturity. Yet, it is a crucial practice that should be implemented at the earlier stages of its lifecycle and progressively applied across the other key processes. The earlier we incorporate quality the full benefit we can get from insights. In this paper, we first identify the key challenges that necessitates quality evaluation. We then survey, classify and discuss the most recent work on Big Data management. Consequently, we propose an across-the-board quality management framework describing the key quality evaluation practices to be conducted through the different Big Data stages. The framework can be used to leverage the quality management and to provide a roadmap for Data scientists to better understand quality practices and highlight the importance of managing the quality. We finally, conclude the paper and point to some future research directions on quality of Big Data.", "articleno": "", "ISSN": "", "month": "July", "number": "", "volume": "", "publicationtype": "inproceedings", "author": ["Taleb, Ikbal", "Serhani, Mohamed", "Dssouli, Rachida"]}]["8276745", {"address": "", "booktitle": "2017 IEEE International Conference on Internet of Things (iThings) and IEEE Green Computing and Communications (GreenCom) and IEEE Cyber, Physical and Social Computing (CPSCom) and IEEE Smart Data (SmartData)", "doi": "10.1109/iThings-GreenCom-CPSCom-SmartData.2017.28", "isbn": "", "keywords": ["Data integrity", "Measurement", "Warranties", "Metadata", "Cleaning", "Big Data", "Heterogeneous Data Sets", "Data Quality", "Metadata", "Data Cleaning", "Data Quality Assessment"], "location": "", "numpages": "", "pages": "155-162", "publisher": "", "series": "", "title": "Towards a Data Quality Framework for Heterogeneous Data", "url": "", "year": "2017", "abstract": "Every industry has significant data output as a product of their working process, and with the recent advent of big data mining and integrated data warehousing it is the case for a robust methodology for assessing the quality for sustainable and consistent processing. In this paper a review is conducted on Data Quality (DQ) in multiple domains in order to propose connections between their methodologies. This critical review suggests that within the process of DQ assessment of heterogeneous data sets, not often are they treated as separate types of data in need of an alternate data quality assessment framework. We discuss the need for such a directed DQ framework and the opportunities that are foreseen in this research area and propose to address it through degrees of heterogeneity.", "articleno": "", "ISSN": "", "month": "June", "number": "", "volume": "", "publicationtype": "inproceedings", "author": ["Micic, Natasha", "Neagu, Daniel", "Campean, Felician", "Zadeh, Esmaeil"]}]["5360632", {"address": "", "booktitle": "2009 Sixth International Conference on Fuzzy Systems and Knowledge Discovery", "doi": "10.1109/FSKD.2009.842", "isbn": "", "keywords": ["Data mining", "Finance", "Quality assurance", "Information analysis", "Resource management", "Quality management", "Project management", "Quality assessment", "Frequency", "Fuzzy systems", "Auto finance marketing", "data mining", "data warehouse", "quality assessment", "information quality"], "location": "", "numpages": "", "pages": "188-191", "publisher": "", "series": "", "title": "Assuring the Information Quality in Data Mining for a Finance Company", "url": "", "year": "2009", "abstract": "This paper describes a information quality assurance exercise undertaken for a finance company as part of a larger project in auto finance marketing. A methodology to estimate the effects of data accuracy, completeness and consistency on the data aggregate functions count, sum and average is presented. This methodology should be of specific interest to quality assurance practitioners for projects that harvest warehouse data for decision support to the management. The assessment comprised ten checks in three broad categories, to ensure the quality of information collected over 1103 attributes. The assessment discovered four critical gaps in the data that had to be corrected before the data could be transitioned to the analysis phase.", "articleno": "", "ISSN": "", "month": "Aug", "number": "", "volume": "5", "publicationtype": "inproceedings", "author": ["Su, Ying", "Peng, Gongqian", "Jin, Zhanming"]}]["7077304", {"address": "", "booktitle": "2014 4th World Congress on Information and Communication Technologies (WICT 2014)", "doi": "10.1109/WICT.2014.7077304", "isbn": "", "keywords": ["Information systems", "Organizations", "Accuracy", "Quality assessment", "Correlation", "Data mining", "Data Quality", "Data quality Dimension", "Business Process Improvement", "Information System", "Data Quality Problem"], "location": "", "numpages": "", "pages": "70-73", "publisher": "", "series": "", "title": "The impact of data quality dimensions on business process improvement", "url": "", "year": "2014", "abstract": "Guaranteeing high data quality level is an important issue to increase the efficiency of the business processes. In fact, poor data quality produces wrong information, which leads to the failure of the business process improvement. Identifying data quality problems has positive impact on overall effectiveness and efficiency of the process improvement. In fact, improving data quality often requires modifying business process enriching them with the most improvement activities. Such activities depend and change based on the data quality dimensions. In this paper, we focus on review of the impact of data quality dimensions on business process improvement in order to support managers to facilitate the implementation of process improvement. The evaluations of this research will use to refine and extend knowledge of relationship between data quality dimensions and business process improvement.", "articleno": "", "ISSN": "", "month": "Dec", "number": "", "volume": "", "publicationtype": "inproceedings", "author": ["Panahy, Payam", "Sidi, Fatimah", "Affendey, Lilly", "Jabar, Marzanah"]}]["5593829", {"address": "", "booktitle": "2010 5th International Conference on Computer Science & Education", "doi": "10.1109/ICCSE.2010.5593829", "isbn": "", "keywords": ["Cotton", "Monitoring", "Data acquisition", "Production", "Raw materials", "Quality management", "Spinning", "Cotton Spinning Corporation", "Quality Data Acquisition", "Monitoring and Management", "Integration"], "location": "", "numpages": "", "pages": "1939-1942", "publisher": "", "series": "", "title": "Integrative system structure of quality data acquisition, monitoring and management oriented to cotton-spinning enterprise", "url": "", "year": "2010", "abstract": "Traditionally, for a cotton spinning corporation, its quality data acquisition, analysis, and monitoring were separated. In order to improve the quality management of the enterprise, this paper completes and depicts the framework construction of integrative system of quality data acquisition, analysis, and monitoring oriented to the cotton spinning corporations. It also elaborates the technology of quality data acquisition, analysis, and monitoring in the process of production management of this enterprise. The executive results shows that this system has enhanced the quality awareness, significantly increased the products quality, and reinforced the core competitiveness of the enterprise.", "articleno": "", "ISSN": "", "month": "Aug", "number": "", "volume": "", "publicationtype": "inproceedings", "author": ["Xiaoping, Zhao", "Zhenwang, Dong", "Chao, Luo"]}]["9026017", {"address": "", "booktitle": "2019 International Conference on Meteorology Observations (ICMO)", "doi": "10.1109/ICMO49322.2019.9026017", "isbn": "", "keywords": ["Quality control", "Inspection", "Process control", "Standards", "Instruments", "Atmosphere", "Data integrity", "Quality Control", "Reactive Gas", " Control Method", "Quality Control, Reactive Gas", "Control Method"], "location": "", "numpages": "", "pages": "1-4", "publisher": "", "series": "", "title": "Analysis of Quality Control Effect of Reactive Gas Observation Data Based on Multiple Data Quality Control Methods", "url": "", "year": "2019", "abstract": "The quality control of observation data is an important link in the entire meteorological observation service system chain. The reactive gas observation is different from the observation of other meteorological elements in general, especially the special operations such as periodic zero / span inspection and multi-point calibration involved in instrument observation, and the quality control corresponding to its data is also special character. The main purpose of this article is to find the most effective quality control method for quality control of reactive gas observation data. Six types of quality control methods are used to perform quality control on reactive gas observation data, including extreme value check, triple standard deviation method, zero-span identification method, internal consistency check, time change check, and comprehensive identification check. By comprehensively identifying the data quality control result identification code given by each method, the quality control status of the data is finally determined. The quality control data used are the observation data of Beijing Shangdianzi atmospheric background station in 2018. Conclusion: A comparison of longsequence, multi-site analysis of the data after quality control using 6 methods and the data processed by a single method can draw conclusions. The multi-method comprehensive quality control results are superior to a single traditional meteorological observation data quality control method, which will cause fewer false judgments on the data and ensure the best quality control results of the observation data.", "articleno": "", "ISSN": "", "month": "Dec", "number": "", "volume": "", "publicationtype": "inproceedings", "author": ["Wenxue, Chi", "Yong, Zhang", "Xiaochun, Zhang", "Junshan, Jing", "Peng, Yan"]}]["9642876", {"address": "", "booktitle": "2021 International Conference on Quality Management, Transport and Information Security, Information Technologies (IT&QM&IS)", "doi": "10.1109/ITQMIS53292.2021.9642876", "isbn": "", "keywords": ["Measurement", "Data integrity", "Biological system modeling", "Ecosystems", "Standards organizations", "Process control", "Companies", "data quality", "digital platforms", "standards", "metrics"], "location": "", "numpages": "", "pages": "282-285", "publisher": "", "series": "", "title": "Data Quality and Standardization for Effective Use of Digital Platforms", "url": "", "year": "2021", "abstract": "Functioning of a company is largely based on digital technologies that form digital platforms - a set of products that implements innovative business models based on modern technologies, new ways of interaction with clients, contractors and partners. The purpose of the paper is to form metrics of data quality when using digital platforms. Studies of data quality metrics in various types of business and digital ecosystems made it possible to form a data quality scale capable of assessing the effectiveness of the role model, competency profiles of participants in digital ecosystems, and the sufficiency of resources. It is also necessary to evaluate the simplicity, efficiency and discipline of data quality control and management processes in order to eliminate the loss of digital trust of users. The paper focuses on the organizational model of data quality management based on the quality metrics system.", "articleno": "", "ISSN": "", "month": "Sep.", "number": "", "volume": "", "publicationtype": "inproceedings", "author": ["Efimova, Olga", "Igolnikov, Boris", "Isakov, Michail", "Dmitrieva, Elizaveta"]}]["9260067", {"address": "", "booktitle": "2020 IEEE 7th International Conference on Data Science and Advanced Analytics (DSAA)", "doi": "10.1109/DSAA49011.2020.00119", "isbn": "", "keywords": ["Big Data", "Data models", "Banking", "Neural networks", "Industries", "Data integrity", "Computational modeling", "Big Data", "Data Noise", "Data Quality", "Prediction and Machine Learning"], "location": "", "numpages": "", "pages": "791-792", "publisher": "", "series": "", "title": "Big Data Quality Prediction on Banking Applications: Extended Abstract", "url": "", "year": "2020", "abstract": "Big data has been transformed into knowledge by information systems to add value in businesses. Enterprises relying on it benefit from risk management to a certain extent. The value, however, depends on the quality of data. The quality needs to be verified before any use of the data. Specifically, measuring the quality by simulating the real life situation and even forecast it accurately turns into a hot topic. In recent years, there have been numerous researches on the measurement and assessment of data quality. These are yet to utilize a scientific computational method for the measurement and prediction. Current methods either fail to make an accurate prediction or do not consider the correlation and time sequence factors of the data. To address this, we design a model to extend machine learning technique to business applications predicting this. Firstly, we implement the model to detect data noises from a risk dataset according to an international data quality standard from banking industry and then estimate their impacts with Gaussian and Bayesian methods. Secondly, we direct sequential learning in multiple deep neural networks for the prediction with an attention mechanism. The model is experimented with various network methodologies to show the predictive power of machine learning technique and is evaluated by validation data to confirm the model effectiveness. The model is scalable to apply to any industries utilizing big data other than the banking industry.", "articleno": "", "ISSN": "", "month": "Oct", "number": "", "volume": "", "publicationtype": "inproceedings", "author": ["Wong, Ka", "Wong, Raymond"]}]["7840906", {"address": "", "booktitle": "2016 IEEE International Conference on Big Data (Big Data)", "doi": "10.1109/BigData.2016.7840906", "isbn": "", "keywords": ["Sensors", "Quality control", "Metadata", "Meteorology", "Interpolation", "Quality assessment", "Time series analysis", "data quality", "data stream processing", "spatial-temporal data", "quality control", "interpolation"], "location": "", "numpages": "", "pages": "2636-2645", "publisher": "", "series": "", "title": "The SMART approach to comprehensive quality assessment of site-based spatial-temporal data", "url": "", "year": "2016", "abstract": "There is a need for comprehensive solutions to address the challenges of spatio-temporal data quality assessment. Emphasis is often placed on the quality assessment of individual observations from sensors but not on the sensors themselves nor upon site metadata such as location and timestamps. The focus of this paper is on the development and evaluation of a representative and comprehensive, interpolation-based methodology for assessment of spatio-temporal data quality. We call our method the SMART method, short for Simple Mappings for the Approximation and Regression of Time series. When applied to a real-world, meteorological data set, we show that our method outperforms standard interpolators and we identify numerous problematic sites that otherwise would not have been flagged as bad. We further identify sites for which metadata is incorrect. We believe that there are many problems with real data sets like these and, in the absence of an approach like ours, these problems have largely gone unidentified. Our results bring into question the validity of provider-based quality control indicators. In addition to providing a comprehensive solution, our approach is novel for the simple but effective way that it accounts for spatial and temporal variation.", "articleno": "", "ISSN": "", "month": "Dec", "number": "", "volume": "", "publicationtype": "inproceedings", "author": ["Angryk, Rafal", "Galarus, Douglas"]}]["7364065", {"address": "", "booktitle": "2015 IEEE International Conference on Big Data (Big Data)", "doi": "10.1109/BigData.2015.7364065", "isbn": "", "keywords": ["Databases", "Big data", "Measurement", "Context", "Cleaning", "Biology", "Computers", "Data quality", "big data", "biological data", "information quality"], "location": "", "numpages": "", "pages": "2654-2660", "publisher": "", "series": "", "title": "Data quality issues in big data", "url": "", "year": "2015", "abstract": "Though the issues of data quality trace back their origin to the early days of computing, the recent emergence of Big Data has added more dimensions. Furthermore, given the range of Big Data applications, potential consequences of bad data quality can be for more disastrous and widespread. This paper provides a perspective on data quality issues in the Big Data context. it also discusses data integration issues that arise in biological databases and attendant data quality issues.", "articleno": "", "ISSN": "", "month": "Oct", "number": "", "volume": "", "publicationtype": "inproceedings", "author": ["Rao, Dhana", "Gudivada, Venkat", "Raghavan, Vijay"]}]["8085513", {"address": "", "booktitle": "2017 12th International Conference on Computer Science and Education (ICCSE)", "doi": "10.1109/ICCSE.2017.8085513", "isbn": "", "keywords": ["Education", "Monitoring", "Big Data", "Data mining", "Servers", "Memory", "Indexes", "big data", "monitoring and evaluation", "system design"], "location": "", "numpages": "", "pages": "337-342", "publisher": "", "series": "", "title": "Design of higher education quality monitoring and evaluation platform based on big data", "url": "", "year": "2017", "abstract": "Through the continuous collection and in-depth analysis of the quality monitoring data of colleges and universities, we combine the efficiency processing of big data and data evaluation, monitor the status of higher education normally, and construct a higher education quality monitoring and evaluation platform based on Spark. This platform is teaching centered with schools as its basis, including subsystems of data acquisition, data analysis, machine learning, data storage, data analysis and other areas. Through the application of the higher education quality monitoring platform, we can understand the current situation of the development of higher education scientifically, and provide the basis for the macro-decision of education administration department.", "articleno": "", "ISSN": "2473-9464", "month": "Aug", "number": "", "volume": "", "publicationtype": "inproceedings", "author": ["Li, Yuqian", "Li, Peng", "Zhu, Feng", "Wang, Ruchuan"]}]["8862267", {"address": "", "booktitle": "2019 International Conference on Machine Learning, Big Data, Cloud and Parallel Computing (COMITCon)", "doi": "10.1109/COMITCon.2019.8862267", "isbn": "", "keywords": ["Big Data", "Data integrity", "Meteorology", "Monitoring", "Data mining", "Organizations", "Big Data", "Big Data Quality", "Data Quality", "preprocessing", "pre-processing"], "location": "", "numpages": "", "pages": "559-563", "publisher": "", "series": "", "title": "Big Data Quality Framework: Pre-Processing Data in Weather Monitoring Application", "url": "", "year": "2019", "abstract": "Big Data has become an imminent part of all industries and business sectors today. All organizations in any sector like energy, banking, retail, hardware, networking, etc all generate huge quantum of heterogenous data which if mined, processed and analyzed accurately can reveal immensely useful patterns for business heads to apply to generate and grow their businesses. Big Data helps in acquiring, processing and analyzing large amounts of heterogeneous data to derive valuable results. Quality of information is affected by size, speed and format in which data is generated. Hence, Quality of Big Data is of great relevance and importance. We propose addressing various aspects of the raw data to improve its quality in the pre-processing stage, as the raw data may not usable as-is. We are exploring process like Cleansing to fix as much data as feasible, Noise filters to remove bad data, as well sub-processes for Integration and Filtering along with Data Transformation/Normalization. We evaluate and profile the Big Data during acquisition stage, which is adapted to expectations to avoid cost overheads later while also improving and leading to accurate data analysis. Hence, it is imperative to improve Data quality even it is absorbed and utilized in an industry's Big Data system. In this paper, we propose a Pre-Processing Framework to address quality of data in a weather monitoring and forecasting application that also takes into account global warming parameters and raises alerts/notifications to warn users and scientists in advance.", "articleno": "", "ISSN": "", "month": "Feb", "number": "", "volume": "", "publicationtype": "inproceedings", "author": ["Juneja, Ashish", "Das, Nripendra"]}]["8825534", {"address": "", "articleno": "", "doi": "10.1109/LGRS.2019.2936220", "issn": "1558-0571", "issue_date": "", "journal": "IEEE Geoscience and Remote Sensing Letters", "keywords": ["Oceans", "Image color analysis", "Data integrity", "Quality control", "Sun", "Sea measurements", "NASA", "Calibration", "data quality", "data quantity", "level-2 flags stray light", "ocean color", "quality control", "remote sensing", "sun glint", "uncertainty", "validation"], "month": "May", "number": "5", "numpages": "", "publisher": "", "title": "On the Interplay Between Ocean Color Data Quality and Data Quantity: Impacts of Quality Control Flags", "url": "", "volume": "17", "year": "2020", "abstract": "Nearly all calibration/validation activities for the satellite ocean color missions have focused on data quality to produce data products of the highest quality (i.e., science quality) for climate-related research. Little attention, however, has been paid to data quantity, particularly on how data quality control during data processing impacts downstream data quality and data quantity. In this letter, we attempt to fill this knowledge gap using measurements from the Visible Infrared Imaging Radiometer Suite (VIIRS) onboard the Suomi National Polar-orbiting Partnership (SNPP). For this sensor, the same level-1B data are processed independently using different quality control methods by NASA and NOAA, respectively, allowing for an in-depth evaluation of the interplay between data quantity and quality. The results indicate that the methods to identify stray light and sun glint are the two primary quality control procedures affecting data quantity, where the criteria for flagging pixels \u201ccontaminated\u201d by stray light and sun glint may be relaxed in the NASA ocean color data processing to increase data quantity without compromising data quality.", "pages": "745-749", "note": "", "ISSN": "1558-0571", "publicationtype": "article", "author": ["Hu, Chuanmin", "Barnes, Brian", "Feng, Lian", "Wang, Menghua", "Jiang, Lide"]}]["8465129", {"address": "", "booktitle": "2018 International Conference on Advances in Big Data, Computing and Data Communication Systems (icABCD)", "doi": "10.1109/ICABCD.2018.8465129", "isbn": "", "keywords": ["Data integrity", "Big Data", "Semantics", "Industries", "Bibliographies", "Libraries", "Production", "Big Data", "data quality", "health data", "data quality dimensions", "latent semantic analysis"], "location": "", "numpages": "", "pages": "1-6", "publisher": "", "series": "", "title": "Discovering Most Important Data Quality Dimensions Using Latent Semantic Analysis", "url": "", "year": "2018", "abstract": "Big Data quality is a field which is emerging. Many authors nowadays agree that data quality is still very relevant, even for Big Data uses. However, there is a lack of frameworks or guidelines about how to carry out those big data quality initiatives. The starting point of any data quality work is to determine the properties of data quality, termed as data quality dimensions (DQDs). Even those dimensions lack precise rigour in terms of definition from existing literature. This current research aims to contribute towards identifying the most important DQDs for big data in the health industry. It is a continuation of a previous work, which already identified five most important DQDs, using a human judgement based technique known as inner hermeneutic cycle. To remove potential bias coming from the human judgement aspect, this research uses the same set of literature but applies a statistical technique known to extract knowledge from a set of documents known as latent semantic analysis. The results confirm only 2 similar most important DQDs, namely accuracy and completeness.", "articleno": "", "ISSN": "", "month": "Aug", "number": "", "volume": "", "publicationtype": "inproceedings", "author": ["Juddoo, Suraj", "George, Carlisle"]}]["9006294", {"address": "", "booktitle": "2019 IEEE International Conference on Big Data (Big Data)", "doi": "10.1109/BigData47090.2019.9006294", "isbn": "", "keywords": ["Tools", "Data models", "Containers", "Software", "Real-time systems", "Big Data applications", "Big Data Applications", "Quality Requirements", "Big Data Goal-oriented Requirements Language", "Requirements Modelling Tool"], "location": "", "numpages": "", "pages": "5977-5979", "publisher": "", "series": "", "title": "QualiBD: A Tool for Modelling Quality Requirements for Big Data Applications", "url": "", "year": "2019", "abstract": "The development of Big Data applications is not well-explored, to our knowledge. Embracing Big Data in system building, questions arise as to how to elicit, specify, analyse, model, and document Big Data quality requirements. In our ongoing research, we explore a requirements modelling language for Big Data software applications. In this paper, we introduce QualiBD, a modelling tool that implements the proposed goal-oriented requirements language that facilitates the modelling of Big Data quality requirements.", "articleno": "", "ISSN": "", "month": "Dec", "number": "", "volume": "", "publicationtype": "inproceedings", "author": ["Arruda, Darlan", "Madhavji, Nazim"]}]["8509662", {"address": "", "articleno": "", "doi": "", "issn": "", "issue_date": "", "journal": "IEEE P1159.3/D19, October 2018", "keywords": ["IEEE Standards", "Power quality", "Protocols", "Data integration", "data interchange", "file format", "IEEE 1159.3", "measurement", "monitoring", "power quality", "PQDIF"], "month": "Oct", "number": "", "numpages": "", "publisher": "", "title": "IEEE Draft Recommended Practice for Power Quality Data Interchange Format (PQDIF)", "url": "", "volume": "", "year": "2018", "abstract": "A file format suitable for exchanging power quality related measurement and simulation data in a vendor independent manner is defined in this recommended practice. The format is designed to represent all power quality phenomena identified in IEEE Std 1159-2009, IEEE Recommended Practice on Monitoring Electric Power Quality, other power related measurement data, and is extensible to other data types as well. The recommended file format utilizes a highly compressed storage scheme to help reduce disk space and transmission times. The utilization of Globally Unique Identifiers (GUID) to represent each element in the file permits the format to be extensible without the need for a central registration authority.", "pages": "1-238", "note": "", "ISSN": "", "publicationtype": "article", "author": []}]["7799648", {"address": "", "booktitle": "2016 13th Web Information Systems and Applications Conference (WISA)", "doi": "10.1109/WISA.2016.14", "isbn": "", "keywords": ["Visualization", "User interfaces", "Data models", "Resource description framework", "W3C", "Prototypes", "Context", "data quality-based filtering and ranking", "datasets", "faceted search", "Data Quality Vocabulary (DQV)", "quality metadata", "data portal"], "location": "", "numpages": "", "pages": "18-23", "publisher": "", "series": "", "title": "DQFIRD: Towards Data Quality-Based Filtering and Ranking of Datasets for Data Portals", "url": "", "year": "2016", "abstract": "The Data on the Web Best Practices Working Group, as part of W3C Data Activity, is standardizing the Data Quality Vocabulary (DQV) for expressing data quality of datasets published on the Web. By exploiting such DQV-based quality metadata associated to the datasets in a data portal, data consumers can achieve data quality-based filtering and ranking of datasets on the portal's conventional search results to obtain desired datasets with high data-quality. Despite the significant progress in standardization, there is a lack of systematic research on approaches and tools for data quality-based filtering and ranking of Web published datasets. This paper therefore proposes a generic software framework for Data Quality-based Filtering and Ranking of Datasets (DQFIRD) in data portals. DQFIRD adopts faceted search (or faceted exploration) techniques to filter the search results of a data portal based on quality metadata about the resulting datasets, and then ranks the filtered datasets according to numeric values of quality measurements in the metadata. We designed the main algorithms of DQFIRD and implemented a prototype of DQFIRD using Java and Jena API. Furthermore, we used the prototype to conduct case study experiments and time efficiency test on the Faceted Taxonomy Materialization (FTM) algorithm, the most time-consuming online operation algorithm in DQFIRD. The results indicate that the proposed DQFIRD approach is implementable and effective, and it has low time complexity because the run-time of the FTM algorithm exhibits approximately a linear growth rate as the size of the relevant dataset quality metadata increases.", "articleno": "", "ISSN": "", "month": "Sep.", "number": "", "volume": "", "publicationtype": "inproceedings", "author": ["Xia, Wenze", "Xu, Zhuoming", "Wei, Jie", "Tian, Haimei"]}]["9148148", {"address": "", "booktitle": "2020 International Conference on Computer Information and Big Data Applications (CIBDA)", "doi": "10.1109/CIBDA50819.2020.00049", "isbn": "", "keywords": ["Time series analysis", "Temperature sensors", "Mobile handsets", "Temperature distribution", "Databases", "Data models", "Machine Learning", "Crowd Sensing", "Big Data Analysis", "Abnormal Data Detection Management", "Clustering Method"], "location": "", "numpages": "", "pages": "185-188", "publisher": "", "series": "", "title": "Quality Management of Crowd Sensing Data Based on Machine Learning", "url": "", "year": "2020", "abstract": "Recently, research on crowd sensing data quality management is a new subject area developed based on wireless sensor network related concepts. Crowd sensing data has also experienced many links in the process of network propagation, so it is inevitable that there are abnormal data in the database. Therefore, how to filter these unreliable data to get more real data is particularly important. This paper takes somatosensory temperature as an example, solves the problem of calculating the similarity of unequal long-term sequences by using DTW technology, and then clusters and compares the data in the database to find out the abnormal data. Thereby, the accuracy of the somatosensory temperature database is improved, and relevant users can obtain more accurate information. The experimental results show that when the minimum DTW value exceeds the threshold t = 0.7, the more the number of simulation sequences, the more stable the accuracy rate, and the faster the growth rate of the running time.", "articleno": "", "ISSN": "", "month": "April", "number": "", "volume": "", "publicationtype": "inproceedings", "author": ["Bai, Zhongxian", "Zhuo, Rongqing"]}]["9006187", {"address": "", "booktitle": "2019 IEEE International Conference on Big Data (Big Data)", "doi": "10.1109/BigData47090.2019.9006187", "isbn": "", "keywords": ["Data integrity", "Machine learning", "Pipelines", "Cleaning", "Libraries", "Buildings", "Data quality", "machine learning", "data cleaning", "scalability", "automation", "data science"], "location": "", "numpages": "", "pages": "2913-2922", "publisher": "", "series": "", "title": "DQA: Scalable, Automated and Interactive Data Quality Advisor", "url": "", "year": "2019", "abstract": "Fueled with growth in the fields of Internet of Things (IoT) and Big Data, data has become one of the most valuable assets in today's world. While we are leveraging this data for analyzing complex systems using machine learning and deep learning, a considerable amount of time and effort is spent on addressing data quality issues. If undetected, data quality issues can cause large deviations in the analysis, misleading data scientists. To ease the effort of identifying and addressing data quality challenges, we introduce DQA, a scalable, automated and interactive data quality advisor. In this paper, we describe the DQA framework, provide detailed description of its components and the benefits of integrating it in a data science process. We propose a programmatic approach for implementing the data quality framework which automatically generates dynamic executable graphs for performing data validations fine-tuned for a given dataset. We discuss the use of DQA to build a library of validation checks common to many applications. We provide insight into how DQA addresses many persistence and usability issues which currently make data cleaning a laborious task for data scientists. Finally, we provide a case study of how DQA is implemented in a realworld system and describe the benefits realized.", "articleno": "", "ISSN": "", "month": "Dec", "number": "", "volume": "", "publicationtype": "inproceedings", "author": ["Shrivastava, Shrey", "Patel, Dhaval", "Bhamidipaty, Anuradha", "Gifford, Wesley", "Siegel, Stuart", "Ganapavarapu, Venkata", "Kalagnanam, Jayant"]}]["7100100", {"address": "", "booktitle": "1999 European Control Conference (ECC)", "doi": "10.23919/ECC.1999.7100100", "isbn": "", "keywords": ["Data analysis", "Databases", "Statistical analysis", "Data mining", "Quality assessment", "Product design", "Safety", "Exploratory data analysis", "data mining", "statistical methods", "independence analysis", "data preprocessing"], "location": "", "numpages": "", "pages": "4824-4829", "publisher": "", "series": "", "title": "Using data mining methods for improvement of product and process quality", "url": "", "year": "1999", "abstract": "Recently, data analysis methods have been successfully employed in various areas of industry. With their help analysts gain valuable information and knowledge about business and technical processes. In addition to established and proven statistical methods, new data analysis techniques hiding behind catchwords such as Data Mining, Knowledge Discovery in Databases, and Computational Intelligence are increasingly used. These new approaches are primarily characterized by their ability to find conspicuous patterns in databases almost on their own. In a particular way, they support the general goal to detect existing and possibly still hidden information in databases. In chemical industry the information obtained is mainly meant to support the design of products and production processes, as well as the development of intelligent process management strategies.", "articleno": "", "ISSN": "", "month": "Aug", "number": "", "volume": "", "publicationtype": "inproceedings", "author": ["Estler, Manfred", "Hilpert, Ralf", "Kiupel, Niels", "Soravia, Sergio"]}]["9073586", {"address": "", "booktitle": "2019 12th International Conference on Developments in eSystems Engineering (DeSE)", "doi": "10.1109/DeSE.2019.00072", "isbn": "", "keywords": ["Distributed databases", "Metadata", "Big Data", "Standards", "File systems", "Data integrity", "Big Data", "data quality", "unstructured Data Distributed data file system", "and statistical model."], "location": "", "numpages": "", "pages": "357-362", "publisher": "", "series": "", "title": "Data Quality Management for Big Data Applications", "url": "", "year": "2019", "abstract": "Currently, as a result of the continuous increase of data, one of the key issues is the development of systems and applications to deal with storage, management and processing of big numbers of data. These data are found in unstructured ways. Data management with traditional approaches is inappropriate because of the large and complex data sizes. Hadoop is a suitable solution for the continuous increase in data sizes. The important characteristics of the Hadoop are distributed processing, high storage space, and easy administration. Hadoop is better known for distributed file systems. In this paper, we have proposed techniques and algorithms that deal with big data including data collecting, data preprocessing, algorithms for data cleaning, A Technique for Converting Unstructured Data to Structured Data using metadata, distributed data file system (fragmentation algorithm) and Quality assurance algorithms by using the model is the statistical model to evaluate the highest educational institutions. We concluded that Metadata accelerates query response required and facilitates query execution, metadata will be content for reports, fields and descriptions. Total time access for three complex queries in distributed processing it is 00: 03: 00 per second while in nondistributed processing it is at 00: 15: 77 per second, average is approximately five minutes per second. Quality assurance note values (T-test) is 0.239 and values (T-dis) is 1.96, as a result of dealing with scientific sets and humanities sets. In the comparison law, it can be deduced that if the t-test is smaller than the t-dis; so there is no difference between the mean of the scientific and humanities samples, the values of C.V for both scientific is (8.585) and humanities sets is (7.427), using the law of homogeneity know whether any sets are more homogeneous whenever the value of a small C.V was more homogeneous however the humanity set is more homogeneity.", "articleno": "", "ISSN": "2161-1351", "month": "Oct", "number": "", "volume": "", "publicationtype": "inproceedings", "author": ["Khaleel, Majida", "Hamad, Murtadha"]}]["8258380", {"address": "", "booktitle": "2017 IEEE International Conference on Big Data (Big Data)", "doi": "10.1109/BigData.2017.8258380", "isbn": "", "keywords": ["Industries", "Rails", "Data models", "Rail transportation", "Systematics", "Decision making", "data quality", "rail", "quality by design", "data quality schema"], "location": "", "numpages": "", "pages": "3792-3799", "publisher": "", "series": "", "title": "Understanding data quality: Ensuring data quality by design in the rail industry", "url": "", "year": "2017", "abstract": "The railways worldwide are increasingly looking to the integration of their data resources coupled with advanced analytics to enhance traffic management, to provide new insights on the health of infrastructure assets, to provide soft linkages to other transport modes, and ultimately to enable them to better serve their customers. As in many industrial sectors, over the past decade the rail industry has been investing heavily in sensing technologies that record every aspect of the operation of the railway network. However, as any data scientist knows, it does not matter how good an algorithm is, if you put rubbish in, you get rubbish out; and as the traditional industry model of working with data only within the system that it was collected by becomes increasingly fragile, the industry is discovering that it knows less than it thought about the data it is gathering. When coupled with legacy data resources of unknown accuracy, such as design diagrams for assets that in many cases are decades old, the rail industry now faces a crisis in which its data may become essentially worthless due to a poor understanding of the quality of its data. This paper reports the findings of the first phase of a three-phase systematic review of literature about how data quality can be managed and evaluated in the rail domain. It begins by discussing why data quality matters in a rail context, before going on to define the quality, introduce and expand the concept of a data quality schema.", "articleno": "", "ISSN": "", "month": "Dec", "number": "", "volume": "", "publicationtype": "inproceedings", "author": ["Fu, Qian", "Easton, John"]}]["9754682", {"address": "", "booktitle": "2021 IEEE 7th International Conference on Cloud Computing and Intelligent Systems (CCIS)", "doi": "10.1109/CCIS53392.2021.9754682", "isbn": "", "keywords": ["Manufacturing industries", "Data integrity", "Computational modeling", "Collaboration", "Organizations", "Aerospace electronics", "Data models", "manufacturing industry", "whole life cycle", "data space", "data quality"], "location": "", "numpages": "", "pages": "315-323", "publisher": "", "series": "", "title": "Construction of Data Quality Evaluation Index for Manufacturing Multi-value Chain Collaborative Data Space Based on the Whole Life Cycle of Data", "url": "", "year": "2021", "abstract": "As a new data management model, data space can effectively manage a large amount of multi-heterogeneous dynamic data, but the construction of data space often needs to be based on accurate and scientific original data and to obtain valuable information in data, which poses a challenge to the data quality control of the whole life cycle of data, so it is especially important to evaluate the data quality. By analyzing the synergistic effect of multi-value chain in manufacturing industry and combining the dynamic system of the whole life cycle of data, the data quality evaluation index system is proposed from three aspects of data provider, data space construction and data user, combining four levels of data itself, technology, data flow layer and data management. Through the construction of AHP-TOPSIS data quality evaluation model, AHP is used to determine the index weight, TOPSIS is used to calculate the ideal solution and relative closeness degree, and the evaluation results are obtained. Through the application analysis of examples, quantitative evaluation of data quality, the construction, access and mining of multi-value chain collaborative data space can provide practical experience.", "articleno": "", "ISSN": "", "month": "Nov", "number": "", "volume": "", "publicationtype": "inproceedings", "author": ["Peng, Sha", "Tian, Zhuxiao", "Siqin, Zhuoya", "Xu, Xiaomin"]}]["7823519", {"address": "", "booktitle": "2016 International Conference on Industrial Informatics - Computing Technology, Intelligent Technology, Industrial Information Integration (ICIICII)", "doi": "10.1109/ICIICII.2016.0052", "isbn": "", "keywords": ["Indexes", "Mathematical model", "Data models", "Manufacturing industries", "Computational modeling", "Big data", "Customer satisfaction", "macro-quality evaluation", "big data", "macro-quality index", "customer satisfaction index", "PLS-SEM"], "location": "", "numpages": "", "pages": "181-186", "publisher": "", "series": "", "title": "Big Data Oriented Macro-Quality Index Based on Customer Satisfaction Index and PLS-SEM for Manufacturing Industry", "url": "", "year": "2016", "abstract": "The aim of this paper was to develop a novel macro-quality index driven by big quality data regarding the macro-quality management demands of manufacturing enterprises in Industry 4.0. Firstly, the connotation of big data of macro-quality management in manufacturing industry is expounded, which is the collection of the quality data in product lifecycle including the product quality data, process quality data and organizational ability data. Secondly, referring to the customer satisfaction index theory, a new big data oriented macro-quality index computation model based on the partial least square-structural equation modeling(PLS-SEM) theory is proposed, and the partial least square(PLS) is adopted to estimate the path parameters. Finally, a case study of macro-quality situation evaluation for the manufacturing industry of a city in China is presented. The final result shows that the proposed macro-quality index model is applicable and predictable.", "articleno": "", "ISSN": "", "month": "Dec", "number": "", "volume": "", "publicationtype": "inproceedings", "author": ["Li, Tao", "He, Yihai", "Zhu, Chunling"]}]["5228165", {"address": "", "booktitle": "2009 4th International Conference on Computer Science & Education", "doi": "10.1109/ICCSE.2009.5228165", "isbn": "", "keywords": ["Data warehouses", "Data mining", "Computer science", "Computer science education", "Data analysis", "Medical services", "Laboratories", "Data conversion", "Artificial intelligence", "Software quality", "data warehouse", "data quality", "metadata"], "location": "", "numpages": "", "pages": "823-827", "publisher": "", "series": "", "title": "Analysis and solution of data quality in data warehouse of Chinese materia medica", "url": "", "year": "2009", "abstract": "The data quality problem often be ignored in the process of data warehouse construction and utilization. In order to avoid the phenomenon of \u201cgarbage in, garbage out\u201d which will influence the decision-making, the problem of data quality must be paid great attention. In this paper, we took the Chinese materia medica (Cmm) data warehouse in China Academy of Chinese Medical Sciences (CACMS) as example. We analyzed the data quality problems in the process of its construction, cited the reasons for bad data quality, and gave the key elements and their relations for data quality analysis and assessment. At the end of article, we proposed a series of assessment standards and solution scheme to improve the data quality in Cmm data warehouse, and carried out in practice to prove it.", "articleno": "", "ISSN": "", "month": "July", "number": "", "volume": "", "publicationtype": "inproceedings", "author": ["Chen, Bing", "Weng, Xuchu", "Wang, Beizhan", "Hu, Xueqin"]}]["7923744", {"address": "", "booktitle": "2017 Data Compression Conference (DCC)", "doi": "10.1109/DCC.2017.57", "isbn": "", "keywords": ["Quality control", "Video coding", "Videos", "Data compression", "Encoding", "Video sequences", "video coding", "quality of experience", "PID-based quality control"], "location": "", "numpages": "", "pages": "461-461", "publisher": "", "series": "", "title": "Watching Videos with Certain and Constant Quality: PID-Based Quality Control Method", "url": "", "year": "2017", "abstract": "In video coding, compressed videos with certain and constant quality can ensure quality of experience (QoE). To this end, we propose in this paper a novel PID-based quality control (PQC) method for video coding. Specifically, a formulation is modelled to control quality of video coding with two objectives: minimizing control error and quality fluctuation. Then, we apply the Laplace domain analysis to model the relationship between quantization parameter (QP) and control error in this formulation. Given the relationship between QP and control error, we propose a solution to the PQC formulation, such that videos can be compressed at certain and constant quality. Finally, experimental results show that our PQC method is effective in both control accuracy and quality fluctuation.", "articleno": "", "ISSN": "2375-0359", "month": "April", "number": "", "volume": "", "publicationtype": "inproceedings", "author": ["Song, Yuhang", "Xu, Mai", "Li, Shengxi"]}]["7761054", {"address": "", "booktitle": "OCEANS 2016 MTS/IEEE Monterey", "doi": "10.1109/OCEANS.2016.7761054", "isbn": "", "keywords": ["Quality control", "Standards", "Sea measurements", "Real-time systems", "Data analysis", "Oceanography", "Oceans"], "location": "", "numpages": "", "pages": "1-7", "publisher": "", "series": "", "title": "Wave data analysis and Quality Control challenges", "url": "", "year": "2016", "abstract": "Since its inception in 1975, quality control has been at the forefront of the Coastal Data Information Program (CDIP). During the early days, with few pressure sensor stations deployed (Imperial Beach and Ocean Beach, CA), timely Quality Assurance/Quality Control (QA/QC) checks were done manually. However, as the program grew, it became evident that in order to maintain high quality standards commensurate with real time data dissemination, it would be necessary to develop automated quality control checks designed to accept qualified data and reject sets that fail to meet minimum standards. Both categories were archived and are available, in the public domain, for further manual or machine processing and inspection.", "articleno": "", "ISSN": "", "month": "Sep.", "number": "", "volume": "", "publicationtype": "inproceedings", "author": ["Thomas, Julie"]}]["9590700", {"address": "", "booktitle": "2021 IEEE 4th International Conference on Information Systems and Computer Aided Education (ICISCAE)", "doi": "10.1109/ICISCAE52414.2021.9590700", "isbn": "", "keywords": ["Training", "Analytical models", "Systematics", "Databases", "Data integrity", "Biological system modeling", "Education", "Educational data", "Data quality", "Statistics"], "location": "", "numpages": "", "pages": "363-367", "publisher": "", "series": "", "title": "Research on the Construction of Educational Data Quality Model Based on Multiple Constraints Model", "url": "", "year": "2021", "abstract": "With the development of Internet and information technology, data has become an important asset related to the development prospects of society and all walks of life. At present, there are many quality problems in the use of educational data, which has brought great obstacles to exerting the value of educational data. Only by using scientific statistical methods, obtaining real, objective, comprehensive, scientific and effective basic data, and carrying out systematic and comprehensive analysis on the obtained data, can we give full play to its command and decision-making role. The development of big data and artificial intelligence technology provides new ideas for the analysis and evaluation of educational data quality, and is committed to restoring the overall picture of the education system and promoting the change of regional educational ecology. In this paper, an educational data quality analysis model based on multiple constraint model is proposed, which classifies the data in the database, divides the information in the database into several different categories according to the data characteristics, and establishes a quality management system for educational data in universities, so as to effectively improve the quality of educational data.", "articleno": "", "ISSN": "", "month": "Sep.", "number": "", "volume": "", "publicationtype": "inproceedings", "author": ["Du, Jinming"]}]["8372743", {"address": "", "booktitle": "2018 27th Wireless and Optical Communication Conference (WOCC)", "doi": "10.1109/WOCC.2018.8372743", "isbn": "", "keywords": ["Semantics", "Big Data", "Air quality", "Data mining", "Urban areas", "Monitoring", "Government", "Air Quality", "Big Data", "Prediction", "Cloud Environment"], "location": "", "numpages": "", "pages": "1-3", "publisher": "", "series": "", "title": "Big data platform for air quality analysis and prediction", "url": "", "year": "2018", "abstract": "With the advance of industry, air quality (AQ) is increasingly becoming worse. There are increasingly AQ monitors device have been deployed around country for monitoring air-quality all year long. To estimate and predict AQ, such as PM (particulate matter) 2.5, become an important issue for government to improve people's quality of life. As we can know, there are many factors can affect the AQ, such as traffic, factory exhaust emissions, weather, incineration of garbage, and so on. In most well-developed countries, these pollution sources are monitored for future environmental policy making. In this paper, we will propose a semantic ETL (Extract-Transform-Load) framework on cloud platform for AQ prediction. In the platform, we exploit ontology to concretize the relationship of PM 2.5 from various data sources and to merge those data with the same concept but different naming into the unified database. We implement the ETL framework on the cloud platform, which includes computing nodes and storage nodes. The computing nodes are used to execute data mining algorithms for predicting, and storage modes are used to store retrieved, preprocessed, and analyzed data. We utilize restful web service as the front end API to retrieve analyzed data, and finally we exploit browser to show the visualized result to demonstrate the estimation and prediction. It shows that the big data access framework on the cloud platform can work well for air quality analysis.", "articleno": "", "ISSN": "2379-1276", "month": "April", "number": "", "volume": "", "publicationtype": "inproceedings", "author": ["Chang, Yue", "Lin, Kuan-Ming", "Tsai, Yi-Ting", "Zeng, Yu-Ren", "Hung, Cheng-Xiang"]}]["8421858", {"address": "", "booktitle": "2018 5th IEEE International Conference on Cyber Security and Cloud Computing (CSCloud)/2018 4th IEEE International Conference on Edge Computing and Scalable Cloud (EdgeCom)", "doi": "10.1109/CSCloud/EdgeCom.2018.00025", "isbn": "", "keywords": ["Big Data", "Inspection", "Data integration", "Semantics", "Hidden Markov models", "Thesauri", "Support vector machines", "Big data of quality inspection", "data fusion", "named entity recognition", "entity resolution", "data conflict resolution"], "location": "", "numpages": "", "pages": "95-102", "publisher": "", "series": "", "title": "Review on Big Data Fusion Methods of Quality Inspection for Consumer Goods", "url": "", "year": "2018", "abstract": "Quality big data comes from a wide range, the problem is how to eliminate the structure barriers between various types of data from different sources, to achieve the effective integration for isolated or fragmented data, and then to mine the information, knowledge and wisdom according to the actual needs. It is urgent for the quality inspection departments to solve the problem of making a decision the first time and take preventive measures. This paper introduced the research status of named entity recognition, solid unified detection, data conflict resolution, data fusion method from the characteristics of quality inspection data. The existence problems are analyzed and the research direction is looking forward of the research on data fusion in this paper.", "articleno": "", "ISSN": "", "month": "June", "number": "", "volume": "", "publicationtype": "inproceedings", "author": ["Jiang, Wei", "Ning, Xiuli", "Xu, Yingcheng"]}]["9368701", {"address": "", "booktitle": "2020 IEEE 2nd International Conference on Civil Aviation Safety and Information Technology (ICCASIT", "doi": "10.1109/ICCASIT50869.2020.9368701", "isbn": "", "keywords": ["Quality assurance", "Atmospheric modeling", "Big Data", "Data models", "Time measurement", "Safety", "Mathematical model", "flight quality", "pitch", "QAR data", "normal distribution", "t test"], "location": "", "numpages": "", "pages": "955-958", "publisher": "", "series": "", "title": "Data Mining on the Flight Quality of an Airline based on QAR Big Data", "url": "", "year": "2020", "abstract": "At present, the airlines have made some achievements in event analysis and investigation by using their quick access record (QAR) data. But where each airline's flight quality is in the industry, and whether there is a problem in itself, the airline can't find. In order to help airlines discover the existing flight quality problems, this article uses the QAR big data of the flight operational quality assurance (FOQA) Station of CAAC, and compares the industry-wide QAR data with the QAR data of individual airlines, and founds that the take-off pitch angle of a certain aircraft of A321 models is too small, by using mathematical statistics t test to verify, found the airline's the take-off pitch angle and the industry's the take-off pitch angle exist significant difference. The correlative speed at rotation and the speed at liftoff are also analyzed, and the significant difference is found. The FOQA Station of CAAC feeds back the problem to the airline and the authority. After the investigation of the airline and the authority, there are problems with the airline. And the airline immediately starts to rectify it.", "articleno": "", "ISSN": "", "month": "Oct", "number": "", "volume": "", "publicationtype": "inproceedings", "author": ["Wang, Xin", "Zhao, Xinbin", "Yu, Liling"]}]["8996332", {"address": "", "booktitle": "2019 Chinese Automation Congress (CAC)", "doi": "10.1109/CAC48633.2019.8996332", "isbn": "", "keywords": ["Air quality", "Big Data", "Data mining", "Photonics", "Optical fiber communication", "Telecommunications", "Clustering algorithms", "Air quality", "Big data", "Data mining", "Data visualization"], "location": "", "numpages": "", "pages": "2042-2046", "publisher": "", "series": "", "title": "Air quality data analysis and forecasting platform based on big data", "url": "", "year": "2019", "abstract": "Nowadays, with the continuous development of big data technology, various industries use big data technology to process and mine massive data, and realize the value of data efficiently. In terms of air quality data processing, big data technology can also play a certain advantage. The platform is based on big data technology to design an air quality data analysis and prediction platform including data layer, business layer, interaction layer and visualization platform. Data is cleaned, calibrated, and stored in the data layer to ensure data consistency, integrity, and security. The air quality data is analyzed and predicted at the business layer. The interaction layer includes the functions of algorithm management, data query, and the data visualization platform provides intuitive information display. This design is a significant application for fully exploiting environmental data information. It has powerful data processing functions and scalability, which is a reliable data analysis and prediction platform.", "articleno": "", "ISSN": "2688-0938", "month": "Nov", "number": "", "volume": "", "publicationtype": "inproceedings", "author": ["Wang, Jinghan", "Zhang, Jinnan", "Yuan, XueGuang", "Tang, Yu", "Hao, Hongyu", "Zuo, Yong", "Tan, Zebin", "Qiao, Min", "Cao, Yang", "Ai, Lingmei", "Wan, Yihang", "Chen, Hao"]}]["9888215", {"address": "", "booktitle": "2022 International Conference on Computation, Big-Data and Engineering (ICCBE)", "doi": "10.1109/ICCBE56101.2022.9888215", "isbn": "", "keywords": ["Cloud computing", "5G mobile communication", "Project management", "Transforms", "Big Data", "Digital twins", "Internet of Things", "Big data", "BIM", "quality management"], "location": "", "numpages": "", "pages": "172-176", "publisher": "", "series": "", "title": "Research and Design of Construction Engineering Quality Management System Based on Big Data and BIM Technology", "url": "", "year": "2022", "abstract": "With the rapid development of information technologies such as big data, cloud computing, the Internet of Things, and 5G in recent years, the construction industry, as the traditional industry, urgently requests the transformation of its project management from extensive and low-efficiency mode to high-quality development mode. Under the background of the gradual development of newly-emerged technologies and concepts such as BIM technology, smart construction, and digital twin and based on the full investigation of the quality management requirements of construction enterprises, closed-loop management is focused on the whole process of construction engineering. From the perspective of construction engineering quality management, current theoretical tools such as the Internet of Things, big data, and BIM technology are combined. Taking Python as a basis, the mature and open-source WEB framework, database, and front-end and back-end technologies are used to design and construct a set of construction engineering quality management systems. We explore the digital potential of construction engineering quality management systems to provide a new way of thinking for the informatization, systematization, and system process-oriented development of construction engineering quality management.", "articleno": "", "ISSN": "", "month": "May", "number": "", "volume": "", "publicationtype": "inproceedings", "author": ["Han, Wei", "Wu, Shenggang", "Liu, Qiang", "Cai, Jianzhen"]}]["9524049", {"address": "", "booktitle": "2020 International Conference on Robots & Intelligent System (ICRIS)", "doi": "10.1109/ICRIS52159.2020.00114", "isbn": "", "keywords": ["Quality assurance", "Databases", "Education", "Decision making", "Data preprocessing", "Data collection", "Data models", "teaching evaluation", "Apriori", "ASP.net", "data mining", "WICA"], "location": "", "numpages": "", "pages": "442-445", "publisher": "", "series": "", "title": "Quality Assurance Scheme for Undergraduate Teaching Evaluation Based on Data Mining", "url": "", "year": "2020", "abstract": "To solve the problems existing in the process of teaching quality evaluation and to improve the accuracy of undergraduate teaching quality evaluation, a teaching quality evaluation model based on data mining algorithm is designed. Aiming at the factors such as the large amount of data to be mined and the quality of mining which is easily affected, the improved Apriori algorithm based on partition is applied in the model. Through the process of data collection and data preprocessing, the database suitable for association rule mining is established. Then we can find out which key factors can affect the teaching quality according to the analysis of association rules, so as to provide a strong basis for teaching decision-making and management. The results show that the data mining algorithm can describe the differences between the teaching quality grades of colleges, and acquire high-precision evaluation results of university teaching quality. Moreover, the error of teaching quality evaluation in colleges is far less than that of the current typical teaching quality evaluation methods.", "articleno": "", "ISSN": "", "month": "Nov", "number": "", "volume": "", "publicationtype": "inproceedings", "author": ["Yachen, Wang"]}]["8730858", {"address": "", "booktitle": "2019 IEEE International Conference on Pervasive Computing and Communications Workshops (PerCom Workshops)", "doi": "10.1109/PERCOMW.2019.8730858", "isbn": "", "keywords": ["Data integrity", "Computational modeling", "Data models", "Tools", "Pervasive computing", "Monitoring", "Quality of service", "data stream processing", "data quality", "sensors", "context"], "location": "", "numpages": "", "pages": "445-446", "publisher": "", "series": "", "title": "Quality-Aware Sensor Data Stream Management in a Living Lab Environment", "url": "", "year": "2019", "abstract": "Sensor data is error-prone. Developers of pervasive applications must take the limitations of sensors into account when processing the data. To relieve the developers from the task of data cleaning and quality monitoring, we need a set of tools to model sensor data quality and to integrate the quality information into the stream data processing. In this dissertation, the goal is to provide a framework of tools to semi-automatically generate sensor models and stream processing queries for sensors with quality and context information for a quality-aware data stream processing.", "articleno": "", "ISSN": "", "month": "March", "number": "", "volume": "", "publicationtype": "inproceedings", "author": ["Benabbas, Aboubakr", "Nicklas, Daniela"]}]["5070851", {"address": "", "booktitle": "2009 Sixth International Conference on Information Technology: New Generations", "doi": "10.1109/ITNG.2009.167", "isbn": "", "keywords": ["Quality management", "Error correction", "Coordinate measuring machines", "Software measurement", "Databases", "Real time systems", "Manufacturing processes", "Process control", "Instruments", "Machining", "quality management", "data collection", "COMERO"], "location": "", "numpages": "", "pages": "1567-1567", "publisher": "", "series": "", "title": "Application of COMERO Data Collecting in Quality Management System", "url": "", "year": "2009", "abstract": "Summary form only given. Exact and real-time collection of quality data from manufacturing process is the most important step in quality management system. It is the foundation of cumulating, analyzing and effectively controlling the processpsilas quality. As an advanced and exact metrical instrument, 3-degree coordinate measuring machining (CMM) has been widely used in machinery manufacturing industry. However, it could only use text file to record the measuring result, which can not be integrated with the quality management system. This paper introduces appropriative interface software, which can automatically read the measurement data from 3-degree CMMpsilas RTF file and can automatically transfer the certain data to database and then let quality management system directly transfer the measurement result in order to analyze the correlative accessorypsilas quality. This analysis is useful to understand the characteristic of random error system, the trend of error, the order of error distributing and the reasons for error and also form the quality report and early warning information for quality control. Meanwhile, informationpsilas automatically transmission can obviously improve working efficiency and effectively avoid the artificial errors. Heterogeneous datapsilas conversion is the key point of system integration. This paper comprehensively analysis the expression mode of every dimension and error in the measurement text files mentioned above, and then applies certain program. In order to understand the meaning of relevant data, key words are needed. Using character string in the measurement files, which match the regular expression and the method of exhaustion, we could get the key words. Then the data, which quality management system needs, is formed by transferring data types with homonymous matching and written into the database. In order to achieve the informationpsilas automatically transition, this paper also studies the data synchronization mechanism and ensure the measurement data can be synchronously updated in the quality management system by setting the trigger rule.", "articleno": "", "ISSN": "", "month": "April", "number": "", "volume": "", "publicationtype": "inproceedings", "author": ["Yan-fang, Yue", "Rui-gang, Zhang", "Guang, Yang", "Guang-le, Ge"]}]["4548030", {"address": "", "booktitle": "2007 Australasian Universities Power Engineering Conference", "doi": "10.1109/AUPEC.2007.4548030", "isbn": "", "keywords": ["Power quality", "Data analysis", "Monitoring", "Power system harmonics", "Voltage fluctuations", "Power measurement", "Aggregates", "Instruments", "Time measurement", "Data processing", "Power Quality", "Power Quality Indices"], "location": "", "numpages": "", "pages": "1-7", "publisher": "", "series": "", "title": "The variation of power quality indices due to data analysis procedure", "url": "", "year": "2007", "abstract": "Power quality data is often reported using statistical confidence levels. This will exclude the most extreme data for a certain length of time depending on the interval over which the confidence level is applied. There is considerable conjecture as to the effect of applying statistical measures over different time intervals, e.g. several days, weeks or one year. If statistical confidence levels are applied over long intervals, the length of time not included in the statistical confidence interval is long. During such intervals disturbance levels may be continuously high and not be accounted for in the statistical parameter. This study investigates the effect different methods of aggregating data to a specific reporting period will have on the calculated index. Several data processing methods are trialled to evaluate the effect of using different aggregation intervals to produce an index to characterise disturbance levels for the whole year.", "articleno": "", "ISSN": "", "month": "Dec", "number": "", "volume": "", "publicationtype": "inproceedings", "author": ["Elphick, Sean", "Gosbell, Vic"]}]["9172875", {"address": "", "booktitle": "2020 IEEE Fifth International Conference on Data Science in Cyberspace (DSC)", "doi": "10.1109/DSC50466.2020.00051", "isbn": "", "keywords": ["Data Governance", "Data Credibility", "Data Traceability"], "location": "", "numpages": "", "pages": "290-294", "publisher": "", "series": "", "title": "A data traceability method to improve data quality in a big data environment", "url": "", "year": "2020", "abstract": "In the actual project of data sharing and data governance, the problems of data heterogeneity and low data quality have not been solved. Data quality detection based on syntax rules can effectively find data quality problems, but it had not improved the data quality, especially in a variety of heterogeneous data source environments. This paper designs a data governance model based on data traceability, and we can get data feedback and revision through this model. The proposed method considers the different ownership of data and may form a closed-loop data service chain including effective data validation, data tracking, and data revision, data release. The analysis of the case shows that the method is effective to improve the data quality and meet the requirements of data security also.", "articleno": "", "ISSN": "", "month": "July", "number": "", "volume": "", "publicationtype": "inproceedings", "author": ["Zhang, Guobao"]}]["7821627", {"address": "", "booktitle": "2016 Future Technologies Conference (FTC)", "doi": "10.1109/FTC.2016.7821627", "isbn": "", "keywords": ["Data science", "Data models", "Reservoirs", "Complexity theory", "Complex systems", "Testing", "Analytical models", "Quality Assurance", "Data Science", "Composition of Services", "Category Theory", "Complex Systems", "Formal Modelling", "Simulation", "Complexity Science", "Scientific Method", "Simulation-as-a-Service", "GUI based hypotheses builders and testers", "QA4DS", "Quality of Knowledge", "Crowd Data Science"], "location": "", "numpages": "", "pages": "307-309", "publisher": "", "series": "", "title": "Quality assurance for data science: Making data science more scientific through engaging scientific method", "url": "", "year": "2016", "abstract": "Credibility of science is fundamentally due to the strenuous efforts made to verify the general consistency among relevant facts, theories, applications, research methodologies, etc. and scientific method which emphasizes the significance of continuously building and testing hypotheses has withstood the test of time as a successful methodology of acquiring a body of knowledge, we can rely on, at least within a certain context. A paradigm based on composition of data rich services to gather data to replicate real world scenarios through complexity science based simulators, where quality of data as well as theories explaining them is primarily assured via building and testing of hypotheses, can improve our understanding of what we try to comprehend by engaging data science. While simulators would at least partially automate the implementation of scientific method, a credibility ranking mechanism, would not only help determining and disseminating rankings pertaining to the quality of data as well as theories explaining them but also receive & publish feedback regarding rankings. Including methods used in complex system analysis as part of simulators would enhance the scientific rigor of establishing the credibility of knowledge we have. Providing simulation as a service and making graphical hypothesis builders & testers available for external parties (sometimes even members of general public) would democratice the process of ascertaining the believability of data & associated theoretical models thereby further enhancing the Quality of Knowledge.", "articleno": "", "ISSN": "", "month": "Dec", "number": "", "volume": "", "publicationtype": "inproceedings", "author": ["Wickramage, Narada"]}]["1151615", {"address": "", "booktitle": "OCEANS 81", "doi": "10.1109/OCEANS.1981.1151615", "isbn": "", "keywords": ["Quality control", "Oceanographic techniques", "Data analysis", "Ocean temperature", "Sea measurements", "Pollution measurement", "Marine pollution", "Biology computing", "Temperature measurement", "Temperature dependence"], "location": "", "numpages": "", "pages": "261-263", "publisher": "", "series": "", "title": "Quality Control Procedures in Processing Oceanographic Data", "url": "", "year": "1981", "abstract": "The National Oceanographic Data Center (NODC) receives significant volumes of oceanographic environmental data for processing. These data vary from the conventional ocean station data and expendable bathythermograph data to a wide variety of biological and pollution data. The need for a more objective environmental quality control became evident as the volume of data continued to increase. At the same time, the resources-enriched computer capability and large climatological files of fully processed data-for developing such a system became available. The NODC procedures, prior to the development of environmental models, were based primarily on the initial quality control by data donors and subjective analysis by NODC oceanographers. This study describes the use of ocean models for quality controlling ocean serial and XBT data.", "articleno": "", "ISSN": "", "month": "Sep.", "number": "", "volume": "", "publicationtype": "inproceedings", "author": ["Perlroth, I.", "Hamilton, D."]}]["8695373", {"address": "", "booktitle": "2019 IEEE Conference on Multimedia Information Processing and Retrieval (MIPR)", "doi": "10.1109/MIPR.2019.00093", "isbn": "", "keywords": ["Quality control", "Servers", "Cloud computing", "Big Data", "Visualization", "Inspection", "Databases", "Big data platform, Surface Enhanced Raman Spectroscopy, sensor quality control"], "location": "", "numpages": "", "pages": "463-466", "publisher": "", "series": "", "title": "A Big Data Platform for Surface Enhanced Raman Spectroscopy Data with an Application on Image-Based Sensor Quality Control", "url": "", "year": "2019", "abstract": "Surface-enhanced Raman spectroscopy (SERS) significantly enhances the Raman scattering by molecules, enabling detection and identification of small quantities of relevant bio-/chemical markers in a wide range of applications. In this paper, we present a big data platform with both a local client and cloud server built for acquiring, processing, visualizing and storing SERS sensor data. The local client controls the hardware (i.e., spectrometer and stage) to collect SERS spectra from HP designed sensors, and offers the options to analyze, visualize and save the spectra with meta-data records, including relevant experimental conditions. The cloud server contains remote databases and web interface for centralized data management to users from different locations. Here we describe how this platform was built and demonstrate its use for automated sensor quality control based on sensor images. Sensor quality control is a common practice, employed in sensor production to select high performing sensors. Image-based approach is a natural way to perform sensor quality control without destructing the sensors. Automating this process using the proposed platform can also reduce the time spent and achieve consistent result by avoiding human visual inspection.", "articleno": "", "ISSN": "", "month": "March", "number": "", "volume": "", "publicationtype": "inproceedings", "author": ["Zuo, Yiming", "Vernica, Rares", "Lei, Yang", "Barcelo, Steven", "Rogacs, Anita"]}]["9664881", {"address": "", "booktitle": "2021 IEEE 7th International Conference on Computing, Engineering and Design (ICCED)", "doi": "10.1109/ICCED53389.2021.9664881", "isbn": "", "keywords": ["Data integrity", "Standards organizations", "Decision making", "Organizations", "Documentation", "Regulation", "Data models", "data quality", "data quality management", "data quality maturity model", "information system"], "location": "", "numpages": "", "pages": "1-5", "publisher": "", "series": "", "title": "Data Quality Management Maturity Model : A Case Study in Higher Education\u2019s Human Resource Department", "url": "", "year": "2021", "abstract": "Data has increasingly become more imperative in organization\u2019s decision-making process. Low data quality can cause extensive organizational problems, such as inaccurate decision-making and dropped business possibilities. This is because low-quality data does not present a clear description of the actual situation. In Human Resource (HR) management, low data quality can cause recruitment, career development, remuneration, and retirement processes. Therefore, proper data quality management must be implemented to produce data that suits the organization's needs. To determine how far the implementation of data quality management in the organization, measurement of the maturity level in data quality management is conducted. This study presented an evaluation of data quality management maturity level in HR of higher education, applying the Loshin data quality management maturity framework. The results of this study indicate that the maturity level in the Data quality expectations area is 2.17, the maturity level in the Data quality dimensions area is 2.16, the maturity level in the Policies area is 1.22, the maturity level in the Data quality protocols area is 2, 11, the maturity level in the Data governance area is 1.77, the maturity level in the Data standards area is 1.67, the maturity level in the Technology area is 1.44, and the maturity level in the Performance management area is 1.67. The result shows that the Policies area is the lowest due to the lack of regulations and good documentation regarding data management. It can be a concern in conducting evaluations for improving data quality management.", "articleno": "", "ISSN": "2767-7826", "month": "Aug", "number": "", "volume": "", "publicationtype": "inproceedings", "author": ["Setiadi, Yusuf", "Hidayanto, Achmad", "Rachmawati, Fitri", "Yohannes, Adhi"]}]["5347884", {"address": "", "booktitle": "2009 International Conference on Sustainable Power Generation and Supply", "doi": "10.1109/SUPERGEN.2009.5347884", "isbn": "", "keywords": ["Data compression", "Power quality", "Monitoring", "Delta modulation", "Huffman coding", "Data acquisition", "Compression algorithms", "Power measurement", "Frequency measurement", "Sampling methods", "data compression", "delta modulation", "Huffman coding", "power quality", "power system"], "location": "", "numpages": "", "pages": "1-4", "publisher": "", "series": "", "title": "A new data compression algorithm for power quality online monitoring", "url": "", "year": "2009", "abstract": "The increasing application of power quality data acquisition devices produces large amount of data that should be compressed. The paper investigates the characteristics of power quality data and delta modulation technique, and it proposes a lossless power data compression algorithm based on high-order delta modulation. The compression algorithm carries on multiple differential operations on power quality data, so it could reduce the magnitude of data and requires fewer bits for coding. The proposed algorithm has high compression ratio and little computation requirements. Furthermore, it is suitable for dealing with the power quality data measured at high sampling frequency, and it can work well with traditional compression methods such as Huffman coding. Simulations for several kinds of power quality data confirm its effectiveness and advantages over traditional Huffman coding method.", "articleno": "", "ISSN": "2156-969X", "month": "April", "number": "", "volume": "", "publicationtype": "inproceedings", "author": ["Zhang, Dahai", "Bi, Yanqiu", "Zhao, Jianguo"]}]["4813519", {"address": "", "booktitle": "2008 Second International Conference on Future Generation Communication and Networking Symposia", "doi": "10.1109/FGCNS.2008.74", "isbn": "", "keywords": ["Data mining", "XML", "Conferences", "Australia", "Data engineering", "Association rules", "Databases", "Proposals", "XML", "DATA QUALITY", "DATA MINING", "CONSTRAINTS IN XML"], "location": "", "numpages": "", "pages": "46-49", "publisher": "", "series": "", "title": "Quality Data for Data Mining and Data Mining for Quality Data: A Constraint Based Approach in XML", "url": "", "year": "2008", "abstract": "As quality data is important for data mining, reversely data mining is necessary to measure the quality of data. Specifically, in XML, the issue of quality data for mining purposes and also using data mining techniques for quality measures is becoming more necessary as a massive amount of data is being stored and represented over the Web. We propose two important interrelated issues: how quality XML data is useful for data mining in XML and how data mining in XML is used to measure the quality data for XML. When we address both issues, we consider XML constraints because constraints in XML can be used for quality measurement in XML data and also for finding some important patterns and association rules in XML data mining. We note that XML constraints can play an important role for data quality and data mining in XML. We address the theoretical framework rather than solutions. Our research framework is towards the broader task of data mining and data quality for XML data integrations.", "articleno": "", "ISSN": "", "month": "Dec", "number": "", "volume": "2", "publicationtype": "inproceedings", "author": ["Shahriar, Md.", "Anam, Sarawat"]}]["7299603", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2015.2490723", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Big data", "Social network services", "Computer architecture", "Meta data", "Online services", "architecture", "big data", "metadata", "quality attribute", "quality of data", "Architecture", "big data", "metadata", "quality attribute", "quality of data"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "Evaluating the Quality of Social Media Data in Big Data Architecture", "url": "", "volume": "3", "year": "2015", "abstract": "The use of freely available online data is rapidly increasing, as companies have detected the possibilities and the value of these data in their businesses. In particular, data from social media are seen as interesting as they can, when properly treated, assist in achieving customer insight into business decision making. However, the unstructured and uncertain nature of this kind of big data presents a new kind of challenge: how to evaluate the quality of data and manage the value of data within a big data architecture? This paper contributes to addressing this challenge by introducing a new architectural solution to evaluate and manage the quality of social media data in each processing phase of the big data pipeline. The proposed solution improves business decision making by providing real-time, validated data for the user. The solution is validated with an industrial case example, in which the customer insight is extracted from social media data in order to determine the customer satisfaction regarding the quality of a product.", "pages": "2028-2043", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Immonen, Anne", "P\u00e4\u00e4kk\u00f6nen, Pekka", "Ovaska, Eila"]}]["9005614", {"address": "", "booktitle": "2019 IEEE International Conference on Big Data (Big Data)", "doi": "10.1109/BigData47090.2019.9005614", "isbn": "", "keywords": ["Data models", "Big Data", "Business", "Analytical models", "Computational modeling", "Lakes", "Solid modeling", "Big Data", "Big Data Lake", "Scalable Data Modeling", "Hadoop", "Spark", "Business Intelligence", "Big Data Analytics"], "location": "", "numpages": "", "pages": "2691-2697", "publisher": "", "series": "", "title": "An Effective and Scalable Data Modeling for Enterprise Big Data Platform", "url": "", "year": "2019", "abstract": "The enormous growth of the internet, enterprise applications, social media, and IoT devices in the current time caused a huge spike in enterprise data growth. Big data platform provided scalable storage to manage enterprise data growth and served easier data access to decision-makers, stakeholders and business users. It is a well-known challenge to classify, organize and store all this data and process it to provide business insights. Due to nature, variety, velocity, volume and value of data make it difficult to effectively process big data. Enterprises face challenges to apply complex business rules, to generate insights and to support data-driven decisions in a timely fashion. As big data lake integrates streams of data from a bunch of business units, stakeholders usually analyze enterprise-wide data from various data models. Data models are a vital component of Big data platform. Users may do complex processing, run queries and perform big table joins to generate required metrics depending on the available data models. It is usually a time consuming and resource-intensive process to find the value from data. It is a no-brainer that big data platform in the enterprise needs high-quality data modeling methods to reach an optimal mix of cost, performance, and quality. This paper addresses these challenges by proposing an effective and scalable way to organize and store data in Big Data Lake. It presents some of the basic principles and methodology to build scalable data models in a distributed environment. It also describes how it overcomes common challenges and presents findings.", "articleno": "", "ISSN": "", "month": "Dec", "number": "", "volume": "", "publicationtype": "inproceedings", "author": ["Patel, Jayesh"]}]["7822785", {"address": "", "booktitle": "2016 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)", "doi": "10.1109/BIBM.2016.7822785", "isbn": "", "keywords": ["Quality control", "Quality Control (QC)", "Microarray", "Omics data", "MicroArray Quality Control(MAQC) project", "High-dimensional data quality control (HidQC) plot"], "location": "", "numpages": "", "pages": "1763-1766", "publisher": "", "series": "", "title": "Quality control plot for high dimensional omics data", "url": "", "year": "2016", "abstract": "Quality control (QC) becomes more important in pre-processing analysis of high dimensional omics data. Several routine QC processes became a standard process in omics data analysis. The standard QC analysis includes calculating quality-related measures, checking the consistency among samples, detecting outlying observations and so forth. QC analysis tends to be more important in the era of high dimensional omics data. Although several QC analysis tools providing simple graphical display have been developed by many researchers, they usually require a subjective decision on QC. Here, we propose high-dimensional data quality control (HidQC) plot which is a simple and efficient QC tool for handling high dimensional omics data. HidQC plot primarily focuses on identifying samples of poor quality by conducting a contrast analysis for the between/within group distances and the summary distances. HidQC plot checks the quality by investigating the consistency of samples for each group. Unlike other QC plots, HidQC plot provides the p-value of each sample based on the permutation test, which can be used as a more objective criterion to determine whether to use the sample or not. We applied HidQC plot to MicroArray Quality Control (MAQC) project 1 data to demonstrate its usefulness.", "articleno": "", "ISSN": "", "month": "Dec", "number": "", "volume": "", "publicationtype": "inproceedings", "author": ["Kim, Gyu-tae", "Kim, Yongkang", "Kwon, Min-Seok", "Park, Taesung"]}]["6758125", {"address": "", "booktitle": "Proceedings of 2013 2nd International Conference on Measurement, Information and Control", "doi": "10.1109/MIC.2013.6758125", "isbn": "", "keywords": ["Power quality", "Wavelet transforms", "Data compression", "Voltage fluctuations", "Interrupters", "Educational institutions", "power power quality", "dq0 transform", "wavelet transform", "data compression"], "location": "", "numpages": "", "pages": "987-990", "publisher": "", "series": "", "title": "Compression method of power quality data based on wavelet transform", "url": "", "year": "2013", "abstract": "The demand for data compression has increased considerably due to the huge amount of the power quality data detected. Aiming at the problem the current power quality data compression methods are short of considering the correlation of the three-phase power quality data, a novel approach of data compression for three-phase power quality based on wavelet transform is presented in the paper. The simulation results show that the proposed method can not only get high signal noise ratio relative to the existing methods of the power quality data compression under the same compression ratio, but also control the power quality compression performance according to the practical requirement flexibly.", "articleno": "", "ISSN": "", "month": "Aug", "number": "", "volume": "02", "publicationtype": "inproceedings", "author": ["Rui, Zhang", "Hong-jiao, Yao", "Chuan-guang, Zhang"]}]["6204987", {"address": "", "booktitle": "2012 International Conference on Information Retrieval & Knowledge Management", "doi": "10.1109/InfRKM.2012.6204987", "isbn": "", "keywords": ["Data models", "Data warehouses", "Accuracy", "Quality management", "Organizations", "Complexity theory", "data quality", "data warehouses", "data quality management", "data quality dimension"], "location": "", "numpages": "", "pages": "268-272", "publisher": "", "series": "", "title": "Data quality comparative model for data warehouse", "url": "", "year": "2012", "abstract": "The growth of daily data and complexity in data warehouse with enhanced the information technology has created new challenges for information user. The demand for quality data has increase an awareness of the quality, reliable and accuracy of information in making fast and reliable decision-making. Nowadays, many organizations are depending on their resources in data warehouse. As that matter of fact, the qualities of data warehouse are greatly concern. The poor and error data will cause more trouble in data warehouse as data accessed from the same resources by the user. Here we present the systematic review comparative model to determine the data quality model as further research in our studies.", "articleno": "", "ISSN": "", "month": "March", "number": "", "volume": "", "publicationtype": "inproceedings", "author": ["Sidi, Fatimah", "Ramli, Abdullah", "Jabar, Marzanah", "Affendey, Lilly", "Mustapha, Aida", "Ibrahim, Hamidah"]}]["9510379", {"address": "", "booktitle": "2021 IEEE 4th International Electrical and Energy Conference (CIEEC)", "doi": "10.1109/CIEEC50170.2021.9510379", "isbn": "", "keywords": ["Fault diagnosis", "Power quality", "Forestry", "Big Data", "Power system harmonics", "Harmonic analysis", "Data models", "transient power quality problem", "power quality big data", "random forest", "data-driven"], "location": "", "numpages": "", "pages": "1-6", "publisher": "", "series": "", "title": "A novel identification and location method for transient power quality disturbance sources", "url": "", "year": "2021", "abstract": "The application of power electronics and high penetration of new energy generation have brought great economic and social benefits. Meanwhile, new power quality phenomena and new issues have come into existence, such as three-phase unbalance, harmonic and low-frequency resonance, etc. In order to solve power quality problems fundamentally. The paper proposes a novel identifying and locating method utilizing transient power quality data. Based on the operational big data of power system, spatio-temporal data model of power quality is established. Then, based on big data, by adopting random forest algorithm, the data is analyzed to identify and locate transient power quality problem disturbance sources. This paper simulates in IEEE 30 bus system, and the results indicate the accuracy in identifying and locating short circuit disturbance sources.", "articleno": "", "ISSN": "", "month": "May", "number": "", "volume": "", "publicationtype": "inproceedings", "author": ["Qu, Xiaoyu", "Dong, Kun", "Zhao, Jianfeng", "Liu, Weicheng", "Shi, Zhan", "Yu, Yue"]}]["7944943", {"address": "", "booktitle": "2017 IEEE Third International Conference on Big Data Computing Service and Applications (BigDataService)", "doi": "10.1109/BigDataService.2017.40", "isbn": "", "keywords": ["Water pollution", "Water resources", "Big Data", "Analytical models", "Data models", "Water", "Indexes", "Water quality evaluation", "big data analytics", "data-driven water quality evaluation", "and water quality prediction"], "location": "", "numpages": "", "pages": "224-232", "publisher": "", "series": "", "title": "Data-Driven Water Quality Analysis and Prediction: A Survey", "url": "", "year": "2017", "abstract": "Water quality becomes one of the important quality factors for the quality life in smart cities. Recently, water quality has been degraded due to diverse forms of pollution caused by disposal of human wastes, industrial wastes, automobile wastes. The increasing pollution affects water quality and the quality of people's life. Hence, water quality evaluation, monitoring, and prediction become an important and hot research subject. In the past, many environmental researchers have dedicated their research efforts on this subject using conventional approaches. Recently, many researchers begin to use the big data analytics approach to studying, evaluating, and predicting water quality due to the advances of big data applications and the availability of environmental sensing networks and sensor data. This paper reviews the published research results relating to water quality evaluation and prediction. Moreover, the paper classifies and compares the applied big data analytics approaches and big data based prediction models for water quality assessment. Furthermore, the paper also discusses the future research needs and challenges.", "articleno": "", "ISSN": "", "month": "April", "number": "", "volume": "", "publicationtype": "inproceedings", "author": ["Kang, Gaganjot", "Gao, Jerry", "Xie, Gang"]}]["8332626", {"address": "", "booktitle": "2017 14th Web Information Systems and Applications Conference (WISA)", "doi": "10.1109/WISA.2017.24", "isbn": "", "keywords": ["Data integrity", "Measurement", "Filtering", "Data models", "Portals", "Quality assessment", "Tools", "data quality-based filtering and ranking", "topical datasets", "overall data quality", "data quality assessment", "Data Quality Vocabulary (DQV)", "open data portal"], "location": "", "numpages": "", "pages": "257-262", "publisher": "", "series": "", "title": "User-Driven Filtering and Ranking of Topical Datasets Based on Overall Data Quality", "url": "", "year": "2017", "abstract": "Finding relevant and high-quality data is the eternal needs for data consumers (i.e., users). Many open data portals have been providing users with simple ways of finding datasets on a particular topic (i.e., topical datasets), which are not a way of filtering and ranking topical datasets based on data quality. Despite the recent advances in the development and standardization of data quality models and vocabulary, there is a lack of systematic research on approaches and tools for user-driven data quality-based filtering and ranking of topical datasets. In this paper we address the problem of user-driven filtering and ranking of topical datasets based on the overall data quality of datasets by developing a generic software architecture and the corresponding approach, called ODQFiRD, for filtering and ranking topical datasets according to user-specified data quality assessment criteria. Additionally, we use our implemented prototype of ODQFiRD to conduct a case study experiment on the U.S. Government's open data portal. The prototype implementation and experimental results show that our proposed ODQFiRD is achievable and effective.", "articleno": "", "ISSN": "", "month": "Nov", "number": "", "volume": "", "publicationtype": "inproceedings", "author": ["Xia, Wenze", "Xu, Zhuoming", "Mao, Chengwang"]}]["9798931", {"address": "", "booktitle": "2021 International Conference on Computational Science and Computational Intelligence (CSCI)", "doi": "10.1109/CSCI54926.2021.00123", "isbn": "", "keywords": ["Industries", "Costs", "Scientific computing", "Data integrity", "Computational modeling", "Organizations", "Data models", "Data quality assessment", "Zero Trust", "Data Quality", "Data quality dimensions", "Trusted Data"], "location": "", "numpages": "", "pages": "305-307", "publisher": "", "series": "", "title": "A Zero Trust Model Based Framework For Data Quality Assessment", "url": "", "year": "2021", "abstract": "Zero trust security model has been picking up adoption in various organizations due to its various advantages. Data quality is still one of the fundamental challenges in data curation in many organizations where data consumers don\u2019t trust data due to associated quality issues. As a result, there is a lack of confidence in making business decisions based on data. We design a model based on the zero trust security model to demonstrate how the trust of data consumers can be established. We present a sample application to distinguish the traditional approach from the zero trust based data quality framework.", "articleno": "", "ISSN": "", "month": "Dec", "number": "", "volume": "", "publicationtype": "inproceedings", "author": ["Mohammed, Mahmood", "Talburt, John", "Dagtas, Serhan", "Hollingsworth, Melissa"]}]["9322931", {"address": "", "booktitle": "2020 International Conference Quality Management, Transport and Information Security, Information Technologies (IT&QM&IS)", "doi": "10.1109/ITQMIS51053.2020.9322931", "isbn": "", "keywords": ["Quality management", "Information technology", "Neural networks", "Tools", "Information security", "Big Data", "Standards organizations", "quality management system", "digital transformation", "digital environment", "non-conforming products", "big data", "neural networks", "internet of things"], "location": "", "numpages": "", "pages": "266-268", "publisher": "", "series": "", "title": "Non-conforming Products Management in a Digital Quality Management System", "url": "", "year": "2020", "abstract": "This article addresses the issue of changes in the quality management system, with the digitalization of the company. More specifically, changes regarding the process of managing non-conforming products. The article reflects how the use of digital tools and new technologies affects the process of detecting non-conformities and how the procedure for working with non-conforming products changes: how is the collection of data on the characteristics of the facility, what decisions need to be made regarding the inconsistencies, how the structure of work with non-conformances is changing and to what extent the use of information tools is beneficial.", "articleno": "", "ISSN": "", "month": "Sep.", "number": "", "volume": "", "publicationtype": "inproceedings", "author": ["Mandrakov, Egor", "Vasiliev, Victor", "Dudina, Diana"]}]["8859426", {"address": "", "booktitle": "2019 IEEE 19th International Conference on Software Quality, Reliability and Security Companion (QRS-C)", "doi": "10.1109/QRS-C.2019.00026", "isbn": "", "keywords": ["Data integrity", "Big Data", "Decision making", "Organizations", "Standards organizations", "data quality", "judicial data governance", "quality measurement"], "location": "", "numpages": "", "pages": "66-70", "publisher": "", "series": "", "title": "Quality Driven Judicial Data Governance", "url": "", "year": "2019", "abstract": "With the development of Smart Court 3.0, the amount of judicial data that can be stored and processed by the computer is increasing rapidly. People gradually realize that judicial data contains tremendous social and business value. However, we need stronger ability to handle with and apply massive, multi-source and heterogeneous judicial data. A complete data governance system should be built in order to make full use of the value of data assets. In such a data governance system, data quality control is one of the key steps of data governance, and also the bottleneck of data service development, because data quality determines the upper limit of data application. This paper proposes a judicial data quality measurement framework by analyzing some judicial business data, followed by a data governance method driven by it.", "articleno": "", "ISSN": "", "month": "July", "number": "", "volume": "", "publicationtype": "inproceedings", "author": ["He, Tieke", "Chen, Shenghao", "Hao, Lian", "Liu, Jia"]}]["7502278", {"address": "", "booktitle": "2016 IEEE 2nd International Conference on Big Data Security on Cloud (BigDataSecurity), IEEE International Conference on High Performance and Smart Computing (HPSC), and IEEE International Conference on Intelligent Data and Security (IDS)", "doi": "10.1109/BigDataSecurity-HPSC-IDS.2016.52", "isbn": "", "keywords": ["Context", "Business", "Metadata", "Big data", "Electronic mail", "Reliability", "Indexes", "Enterprise data", "Data catalogue", "Metadata of data", "Data context", "Data quality", "Data governance", "Data as a service"], "location": "", "numpages": "", "pages": "134-139", "publisher": "", "series": "", "title": "Aspects of Data Cataloguing for Enterprise Data Platforms", "url": "", "year": "2016", "abstract": "As the adoption of enterprise Big Data platforms mature, the necessity to maintain systematic catalogue of data being processed and managed by these platforms becomes imperative. Enterprise data catalogues serve as centralized repositories of storing such metadata about data being handled by such platforms, enabling better data governance, security and control. Standardized approaches, methodologies and tools for data catalogues are being discussed and evolved. This paper introduces some key variables, attributes and indexes that need to be handled in such cataloguing solutions -- such as data contexts, data-system relationships, data quality, reliability, sensitivity and accessibility. The paper also discusses specific approaches on how each of these aspects can be adopted and applied to different enterprise contexts effectively.", "articleno": "", "ISSN": "", "month": "April", "number": "", "volume": "", "publicationtype": "inproceedings", "author": ["Shanmugam, Srinivasan", "Seshadri, Gokul"]}]["9150244", {"address": "", "booktitle": "2020 International Conference on Big Data and Informatization Education (ICBDIE)", "doi": "10.1109/ICBDIE50010.2020.00015", "isbn": "", "keywords": ["Data models", "Neural networks", "Analytical models", "Predictive models", "Rough surfaces", "Surface roughness", "Mathematical model", "Product quality", "BP neural network", "data analysis", "surface roughness", "thickness error"], "location": "", "numpages": "", "pages": "34-38", "publisher": "", "series": "", "title": "Product quality prediction of rolling mill in big data environment", "url": "", "year": "2020", "abstract": "With the wide use of rolling mill in iron and steel industry, the quality of rolling mill products has become the primary goal of people. However, due to design defects and manufacturing quality problems, the quality of steel products is seriously affected, and the surface roughness and thickness of steel plate are important quality indicators. In this paper, by analyzing a large number of monitoring data of rolling mill condition and using BP neural network model [1], the discrete system model between monitoring data and \u201csurface roughness\u201d and \u201cthickness error\u201d of rolling steel plate is further established.", "articleno": "", "ISSN": "", "month": "April", "number": "", "volume": "", "publicationtype": "inproceedings", "author": ["Zhang, Lanlan", "Zou, Du"]}]["9660957", {"address": "", "booktitle": "2021 11th IEEE International Conference on Intelligent Data Acquisition and Advanced Computing Systems: Technology and Applications (IDAACS)", "doi": "10.1109/IDAACS53288.2021.9660957", "isbn": "", "keywords": ["Measurement", "Uncertainty", "Systematics", "Data integrity", "Soft sensors", "Decision making", "Data acquisition", "data quality", "data source", "data acquisition", "manufacturing", "decision-making", "review"], "location": "", "numpages": "", "pages": "1200-1205", "publisher": "", "series": "", "title": "Issues of Intelligent Data Acquisition and Quality for Manufacturing Decision-Support in an Industry 4.0 Context", "url": "", "year": "2021", "abstract": "Data quality plays an essential role in decision-making, as the latter may incorporate some risks in different application areas. In the context of industry 4.0, the amount, the versatility, and the speed of information flow for decision-making are important issues. The quality and, in particular, the dependability of data is paramount. This paper investigates the leading data quality characteristics in the industry 4.0 environment with the related issues due to various interactions. It proposes a taxonomy of data sources and flows, from acquisition to the information extraction level for decision-making. The authors highlight the specific issues of error and uncertainty propagation management as significant research challenges for designing intelligent data collection and acquisition systems for industrial manufacturing decision support within the framework of the new generation of industry development. They review data quality characteristics definition and assessment requirements, and suggest classifying these characteristics into four categories. Data quality assessment methods and models with relation to decision-making are also examined. They willfully left aside the investigation of data quality improvement processes for a future more detailed paper.", "articleno": "", "ISSN": "2770-4254", "month": "Sep.", "number": "", "volume": "2", "publicationtype": "inproceedings", "author": ["Yuan, Tangxiao", "Adjallah, Kondo", "Sava, Alexandre", "Wang, Huifen", "Liu, Linyan"]}]["8964872", {"address": "", "booktitle": "2019 10th International Conference on Information Technology in Medicine and Education (ITME)", "doi": "10.1109/ITME.2019.00149", "isbn": "", "keywords": ["Cleaning", "Data integrity", "Filling", "Microsoft Windows", "Data mining", "Instruction sets", "data flow", "data quality", "data cleaning", "cleaning evaluation"], "location": "", "numpages": "", "pages": "645-648", "publisher": "", "series": "", "title": "Design and Implementation of a Universal Data Quality Management Software Based on Data Flow", "url": "", "year": "2019", "abstract": "Data quality problems are analyzed to get several typical problems, such as data missing, data duplication, data abnormality, data inconsistency and data logic error. In order to resolve these problems, a universal data quality management software is proposed. This software provides data cleaning method for each problem and evaluates the effect of these methods, and manages the plug-in components for use. The architecture and critical technologies are introduced in detail, and main steps are shown with a specific application. According to the feedback of users, this new software is powerful, adaptable, simple to use, easy to set processes, with high data cleaning efficiency and accurate effect evaluations.", "articleno": "", "ISSN": "2474-3828", "month": "Aug", "number": "", "volume": "", "publicationtype": "inproceedings", "author": ["Song, Jinyu", "Hao, Jiandong", "Gang, Chen", "Suojuan, Zhang", "Yiping, Guo"]}]["7374131", {"address": "", "booktitle": "2015 International Conference on Computing, Communication and Security (ICCCS)", "doi": "10.1109/CCCS.2015.7374131", "isbn": "", "keywords": ["Big data", "Context", "Quality management", "Frequency measurement", "Organizations", "Big Data", "Data quality", "Data profiling", "Data cleansing", "data quality rules", "dimensions", "metrics"], "location": "", "numpages": "", "pages": "1-9", "publisher": "", "series": "", "title": "Overview of data quality challenges in the context of Big Data", "url": "", "year": "2015", "abstract": "Data quality management systems are thoroughly researched topics and have resulted in many tools and techniques developed by both academia and industry. However, the advent of Big Data might pose some serious questions pertaining to the applicability of existing data quality concepts. There is a debate concerning the importance of data quality for Big Data; one school of thought argues that high data quality methods are essential for deriving higher level analytics while another school of thought argues that data quality level will not be so important as the volume of Big Data would be used to produce patterns and some amount of dirty data will not mask the analytic results which might be derived. This paper aims to investigate various components and activities forming part of data quality management such as dimensions, metrics, data quality rules, data profiling and data cleansing. The result list existing challenges and future research areas associated with Big Data for data quality management.", "articleno": "", "ISSN": "", "month": "Dec", "number": "", "volume": "", "publicationtype": "inproceedings", "author": ["Juddoo, Suraj"]}]["8453066", {"address": "", "booktitle": "2018 IEEE/ACM 40th International Conference on Software Engineering (ICSE)", "doi": "10.1145/3180155.3182534", "isbn": "", "keywords": ["Big Data", "Databases", "Quality of service", "Tools", "Fault tolerance", "Fault tolerant systems", "Software", "Data intensive applications", "Experiment driven action research", "Big data", "Data migration"], "location": "", "numpages": "", "pages": "93-93", "publisher": "", "series": "", "title": "[Journal First] Experiences and Challenges in Building a Data Intensive System for Data Migration", "url": "", "year": "2018", "abstract": "Recent analyses[2, 4, 5] report that many sectors of our economy and society are more and more guided by data-driven decision processes (e.g., health care, public administrations, etc.). As such, Data Intensive (DI) applications are becoming more and more important and critical. They must be fault-tolerant, they should scale with the amount of data, and be able to elastically leverage additional resources as and when these last ones are provided [3]. Moreover, they should be able to avoid data drops introduced in case of sudden overloads and should offer some Quality of Service (QoS) guarantees. Ensuring all these properties is, per se, a challenge, but it becomes even more difficult for DI applications, given the large amount of data to be managed and the significant level of parallelism required for its components. Even if today some technological frameworks are available for the development of such applications (for instance, think of Spark, Storm, Flink), we still lack solid software engineering approaches to support their development and, in particular, to ensure that they offer the required properties in terms of availability, throughput, data loss, etc. In fact, at the time of writing, identifying the right solution can require several rounds of experiments and the adoption of many different technologies. This implies the need for highly skilled persons and the execution of experiments with large data sets and a large number of resources, and, consequently, a significant amount of time and budget. To experiment with currently available approaches, we performed an action research experiment focusing on developing- testing-reengineering a specific DI application, Hegira4Cloud, that migrates data between widely used NoSQL databases, including so-called Database as a Service (DaaS), as well as on-premise databases. This is a representative DI system because it has to handle large volumes of data with different structures and has to guarantee that some important characteristics, in terms of data types and transactional properties, are preserved. Also, it poses stringent requirements in terms of correctness, high performance, fault tolerance, and fast and effective recovery. In our action research, we discovered that the literature offered some high level design guidelines for DI applications, as well as some tools to support modelling and QoS analysis/simulation of complex architectures, however the available tools were not yet. suitable to support DI systems. Moreover, we realized that the available big data frameworks we could have used were not flexible enough to cope with all possible application-specific aspects of our system. Hence, to achieve the desired level of performance, fault tolerance and recovery, we had to adopt a time-consuming, experiment-based approach [1, 6], which, in our case, consisted of three iterations: (1) the design and implementation of a Mediation Data Model capable of managing data extracted from different databases, together with a first monholitical prototype of Hegira4Cloud; (2) the improvement of performance of our prototype when managing and transferring huge amounts of data; (3) the introduction of fault-tolerant data extraction and management mechanisms, which are independent from the targeted databases. Among the others, an important issue that has forced us to reiterate in the development of Hegira4Cloud concerned the DaaS we interfaced with. In particular these DaaS, which are well-known services with a large number of users: (1) were missing detailed information regarding the behaviour of their APIs; (2) did not offer a predictable service; (3) were suffering of random downtimes not correlated with the datasets we were experimenting with. In this journal first presentation, we describe our experience and the issues we encountered that led to some important decisions during the software design and engineering process. Also, we analyse the state of the art of software design and verification tools and approaches in the light of our experience, and identify weaknesses, alternative design approaches and open challenges that could generate new research in these areas. More details can be found in the journal publication.", "articleno": "", "ISSN": "1558-1225", "month": "May", "number": "", "volume": "", "publicationtype": "inproceedings", "author": ["Scavuzzo, Marco", "Di, Elisabetta", "Ardagna, Danilo"]}]["8935096", {"address": "", "articleno": "", "doi": "10.26599/BDMA.2019.9020019", "issn": "2096-0654", "issue_date": "", "journal": "Big Data Mining and Analytics", "keywords": ["Big Data", "Training", "Data mining", "Cleaning", "Sampling methods", "Heuristic algorithms", "Fault tolerance", "data mining", "conditional functional dependency", "big data", "data quality"], "month": "March", "number": "1", "numpages": "", "publisher": "", "title": "Mining conditional functional dependency rules on big data", "url": "", "volume": "3", "year": "2020", "abstract": "Current Conditional Functional Dependency (CFD) discovery algorithms always need a well-prepared training dataset. This condition makes them difficult to apply on large and low-quality datasets. To handle the volume issue of big data, we develop the sampling algorithms to obtain a small representative training set. We design the fault-tolerant rule discovery and conflict-resolution algorithms to address the low-quality issue of big data. We also propose parameter selection strategy to ensure the effectiveness of CFD discovery algorithms. Experimental results demonstrate that our method can discover effective CFD rules on billion-tuple data within a reasonable period.", "pages": "68-84", "note": "", "ISSN": "2096-0654", "publicationtype": "article", "author": ["Li, Mingda", "Wang, Hongzhi", "Li, Jianzhong"]}]["7979931", {"address": "", "booktitle": "2016 7th International Conference on Cloud Computing and Big Data (CCBD)", "doi": "10.1109/CCBD.2016.073", "isbn": "", "keywords": ["Detectors", "Image edge detection", "Radiography", "Standards", "Image quality", "X-ray imaging", "Indexes", "image quality", "in-service", "radiographic product", "routine data", "quality control"], "location": "", "numpages": "", "pages": "341-346", "publisher": "", "series": "", "title": "Big Data Analysis on Radiographic Image Quality", "url": "", "year": "2016", "abstract": "Mass data generated from in-service radiographic product contain assignable information on Image Quality (IQ). Analyzing data from routine work might supplement the time-consuming Image Quality Assurance Test Procedure (IQATP) to evaluate IQ and to know product type performance on site, which can also locate risks and give manufacturer directions for the further actions as well. This article illustrates methodologies of extracting IQ information from mass data and visual quality track, analysis, control, and risk mitigation in Big Data environments.", "articleno": "", "ISSN": "", "month": "Nov", "number": "", "volume": "", "publicationtype": "inproceedings", "author": ["Wen, Hongsheng", "Chen, Zhiqiang", "Gu, Jianping", "Zhu, Qiangqiang"]}]["9106668", {"address": "", "booktitle": "2020 Systems and Information Engineering Design Symposium (SIEDS)", "doi": "10.1109/SIEDS49339.2020.9106668", "isbn": "", "keywords": ["Tracking", "Image color analysis", "Data integrity", "Gaze tracking", "Data collection", "Light emitting diodes", "Real-time systems", "eye-tracking", "feedback", "data quality"], "location": "", "numpages": "", "pages": "1-3", "publisher": "", "series": "", "title": "Improving Data Quality from Remote Eye Tracking Systems Using Real Time Feedback", "url": "", "year": "2020", "abstract": "This study proposes a solution to improve data quality from remote desktop eye trackers. Poor data quality from these systems regularly occurs as a result of participants unknowingly moving outside of the functional data collection area, i.e. the eye tracking box. Researchers are often not aware of the low quality data until after it has been recorded. As a result potentially large amounts of data are unusable. To alleviate this concern, we propose a real-time feedback system that alerts participants when poor eye tracking data are detected, thus enabling them to adjust their position in front of the eye tracker as soon as they move out of the functional data collection area. This capability allows researchers to acquire a higher percentage of useful data over the course of an experiment. Our approach utilized a Raspberry Pi that collected and interpreted data quality from an eye tracker in real time. Data quality from each eye was mapped to a light emitting diode (LED) placed above the computer monitor. The color of LED reflected the current quality of eye tracking data with green and red indicating high and low quality respectively. To determine if the system was effective, we compared the data quality for participants who used the system relative to participants who did not while they performed a cognitive task. Results show increased data quality for those participants using the feedback system. Our results suggest that future studies using remote desktop eye trackers can increase data quality by providing real-time data quality feedback to the participants.", "articleno": "", "ISSN": "", "month": "April", "number": "", "volume": "", "publicationtype": "inproceedings", "author": ["Shevchenko, Peter", "Faurot, Noah", "Barentine, Christian", "Ries, Anthony"]}]