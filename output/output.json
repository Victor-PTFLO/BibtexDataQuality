["8935096", {"address": "", "articleno": "", "doi": "10.26599/BDMA.2019.9020019", "issn": "2096-0654", "issue_date": "", "journal": "Big Data Mining and Analytics", "keywords": ["Big Data", "Training", "Data mining", "Cleaning", "Sampling methods", "Heuristic algorithms", "Fault tolerance", "data mining", "conditional functional dependency", "big data", "data quality"], "month": "March", "number": "1", "numpages": "", "publisher": "", "title": "Mining conditional functional dependency rules on big data", "url": "", "volume": "3", "year": "2020", "abstract": "Current Conditional Functional Dependency (CFD) discovery algorithms always need a well-prepared training dataset. This condition makes them difficult to apply on large and low-quality datasets. To handle the volume issue of big data, we develop the sampling algorithms to obtain a small representative training set. We design the fault-tolerant rule discovery and conflict-resolution algorithms to address the low-quality issue of big data. We also propose parameter selection strategy to ensure the effectiveness of CFD discovery algorithms. Experimental results demonstrate that our method can discover effective CFD rules on billion-tuple data within a reasonable period.", "pages": "68-84", "note": "", "ISSN": "2096-0654", "publicationtype": "article", "author": ["Li, Mingda", "Wang, Hongzhi", "Li, Jianzhong"]}]["9523499", {"address": "", "articleno": "", "doi": "10.26599/BDMA.2021.9020009", "issn": "2096-0654", "issue_date": "", "journal": "Big Data Mining and Analytics", "keywords": ["Structured Query Language", "Optimization", "Engines", "C++ languages", "Sparks", "Big Data", "Query processing", "big data", "C++", "Structured Query Language (SQL)", "query optimization"], "month": "Dec", "number": "4", "numpages": "", "publisher": "", "title": "LotusSQL: SQL engine for high-performance big data systems", "url": "", "volume": "4", "year": "2021", "abstract": "In recent years, Apache Spark has become the de facto standard for big data processing. SparkSQL is a module offering support for relational analysis on Spark with Structured Query Language (SQL). SparkSQL provides convenient data processing interfaces. Despite its efficient optimizer, SparkSQL still suffers from the inefficiency of Spark resulting from Java virtual machine and the unnecessary data serialization and deserialization. Adopting native languages such as C++ could help to avoid such bottlenecks. Benefiting from a bare-metal runtime environment and template usage, systems with C++ interfaces usually achieve superior performance. However, the complexity of native languages also increases the required programming and debugging efforts. In this work, we present LotusSQL, an engine to provide SQL support for dataset abstraction on a native backend Lotus. We employ a convenient SQL processing framework to deal with frontend jobs. Advanced query optimization technologies are added to improve the quality of execution plans. Above the storage design and user interface of the compute engine, LotusSQL implements a set of structured dataset operations with high efficiency and integrates them with the frontend. Evaluation results show that LotusSQL achieves a speedup of up to 9\u00d7 in certain queries and outperforms Spark SQL in a standard query benchmark by more than 2\u00d7 on average.", "pages": "252-265", "note": "", "ISSN": "2096-0654", "publicationtype": "article", "author": ["Li, Xiaohan", "Yu, Bowen", "Feng, Guanyu", "Wang, Haojie", "Chen, Wenguang"]}]["7299603", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2015.2490723", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Big data", "Social network services", "Computer architecture", "Meta data", "Online services", "architecture", "big data", "metadata", "quality attribute", "quality of data", "Architecture", "big data", "metadata", "quality attribute", "quality of data"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "Evaluating the Quality of Social Media Data in Big Data Architecture", "url": "", "volume": "3", "year": "2015", "abstract": "The use of freely available online data is rapidly increasing, as companies have detected the possibilities and the value of these data in their businesses. In particular, data from social media are seen as interesting as they can, when properly treated, assist in achieving customer insight into business decision making. However, the unstructured and uncertain nature of this kind of big data presents a new kind of challenge: how to evaluate the quality of data and manage the value of data within a big data architecture? This paper contributes to addressing this challenge by introducing a new architectural solution to evaluate and manage the quality of social media data in each processing phase of the big data pipeline. The proposed solution improves business decision making by providing real-time, validated data for the user. The solution is validated with an industrial case example, in which the customer insight is extracted from social media data in order to determine the customer satisfaction regarding the quality of a product.", "pages": "2028-2043", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Immonen, Anne", "P\u00e4\u00e4kk\u00f6nen, Pekka", "Ovaska, Eila"]}]["8787229", {"address": "", "articleno": "", "doi": "10.26599/BDMA.2018.9020037", "issn": "2096-0654", "issue_date": "", "journal": "Big Data Mining and Analytics", "keywords": ["Clustering algorithms", "Big Data", "Ocean temperature", "Data mining", "Meteorology", "Partitioning algorithms", "Temperature control", "clustering", "Hadoop", "big data", "k-means", "hierarchical"], "month": "Dec", "number": "4", "numpages": "", "publisher": "", "title": "A novel clustering technique for efficient clustering of big data in Hadoop Ecosystem", "url": "", "volume": "2", "year": "2019", "abstract": "Big data analytics and data mining are techniques used to analyze data and to extract hidden information. Traditional approaches to analysis and extraction do not work well for big data because this data is complex and of very high volume. A major data mining technique known as data clustering groups the data into clusters and makes it easy to extract information from these clusters. However, existing clustering algorithms, such as k-means and hierarchical, are not efficient as the quality of the clusters they produce is compromised. Therefore, there is a need to design an efficient and highly scalable clustering algorithm. In this paper, we put forward a new clustering algorithm called hybrid clustering in order to overcome the disadvantages of existing clustering algorithms. We compare the new hybrid algorithm with existing algorithms on the bases of precision, recall, F-measure, execution time, and accuracy of results. From the experimental results, it is clear that the proposed hybrid clustering algorithm is more accurate, and has better precision, recall, and F-measure values.", "pages": "240-247", "note": "", "ISSN": "2096-0654", "publicationtype": "article", "author": ["Kumar, Sunil", "Singh, Maninder"]}]["8782595", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2019.2932259", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Nonvolatile memory", "Quality of service", "Data analysis", "Data centers", "Bandwidth", "Big Data", "Robustness", "Big data analysis", "data recovery", "IC-IoT", "NVM", "QoS improvement"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "Active Data Replica Recovery for Quality-Assurance Big Data Analysis in IC-IoT", "url": "", "volume": "7", "year": "2019", "abstract": "QoS-aware big data analysis is critical in Information-Centric Internet of Things (IC-IoT) system to support various applications like smart city, smart grid, smart health, intelligent transportation systems, and so on. The employment of non-volatile memory (NVM) in cloud or edge system provides good opportunity to improve quality of data analysis tasks. However, we have to face the data recovery problem led by NVM failure due to the limited write endurance. In this paper, we investigate the data recovery problem for QoS guarantee and system robustness, followed by proposing a rarity-aware data recovery algorithm. The core idea is to establish the rarity indicator to evaluate the replica distribution and service requirement comprehensively. With this idea, we give the lost replicas with distinguishing priority and eliminate the unnecessary replicas. Then, the data replicas are recovered stage by stage to guarantee QoS and provide system robustness. From our extensive experiments and simulations, it is shown that the proposed algorithm has significant performance improvement on QoS and robustness than the traditional direct data recovery method. Besides, the algorithm gives an acceptable data recovery time.", "pages": "106997-107005", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Wang, Songyun", "Yuan, Jiabin", "Li, Xin", "Qian, Zhuzhong", "Arena, Fabio", "You, Ilsun"]}]["8822937", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2019.2939196", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Data science", "Data visualization", "Economics", "Training", "Big Data applications", "Applied Technical Colleges and Universities", "big data education", "curriculum reform"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "Curriculum Reform in Big Data Education at Applied Technical Colleges and Universities in China", "url": "", "volume": "7", "year": "2019", "abstract": "With the boom in data science, big data education has received increasing attention from all kinds of colleges and universities in China, and many of them are in a rush to offer big data education. This paper first analyzes the major areas of big data capability training and the Chinese market needs for various kinds of data science talent. Then, it discusses the curriculum design process for the \u201cData Science & Big Data Technology\u201d bachelor's degree program, and summarizes some detailed approaches to improving teaching experiments. Finally, this paper proposes a graduating student profile for big data education at applied technical colleges and universities in China. The authors' main ideas include that, at the applied technical colleges and universities, a) a suitable graduating student orientation should be determined as the big data talent needs are hierarchical; b) the redesigned curriculum in big data education should provide students more practical capabilities and knowledge; c) the teaching of the existing mainstream big data technologies and tools should be significant components in the syllabi of big data education.", "pages": "125511-125521", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Li, Xin", "Fan, Xiaoping", "Qu, Xilong", "Sun, Guang", "Yang, Chen", "Zuo, Biao", "Liao, Zhifang"]}]["8950481", {"address": "", "articleno": "", "doi": "10.23919/CJEE.2019.000025", "issn": "2096-1529", "issue_date": "", "journal": "Chinese Journal of Electrical Engineering", "keywords": ["Filling", "Power systems", "Random forests", "Interpolation", "Data models", "Big Data", "Data mining", "Big data cleaning", "missing data filling", "data preprocessing", "random forest", "data quality"], "month": "Dec", "number": "4", "numpages": "", "publisher": "", "title": "A missing power data filling method based on improved random forest algorithm", "url": "", "volume": "5", "year": "2019", "abstract": "Missing data filling is a key step in power big data preprocessing, which helps to improve the quality and the utilization of electric power data. Due to the limitations of the traditional methods of filling missing data, an improved random forest filling algorithm is proposed. As a result of the horizontal and vertical directions of the electric power data are based on the characteristics of time series. Therefore, the method of improved random forest filling missing data combines the methods of linear interpolation, matrix combination and matrix transposition to solve the problem of filling large amount of electric power missing data. The filling results show that the improved random forest filling algorithm is applicable to filling electric power data in various missing forms. What's more, the accuracy of the filling results is high and the stability of the model is strong, which is beneficial in improving the quality of electric power data.", "pages": "33-39", "note": "", "ISSN": "2096-1529", "publicationtype": "article", "author": ["Deng, Wei", "Guo, Yixiu", "Liu, Jie", "Li, Yong", "Liu, Dingguo", "Zhu, Liang"]}]["8667300", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2019.2904286", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Standards", "Uncertainty", "Big Data", "Metrology", "Reliability", "Measurement uncertainty", "Biomedical measurement", "Big data", "data quality", "data traceability", "metrology", "standard reference data", "uncertainty"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "Big Data Quality Assurance Through Data Traceability: A Case Study of the National Standard Reference Data Program of Korea", "url": "", "volume": "7", "year": "2019", "abstract": "In the era of big data, the scientific and social demand for quality data is aggressive and urgent. This paper sheds light on the expanded role of metrology of verifying validated procedures of data production and developing adequate uncertainty evaluation methods to ensure the trustworthiness of data and information. In this regard, I explore the mechanism of the national standard reference data (SRD) program of Korea, which connects various scientific and social sectors to metrology by applying useful metrological concepts and methods to produce reliable data and convert such data into national standards. In particular, the changing interpretation of metrological key concepts, such as \u201cmeasurement,\u201d \u201ctraceability,\u201d and \u201cuncertainty,\u201d will be explored and reconsidered from the perspective of data quality assurance. As a result, I suggest the concept of \u201cdata traceability\u201d with \u201cthe matrix of data quality evaluation\u201d according to the elements of a data production system and related evaluation criteria. To conclude, I suggest social and policy implications for the new role of metrology and standards for producing and disseminating reliable knowledge sources from big data.", "pages": "36294-36299", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Lee, Doyoung"]}]["9399411", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2021.3072196", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Watermarking", "Blockchain", "Compressed sensing", "Big Data", "Image coding", "Privacy", "Peer-to-peer computing", "Blockchain", "IPFS", "compressed sensing", "watermarking", "data hiding"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "Blockchain-Watermarking for Compressive Sensed Images", "url": "", "volume": "9", "year": "2021", "abstract": "With the application of multimedia big data, the problems such as information leakage and data tampering have emerged. The security of images which is one of the most typical multimedia has become a major problem facing the large-scale open network environment. This paper proposed a blockchain-watermarking scheme to protect the privacy, integrity and availability of compressed sensed images, which effectively combines multimedia watermarking, compressed sensing, Interplanetary File System (IPFS) and blockchain technologies. Based on the reliable authentication of watermarking, the confidentiality protection of compressed sensing, the secure storage of IPFS, and the decentralization and non-tamperability of blockchain, the all-round security protection of the image big data based on compressive sensing can be realized. Experiments show that the proposed scheme is effective and feasible.", "pages": "56457-56467", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Li, Ming", "Zeng, Leilei", "Zhao, Le", "Yang, Renlin", "An, Dezhi", "Fan, Haiju"]}]["9259196", {"address": "", "articleno": "", "doi": "10.26599/BDMA.2020.9020024", "issn": "2096-0654", "issue_date": "", "journal": "Big Data Mining and Analytics", "keywords": ["Two dimensional displays", "Task analysis", "Sentiment analysis", "Data models", "Data mining", "Context modeling", "Analytical models", "educational big data", "sentiment analysis", "aspect-level", "attention"], "month": "Dec", "number": "4", "numpages": "", "publisher": "", "title": "Multi-attention fusion modeling for sentiment analysis of educational big data", "url": "", "volume": "3", "year": "2020", "abstract": "As an important branch of natural language processing, sentiment analysis has received increasing attention. In teaching evaluation, sentiment analysis can help educators discover the true feelings of students about the course in a timely manner and adjust the teaching plan accurately and timely to improve the quality of education and teaching. Aiming at the inefficiency and heavy workload of college curriculum evaluation methods, a Multi-Attention Fusion Modeling (Multi-AFM) is proposed, which integrates global attention and local attention through gating unit control to generate a reasonable contextual representation and achieve improved classification results. Experimental results show that the Multi-AFM model performs better than the existing methods in the application of education and other fields.", "pages": "311-319", "note": "", "ISSN": "2096-0654", "publicationtype": "article", "author": ["Zhai, Guanlin", "Yang, Yan", "Wang, Heng", "Du, Shengdong"]}]["9344686", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2021.3056486", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Big Data", "Government", "Decision making", "Data models", "Mathematical model", "Technological innovation", "Information technology", "Big data system capabilities", "big data human capabilities", "big data management capabilities", "smart service performance", "structural equation model"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "Research on the Impact of Big Data Capabilities on Government\u2019s Smart Service Performance: Empirical Evidence From China", "url": "", "volume": "9", "year": "2021", "abstract": "The government of China seeks to improve e-government service quality and build a service-oriented government that citizens find satisfactory. To this end, big data is being used as a new tool of government service innovation. However, there is a lack of research on how big data affects the performance of government smart services. This article explores the influence mechanisms of government big data capabilities on the performance of smart service provision, utilizing the carding analysis of relevant literature, published both in China and abroad. To this end, a structural equation model was constructed. Using data from 289 valid questionnaires in Jiangsu, Shandong, Zhejiang, and other provinces and cities in China, the study tests internal mechanisms of big data capabilities and its effect on smart service performance. Following a new definition of government big data capability, the paper divides the capability into three dimensions: big data system capability, big data human capability and big data management capability. The main conclusions are as follows: (1) Big data management capability has a significant positive impact on big data human capability and big data system capability. (2) Big data system capability has a significant positive impact on big data human capability. (3) Big data system capability and big data management capability have a significant positive effect on smart service performance. (4) The impact of big data human capability on smart service performance is not however significant enough to bring about the improvements which the government seeks.", "pages": "50523-50537", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Zhang, Airong", "Lv, Na"]}]["8809689", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2019.2936941", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Urban areas", "Big Data", "Data analysis", "Transportation", "Cloud computing", "Data mining", "Europe", "Big data", "cloud computing", "data analytics", "data privacy", "data quality", "distributed environment", "public transport management", "smart city"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "An Integrated Big and Fast Data Analytics Platform for Smart Urban Transportation Management", "url": "", "volume": "7", "year": "2019", "abstract": "Smart urban transportation management can be considered as a multifaceted big data challenge. It strongly relies on the information collected into multiple, widespread, and heterogeneous data sources as well as on the ability to extract actionable insights from them. Besides data, full stack (from platform to services and applications) Information and Communications Technology (ICT) solutions need to be specifically adopted to address smart cities challenges. Smart urban transportation management is one of the key use cases addressed in the context of the EUBra-BIGSEA (Europe-Brazil Collaboration of Big Data Scientific Research through Cloud-Centric Applications) project. This paper specifically focuses on the City Administration Dashboard, a public transport analytics application that has been developed on top of the EUBra-BIGSEA platform and used by the Municipality stakeholders of Curitiba, Brazil, to tackle urban traffic data analysis and planning challenges. The solution proposed in this paper joins together a scalable big and fast data analytics platform, a flexible and dynamic cloud infrastructure, data quality and entity matching algorithms as well as security and privacy techniques. By exploiting an interoperable programming framework based on Python Application Programming Interface (API), it allows an easy, rapid and transparent development of smart cities applications.", "pages": "117652-117677", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Fiore, Sandro", "Elia, Donatello", "Pires, Carlos", "Mestre, Demetrio", "Cappiello, Cinzia", "Vitali, Monica", "Andrade, Nazareno", "Braz, Tarciso", "Lezzi, Daniele", "Moraes, Regina", "Basso, Tania", "Kozievitch, N\u00e1dia", "Fonseca, Keiko", "Antunes, Nuno", "Vieira, Marco", "Palazzo, Cosimo", "Blanquer, Ignacio", "Meira, Wagner", "Aloisio, Giovanni"]}]["9139506", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2020.3009006", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Insurance", "Medical diagnostic imaging", "Big Data", "Data mining", "Data models", "Medical treatment", "Big data", "abnormal behavior", "healthcare insurance", "association rules"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "Big Data-Driven Abnormal Behavior Detection in Healthcare Based on Association Rules", "url": "", "volume": "8", "year": "2020", "abstract": "Healthcare insurance frauds are causing millions of dollars of public healthcare fund losses around the world in various ways, which makes it very important to strengthen the management of medical insurance in order to guarantee the steady operation of medical insurance funds. Healthcare fraud detection methods can reduce the losses of healthcare insurance funds and improve medical quality. Existing fraud detection studies mostly focus on finding normal behavior patterns and treat those violating normal behavior patterns as fraudsters. However, fraudsters can often disguise themselves with some normal behaviors, such as some consistent behaviors when they seek medical treatments. To address these issues, we combined a MapReduce distributed computing model and association rule mining to propose a medical cluster behavior detection algorithm based on frequent pattern mining. It can detect certain consistent behaviors of patients in medical treatment activities. By analyzing 1.5 million medical claim records, we have verified the effectiveness of the method. Experiments show that this method has better performance than several benchmark methods.", "pages": "129002-129011", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Zhou, Shengyao", "He, Jie", "Yang, Hui", "Chen, Donghua", "Zhang, Runtong"]}]["9852477", {"address": "", "articleno": "", "doi": "10.1109/OAJPE.2022.3197553", "issn": "2687-7910", "issue_date": "", "journal": "IEEE Open Access Journal of Power and Energy", "keywords": ["Phasor measurement units", "Data integrity", "Big Data", "Power systems", "Machine learning", "Training", "Statistical analysis", "Phasor measurement units", "synchrophasor datasets", "statistical analysis", "data quality analysis", "label quality", "nonymized datasets", "supervised machine learning", "big data", "wide area monitoring system"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "Application of Big Data Analytics and Machine Learning to Large-Scale Synchrophasor Datasets: Evaluation of Dataset \u2018Machine Learning-Readiness\u2019", "url": "", "volume": "9", "year": "2022", "abstract": "This manuscript presents a data quality analysis and holistic \u2018machine learning-readiness\u2019 evaluation of a representative set of large-scale, real-world phasor measurement unit (PMU) datasets provided under the United States Department of Energy-funded FOA 1861 research program. A major focus of this study is to understand the present-day suitability of large-scale, real-world synchrophasor datasets for application of commercially-available, off-the-shelf big data and supervised or semi-supervised machine learning (ML) tools and catalogue any major obstacles to their application. To this end, dataset quality is methodically examined through an interconnect-wide quantifications of basic bad data occurrences, a summary of several harder-to-detect data quality issues that can jeopardize successful application of machine learning, and an evaluation of the adequacy of event log labeling for supervised training of models used for online event classification. A global \u2018six-point\u2019 statistical analyses of several key dataset variables is demonstrated as a means by which to identify additional hard-to-detect data quality issues, also providing an example successful application of big data technology to extract insights regarding reasonable operational bounds of the US power system. Obstacles for application of commercial ML technologies are summarized, with a particular focus on supervised and semi-supervised ML. Lessons-learned are provided regarding challenges associated with present-day event labeling practices, large spatial scope of the dataset, and dataset anonymization. Finally, insight into efficacy of employed mitigation strategies are discussed, and recommendations for future work are made.", "pages": "386-397", "note": "", "ISSN": "2687-7910", "publicationtype": "article", "author": ["Hart, Philip", "He, Lijun", "Wang, Tianyi", "Kumar, Vijay", "Aggour, Kareem", "Subramanian, Arun", "Yan, Weizhong"]}]["9475115", {"address": "", "articleno": "", "doi": "10.23919/ICN.2021.0002", "issn": "2708-6240", "issue_date": "", "journal": "Intelligent and Converged Networks", "keywords": ["Artificial intelligence", "5G mobile communication", "Optimization", "Wireless communication", "Engines", "Data acquisition", "Radio access networks", "true-data testbed", "wireless communication networks", "artificial intelligence (AI)", "big data", "internet of everything (IoE)"], "month": "June", "number": "2", "numpages": "", "publisher": "", "title": "True-data testbed for 5G/B5G intelligent network", "url": "", "volume": "2", "year": "2021", "abstract": "Future beyond fifth-generation (B5G) and sixth-generation (6G) mobile communications will shift from facilitating interpersonal communications to supporting internet of everything (IoE), where intelligent communications with full integration of big data and artificial intelligence (AI) will play an important role in improving network efficiency and provi di ng hi gh-quality servi ce. As a rapi d evolvi ng paradi gm, the AI-empowered mob i le communi cati ons demand large amounts of data acquired from real network environment for systematic test and verification. Hence, we build the world's first true-data testbed for 5G/B5G intelligent network (TTIN), which comprises 5G/B5G on-site experimental networks, data acquisition & data warehouse, and AI engine & network optimization. In the TTIN, true network data acquisition, storage, standardization, and analysis are available, which enable system-level online verification of B5G/6G-orientated key technologies and support data-driven network optimization through the closed-loop control mechanism. This paper elaborates on the system architecture and module design of TTIN. Detailed technical specifications and some of the established use cases are also showcased.", "pages": "133-149", "note": "", "ISSN": "2708-6240", "publicationtype": "article", "author": ["Huang, Yongming", "Liu, Shengheng", "Zhang, Cheng", "You, Xiaohu", "Wu, Hequan"]}]["8361574", {"address": "", "articleno": "", "doi": "10.26599/BDMA.2018.9020020", "issn": "2096-0654", "issue_date": "", "journal": "Big Data Mining and Analytics", "keywords": ["Big Data", "Quality of experience", "Edge computing", "Quality of service", "Computational modeling", "Training", "Streaming media", "Quality-of-Experience (QoE)", " high-dimensional big data management", " deep learning", " pervasive edge"], "month": "Sep.", "number": "3", "numpages": "", "publisher": "", "title": "QoE-driven big data management in pervasive edge computing environment", "url": "", "volume": "1", "year": "2018", "abstract": "In the age of big data, services in the pervasive edge environment are expected to offer end-users better Quality-of-Experience (QoE) than that in a normal edge environment. However, the combined impact of the storage, delivery, and sensors used in various types of edge devices in this environment is producing volumes of high-dimensional big data that are increasingly pervasive and redundant. Therefore, enhancing the QoE has become a major challenge in high-dimensional big data in the pervasive edge computing environment. In this paper, to achieve high QoE, we propose a QoE model for evaluating the qualities of services in the pervasive edge computing environment. The QoE is related to the accuracy of high-dimensional big data and the transmission rate of this accurate data. To realize high accuracy of high-dimensional big data and the transmission of accurate data through out the pervasive edge computing environment, in this study we focused on the following two aspects. First, we formulate the issue as a high-dimensional big data management problem and test different transmission rates to acquire the best QoE. Then, with respect to accuracy, we propose a Tensor-Fast Convolutional Neural Network (TF-CNN) algorithm based on deep learning, which is suitable for high-dimensional big data analysis in the pervasive edge computing environment. Our simulation results reveal that our proposed algorithm can achieve high QoE performance.", "pages": "222-233", "note": "", "ISSN": "2096-0654", "publicationtype": "article", "author": ["Meng, Qianyu", "Wang, Kun", "He, Xiaoming", "Guo, Minyi"]}]["9096305", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2020.2995572", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Big Data", "Diseases", "Data mining", "Medical diagnostic imaging", "Data models", "Healthcare", "big data", "big data management", "big data analytics"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "A Comprehensive Analysis of Healthcare Big Data Management, Analytics and Scientific Programming", "url": "", "volume": "8", "year": "2020", "abstract": "Healthcare systems are transformed digitally with the help of medical technology, information systems, electronic medical records, wearable and smart devices, and handheld devices. The advancement in the medical big data, along with the availability of new computational models in the field of healthcare, has enabled the caretakers and researchers to extract relevant information and visualize the healthcare big data in a new spectrum. The role of medical big data becomes a challenging task in the form of storage, required information retrieval within a limited time, cost efficient solutions in terms care, and many others. Early decision making based healthcare system has massive potential for dropping the cost of care, refining quality of care, and reducing waste and error. Scientific programming play a significant role to overcome the existing issues and future problems involved in the management of large scale data in healthcare, such as by assisting in the processing of huge data volumes, complex system modelling, and sourcing derivations from healthcare data and simulations. Therefore, to address this problem efficiently a detailed study and analysis of the available literature work is required to facilitate the doctors and practitioners for making the decisions in identifying the disease and suggest treatment accordingly. The peer reviewed reputed journals are selected for the accumulated of published research work during the period ranges from 2015 - 2019 (a portion of 2020 is also included). A total of 127 relevant articles (conference papers, journal papers, book section, and survey papers) are selected for the assessment and analysis purposes. The proposed research work organizes and summarizes the existing published research work based on the research questions defined and keywords identified for the search process. This analysis on the existence research work will help the doctors and practitioners to make more authentic decisions, which ultimately will help to use the study as evidence for treating patients and suggest medicines accordingly.", "pages": "95714-95733", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Nazir, Shah", "Khan, Sulaiman", "Khan, Habib", "Ali, Shaukat", "Garc\u00eda-Magari\u00f1o, Iv\u00e1n", "Atan, Rodziah", "Nawaz, Muhammad"]}]["8805062", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2019.2936133", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Data visualization", "Cardiology", "Big Data", "Medical services", "Systematics", "Protocols", "Libraries", "Big data", "medical big data", "visualization", "healthcare", "cardiology", "systematic literature review"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "Big Data Visualization in Cardiology\u2014A Systematic Review and Future Directions", "url": "", "volume": "7", "year": "2019", "abstract": "The digital transformations and use of healthcare information system, electronic medical records, wearable technology, and smart devices are increasing with the passage of time. A variety of sources of big data in healthcare are available, such as biometric data, registration data, electronic health record, medical imaging, patient reported data, biomarker data, clinical data, and administrative data. Visualization of data is a key tool for producing images, diagrams, or animations to convey messages from the viewed insight. The role of cardiology in healthcare is obvious for living and life. The function of heart is the control of blood supply to the entire parts of the body. Recent speedy growth in healthcare and the development of computation in the field of cardiology enable researchers and practitioners to mine and visualize new insights from patient data. The role of visualization is to capture the important information from the data and to visualize it for the easiness of doctors and practitioners. To help the doctors and practitioners, the proposed study presents a detailed report of the existing literature on visualization of data in the field of cardiology. This report will support the doctors and practitioners in decision-making process and to make it easier. This detailed study will eventually summarize the results of the existing literature published related to visualization of data in the cardiology. This research uses the systematic literature protocol and the data was collected from the studies published during the year 2009 to 2018 (10 years). The proposed study selected 53 primary studies from different repositories according to the defined exclusion, inclusion, and quality criteria. The proposed study focused mainly on the research work been done on visualization of big data in the field of cardiology, presented a summary of the techniques used for visualization of data in cardiology, and highlight the benefits of visualizations in cardiology. The current research summarizes and organizes the available literature in the form of published materials related to big data visualization in cardiology. The proposed research will help the researchers to view the available research studies on the subject of medical big data in cardiology and then can ultimately be used as evidence in future research. The results of the proposed research show that there is an increase in articles published yearly wise and several studies exist related to medical big data in cardiology. The derivations from the studies are presented in the paper.", "pages": "115945-115958", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Nazir, Shah", "Nawaz, Muhammad", "Anwar, Sajid", "Adnan, Awais", "Asadi, Shahla", "Shahzad, Sara", "Ali, Shaukat"]}]["9454431", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2021.3089100", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Usability", "Big Data", "Data models", "Data mining", "Bibliographies", "Protocols", "Systematics", "Big data", "data transformation", "data usability", "text data", "unstructured data", "usability enhancement"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "Development of Usability Enhancement Model for Unstructured Big Data Using SLR", "url": "", "volume": "9", "year": "2021", "abstract": "Unstructured text contains valuable information for a range of enterprise applications and informed decision making. Text analytics is used to extract valuable insights from unstructured big data. Among the most significant challenges of text analytics, quality and usability are critical in affecting the outcome of the analytical process. The enhancement in usability is important for the exploitation of unstructured data. Most of the existing literature focuses on the usability of structured data as compared to unstructured data whereas big data usability has been discussed merely in the context of its assessment. The existing approaches do not provide proper guidelines on the usability enhancement of unstructured data. In this study, a rigorous systematic literature review using PRISMA framework has been conducted to develop a model enhancing the usability of unstructured data bridging the research gap. The recent approaches and solutions for text analytics have been investigated thoroughly. The usability issues of unstructured text data and their consequences on data preparation for analytics have been identified. Defining the usability dimensions for unstructured big data, identification of the usability determinants, and developing a relationship between usability dimension and determinants to derive usability rules are the significant contributions of this research and are integrated to formulate the usability enhancement model. The proposed model is the major outcome of the research. It contributes to make unstructured data usable and facilitates the data preparation activities with more valuable data that eventually improve the analytical process.", "pages": "87391-87409", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Adnan, Kiran", "Akbar, Rehan", "Wang, Khor"]}]["9162126", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2020.3015016", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Tools", "Quality control", "Big Data", "Bioinformatics", "Sparks", "Sequential analysis", "Acceleration", "Big data", "next-generation sequencing (NGS)", "bioinformatics", "quality control", "apache spark"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "SeQual: Big Data Tool to Perform Quality Control and Data Preprocessing of Large NGS Datasets", "url": "", "volume": "8", "year": "2020", "abstract": "This paper presents SeQual, a scalable tool to efficiently perform quality control of large genomic datasets. Our tool currently supports more than 30 different operations (e.g., filtering, trimming, formatting) that can be applied to DNA/RNA reads in FASTQ/FASTA formats to improve subsequent downstream analyses, while providing a simple and user-friendly graphical interface for non-expert users. Furthermore, SeQual takes full advantage of Big Data technologies to process massive datasets on distributed-memory systems such as clusters by relying on the open-source Apache Spark cluster computing framework. Our scalable Spark-based implementation allows to reduce the runtime from more than three hours to less than 20 minutes when processing a paired-end dataset with 251 million reads per input file on an 8-node multi-core cluster.", "pages": "146075-146084", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Exp\u00f3sito, Roberto", "Galego-Torreiro, Roi", "Gonz\u00e1lez-Dom\u00ednguez, Jorge"]}]["9126812", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2020.3005268", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Real-time systems", "Big Data", "Libraries", "Systematics", "Data mining", "Bibliographies", "Quality assessment", "Real-time stream processing", "big data streaming", "structured/un-structured data", "ETL", "systematic literature review"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "Challenges and Solutions for Processing Real-Time Big Data Stream: A Systematic Literature Review", "url": "", "volume": "8", "year": "2020", "abstract": "Contribution: Recently, real-time data warehousing (DWH) and big data streaming have become ubiquitous due to the fact that a number of business organizations are gearing up to gain competitive advantage. The capability of organizing big data in efficient manner to reach a business decision empowers data warehousing in terms of real-time stream processing. A systematic literature review for real-time stream processing systems is presented in this paper which rigorously look at the recent developments and challenges of real-time stream processing systems and can serve as a guide for the implementation of real-time stream processing framework for all shapes of data streams. Background: Published surveys and reviews either cover papers focusing on stream analysis in applications other than real-time DWH or focusing on extraction, transformation, loading (ETL) challenges for traditional DWH. This systematic review attempts to answer four specific research questions. Research Questions: 1)Which are the relevant publication channels for real-time stream processing research? 2) Which challenges have been faced during implementation of real-time stream processing? 3) Which approaches/tools have been reported to address challenges introduced at ETL stage while processing real-time stream for real-time DWH? 4) What evidence have been reported while addressing different challenges for processing real-time stream? Methodology: A systematic literature was conducted to compile studies related to publication channels targeting real-time stream processing/joins challenges and developments. Following a formal protocol, semi-automatic and manual searches were performed for work from 2011 to 2020 excluding research in traditional data warehousing. Of 679,547 papers selected for data extraction, 74 were retained after quality assessment. Findings: This systematic literature highlights implementation challenges along with developed approaches for real-time DWH and big data stream processing systems and provides their comparisons. This study found that there exists various algorithms for implementing real-time join processing at ETL stage for structured data whereas less work for un-structured data is found in this subject matter.", "pages": "119123-119143", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Mehmood, Erum", "Anees, Tayyaba"]}]["8416666", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2018.2857845", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Data analysis", "Tools", "Data visualization", "Big Data", "Statistical analysis", "Synthetic aperture sonar", "Data analysis", "data visualization", "reproducibility of results", "clouds", "data refinement", "R"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "VPL-Based Big Data Analysis System: UDAS", "url": "", "volume": "6", "year": "2018", "abstract": "Over the past five years, research on big data analysis has been actively conducted, and many services have been developed to find valuable data. However, low quality of raw data and data loss problem during data analysis make it difficult to perform accurate data analysis. With the enormous generation of both unstructured and structured data, refinement of data is becoming increasingly difficult. As a result, data refinement plays an important role in data analysis. In addition, as part of efforts to ensure research reproducibility, the importance of reuse of researcher data and research methods is increasing; however, the research on systems supporting such roles has not been conducted sufficiently. Therefore, in this paper, we propose a big data analysis system named the unified data analytics suite (UDAS) that focuses on data refinement. UDAS performs data refinement based on the big data platform and ensures the reusability and reproducibility of refinement and analysis through the visual programming language interface. It also recommends open source and visualization libraries to users for statistical analysis. The qualitative evaluation of UDAS using the functional evaluation factor of the big data analysis platform demonstrated that the average satisfaction of the users is significantly high.", "pages": "40883-40897", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Choi, Hyunjin", "Gim, Jangwon", "Seo, Young-Duk", "Baik, Doo-Kwon"]}]["7587347", {"address": "", "articleno": "", "doi": "10.1109/TBME.2016.2573285", "issn": "1558-2531", "issue_date": "", "journal": "IEEE Transactions on Biomedical Engineering", "keywords": ["Big data", "Bioinformatics", "Genomics", "Data mining", "Medical services", "DNA", "Feature extraction", "Big data analytics", "bioinformatics", "electronic health records (EHRs)", "health informatics", "\u2013omic data", "precision medicine"], "month": "Feb", "number": "2", "numpages": "", "publisher": "", "title": "\u2013Omic and Electronic Health Record Big Data Analytics for Precision Medicine", "url": "", "volume": "64", "year": "2017", "abstract": "Objective: Rapid advances of high-throughput technologies and wide adoption of electronic health records (EHRs) have led to fast accumulation of -omic and EHR data. These voluminous complex data contain abundant information for precision medicine, and big data analytics can extract such knowledge to improve the quality of healthcare. Methods: In this paper, we present -omic and EHR data characteristics, associated challenges, and data analytics including data preprocessing, mining, and modeling. Results: To demonstrate how big data analytics enables precision medicine, we provide two case studies, including identifying disease biomarkers from multi-omic data and incorporating -omic information into EHR. Conclusion: Big data analytics is able to address -omic and EHR data challenges for paradigm shift toward precision medicine. Significance: Big data analytics makes sense of -omic and EHR data to improve healthcare outcome. It has long lasting societal impact.", "pages": "263-273", "note": "", "ISSN": "1558-2531", "publicationtype": "article", "author": ["Wu, Po-Yen", "Cheng, Chih-Wen", "Kaddi, Chanchala", "Venugopalan, Janani", "Hoffman, Ryan", "Wang, May"]}]["9496639", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2021.3100287", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Cloud computing", "Security", "Big Data", "Software", "Organizations", "Social networking (online)", "STEM", "Security challenges", "big data", "cloud computing", "SLR", "vendor", "SPSS"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "Analyzing and Evaluating Critical Challenges and Practices for Software Vendor Organizations to Secure Big Data on Cloud Computing: An AHP-Based Systematic Approach", "url": "", "volume": "9", "year": "2021", "abstract": "Recently, its becomes easy to track down the data due to its availability in a large number. Although for data management, processing, and obtainability, cloud computing is considered a well-known approach for organizational development on the internet. Despite many advantages, cloud computing has still numerous security challenges that can affect the big-data usage on cloud computing. To find the security issues/challenges that are faced by software vendors\u2019 organizations we conducted a systematic literature review (SLR) through which we have find out 103 relevant research publications by developing a search string that is inspired by the research questions. This relevant data was comprised from different databases e.g. Google Scholar, IEEE Explore, ScienceDirect, ACM Digital Library, and SpringerLink. Furthermore, for the detailed literature review, we have accomplished all the steps in SLR, for example, development of SLR protocol, Initials and final assortment of the relevant data, data extraction, data quality assessment, and data synthesis. We identified fifteen (15) critical security challenges which are: data secrecy, geographical data location, unauthorized data access, lack of control, lack of data management, network-level issues, data integrity, data recovery, lack of trust, data sharing, data availability, asset issues, legal amenabilities, lack of quality, and lack of consistency. Furthermore, sixty four (64) standard practices are identified for these critical security challenges using the proposed SLR that could help vendor organizations to overcome the security challenges for big data. The findings of our research study demonstrate the resemblances and divergences in the identified security challenges in different periods, continents, databases, and methods. The proposed SLR will also support software vendor organizations for securing big data on the cloud computing platforms. This paper has the following content: in Section II, we have describe the Literature review; in Section III, research methodology is specified; in Section IV, the findings of the SLR and the analysis of result are discussed; in Section V, the limitations of this research are given; in Section VI, we discussed our conclusions and future work.", "pages": "107309-107332", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Khan, Abudul", "Khan, Maseeh", "Khan, Javed", "Ahmad, Arshad", "Khan, Khalil", "Zamir, Muhammad", "Kim, Wonjoon", "Ijaz, Muhammad"]}]["8537879", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2018.2881759", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Big Data", "Task analysis", "Data models", "Organizations", "Databases", "Data mining", "Business process analytics", "Big Data systems", "process data-intensive operations"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "Business Process Analytics and Big Data Systems: A Roadmap to Bridge the Gap", "url": "", "volume": "6", "year": "2018", "abstract": "Business processes represent a cornerstone to the operation of any enterprise. They are the operational means for such organizations to fulfill their goals. Nowadays, enterprises are able to gather massive amounts of event data. These are generated as business processes are executed and stored in transaction logs, databases, e-mail correspondences, free form text on (enterprise) social media, and so on. Taping into these data, enterprises would like to weave data analytic techniques into their decision making capabilities. In recent years, the IT industry has witnessed significant advancements in the domain of Big Data analytics. Unfortunately, the business process management (BPM) community has not kept up to speed with such developments and often rely merely on traditional modeling-based approaches. New ways of effectively exploiting such data are not sufficiently used. In this paper, we advocate that a good understanding of the business process and Big Data worlds can play an effective role in improving the efficiency and the quality of various data-intensive business operations using a wide spectrum of emerging Big Data systems. Moreover, we coin the term process footprint as a wider notion of process data than that is currently perceived in the BPM community. A roadmap towards taking business process data intensive operations to the next level is shaped in this paper.", "pages": "77308-77320", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Sakr, Sherif", "Maamar, Zakaria", "Awad, Ahmed", "Benatallah, Boualem", "Van, Wil"]}]["8715359", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2019.2916912", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Big Data", "NoSQL databases", "Transforms", "Scalability", "Servers", "Tools", "Relational databases", "NoSQL", "big data", "data cleansing"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "Intelligent Data Engineering for Migration to NoSQL Based Secure Environments", "url": "", "volume": "7", "year": "2019", "abstract": "In an era of super computing, data is increasing exponentially requiring more proficiency from the available technologies of data storage, data processing, and analysis. Such continuous massive growth of structured and unstructured data is referred to as a \u201cBig data\u201d. The processing and storage of big data through a conventional technique is not possible. Due to improved proficiency of Big Data solution in handling data, such as NoSQL caused the developers in the previous decade to start preferring big data databases, such as Apache Cassandra, Oracle, and NoSQL. NoSQL is a modern database technology that is designed to provide scalability to support voluminous data, leading to the rise of NoSQL as the most viable database solution. These modern databases aim to overcome the limitations of relational databases such as unlimited scalability, high performance, data modeling, data distribution, and continuous availability. These days, the larger enterprises need to shift NoSQL databases due to their more flexible models. It is a great challenge for business organizations and enterprises to transform their existing databases to NoSQL databases considering heterogeneity and complexity in relational data. In addition, with the emergence of big data, data cleansing has become a great challenge. In this paper, we proposed an approach that has two modules: data transformation and data cleansing module. The first phase is the transformation of a relational database to Oracle NoSQL database through model transformation. The second phase provides data cleansing ability to improve data quality and prepare it for big data analytics. The experiments show the proposed approach successfully transforms the relational database to a big data database and improve data quality.", "pages": "69042-69057", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Ramzan, Shabana", "Bajwa, Imran", "Ramzan, Bushra", "Anwar, Waheed"]}]["9235580", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2020.3033068", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Education", "Big Data", "Data mining", "Information technology", "Symbiosis", "Learning (artificial intelligence)", "Eco-environment", "English teaching", "big data", "data mining", "influence factor", "comprehensive ability"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "Eco-Environment Construction of English Teaching Using Artificial Intelligence Under Big Data Environment", "url": "", "volume": "8", "year": "2020", "abstract": "Application of big data and artificial intelligence has become one influence factor of English teaching, which have broken the balance of the teaching Eco-environment for English. In this article, the artificial intelligence and big data are introduced into English teaching to propose a new teaching Eco-environment construction method to meet the needs of the social development and international communication in English. In the proposed method, the characteristics of English teaching under big data environment are analyzed in detail. Then the big data technology is used to construct a new Eco-environment of English teaching to improve the teaching and learning quality. The data mining method is one of artificial intelligence methods, which is used to analyze the relationship of interdependence and mutual restriction among various factors in English teaching in order to build and implement a new Eco-environment with the information sharing, quality teaching and personalized learning of English. Finally, through the practical application of the constructed Eco-environment, the experiment results show that the proposed method can help students update their learning concepts, methods and contents of English, inspire their interest and initiative by comparing with some existed teaching methods, so as to improve their learning effects and application ability of English. Therefore, the constructed Eco-environment provides a new idea and direction for English teaching reform by application of big data and artificial intelligence.", "pages": "193955-193965", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Sun, Maohua", "Li, Yuangang"]}]["8890933", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2019.2951364", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Differential privacy", "Publishing", "Big Data", "Data models", "Vehicle dynamics", "Big location data", "privacy preserving data publishing", "adaptive sampling", "differential privacy", "heuristic quad-tree partitioning"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "Dynamic Release of Big Location Data Based on Adaptive Sampling and Differential Privacy", "url": "", "volume": "7", "year": "2019", "abstract": "Data releasing is a key part bridging between the collection of big data and their applications. Traditional methods release the static version of dataset or publish the snapshot with a fixed sampling interval, which cannot meet the dynamic query requirements and query precision for big data. Moreover, the quality of published data cannot reflect the characteristics of the dynamic changes of big data, which often leads to subsequent data analysis and mining errors. This paper proposes an adaptive sampling mechanism and privacy protection method for the release of big location data. In order to reflect the dynamic change of data in time, we design an adaptive sampling mechanism based on the proportional-integral-derivative (PID) controller according to the temporal and spatial correlation of the location data. To ensure the privacy of published data, we propose a heuristic quad-tree partitioning method as well as a corresponding privacy budget allocation strategy. Experiments and analysis prove that the adaptive sampling mechanism proposed in this paper can effectively track the trend of dynamic changes of data, and the designed differential privacy method can improve the accuracy of counting query and enhance the availability of published data under the premise of certain privacy intensity. The proposed methods can also be readily extended to other areas of big data release applications.", "pages": "164962-164974", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Yan, Yan", "Zhang, Lianxiu", "Sheng, Quan", "Wang, Bingqian", "Gao, Xin", "Cong, Yiming"]}]["9409047", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2021.3074559", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Big Data", "Organizations", "Radio frequency", "Predictive models", "Support vector machines", "Data models", "Analytical models", "Deep people analytics", "employee attrition", "retention", "prediction", "interpretation", "policies recommendation"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "From Big Data to Deep Data to Support People Analytics for Employee Attrition Prediction", "url": "", "volume": "9", "year": "2021", "abstract": "In the era of data science and big data analytics, people analytics help organizations and their human resources (HR) managers to reduce attrition by changing the way of attracting and retaining talent. In this context, employee attrition presents a critical problem and a big risk for organizations as it affects not only their productivity but also their planning continuity. In this context, the salient contributions of this research are as follows. Firstly, we propose a people analytics approach to predict employee attrition that shifts from a big data to a deep data context by focusing on data quality instead of its quantity. In fact, this deep data-driven approach is based on a mixed method to construct a relevant employee attrition model in order to identify key employee features influencing his/her attrition. In this method, we started thinking `big' by collecting most of the common features from the literature (an exploratory research) then we tried thinking `deep' by filtering and selecting the most important features using survey and feature selection algorithms (a quantitative method). Secondly, this attrition prediction approach is based on machine, deep and ensemble learning models and is experimented on a large-sized and a medium-sized simulated human resources datasets and then a real small-sized dataset from a total of 450 responses. Our approach achieves higher accuracy (0.96, 0.98 and 0.99 respectively) for the three datasets when compared previous solutions. Finally, while rewards and payments are generally considered as the most important keys to retention, our findings indicate that `business travel', which is less common in the literature, is the leading motivator for employees and must be considered within HR policies to retention.", "pages": "60447-60458", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Yahia, Nesrine", "Hlel, Jihen", "Colomo-Palacios, Ricardo"]}]["8641478", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2019.2898707", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Big Data", "Data integrity", "Detection algorithms", "Databases", "Time complexity", "Hazards", "Business", "Inconsistency detection", "big data", "one-pass algorithm", "data quality", "denial constraint"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "One-Pass Inconsistency Detection Algorithms for Big Data", "url": "", "volume": "7", "year": "2019", "abstract": "Data in the real world is often dirty. Inconsistency is an important kind of dirty data; before repairing inconsistency, we need to detect them first. The time complexities of the current inconsistency detection algorithms are super-linear to the size of data and not suitable for the big data. For the inconsistency detection of big data, we develop an algorithm that detects inconsistency within the one-pass scan of the data according to both the functional dependency (FD) and the conditional functional dependency (CFD) in our previous work. In this paper, we propose inconsistency detection algorithms in terms of FD, CFD, and Denial Constraint (DC). DCs are more expressive than FDs and CFDs. Developing the algorithm to detect the violation of DCs increases the applicability of our inconsistency detection algorithms. We compare the performance of our algorithm with the performance of implementing SQL queries in MySQL and BigQuery. The experimental results indicate the high efficiency of our algorithms.", "pages": "22377-22394", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Zhang, Meifan", "Wang, Hongzhi", "Li, Jianzhong", "Gao, Hong"]}]["8561268", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2018.2885142", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Monitoring", "Big Data", "Forecasting", "Indexes", "Cloud computing", "Real-time systems", "Brain modeling", "Brain health quality", "intelligent forecasting", "air quality forecast monitoring", "big data", "Internet of Things"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "Air Quality Forecast Monitoring and Its Impact on Brain Health Based on Big Data and the Internet of Things", "url": "", "volume": "6", "year": "2018", "abstract": "Brain health quality pre-monitoring has become an urgent need, and this is a system of complex engineering. From the perspective of intelligent decision-making based on big data, the intelligent air index prediction is introduced, the popular classification algorithm is introduced, the hidden information of historical data is mined, and the brain health quality prediction is realized. The brain health quality monitoring system based on the Internet of Things is constructed, and the classification algorithm is used to realize real-time acquisition, intelligent processing of data. In order to improve the data processing speed and enhance the real-time performance of brain health quality prediction, this paper introduces cloud computing technology to accelerate data processing. In order to enable users to understand the air index, anytime and anywhere, it is also designed based on the problem of large historical data of air index and real-time data collection. The Android platform develops an air index forecast client.", "pages": "78678-78688", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Huang, Yuan", "Zhao, Qiang", "Zhou, Qianyu", "Jiang, Wanchang"]}]["8412190", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2018.2856623", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Cloud computing", "Big Data", "Computational modeling", "Data models", "Adaptation models", "Task analysis", "Big data", "big data processing", "cloud computing", "cloud selection", "trust model", "quality of cloud services", "service evaluation", "community"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "A Multi-Dimensional Trust Model for Processing Big Data Over Competing Clouds", "url": "", "volume": "6", "year": "2018", "abstract": "Cloud computing has emerged as a powerful paradigm for delivering data-intensive services over the Internet. Cloud computing has enabled the implementation and success of big data, a recent phenomenon handling huge data being generated from different sources. Competing clouds have made it challenging to select a cloud provider that guarantees quality of cloud service (QoCS). Also, cloud providers' claims of guaranteeing QoCS are exaggerated for marketing purposes; hence, they cannot often be trusted. Therefore, a comprehensive trust model is necessary to evaluate the QoCS prior to making any selection decision. In this paper, we propose a multi-dimensional trust model for big data workflow processing over different clouds. It evaluates the trustworthiness of cloud providers based on: the most up-to-date cloud resource capabilities, the reputation evidence measured by neighboring users, and a recorded personal history of experiences with the cloud provider. The ultimate goal is to ensure an efficient selection of trustworthiness cloud provider who eventually will guarantee high QoCS and fulfills key big data workflow requirements. Various experiments were conducted to validate our proposed model. The results show that our model captures the different components of trust, ensures high QoCS, and effectively adapts to the dynamic nature of the cloud.", "pages": "39989-40007", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["El, Hadeel", "Serhani, Mohamed", "Dssouli, Rachida", "Benatallah, Boualem"]}]["8012376", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2017.2741105", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Manufacturing", "Big Data", "Ontologies", "Production facilities", "Cloud computing", "Data models", "Machine learning", "Industrial big data", "smart factory", "data analysis", "cyber-physical systems"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "Industrial Big Data Analysis in Smart Factory: Current Status and Research Strategies", "url": "", "volume": "5", "year": "2017", "abstract": "Under the background of cyber-physical systems and Industry 4.0, intelligent manufacturing has become an orientation and produced a revolutionary change. Compared with the traditional manufacturing environments, the intelligent manufacturing has the characteristics as highly correlated, deep integration, dynamic integration, and huge volume of data. Accordingly, it still faces various challenges. In this paper, we summarize and analyze the current research status in both domestic and aboard, including industrial big data collection, modeling of the intelligent product lines based on ontology, the predictive diagnosis based on industrial big data, group learning of product line equipment and the product line reconfiguration of intelligent manufacturing. Based on the research status and the problems, we propose the research strategies, including acquisition schemes of industrial big data under the environment of intelligent, ontology modeling and deduction method based intelligent product lines, predictive diagnostic methods on production lines based on deep neural network, deep learning among devices based on cloud supplements and 3-D selforganized reconfiguration mechanism based on the supplements of cloud. In our view, this paper will accelerate the implementation of smart factory.", "pages": "17543-17551", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Xu, Xiaoya", "Hua, Qingsong"]}]["8840830", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2019.2941898", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Big Data", "Cardiology", "Diseases", "Protocols", "Data visualization", "Bibliographies", "Big data", "big data features", "analytics in cardiology", "healthcare", "systematic literature review (SLR)"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "Big Data Features, Applications, and Analytics in Cardiology\u2014A Systematic Literature Review", "url": "", "volume": "7", "year": "2019", "abstract": "In today's digital world the information surges with the widespread use of the internet and global communication systems. Healthcare systems are also facing digital transformations with the enhancement in the utilization of healthcare information systems, electronic records in medical, wearable, smart devices, handheld devices, and so on. A bulk of data is produced from these digital transformations. The recent increase in medical big data and the development of computational techniques in the field of cardiology enables researchers and practitioners to extract and visualize medical big data in a new spectrum. The role of medical big data in cardiology becomes a challenging task. Early decision making in cardiac healthcare system has massive potential for dropping the cost of care, refining quality of care, and reducing waste and error. Therefore, to facilitate this process a detailed report of the existing literature will be feasible to help the doctors and practitioners in decision making for the purpose of identifying and treating cardiac diseases. This detailed study will summarize results from the existing literature on big data in the field cardiac disease. This research uses the systematic literature protocol as presented by Kitchenham et al. The data was collected from the published materials from 2008 to 2018 as conference or journal publications, books, magazines and other online sources. 190 papers were included relying on the defined inclusion, exclusion, and checking the quality criteria. The current study helped to identify medical big data features, the application of medical big data, and the analytics of the big data in cardiology. The results of the proposed research shows that several studies exist that are associated to medical big data specifically to cardiology. This research summarizes and organizes the existing literature based on the defined keywords and research questions. The analysis will help doctors to make more authentic decisions, which ultimately will help to use the study as evidence for treating patients with heart related diseases.", "pages": "143742-143771", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Nazir, Shah", "Nawaz, Muhammad", "Adnan, Awais", "Shahzad, Sara", "Asadi, Shahla"]}]["8946609", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2019.2963283", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Seaports", "Big Data", "Industries", "Stakeholders", "Logistics", "Data models", "Data mining", "Analytics", "big data", "industry 4.0", "industrial data spaces", "Internet of Things", "maritime", "seaport", "intelligent transport"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "Seaport Data Space for Improving Logistic Maritime Operations", "url": "", "volume": "8", "year": "2020", "abstract": "The maritime industry expects several improvements to efficiently manage the operation processes by introducing Industry 4.0 enabling technologies. Seaports are the most critical point in the maritime logistics chain because of its multimodal and complex nature. Consequently, coordinated communication among any seaport stakeholders is vital to improving their operations. Currently, Electronic Data Interchange (EDI) and Port Community Systems (PCS), as primary enablers of digital seaports, have demonstrated their limitations to interchange information on time, accurately, efficiently, and securely, causing high operation costs, low resource management, and low performance. For these reasons, this contribution presents the Seaport Data Space (SDS) based on the Industrial Data Space (IDS) reference architecture model to enable a secure data sharing space and promote an intelligent transport multimodal terminal. Each seaport stakeholders implements the IDS connector to take part in the SDS and share their data. On top of SDS, a Big Data architecture is integrated to manage the massive data shared in the SDS and extract useful information to improve the decision-making. The architecture has been evaluated by enabling a port authority and a container terminal to share its data with a shipping company. As a result, several Key Performance Indicators (KPIs) have been developed by using the Big Data architecture functionalities. The KPIs have been shown in a dashboard to allow easy interpretability of results for planning vessel operations. The SDS environment may improve the communication between stakeholders by reducing the transaction costs, enhancing the quality of information, and exhibiting effectiveness.", "pages": "4372-4382", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Sarabia-J\u00e1come, David", "Palau, Carlos", "Esteve, Manuel", "Boronat, Fernando"]}]["8766970", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2019.2930004", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Data mining", "Itemsets", "Air quality", "Data structures", "Urban areas", "Location awareness", "Air quality", "data mining", "frequent", "itemset", "spatio-temporal"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "Frequent Pattern Mining on Time and Location Aware Air Quality Data", "url": "", "volume": "7", "year": "2019", "abstract": "With the advent of big data era, enormous volumes of data are generated every second. Varied data processing algorithms and architectures have been proposed in the past to achieve better execution of data mining algorithms. One such algorithm is extracting most frequently occurring patterns from the transactional database. Dependency of transactions on time and location further makes frequent itemset mining task more complex. The present work targets to identify and extract the frequent patterns from such time and location-aware transactional data. Primarily, the spatio-temporal dependency of air quality data is leveraged to find out frequently co-occurring pollutants over several locations of Delhi, the capital city of India. Varied approaches have been proposed in the past to extract frequent patterns efficiently, but this work suggests a generalized approach that can be applied to any numeric spatio-temporal transactional data, including air quality data. Furthermore, a comprehensive description of the algorithm along with a sample running example on air quality dataset is shown in this work. A detailed experimental evaluation is carried out on the synthetically generated datasets, benchmark datasets, and real world datasets. Furthermore, a comparison with spatio-temporal apriori as well as the other state-of-the-art non-apriori-based algorithms is shown. Results suggest that the proposed algorithm outperformed the existing approaches in terms of execution time of algorithm and memory resources.", "pages": "98921-98933", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Aggarwal, Apeksha", "Toshniwal, Durga"]}]["8993712", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2020.2973177", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Green products", "Urban areas", "Social networking (online)", "Spatiotemporal phenomena", "Data mining", "Big Data", "Air quality", "Urban green parks", "big data", "social networks", "spatiotemporal", "KDE", "data mining"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "Spatiotemporal Patterns of Visitors in Urban Green Parks by Mining Social Media Big Data Based Upon WHO Reports", "url": "", "volume": "8", "year": "2020", "abstract": "Green parks in urban areas are believed to enhance the well-being of residents. The importance of green spaces to support health and fitness in urban areas has recently regained interest. Reports released in 2010-2016 by the World Health Organization (WHO) on urban planning, environment, and health stated that green spaces can have a positive impact on physical activity, social and mental well-being, enhance air quality and decrease noise exposure. We analyzed the number of check-ins in various parks of Shanghai by utilizing geotagged social media network check-in data. This article presents a descriptive study using social media data by obtaining the three-year comparison of spatial and temporal patterns of park visits to raise public awareness that green parks provide a healthy environment that can be beneficial for the well-being of urban citizens. We investigated the visitor spatiotemporal behavior in more than 115 green parks in 10 districts of Shanghai with approximately 250,000 check-ins. We examined 3 years of geotagged data and our main findings are: (i) the spatial and temporal variations of users in urban green parks (ii) the gender differences in space and time with relation to urban green parks. The main objective of this article is to present evident data for policymakers on the advantages of providing green spaces access to urban citizens and to facilitate cities with systematic approaches to provide green space access to improve the health of urban citizens.", "pages": "39197-39211", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Ullah, Hidayat", "Wan, Wanggen", "Haidery, Saqib", "Khan, Naimat", "Ebrahimpour, Zeinab", "Muzahid, A."]}]["8913543", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2019.2955992", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Clustering algorithms", "Edge computing", "Real-time systems", "Big Data", "Cloud computing", "Internet of Things", "Classification algorithms", "Real-time streaming data", "clustering", "edge computing", "algorithm"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "Research and Analysis for Real-Time Streaming Big Data Based on Controllable Clustering and Edge Computing Algorithm", "url": "", "volume": "7", "year": "2019", "abstract": "Aiming at the low efficiency, poor performance and weak stability of traditional clustering algorithms and the poor response to the processing of massive data in real time, a real-time streaming controllable clustering edge computing algorithm (SCCEC) is proposed. First, the data tuples that arrive in real time are pre-processed by coarse clustering, the number of clusters, and the position of the center point are determined, and a set formed by macro clusters having differences is formed. Secondly, the macro cluster set obtained by the coarse clustering is sampled, and then K-means parallel clustering is performed with the largest and smallest distances, thereby realizing fine clustering of data. Finally, the completely clustering algorithm and the edge-computing algorithm are combined to realize the clustering analysis under the edge-computing framework. The experimental results show that the proposed algorithm has the advantages of high efficiency, good quality, and strong stability. It can quickly obtain the global optimal solution, and deal with massive data with high real-time performance. It can be used for real-time streaming data aggregation under big data background.", "pages": "171621-171632", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Li, Xiang", "Zhang, Zijia"]}]["8664564", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2019.2904248", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Data models", "Pricing", "Data privacy", "Sensors", "Economics", "Data integrity", "Data brokers", "profit maximization", "willingness-to-buy", "willingness-to-sell"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "Personal Data Trading Scheme for Data Brokers in IoT Data Marketplaces", "url": "", "volume": "7", "year": "2019", "abstract": "With the widespread use of the Internet of Things, data-driven services take the lead of both online and off-line businesses. Especially, personal data draw heavy attention of service providers because of the usefulness in value-added services. With the emerging big-data technology, a data broker appears, which exploits and sells personal data about individuals to other third parties. Due to little transparency between providers and brokers/consumers, people think that the current ecosystem is not trustworthy, and new regulations with strengthening the rights of individuals were introduced. Therefore, people have an interest in their privacy valuation. In this sense, the willingness-to-sell (WTS) of providers becomes one of the important aspects for data brokers; however, conventional studies have mainly focused on the willingness-to-buy (WTB) of consumers. Therefore, this paper proposes an optimized trading model for data brokers who buy personal data with proper incentives based on the WTS, and they sell valuable information from the refined dataset by considering the WTB and the dataset quality. This paper shows that the proposed model has a global optimal point by the convex optimization technique and proposes a gradient ascent-based algorithm. Consequently, it shows that the proposed model is feasible even if the data brokers spend costs to gather personal data.", "pages": "40120-40132", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Oh, Hyeontaek", "Park, Sangdon", "Lee, Gyu", "Heo, Hwanjo", "Choi, Jun"]}]["9261414", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2020.3038394", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Manufacturing processes", "Production", "Quality control", "Product design", "Real-time systems", "Manufacturing", "Quality assessment", "Quality management", "production control", "prediction methods"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "A Real-Time Quality Control System Based on Manufacturing Process Data", "url": "", "volume": "8", "year": "2020", "abstract": "Quality prediction is one of the key links of quality control. Benefitting from the development of digital manufacturing, manufacturing process data have grown rapidly, which allows product quality predictions to be made based on a real-time manufacturing process. A real-time quality control system (RTQCS) based on manufacturing process data is presented in this paper. In this study, the relationship between the product real-time quality status and processing task process was established by analyzing the relationship between the product manufacturing resources and the quality status. The key quality characteristics of the product were identified by analyzing the similarity of the product quality characteristic variations in the manufacturing process based on the big data technology, and a quality-resource matrix was constructed. Based on the quality-resource matrix, the RTQCS was established by introducing an association-rule incremental-update algorithm. Finally, the RTQCS was applied in actual production, and the performance of RTQCS was verified by experiments. The experiments showed that the RTQCS can effectively guarantee the quality of product manufacturing and improve the manufacturing efficiency during production.", "pages": "208506-208517", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Duan, Gui-Jiang", "Yan, Xin"]}]["7914196", {"address": "", "articleno": "", "doi": "10.23919/TST.2017.7914196", "issn": "1007-0214", "issue_date": "", "journal": "Tsinghua Science and Technology", "keywords": ["Heuristic algorithms", "Remuneration", "Real-time systems", "Databases", "Big Data", "data quality management", " data currency", " dynamic determining"], "month": "June", "number": "3", "numpages": "", "publisher": "", "title": "Efficient currency determination algorithms for dynamic data", "url": "", "volume": "22", "year": "2017", "abstract": "Data quality is an important aspect in data application and management, and currency is one of the major dimensions influencing its quality. In real applications, datasets timestamps are often incomplete and unavailable, or even absent. With the increasing requirements to update real-time data, existing methods can fail to adequately determine the currency of entities. In consideration of the velocity of big data, we propose a series of efficient algorithms for determining the currency of dynamic datasets, which we divide into two steps. In the preprocessing step, to better determine data currency and accelerate dataset updating, we propose the use of a topological graph of the processing order of the entity attributes. Then, we construct an Entity Query B-Tree (EQB-Tree) structure and an Entity Storage Dynamic Linked List (ES-DLL) to improve the querying and updating processes of both the data currency graph and currency scores. In the currency determination step, we propose definitions of the currency score and currency information for tuples referring to the same entity and use examples to discuss methods and algorithms for their computation. Based on our experimental results with both real and synthetic data, we verify that our methods can efficiently update data in the correct order of currency.", "pages": "227-242", "note": "", "ISSN": "1007-0214", "publicationtype": "article", "author": ["Ding, Xiaoou", "Wang, Hongzhi", "Gao, Yitong", "Li, Jianzhong", "Gao, Hong"]}]["9142149", {"address": "", "articleno": "", "doi": "10.26599/BDMA.2020.9020001", "issn": "2096-0654", "issue_date": "", "journal": "Big Data Mining and Analytics", "keywords": ["Measurement errors", "Measurement uncertainty", "Data analysis", "Loss measurement", "Newton method", "Covariance matrices", "Current measurement", "Cram\u00e9r-Rao Lower Bound (CRLB)", "error data analytics", "generalized least squares", "Received Signal Strength (RSS)"], "month": "Sep.", "number": "3", "numpages": "", "publisher": "", "title": "Error data analytics on RSS range-based localization", "url": "", "volume": "3", "year": "2020", "abstract": "The quality of measurement data is critical to the accuracy of both outdoor and indoor localization methods. Due to the inevitable measurement error, the analytics on the error data is critical to evaluate localization methods and to find the effective ones. For indoor localization, Received Signal Strength (RSS) is a convenient and low-cost measurement that has been adopted in many localization approaches. However, using RSS data for localization needs to solve a fundamental problem, that is, how accurate are these methods? The reason of the low accuracy of the current RSS-based localization methods is the oversimplified analysis on RSS measurement data. In this proposed work, we adopt a generalized measurement model to find optimal estimators whose estimated error is equal to the Cram\u00e9r-Rao Lower Bound (CRLB). Through mathematical techniques, the key factors that affect the accuracy of RSS-based localization methods are revealed, and the analytics expression that discloses the proportional relationship between the localization accuracy and these factors is derived. The significance of our discovery has two folds: First, we present a general expression for localization error data analytics, which can explain and predict the accuracy of range-based localization algorithms; second, the further study on the general analytics expression and its minimum can be used to optimize current localization algorithms.", "pages": "155-170", "note": "", "ISSN": "2096-0654", "publicationtype": "article", "author": ["Yang, Shuhui", "Yuan, Zimu", "Li, Wei"]}]["9389541", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2021.3069449", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Agriculture", "Digital agriculture", "Machine learning", "Machine learning algorithms", "Market research", "Big Data", "Internet of Things", "Rice production", "big data analytics", "Internet of Things", "machine learning", "smart farming", "precision agriculture", "agriculture supply chain"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "Towards Paddy Rice Smart Farming: A Review on Big Data, Machine Learning, and Rice Production Tasks", "url": "", "volume": "9", "year": "2021", "abstract": "Big Data (BD), Machine Learning (ML) and Internet of Things (IoT) are expected to have a large impact on Smart Farming and involve the whole supply chain, particularly for rice production. The increasing amount and variety of data captured and obtained by these emerging technologies in IoT offer the rice smart farming strategy new abilities to predict changes and identify opportunities. The quality of data collected from sensors greatly influences the performance of the modelling processes using ML algorithms. These three elements (e.g., BD, ML and IoT) have been used tremendously to improve all areas of rice production processes in agriculture, which transform traditional rice farming practices into a new era of rice smart farming or rice precision agriculture. In this paper, we perform a survey of the latest research on intelligent data processing technology applied in agriculture, particularly in rice production. We describe the data captured and elaborate role of machine learning algorithms in paddy rice smart agriculture, by analyzing the applications of machine learning in various scenarios, smart irrigation for paddy rice, predicting paddy rice yield estimation, monitoring paddy rice growth, monitoring paddy rice disease, assessing quality of paddy rice and paddy rice sample classification. This paper also presents a framework that maps the activities defined in rice smart farming, data used in data modelling and machine learning algorithms used for each activity defined in the production and post-production phases of paddy rice. Based on the proposed mapping framework, our conclusion is that an efficient and effective integration of all these three technologies is very crucial that transform traditional rice cultivation practices into a new perspective of intelligence in rice precision agriculture. Finally, this paper also summarizes all the challenges and technological trends towards the exploitation of multiple sources in the era of big data in agriculture.", "pages": "50358-50380", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Alfred, Rayner", "Obit, Joe", "Chin, Christie", "Haviluddin, Haviluddin", "Lim, Yuto"]}]["8824131", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2019.2939158", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Big Data", "Hospitals", "Organizations", "Electronic medical records", "Biomedical imaging", "Big data analytics", "electronic health records and impacts", "knowledge-based view"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "Optimizing the Electronic Health Records Through Big Data Analytics: A Knowledge-Based View", "url": "", "volume": "7", "year": "2019", "abstract": "Many hospitals are suffering from ineffective use of big data analytics with electronic health records (EHRs) to generate high quality insights for their clinical practices. Organizational learning has been a key role in improving the use of big data analytics with EHRs. Drawing on the knowledge-based view and big data lifecycle, we investigate how the three modes of knowledge can achieve meaningful use of big data analytics with EHRs. To test the associations in the proposed research model, we surveyed 580 nurses of a large hospital in China in 2019. Structural equation modelling was used to examine relationships between knowledge mode of EHRs and meaningful use of EHRs. The results reveal that know-what about EHRs utilization, know-how EHRs storage and utilization, and know-why storage and utilization can improve nurses' meaningful use of big data analytics with EHRs. This study contributes to the existing digital health and big data literature by exploring the proper adaptation of analytical tools to EHRs from the different knowledge mode in order to shape meaningful use of big data analytics with EHRs.", "pages": "136223-136231", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Zhang, Caifeng", "Ma, Rui", "Sun, Shiwei", "Li, Yujie", "Wang, Yichuan", "Yan, Zhijun"]}]["9427143", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2021.3078536", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Safety", "Big Data", "Internet of Things", "Production", "Radiofrequency identification", "Python", "Epidemics", "Two-dimensional code technology", "Internet of Things", "big data", "artificial intelligence", "food safety traceability system"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "Construct Food Safety Traceability System for People\u2019s Health Under the Internet of Things and Big Data", "url": "", "volume": "9", "year": "2021", "abstract": "In the context of epidemic prevention and control, food safety monitoring, data analysis and food safety traceability have become more important. At the same time, the most important reason for food safety issues is incomplete, opaque, and asymmetric information. The most fundamental way to solve these problems is to do a good job of traceability, and establish a reasonable and reliable food safety traceability system. The traceability system is currently an important means to ensure food quality and safety and solve the crisis of trust between consumers and the market. Research on food safety traceability systems based on big data, artificial intelligence and the Internet of Things provides ideas and methods to solve the problems of low credibility and difficult data storage in the application of traditional traceability systems. Therefore, this research takes rice as an example and proposes a food safety traceability system based on RFID two-dimensional code technology and big data storage technology in the Internet of Things. This article applies RFID technology to the entire system by analyzing the requirements of the system, designing the system database and database tables, encoding the two-dimensional code and generating the design for information entry. Using RFID radio frequency technology and the data storage function in big data to obtain information in the food production process. Finally, the whole process of food production information can be traced through the design of dynamic query platform and mobile terminal. In this research, the food safety traceability system based on big data and the Internet of Things guarantees the integrity, reliability and safety of traceability information from a technical level. This is an effective solution for enhancing the credibility of traceability information, ensuring the integrity of information, and optimizing the data storage structure.", "pages": "70571-70583", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Zheng, Miaomiao", "Zhang, Shanshan", "Zhang, Yidan", "Hu, Baozhong"]}]["9884973", {"address": "", "articleno": "", "doi": "10.1109/JSEN.2022.3203853", "issn": "1558-1748", "issue_date": "", "journal": "IEEE Sensors Journal", "keywords": ["Data integrity", "Measurement", "Internet of Things", "Big Data", "Data models", "Sensor phenomena and characterization", "Quality assessment", "Big data model", "data quality", "Internet of Things (IoT)", "machine learning", "trust"], "month": "Oct", "number": "20", "numpages": "", "publisher": "", "title": "End-to-End Data Quality Assessment Using Trust for Data Shared IoT Deployments", "url": "", "volume": "22", "year": "2022", "abstract": "Continued development of communication technologies has led to widespread Internet-of-Things (IoT) integration into various domains, including health, manufacturing, automotive, and precision agriculture. This has further led to the increased sharing of data among such domains to foster innovation. Most of these IoT deployments, however, are based on heterogeneous, pervasive sensors, which can lead to quality issues in the recorded data. This can lead to sharing of inaccurate or inconsistent data. There is a significant need to assess the quality of the collected data, should it be shared with multiple application domains, as inconsistencies in the data could have financial or health ramifications. This article builds on the recent research on trust metrics and presents a framework to integrate such metrics into the IoT data cycle for real-time data quality assessment. Critically, this article adopts a mechanism to facilitate end-user parameterization of a trust metric tailoring its use in the framework. Trust is a well-established metric that has been used to determine the validity of a piece or source of data in crowd-sourced or other unreliable data collection techniques such as that in IoT. The article further discusses how the trust-based framework eliminates the requirement for a gold standard and provides visibility into data quality assessment throughout the big data model. To qualify the use of trust as a measure of quality, an experiment is conducted using data collected from an IoT deployment of sensors to measure air quality in which low-cost sensors were colocated with a gold standard reference sensor. The calculated trust metric is compared with two well-understood metrics for data quality, root mean square error (RMSE), and mean absolute error (MAE). A strong correlation between the trust metric and the comparison metrics shows that trust may be used as an indicative quality metric for data quality. The metric incorporates the additional benefit of its ability for use in low context scenarios, as opposed to RMSE and MAE, which require a reference for comparison.", "pages": "19995-20009", "note": "", "ISSN": "1558-1748", "publicationtype": "article", "author": ["Byabazaire, John", "O\u2019Hare, Gregory", "Delaney, Declan"]}]["8746175", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2019.2924979", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Crowdsourcing", "Databases", "Task analysis", "Object recognition", "Monitoring", "Error analysis", "Big Data", "Quality management", "quality control", "data quality", "duplicate detection", "in-house crowdsourcing"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "Quality Management of Workers in an In-House Crowdsourcing-Based Framework for Deduplication of Organizations\u2019 Databases", "url": "", "volume": "7", "year": "2019", "abstract": "While organizations in the current era of big data are generating massive volumes of data, they also need to ensure that its quality is maintained for it to be useful in decision-making purposes. The problem of dirty data plagues every organization. One aspect of dirty data is the presence of duplicate data records that negatively impact the organization's operations in many ways. Many existing approaches attempt to address this problem by using traditional data cleansing methods. In this paper, we address this problem by using an in-house crowdsourcing-based framework, namely, DedupCrowd. One of the main obstacles of crowdsourcing-based approaches is to monitor the performance of the crowd, by which the integrity of the whole process is maintained. In this paper, a statistical quality control-based technique is proposed to regulate the performance of the crowd. We apply our proposed framework in the context of a contact center, where the Customer Service Representatives are used as the crowd to assist in the process of deduplicating detection. By using comprehensive working examples, we show how the different modules of the DedupCrowd work not only to monitor the performance of the crowd but also to assist in duplicate detection.", "pages": "90715-90730", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Saberi, Morteza", "Hussain, Omar", "Chang, Elizabeth"]}]["9845398", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2022.3195338", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Measurement", "Time series analysis", "Data integrity", "Decision support systems", "Standards", "Monitoring", "Internet of Things", "Data quality", "streaming time series", "decision support system"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "On the Evaluation, Management and Improvement of Data Quality in Streaming Time Series", "url": "", "volume": "10", "year": "2022", "abstract": "The Internet of Things (IoT) technologies plays a key role in the Fourth Industrial Revolution (Industry 4.0). This implies the digitisation of the industry and its services to improve productivity. To obtain the necessary information throughout the different processes, useful data streams are obtained to provide Artificial Intelligence and Big Data algorithms. However, strategic decision-making based on these algorithms may not be successful if they have been developed based on inadequate low-quality data. This research work proposes a set of metrics to measure Data Quality (DQ) in streaming time series, and implements and validates a set of techniques and tools that allow monitoring and improving the quality of the information. These techniques allow the early detection of problems that arise in relation to the quality of the data collected; and, in addition, they provide some mechanisms to solve these problems. Later, as part of the work, a use case related to industrial field is presented, where these techniques and tools have been deployed into a data management, monitoring and data analysis platform. This integration provides additional functionality to the platform, a Decision Support System (DSS) named DQ-REMAIN (Data Quality REport MAnagement and ImprovemeNt), for decision-making regarding the quality of data obtained from streaming time series.", "pages": "81458-81475", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Meritxell, G\u00f3mez-Omella", "Sierra, Basilio", "Ferreiro, Susana"]}]["9658542", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2021.3137398", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Histograms", "Videos", "Feature extraction", "Distortion", "Sensors", "Data mining", "Compression Sensing", "multiple histograms shifting (MHS)", "reversible data hiding"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "Multiple Histograms Shifting-Based Video Data Hiding Using Compression Sensing", "url": "", "volume": "10", "year": "2022", "abstract": "With the development of multimedia editing technologies, the copyright protection has attacked more attentions. Reversible data hiding (RDH), in which the cover can be recovered losslessly, is an effect method to eliminate embedding distortions. As a typical RDH method, histogram shifting (HS) is used widely. Most existing RDH schemes based on HS usually build sharp histograms by predicting and sorting techniques. To make use of spatial correlations of multimedia, several RDH schemes based on multiple HS (MHS) are proposed to protect copyright, in which some rigid rules are used to build multiple histograms. Against images, videos have more spatial and temple correlations and it is easier to acquire sharper histograms. In this paper, a video MHS scheme based on compression sensing (CS) is proposed. As a linear sensing algorithm, CS can measure macroblock residuals by reducing corrections among pixels to acquire distinguishable macroblock features, while keeping their statistical characteristics immutable. By employing CS, macroblocks with similar characteristics cluster together to formulate multiple histograms. For each of these histograms, data embedding is implemented to reduce shifting distortions by expanding the outermost bins while other bins are unchanged. Experimental results show that the quality of most test videos in our scheme are higher than that in the state-of-art schemes.", "pages": "699-707", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Chen, Yanli", "Zhou, Limengnan", "Zhou, Yonghui", "Chen, Yi", "Hu, Shengbo", "Dong, Zhicheng"]}]["8667817", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2019.2904759", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Big Data", "Reliability", "Manufacturing", "Failure analysis", "Analytical models", "Production", "Computational modeling", "Infant failure", "big data", "root cause analysis", "associated tree", "fuzzy DEA"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "Big Data-Oriented Product Infant Failure Intelligent Root Cause Identification Using Associated Tree and Fuzzy DEA", "url": "", "volume": "7", "year": "2019", "abstract": "Infant failure analyzing is an effective approach to improve production quality continuously. The root causes of infant failure have always been a puzzle to manufacturers. To satisfy the increasing demand for the fuzzy root cause analysis of product infant failure in the era of big data, a novel root cause identification approach based on the associated tree and fuzzy data envelopment analysis (DEA) is presented for product infant failure. First, to decrease fuzziness with regard to the mechanism of infant failure, the associated tree is adapted to guide the analysis process for possible root causes based on axiomatic domain mapping. Second, considering the fuzzy mechanism and massive data, the fuzzy DEA technique is adopted to cluster all the potential factors of functional parameters, physical parameters, and process parameters from big data regarding product life cycle. Third, the ranking method of decision-making unit efficiency in fuzzy DEA is used to model and rank the weight of each node in the established associated tree of infant failure. Finally, a case study of root cause identification for a typical infant failure of the vibration and noise of a washing machine is presented to demonstrate the feasibility and validity of the proposed method.", "pages": "34687-34698", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["He, Zhenzhen", "He, Yihai", "Liu, Fengdi", "Zhao, Yixiao"]}]["7298399", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2015.2490085", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Big data", "Data analystics", "Software engineering", "Cloud computing", "Reliability engineering", "Semantics", "Context modeling", "Big data analytics", "Cloud computing", "conceptual framework", "software engineering", "Big data analytics", "cloud computing", "conceptual framework", "software engineering"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "CF4BDA: A Conceptual Framework for Big Data Analytics Applications in the Cloud", "url": "", "volume": "3", "year": "2015", "abstract": "Building big data analytics (BDA) applications in the cloud introduces inevitable challenges, such as loss of control and uncertainty. To address the existing challenges, numerous efforts have been made on BDA application engineering to optimize the quality of BDA applications in the cloud, such as performance and reliability. However, there is still a lack of systematic view on engineering BDA applications in the cloud. Therefore, in this paper, we present a conceptual framework named CF4BDA to analyze the existing work on BDA applications from two perspectives: 1) the lifecycle of BDA applications and 2) the objects involved in the context of BDA applications in the cloud. The framework can help researchers and practitioners identify the research opportunities in a structured way and guide implementing BDA applications in the cloud. We perform a preliminary evaluation of the usefulness of CF4BDA by applying it to analyze a set of representative studies.", "pages": "1944-1952", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Lu, Qinghua", "Li, Zheng", "Kihl, Maria", "Zhu, Liming", "Zhang, Weishan"]}]["8392685", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2018.2849822", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Sleep apnea", "Monitoring", "Biomedical monitoring", "Real-time systems", "Computer architecture", "Big Data", "Sensors", "Internet-of-Things", "big data", "interoperability", "sleep monitoring", "health monitoring", "open data", "fog computing", "cloud computing"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "A Smart System for Sleep Monitoring by Integrating IoT With Big Data Analytics", "url": "", "volume": "6", "year": "2018", "abstract": "Obtrusive sleep apnea (OSA) is one of the most important sleep disorders because it has a direct adverse impact on the quality of life. Intellectual deterioration, decreased psychomotor performance, behavior, and personality disorders are some of the consequences of OSA. Therefore, a real-time monitoring of this disorder is a critical need in healthcare solutions. There are several systems for OSA detection. Nevertheless, despite their promising results, these systems not guiding their treatment. For these reasons, this research presents an innovative system for both to detect and support of treatment of OSA of elderly people by monitoring multiple factors such as sleep environment, sleep status, physical activities, and physiological parameters as well as the use of open data available in smart cities. Our system architecture performs two types of processing. On the one hand, a pre-processing based on rules that enables the sending of real-time notifications to responsible for the care of elderly, in the event of an emergency situation. This pre-processing is essentially based on a fog computing approach implemented in a smart device operating at the edge of the network that additionally offers advanced interoperability services: technical, syntactic, and semantic. On the other hand, a batch data processing that enables a descriptive analysis that statistically details the behavior of the data and a predictive analysis for the development of services, such as predicting the least polluted place to perform outdoor activities. This processing uses big data tools on cloud computing. The performed experiments show a 93.3% of effectivity in the air quality index prediction to guide the OSA treatment. The system's performance has been evaluated in terms of latency. The achieved results clearly demonstrate that the pre-processing of data at the edge of the network improves the efficiency of the system.", "pages": "35988-36001", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Yacchirema, Diana", "Sarabia-J\u00e1Come, David", "Palau, Carlos", "Esteve, Manuel"]}]["8788596", {"address": "", "articleno": "", "doi": "10.1109/TSUSC.2019.2929953", "issn": "2377-3782", "issue_date": "", "journal": "IEEE Transactions on Sustainable Computing", "keywords": ["Water quality", "Water resources", "Lakes", "Water pollution", "Biology", "Urban areas", "Sensors", "Sustainable water supply", "water quality control", "data perception", "risk evaluation", "frequency analysis", "scalability"], "month": "July", "number": "3", "numpages": "", "publisher": "", "title": "Quality Risk Analysis for Sustainable Smart Water Supply Using Data Perception", "url": "", "volume": "5", "year": "2020", "abstract": "Constructing Sustainable Smart Water Supply systems are facing serious challenges all around the world with the fast expansion of modern cities. Water quality is influencing our life ubiquitously and prioritizing all the urban management. Traditional urban water quality control mostly focused on routine tests of quality indicators, which include physical, chemical, and biological groups. However, the inevitable delay for biological indicators has increased the health risk and leads to accidents such as massive infections in many big cities. In this paper, we first analyze the problem, technical challenges, and research questions. Then, we provide a possible solution by building a risk analysis framework for the urban water supply system. It takes indicator data we collected from industrial processes to perceive water quality changes, and further for risk detection. In order to provide explainable results, we propose an Adaptive Frequency Analysis (Adp-FA) method to resolve the data using indicators' frequency domain information for their inner relationships and individual prediction. We also investigate the scalability properties of this method from indicator, geography, and time domains. For the application, we select industrial quality data sets collected from a Norwegian project in four different urban water supply systems, as Oslo, Bergen, Str\u00f8mmen, and \u00c5lesund. We employ the proposed method to test spectrogram, prediction accuracy, and time consumption, comparing with classical Artificial Neural Network and Random Forest methods. The results show our method better perform in most of the aspects. It is feasible to support industrial water quality risk early warnings and further decision support.", "pages": "377-388", "note": "", "ISSN": "2377-3782", "publicationtype": "article", "author": ["Wu, Di", "Wang, Hao", "Mohammed, Hadi", "Seidu, Razak"]}]["9299499", {"address": "", "articleno": "", "doi": "10.17775/CSEEJPES.2020.04080", "issn": "2096-0042", "issue_date": "", "journal": "CSEE Journal of Power and Energy Systems", "keywords": ["Cleaning", "Distribution networks", "Big Data", "Prediction algorithms", "Clustering algorithms", "Data models", "Anomaly detection", "Data cleaning", "Outliers detection", "missing data imputation", "LOF", "DBSCAN", "Random Forest"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "A big data cleaning method based on improved CLOF and Random Forest for distribution network", "url": "", "volume": "", "year": "2020", "abstract": "In order to improve the data quality, the big data cleaning method of distribution network was studied in this paper. First, the Local Outlier Factor (LOF) algorithm based on DBSCAN clustering was used to detect outliers. However, due to the difficulty in determining the LOF threshold, a method of dynamically calculating the threshold based on the transformer districts and time was proposed. Besides, the LOF algorithm combines the statistical distribution method to reduce the \"misjudgment rate\". Aiming at the diversity and complexity of data missing forms in power big data, this paper improved the Random Forest imputation algorithm, which can be applied to various forms of missing data, especially the blocked missing data and even some horizontal or vertical data completely missing. The data in this paper were from real data of 44 transformer districts of a certain 10kV line in distribution network. Experimental results showed that outlier detection was accurate and suitable for any shape and multidimensional power big data. The improved Random Forest imputation algorithm was suitable for all missing forms, with higher imputation accuracy and better model stability. By comparing the network loss prediction between the data using this data cleaning method and the data removing outliers and missing values, it was found that the accuracy of network loss prediction had been improved by nearly 4 percentage points using the data cleaning method mentioned in this paper. Additionally, as the proportion of bad data increased, the difference between the prediction accuracy of cleaned data and that of uncleaned data was greater.", "pages": "1-10", "note": "", "ISSN": "2096-0042", "publicationtype": "article", "author": ["Liu, Jie", "Cao, Yijia", "Li, Yong", "Guo, Yixiu", "Deng, Wei"]}]["7912315", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2017.2694446", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Diseases", "Hospitals", "Prediction algorithms", "Machine learning algorithms", "Big Data", "Data models", "Big data analytics", "machine learning", "healthcare"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "Disease Prediction by Machine Learning Over Big Data From Healthcare Communities", "url": "", "volume": "5", "year": "2017", "abstract": "With big data growth in biomedical and healthcare communities, accurate analysis of medical data benefits early disease detection, patient care, and community services. However, the analysis accuracy is reduced when the quality of medical data is incomplete. Moreover, different regions exhibit unique characteristics of certain regional diseases, which may weaken the prediction of disease outbreaks. In this paper, we streamline machine learning algorithms for effective prediction of chronic disease outbreak in disease-frequent communities. We experiment the modified prediction models over real-life hospital data collected from central China in 2013-2015. To overcome the difficulty of incomplete data, we use a latent factor model to reconstruct the missing data. We experiment on a regional chronic disease of cerebral infarction. We propose a new convolutional neural network (CNN)-based multimodal disease risk prediction algorithm using structured and unstructured data from hospital. To the best of our knowledge, none of the existing work focused on both data types in the area of medical big data analytics. Compared with several typical prediction algorithms, the prediction accuracy of our proposed algorithm reaches 94.8% with a convergence speed, which is faster than that of the CNN-based unimodal disease risk prediction algorithm.", "pages": "8869-8879", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Chen, Min", "Hao, Yixue", "Hwang, Kai", "Wang, Lu", "Wang, Lin"]}]["8374410", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2018.2845105", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Data models", "Games", "Analytical models", "Internet of Things", "Data mining", "Nash equilibrium", "Numerical models", "Data market", "Internet of Things", "stackelberg game", "industrial informatics"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "Three Hierarchical Levels of Big-Data Market Model Over Multiple Data Sources for Internet of Things", "url": "", "volume": "6", "year": "2018", "abstract": "This paper proposes three hierarchical levels of a competitive big-data market model. We consider that a service provider gathers data from multiple data sources and provides valuable information from refined data as a service to its customers. Under our approach, a service provider determines optimal data procurement from multiple data sources within its budget constraint. The multiple data sources follow the service provider's action by independently submitting bidding prices to the service provider. Further, customers decide whether to subscribe or not based on the subscription fee, their willingness-to-pay, and the quality of the refined data. We study the economic benefits of such a market model by analyzing the hierarchical decision making procedures as a Stackelberg game. We show the existence and the uniqueness of the Nash equilibrium (NE), and the NE solution is given as a closed form. Finally, we reveal that the obtained unique equilibrium solution maximizes the payoff of all market participants.", "pages": "31269-31280", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Jang, Busik", "Park, Sangdon", "Lee, Joohyung", "Hahn, Sang-Geun"]}]["7978034", {"address": "", "articleno": "", "doi": "10.21629/JSEE.2017.03.21", "issn": "1004-4132", "issue_date": "", "journal": "Journal of Systems Engineering and Electronics", "keywords": ["Algorithm design and analysis", "Reliability", "Internet", "Telecommunications", "Big Data", "Data models", "truth finder", "data reliability", "entity attribute", "data conflict"], "month": "June", "number": "3", "numpages": "", "publisher": "", "title": "Truth finder algorithm based on entity attributes for data conflict solution", "url": "", "volume": "28", "year": "2017", "abstract": "The Internet now is a large-scale platform with big data. Finding truth from a huge dataset has attracted extensive attention, which can maintain the quality of data collected by users and provide users with accurate and efficient data. However, current truth finder algorithms are unsatisfying, because of their low accuracy and complication. This paper proposes a truth finder algorithm based on entity attributes (TFAEA). Based on the iterative computation of source reliability and fact accuracy, TFAEA considers the interactive degree among facts and the degree of dependence among sources, to simplify the typical truth finder algorithms. In order to improve the accuracy of them, TFAEA combines the one-way text similarity and the factual conflict to calculate the mutual support degree among facts. Furthermore, TFAEA utilizes the symmetric saturation of data sources to calculate the degree of dependence among sources. The experimental results show that TFAEA is not only more stable, but also more accurate than the typical truth finder algorithms.", "pages": "617-626", "note": "", "ISSN": "1004-4132", "publicationtype": "article", "author": ["Xu, Xiaolong", "Liu, Xinxin", "Liu, Xiaoxiao", "Sun, Yanfei"]}]["9906976", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2022.3211396", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Social networking (online)", "Data models", "Blogs", "Data integrity", "Sentiment analysis", "Information integrity", "Big Data", "Coherence", "Social media", "big data", "microblogging platforms", "topic modeling", "data cleansing", "data quality", "topic coherence", "purity"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "Enhancing Big Social Media Data Quality for Use in Short-Text Topic Modeling", "url": "", "volume": "10", "year": "2022", "abstract": "With the emergence of microblogging platforms and social media applications, large amounts of user-generated data in the form of comments, reviews, and brief text messages are produced every day. Microblog data is typically of poor quality; hence improving the quality of the data is a significant scientific and practical challenge. In spite of the relevance of the problem, there has been not much work so far, especially in regard to microblog data quality for Short-Text Topic Modelling (STTM) purposes. This paper addresses this problem and proposes an approach called the Social Media Data Cleansing Model (SMDCM) to improve data quality for STTM. We evaluate SMDCM using six topic modelling methods, namely the Latent Dirichlet Allocation (LDA), Word-Network Topic Model (WNTM), Pseudo-document-based Topic Modelling (PTM), Biterm Topic Model (BTM), Global and Local word embedding-based Topic Modeling (GLTM), and Fuzzy Topic modelling (FTM). We used the Real-world Cyberbullying Twitter (RW-CB-Twitter) and the Cyberbullying Mendeley (CB-MNDLY) datasets in the evaluation. The results proved the efficiency of the GLTM and WNTM over the other STTM models when applying the SMDCM techniques, which achieved optimum topic coherence and high accuracy values on RW-CB-Twitter and CB-MNDLY datasets.", "pages": "105328-105351", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Murshed, Belal", "Abawajy, Jemal", "Mallappa, Suresha", "Saif, Mufeed", "Al-Ghuribi, Sumaia", "Ghanem, Fahd"]}]["9663261", {"address": "", "articleno": "", "doi": "10.26599/BDMA.2021.9020021", "issn": "2096-0654", "issue_date": "", "journal": "Big Data Mining and Analytics", "keywords": ["Task analysis", "Feature extraction", "Portfolios", "Training", "Investment", "Amplitude modulation", "Data mining", "Intelligent Financial Advisor (IFA)", "potential client identification", "MultiTask Learning (MTL)", "feature selection"], "month": "March", "number": "1", "numpages": "", "publisher": "", "title": "Toward intelligent financial advisors for identifying potential clients: A multitask perspective", "url": "", "volume": "5", "year": "2022", "abstract": "Intelligent Financial Advisors (IFAs) in online financial applications (apps) have brought new life to personal investment by providing appropriate and high-quality portfolios for users. In real-world scenarios, identifying potential clients is a crucial issue for IFAs, i.e., identifying users who are willing to purchase the portfolios. Thus, extracting useful information from various characteristics of users and further predicting their purchase inclination are urgent. However, two critical problems encountered in real practice make this prediction task challenging, i.e., sample selection bias and data sparsity. In this study, we formalize a potential conversion relationship, i.e., user ! activated user ! client and decompose this relationship into three related tasks. Then, we propose a Multitask Feature Extraction Model (MFEM), which can leverage useful information contained in these related tasks and learn them jointly, thereby solving the two problems simultaneously. In addition, we design a two-stage feature selection algorithm to select highly relevant user features efficiently and accurately from an incredibly huge number of user feature fields. Finally, we conduct extensive experiments on a real-world dataset provided by a famous fintech bank. Experimental results clearly demonstrate the effectiveness of MFEM.", "pages": "64-78", "note": "", "ISSN": "2096-0654", "publicationtype": "article", "author": ["Shao, Qixiang", "Yu, Runlong", "Zhao, Hongke", "Liu, Chunli", "Zhang, Mengyi", "Song, Hongmei", "Liu, Qi"]}]["8674747", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2019.2907573", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Image segmentation", "Clustering algorithms", "Cluster computing", "Big Data", "Partitioning algorithms", "Sparks", "Task analysis", "Fuzzy C-means", "image segmentation", "image big data", "Apache Spark", "parallel algorithm"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "A Spark-Based Parallel Fuzzy $c$ -Means Segmentation Algorithm for Agricultural Image Big Data", "url": "", "volume": "7", "year": "2019", "abstract": "With the explosive growth of image big data in the agriculture field, image segmentation algorithms are confronted with unprecedented challenges. As one of the most important images segmentation technologies, the fuzzy c-means (FCMs) algorithm has been widely used in the field of agricultural image segmentation as it provides simple computation and high-quality segmentation. However, due to its large amount of computation, the sequential FCM algorithm is too slow to finish the segmentation task within an acceptable time. This paper proposes a parallel FCM segmentation algorithm based on the distributed memory computing platform Apache Spark for agricultural image big data. The input image is first converted from the RGB color space to the lab color space and generates point cloud data. Then, point cloud data are partitioned and stored in different computing nodes, in which the membership degrees of pixel points to different cluster centers are calculated and the cluster centers are updated iteratively in a data-parallel form until the stopping condition is satisfied. Finally, point cloud data are restored after clustering for reconstructing the segmented image. On the Spark platform, the performance of the parallel FCMs algorithm is evaluated and reaches an average speedup of 12.54 on ten computing nodes. The experimental results show that the Spark-based parallel FCMs algorithm can obtain a significant increase in speedup, and the agricultural image testing set delivers a better performance improvement of 128% than the Hadoop-based approach. This paper indicates that the Spark-based parallel FCM algorithm provides faster speed of segmentation for agricultural image big data and has better scale-up and size-up rates.", "pages": "42169-42180", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Liu, Bin", "He, Songrui", "He, Dongjian", "Zhang, Yin", "Guizani, Mohsen"]}]["8319403", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2018.2817022", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Data mining", "Feature extraction", "Decision trees", "Knowledge acquisition", "Data models", "Production", "Knowledge engineering", "data mining", "features ranking", "algorithm selection", "decision tree", "production rule", "user experience"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "A Data-Driven Knowledge Acquisition System: An End-to-End Knowledge Engineering Process for Generating Production Rules", "url": "", "volume": "6", "year": "2018", "abstract": "Data-driven knowledge acquisition is one of the key research fields in data mining. Dealing with large amounts of data has received a lot of attention in the field recently, and a number of methodologies have been proposed to extract insights from data in an automated or semi-automated manner. However, these methodologies generally target a specific aspect of the data mining process, such as data acquisition, data preprocessing, or data classification. However, a comprehensive knowledge acquisition method is crucial to support the end-to-end knowledge engineering process. In this paper, we introduce a knowledge acquisition system that covers all major phases of the cross-industry standard process for data mining. Acknowledging the importance of an end-to-end knowledge engineering process, we designed and developed an easy-to-use data-driven knowledge acquisition tool (DDKAT). The major features of the DDKAT are: (1) a novel unified features scoring approach for data selection; (2) a user-friendly data processing interface to improve the quality of the raw data; (3) an appropriate decision tree algorithm selection approach to build a classification model; and (4) the generation of production rules from various decision tree classification models in an automated manner. Furthermore, two diabetes studies were performed to assess the value of the DDKAT in terms of user experience. A total of 19 experts were involved in the first study and 102 students in the artificial intelligence domain were involved in the second study. The results showed that the overall user experience of the DDKAT was positive in terms of its attractiveness, as well as its pragmatic and hedonic quality factors.", "pages": "15587-15607", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Ali, Maqbool", "Ali, Rahman", "Khan, Wajahat", "Han, Soyeon", "Bang, Jaehun", "Hur, Taeho", "Kim, Dohyeong", "Lee, Sungyoung", "Kang, Byeong"]}]["8951087", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2020.2964707", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Vehicle-to-everything", "Big Data", "Protocols", "Edge computing", "Wireless communication", "Unicast", "Quality of service", "Edge computing", "traffic big data", "vehicular ad hoc networks", "V2X communications"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "Edge-Based V2X Communications With Big Data Intelligence", "url": "", "volume": "8", "year": "2020", "abstract": "Vehicular Internet-of-Things applications require an efficient Vehicle-to-Everything (V2X) communication scheme. However, it is particularly challenging to achieve a high throughput and low latency with limited wireless resources in highly dynamic vehicular networks. In this article, we propose a scheme that enhances V2V communications through integration of vehicle edge-based forwarding and learning-based edge selection policy optimization. The proposed scheme has three main characteristics. First, the Hierarchical edge-based preemptive route creation is introduced to create hierarchical edges and conduct efficient packet forwarding as well as route aggregation. Second, Two-stage learning is introduced to select efficient edge nodes using big data driven traffic prediction and reinforcement learning-based edge node selection. Third, Context-aware edge selection is employed to improve the performance of edge-based forwarding in various contexts. We use real traffic big data and realistic vehicular network simulations to evaluate the performance of the proposed scheme and show the advantage over other baseline approaches.", "pages": "8603-8613", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Guleng, Siri", "Wu, Celimuge", "Liu, Zhi", "Chen, Xianfu"]}]["8449924", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2018.2867633", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Quality of service", "Big Data", "Traffic congestion", "Task analysis", "Optimization", "Heuristic algorithms", "Web services", "Big Data space", "multiobjective service selection", "QoS preferences", "rule-based", "traffic-efficient"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "QoS-Aware Rule-Based Traffic-Efficient Multiobjective Service Selection in Big Data Space", "url": "", "volume": "6", "year": "2018", "abstract": "The number of Web services has increased dramatically during the last few years. This has resulted in an increase in the volume of candidate services for tasks in composition systems. This has led to growth in the variety of nonfunctional properties in service selection, resulting in uncertainty (veracity issues) among such properties, which has severely affected the NP-hard aspects of service selection. Despite this, consumers in many areas would like access to a variety of selection methods such as linear programming and dynamic programming techniques. An additional problem is that the composition length (the number of tasks) of the workflow has increased, with the incorporation of research domains such as data science. These trending composition issues are challenging the computational power of existing methods. Such concerns have opened the door to research involving Big Data space. We propose a flexible, distributed selection algorithm that facilitates heterogeneous-selection methods to satisfy multiobjective composition requirements rather than rigid, specific composition requirements. However, service-selection processes in a Big Data space will inevitably increase traffic congestion caused by the increased volume of internal communication, particularly external traffic, such as Zipf and Pareto phenomena, and internal traffic during shuffling. To address these concerns, we propose solutions for each case. Our experiments demonstrate that the proposed traffic-efficient multiobjective method is well behaved when selecting services in Big Data space.", "pages": "48797-48814", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Akila, T.H.", "Paik, Incheon", "Siriweera, S."]}]["9815858", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2022.3188871", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Generative adversarial networks", "Electronics industry", "Data models", "Training", "Support vector machines", "Random forests", "Neurons", "Data imputation", "deep learning", "fault classification and detection", "generative adversarial networks", "machine learning", "missing data", "semiconductor equipment"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "Semi-GAN: An Improved GAN-Based Missing Data Imputation Method for the Semiconductor Industry", "url": "", "volume": "10", "year": "2022", "abstract": "Complete data are required for the operation, maintenance, and detection of faults in semiconductor equipment. Missing data occur frequently because of defects such as sensor, data storage, and communication faults, leading to reductions in yield, quality, and productivity. Although many attempts have been made to solve this problem in other fields, few studies have specifically addressed data imputation in the semiconductor industry. In this study, an improved generative adversarial network (GAN)-based missing data imputation for the semiconductor industry called Semi-GAN is proposed. This study introduces a machine learning approach for dealing with data imputation in the semiconductor industry. The proposed method was applied to real data and evaluated using traditional techniques. In particular, the proposed method showed excellent results compared to traditional attribution methods when all missing data ratios in the experiments were less than 20%. It was also observed to be superior when simple and repetitive patterns were omitted rather than repetitive but not simple patterns.", "pages": "72328-72338", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Lee, Sun-Yong", "Connerton, Timothy", "Lee, Yeon-Woo", "Kim, Daeyoung", "Kim, Donghwan", "Kim, Jin-Ho"]}]["9115006", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2020.3001749", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Classification algorithms", "Heuristic algorithms", "Support vector machines", "Text categorization", "Training", "Data mining", "dynamic data", "students\u2019 learning degree", "subjective weighting method", "clustering algorithm"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "An Optimized Mining Algorithm for Analyzing Students\u2019 Learning Degree Based on Dynamic Data", "url": "", "volume": "8", "year": "2020", "abstract": "With the rapid development of educational informatization, it has enabled education to enter the era of big data. How to extract effective information from educational big data and realize adaptive personalized learning goals have become the current research hotspot. The traditional static data only analyzes the students' learning degree based on the students' final answer, but ignores the dynamic data in the process of answering questions, such as the modification and the time it answered on the question, which makes it difficult to fully and accurately mine the correlation between the massive data, so it turns from static data mining to dynamic data mining. The paper proposes an optimized mining algorithm for analyzing students' learning degree based on dynamic data. The algorithm first uses the optimized text classification technology to match the question texts to the knowledge points automatically, so as to improves the efficiency and quality. Then, it uses the subjective weighting method combined with the expert experience to generate the learning degree matrix of students on knowledge points based on dynamic data of the students' records. Finally, the DBSCAN clustering algorithm is used to cluster the personalized learning characteristics of students according to the learning degree matrix. The experimental result shows that the algorithm can deal with massive data automatically and effectively, and analyze the students' learning degree on knowledge points comprehensively and accurately, so as to classify students and realize personalized teaching.", "pages": "113543-113556", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Shao, Zengzhen", "Sun, Hongxu", "Wang, Xiao", "Sun, Zhongzhi"]}]["8979364", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2020.2971244", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Predictive models", "Biological system modeling", "Algae", "Adaptation models", "Mathematical model", "Water quality", "Data models", "Algal bloom", "action dependent heuristic dynamic programming", "time-varying parameters identification", "prediction"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "An Action Dependent Heuristic Dynamic Programming Approach for Algal Bloom Prediction With Time-Varying Parameters", "url": "", "volume": "8", "year": "2020", "abstract": "Algal bloom is a nonlinear and time-varying process, which brings challenges for the accurate prediction. For the existing mechanism model of algae ignores the external key factors, we propose an algae growth model (AGM) optimized by action dependent heuristic dynamic programming (ADHDP). This model has the structure of information interaction with the outside, which can predict algal bloom with well adaptive ability. In this paper, chlorophyll-a concentration is used as the representative factor of algal bloom. We use ADHDP approach to map the external key factors to the time-varying parameters, so the AGM can be adjusted to realize the self-adaptive prediction with the changes in external environments. Compared with different prediction methods, the simulation result shows that the ADHDP-AGM prediction model can accurately predict the chlorophyll-a concentration under different data distributions. Moreover, the prediction process shows that the time-varying parameters in AGM conform to the evolution trend of chlorophyll-a concentration in fact, which further improves the interpretability of prediction model. It provides a new perspective for building a data-driven prediction model with clear physical significance, and makes the mechanism research and data science further fusion.", "pages": "26235-26246", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Zhang, Huiyan", "Hu, Bo", "Wang, Xiaoyi", "Xu, Jiping", "Wang, Li", "Sun, Qian", "Zhao, Zhiyao"]}]["9104899", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2020.2995763", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Pain", "Twitter", "Tagging", "Crawlers", "Big Data", "Patient insights", "thick data", "learning patient preferences", "graph based algorithms", "graph-based machine learning", "graph-based transfer learning", "Neo4j", "twitter conversation communities"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "Envisioning Insight-Driven Learning Based on Thick Data Analytics With Focus on Healthcare", "url": "", "volume": "8", "year": "2020", "abstract": "Detecting and analyzing patient insights from social media enables healthcare givers to better understand what patients want and also to identify their pain points. Healthcare institutions cannot neglect the need to monitor and analyze popular social media outlets such as Twitter and Facebook. To have a study success, a healthcare giver needs to be able to engage with their patients and adapt to their preferences effectively. However, data-driven decision-making is no longer enough, as the best-in-class organizations struggle to realize tangible benefits from their data-driven analytics investments. Relying on simplistic textual analytics that use big data technologies to learn consumer/patient insights is no longer sufficient as most of these analytics utilize sort of bag-of-words counting algorithms. The majority of projects utilizing big data analytics have failed due to the obsession with metrics at the expense of capturing the customer's perspective data, as well as the failure in turning consumer insights into actions. Most of the consumer insights can be captured with qualitative research methods that work with small, even statistically insignificant, sample sizes. Employing qualitative analytics provide some kind of actionable intelligence which acquires understanding to broad questions about the consumer needs in tandem with analytical power. Generating insight, on one hand, requires sound techniques to measure consumers' engagement more precisely and offers depth analytics to the consumer data story. On the other hand, turning relevant insights into actions requires incorporating actionable intelligence across the business by verify hypotheses based on qualitative findings by using web analytics to see if these axioms apply to a large number of customers. The first component of our visionary approach is dedicated to identifying the relationships between constituents of the healthcare pain points as echoed by the social media conversation in terms of sociographic network where the elements composing these conversations are described as nodes and their interactions as links. In this part, conversation groups of nodes that are heavily connected will be identified representing what we call conversation communities. By identifying these conversation communities several consumer hidden insights can be inferred from using techniques such as visualizing conversation graphs relevant to given pain point, conversation learning from question answering, conversations summaries, conversation timelines, conversation anomalies and other conversation pattern learning techniques. These techniques will identify and learn the patient insights without forgetting from the context of conversation communities, are tagged as \u201cthick data analytics\u201d. Additionally machine learning methods can be used as assistive techniques to learn from the identified thick data and build models around identified thick data. With the use of transfer learning we also can fine tune these models with the arrival of new conversations. The author is currently experimenting with these seven insights driven learning methods described in this paper with massive geo-located Twitter data to infer the quality of care related to the current COVID-19 outbreak.", "pages": "114998-115004", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Fiaidhi, Jinan"]}]["9159116", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2020.3014301", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Wireless networks", "Quality of service", "Optimization", "Resource management", "Real-time systems", "Artificial intelligence", "Complexity theory", "Wireless network personalization", "machine learning (ML)", "big data-driven", "wireless networks", "user satisfaction", "quality-of-experience (QoE)", "deep learning (DL)", "artificial intelligence (AI)", "resource allocation", "evolutionary multi-objective optimization (EMOO)"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "Personalized Resource Allocation in Wireless Networks: An AI-Enabled and Big Data-Driven Multi-Objective Optimization", "url": "", "volume": "8", "year": "2020", "abstract": "The design and optimization of wireless networks have mostly been based on strong mathematical and theoretical modeling. Nonetheless, as novel applications emerge in the era of 5G and beyond, unprecedented levels of complexity will be encountered in the design and optimization of the network. As a result, the use of Artificial Intelligence (AI) is envisioned for wireless network design and optimization due to the flexibility and adaptability it offers in solving extremely complex problems in real-time. One of the main future applications of AI is enabling user-level personalization for numerous use cases. AI will revolutionize the way we interact with computers in which computers will be able to sense commands and emotions from humans in a non-intrusive manner, making the entire process transparent to users. By leveraging this capability, and accelerated by the advances in computing technologies, wireless networks can be redesigned to enable the personalization of network services to the user level in real-time. While current wireless networks are being optimized to achieve a predefined set of quality requirements, the personalization technology advocated in this article is supported by an intelligent big data-driven layer designed to micro-manage the scarce network resources. This layer provides the intelligence required to decide the necessary service quality that achieves the target satisfaction level for each user. Due to its dynamic and flexible design, personalized networks are expected to achieve unprecedented improvements in optimizing two contradicting objectives in wireless networks: saving resources and improving user satisfaction levels. This article presents some foundational background on the proposed network personalization technology and its enablers. Then, an AI-enabled big data-driven surrogate-assisted multi-objective optimization formulation is proposed and tested to illustrate the feasibility and prominence of this technology.", "pages": "144592-144609", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Alkurd, Rawan", "Abualhaol, Ibrahim", "Yanikomeroglu, Halim"]}]["9826667", {"address": "", "articleno": "", "doi": "10.1108/IJCS-09-2019-0023", "issn": "2398-7294", "issue_date": "", "journal": "International Journal of Crowd Science", "keywords": ["Data analysis", "Databases", "Government", "Data visualization", "Interference", "Filtering algorithms", "Big Data", "Data management", "Data mining", "Crowdsourced big data and analytics", "Knowledge discovery"], "month": "October", "number": "3", "numpages": "", "publisher": "", "title": "Knowledge discovery in sociological databases: An application on general society survey dataset", "url": "", "volume": "3", "year": "2019", "abstract": "Purpose \u2013 The General Society Survey(GSS) is a kind of government-funded survey which aims at examining the Socio-economic status, quality of life, and structure of contemporary society. GSS data set is regarded as one of the authoritative source for the government and organization practitioners to make data-driven policies. The previous analytic approaches for GSS data set are designed by combining expert knowledges and simple statistics. By utilizing the emerging data mining algorithms, we proposed a comprehensive data management and data mining approach for GSS data sets. Design/methodology/approach \u2013 The approach are designed to be operated in a two-phase manner: a data management phase which can improve the quality of GSS data by performing attribute pre-processing and filter-based attribute selection; a data mining phase which can extract hidden knowledge from the data set by performing data mining analysis including prediction analysis, classification analysis, association analysis and clustering analysis. Findings \u2013 According to experimental evaluation results, the paper have the following findings: Performing attribute selection on GSS data set can increase the performance of both classification analysis and clustering analysis; all the data mining analysis can effectively extract hidden knowledge from the GSS data set; the knowledge generated by different data mining analysis can somehow cross-validate each other. Originality/value \u2013 By leveraging the power of data mining techniques, the proposed approach can explore knowledge in a fine-grained manner with minimum human interference. Experiments on Chinese General Social Survey data set are conducted at the end to evaluate the performance of our approach.", "pages": "315-332", "note": "", "ISSN": "2398-7294", "publicationtype": "article", "author": ["Pan, Zhiwen", "Li, Jiangtian", "Chen, Yiqiang", "Pacheco, Jesus", "Dai, Lianjun", "Zhang, Jun"]}]["8413084", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2018.2857499", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Data mining", "Medical services", "Sensors", "Unified modeling language", "Medical diagnostic imaging", "Ontologies", "Telemedicine", "Ontology-oriented architecture", "heterogeneous data", "health sector", "artificial intelligence methods", "personalized medicine", "telemedicine system", "diabetes\u2019 treatment"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "An Ontology-Oriented Architecture for Dealing With Heterogeneous Data Applied to Telemedicine Systems", "url": "", "volume": "6", "year": "2018", "abstract": "Current trends in medicine regarding issues of accessibility to and the quantity and quality of information and quality of service are very different compared to former decades. The current state requires new methods for addressing the challenge of dealing with enormous amounts of data present and growing on the Web and other heterogeneous data sources such as sensors and social networks and unstructured data, normally referred to as big data. Traditional approaches are not enough, at least on their own, although they were frequently used in hybrid architectures in the past. In this paper, we propose an architecture to process big data, including heterogeneous sources of information. We have defined an ontology-oriented architecture, where a core ontology has been used as a knowledge base and allows data integration of different heterogeneous sources. We have used natural language processing and artificial intelligence methods to process and mine data in the health sector to uncover the knowledge hidden in diverse data sources. Our approach has been applied to the field of personalized medicine (study, diagnosis, and treatment of diseases customized for each patient) and it has been used in a telemedicine system. A case study focused on diabetes is presented to prove the validity of the proposed model.", "pages": "41118-41138", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Peral, Jes\u00fas", "Ferr\u00e1ndez, Antonio", "Gil, David", "Mu\u00f1oz-Terol, Rafael", "Mora, Higinio"]}]["9826632", {"address": "", "articleno": "", "doi": "10.1108/IJCS-09-2018-0020", "issn": "2398-7294", "issue_date": "", "journal": "International Journal of Crowd Science", "keywords": ["Correlation", "Sociology", "Employment", "Decision making", "Big Data", "Data mining", "Statistics", "Decision support systems", "Data analytics", "anomaly data detection", "Data management systems"], "month": "October", "number": "2", "numpages": "", "publisher": "", "title": "Anomaly data management and big data analytics: an application on disability datasets", "url": "", "volume": "2", "year": "2018", "abstract": "Purpose \u2013 The disability datasets are the datasets that contain the information of disabled populations. By analyzing these datasets, professionals who work with disabled populations can have a better understanding of the inherent characteristics of the disabled populations, so that working plans and policies, which can effectively help the disabled populations, can be made accordingly. Design/methodology/approach \u2013 In this paper, the authors proposed a big data management and analytic approach for disability datasets. Findings \u2013 By using a set of data mining algorithms, the proposed approach can provide the following services. The data management scheme in the approach can improve the quality of disability data by estimating miss attribute values and detecting anomaly and low-quality data instances. The data mining scheme in the approach can explore useful patterns which reflect the correlation, association and interactional between the disability data attributes. Experiments based on real-world dataset are conducted at the end to prove the effectiveness of the approach. Originality/value \u2013 The proposed approach can enable data-driven decision-making for professionals who work with disabled populations.", "pages": "164-176", "note": "", "ISSN": "2398-7294", "publicationtype": "article", "author": ["Pan, Zhiwen", "Ji, Wen", "Chen, Yiqiang", "Dai, Lianjun", "Zhang, Jun"]}]["7882669", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2017.2682640", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Cloud computing", "Wireless communication", "Data analysis", "Big Data", "Wireless sensor networks", "Edge computing", "Distributed databases", "Big data", "data analytics", "internet of things (IoT)", "cloud computing", "edge computing", "fog computing"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "Live Data Analytics With Collaborative Edge and Cloud Processing in Wireless IoT Networks", "url": "", "volume": "5", "year": "2017", "abstract": "Recently, big data analytics has received important attention in a variety of application domains including business, finance, space science, healthcare, telecommunication and Internet of Things (IoT). Among these areas, IoT is considered as an important platform in bringing people, processes, data and things/objects together in order to enhance the quality of our everyday lives. However, the key challenges are how to effectively extract useful features from the massive amount of heterogeneous data generated by resource-constrained IoT devices in order to provide real-time information and feedback to the end-users, and how to utilize this data-aware intelligence in enhancing the performance of wireless IoT networks. Although there are parallel advances in cloud computing and edge computing for addressing some issues in data analytics, they have their own benefits and limitations. The convergence of these two computing paradigms, i.e., massive virtually shared pool of computing and storage resources from the cloud and real-time data processing by edge computing, could effectively enable live data analytics in wireless IoT networks. In this regard, we propose a novel framework for coordinated processing between edge and cloud computing/processing by integrating advantages from both the platforms. The proposed framework can exploit the network-wide knowledge and historical information available at the cloud center to guide edge computing units towards satisfying various performance requirements of heterogeneous wireless IoT networks. Starting with the main features, key enablers and the challenges of big data analytics, we provide various synergies and distinctions between cloud and edge processing. More importantly, we identify and describe the potential key enablers for the proposed edge-cloud collaborative framework, the associated key challenges and some interesting future research directions.", "pages": "4621-4635", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Sharma, Shree", "Wang, Xianbin"]}]["8419685", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2018.2859756", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Big Data", "Mobile handsets", "Data analysis", "Data models", "Anomaly detection", "Quality of service", "Predictive models", "Anomaly", "call data records", "data analytics"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "Call Detail Records Driven Anomaly Detection and Traffic Prediction in Mobile Cellular Networks", "url": "", "volume": "6", "year": "2018", "abstract": "Mobile networks possess information about the users as well as the network. Such information is useful for making the network end-to-end visible and intelligent. Big data analytics can efficiently analyze user and network information, unearth meaningful insights with the help of machine learning tools. Utilizing big data analytics and machine learning, this paper contributes in three ways. First, we utilize the call detail records data to detect anomalies in the network. For authentication and verification of anomalies, we use k-means clustering, an unsupervised machine learning algorithm. Through effective detection of anomalies, we can proceed to suitable design for resource distribution as well as fault detection and avoidance. Second, we prepare anomaly free data by removing anomalous activities and train a neural network model. By passing the anomaly and anomaly free data through this model, we observe the effect of anomalous activities in training of the model and also observe mean square error of the anomaly and anomaly free data. At last, we use an autoregressive integrated moving average model to predict future traffic for a user. Through simple visualization, we show that the anomaly free data better generalizes the learning models and performs better on prediction task.", "pages": "41728-41737", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Sultan, Kashif", "Ali, Hazrat", "Zhang, Zhongshan"]}]["8066282", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2017.2757841", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Smart cities", "Resilience", "Market research", "Data models", "Sensors", "Predictive models", "Adaptive algorithms", "geospatial analysis", "predictive models", "statistical learning"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "Dynamic Network Model for Smart City Data-Loss Resilience Case Study: City-to-City Network for Crime Analytics", "url": "", "volume": "5", "year": "2017", "abstract": "Today's cities generate tremendous amounts of data, thanks to a boom in affordable smart devices and sensors. The resulting big data creates opportunities to develop diverse sets of context-aware services and systems, ensuring smart city services are optimized to the dynamic city environment. Critical resources in these smart cities will be more rapidly deployed to regions in need, and those regions predicted to have an imminent or prospective need. For example, crime data analytics may be used to optimize the distribution of police, medical, and emergency services. However, as smart city services become dependent on data, they also become susceptible to disruptions in data streams, such as data loss due to signal quality reduction or due to power loss during data collection. This paper presents a dynamic network model for improving service resilience to data loss. The network model identifies statistically significant shared temporal trends across multivariate spatiotemporal data streams and utilizes these trends to improve data prediction performance in the case of data loss. Dynamics also allow the system to respond to changes in the data streams such as the loss or addition of new information flows. The network model is demonstrated by city-based crime rates reported in Montgomery County, MD, USA. A resilient network is developed utilizing shared temporal trends between cities to provide improved crime rate prediction and robustness to data loss, compared with the use of single city-based auto-regression. A maximum improvement in performance of 7.8 % for Silver Spring is found and an average improvement of 5.6 % among cities with high crime rates. The model also correctly identifies all the optimal network connections, according to prediction error minimization. City-to-city distance is designated as a predictor of shared temporal trends in crime and weather is shown to be a strong predictor of crime in Montgomery County.", "pages": "20524-20535", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Kotevska, Olivera", "Kusne, A.", "Samarov, Daniel", "Lbath, Ahmed", "Battou, Abdella"]}]["7933943", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2017.2707439", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Cloud computing", "Medical services", "Mobile communication", "Smart cities", "Servers", "Real-time systems", "Quality of service", "Smart health care", "smart city", "big data", "quality of service (QoS)", "virtual machine migration", "ant colony optimization"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "Mobile Cloud-Based Big Healthcare Data Processing in Smart Cities", "url": "", "volume": "5", "year": "2017", "abstract": "In recent years, the Smart City concept has become popular for its promise to improve the quality of life of urban citizens. The concept involves multiple disciplines, such as Smart health care, Smart transportation, and Smart community. Most services in Smart Cities, especially in the Smart healthcare domain, require the real-time sharing, processing, and analyzing of Big Healthcare Data for intelligent decision making. Therefore, a strong wireless and mobile communication infrastructure is necessary to connect and access Smart healthcare services, people, and sensors seamlessly, anywhere at any time. In this scenario, mobile cloud computing (MCC) can play a vital role by offloading Big Healthcare Data related tasks, such as sharing, processing, and analysis, from mobile applications to cloud resources, ensuring quality of service demands of end users. Such resource migration, which is also termed virtual machine (VM) migration, is effective in the Smart healthcare scenario in Smart Cities. In this paper, we propose an ant colony optimization-based joint VM migration model for a heterogeneous, MCC-based Smart Healthcare system in Smart City environment. In this model, the user\u2019s mobility and provisioned VM resources in the cloud address the VM migration problem. We also present a thorough performance evaluation to investigate the effectiveness of our proposed model compared with the state-of-the-art approaches.", "pages": "11887-11899", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Islam, MD.", "Razzaque, MD.", "Hassan, Mohammad", "Ismail, Walaa", "Song, Biao"]}]["9730861", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2022.3157823", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Data visualization", "Rendering (computer graphics)", "Clustering algorithms", "Prediction algorithms", "Predictive models", "Geology", "Data models", "3D visualization of massive data", "deep learning", "Hilbert R-tree", "deep clustering", "time-series forecasting", "view frustum culling"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "Fast 3D Visualization of Massive Geological Data Based on Clustering Index Fusion", "url": "", "volume": "10", "year": "2022", "abstract": "With the development of $3D$ visualization technology, the amount of geological data information is increasing, and the interactive display of big data faces severe challenges. Because traditional volume rendering methods cannot entirely load large-scale data into the memory owing to hardware limitations, a visualization method based on variational deep embedding clustering fusion Hilbert R-tree is proposed to solve slow display and stuttering issues when rendering massive geological data. By constructing an efficient data index structure, deep clustering algorithms and space-filling curves can be integrated into the data structure to improve the indexing efficiency. In addition, this method combines time forecasting, data scheduling, and loading modules to improve the accuracy and real-time data display rate, thereby improving the stability of $3D$ visualization of large-scale geological data. This method uses real geological data as the experimental dataset, comparing and analyzing the existing index structure and time-series prediction method. The experimental results indicate that when comparing the index of the variational deep embedded clustering-Hilbert R-tree ( $VDEC-HRT$ ) with that of the K-means Hilbert R-tree ( $KHRT$ ), the time required is reduced by 55.67%, the viewpoint prediction correctness of the proposed method is improved by 22.7% compared with Lagrange interpolation algorithm. And the overall rendering performance and quality of the system achieve the expected results. Ours experiments prove the feasibility and effectiveness of the proposed scheme in the visualization of large-scale geological data.", "pages": "28821-28831", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Zhang, Yu-Hang", "Wen, Chang", "Zhang, Min", "Xie, Kai", "He, Jian-Biao"]}]["8674542", {"address": "", "articleno": "", "doi": "10.1109/TBDATA.2019.2907624", "issn": "2332-7790", "issue_date": "", "journal": "IEEE Transactions on Big Data", "keywords": ["Clustering algorithms", "Partitioning algorithms", "Programming", "Data models", "Machine learning algorithms", "Big Data", "Computational modeling", "Density-based hierarchical clustering", "MapReduce", "big data"], "month": "March", "number": "1", "numpages": "", "publisher": "", "title": "Hierarchical Density-Based Clustering Using MapReduce", "url": "", "volume": "7", "year": "2021", "abstract": "Hierarchical density-based clustering is a powerful tool for exploratory data analysis, which can play an important role in the understanding and organization of datasets. However, its applicability to large datasets is limited because the computational complexity of hierarchical clustering methods has a quadratic lower bound in the number of objects to be clustered. MapReduce is a popular programming model to speed up data mining and machine learning algorithms operating on large, possibly distributed datasets. In the literature, there have been attempts to parallelize algorithms such as Single-Linkage, which in principle can also be extended to the broader scope of hierarchical density-based clustering, but hierarchical clustering algorithms are inherently difficult to parallelize with MapReduce. In this paper, we discuss why adapting previous approaches to parallelize Single-Linkage clustering using MapReduce leads to very inefficient solutions when one wants to compute density-based clustering hierarchies. Preliminarily, we discuss one such solution, which is based on an exact, yet very computationally demanding, random blocks parallelization scheme. To be able to efficiently apply hierarchical density-based clustering to large datasets using MapReduce, we then propose a different parallelization scheme that computes an approximate clustering hierarchy based on a much faster, recursive sampling approach. This approach is based on HDBSCAN*, the state-of-the-art hierarchical density-based clustering algorithm, combined with a data summarization technique called data bubbles. The proposed method is evaluated in terms of both runtime and quality of the approximation on a number of datasets, showing its effectiveness and scalability.", "pages": "102-114", "note": "", "ISSN": "2332-7790", "publicationtype": "article", "author": ["Santos, Joelson", "Syed, Talat", "Naldi, Murilo", "Campello, Ricardo", "Sander, Joerg"]}]["9471831", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2021.3094231", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Data models", "Communication cables", "Transfer learning", "Data integration", "Convolution", "Training", "Analytical models", "Quality of transmission cable", "transfer learning", "data fusion"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "Fault Judgment of Transmission Cable Based on Multi-Channel Data Fusion and Transfer Learning", "url": "", "volume": "9", "year": "2021", "abstract": "Non-intrusive transmission cable monitoring is the latest advanced measurement technology for smart grids. It only samples the voltage on a certain part of the transmission cable, and uses intelligent algorithms to identify the quality, which has obvious advantages of low construction and maintenance costs. This paper established a model based on multi-channel data fusion and transfer learning to classify the quality of transmission cable. First, we used the ANSYS Maxwell simulation platform to obtain ten kinds of specific fault data, which solved the time cost of manual labeling. Then, we performed multi-channel data fusion on the original data, which strengthened the expression of important features and was more conducive to the training of the model. Next, we used Depthwise Separable Convolution (DSC) to speed up the learning of the model, and improve the accuracy of the classification. Finally, we transferred the model trained with simulation data into the real scene, realized the transfer from multi classes to two classes, the effectiveness was proved in experiments. The accuracy of the model built in the article to classify the quality of the transmission cables is 98.1%.", "pages": "98161-98168", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Zhang, Fujie", "Yao, Degui", "Zhang, Xiaofei", "Hu, Zhouming", "Zhu, Wenjun", "Ju, Yun"]}]["9486501", {"address": "", "articleno": "", "doi": "10.1109/TGRS.2021.3091409", "issn": "1558-0644", "issue_date": "", "journal": "IEEE Transactions on Geoscience and Remote Sensing", "keywords": ["Calibration", "Hyperspectral imaging", "Radiometry", "Atmospheric measurements", "Atmospheric modeling", "Artificial intelligence", "Cameras", "Bidirectional reflectance distribution function (BRDF) correction", "high-throughput phenotyping", "image quality assessment", "shadow compensation", "soil removal"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "Data-Driven Artificial Intelligence for Calibration of Hyperspectral Big Data", "url": "", "volume": "60", "year": "2022", "abstract": "Near-earth hyperspectral big data present both huge opportunities and challenges for spurring developments in agriculture and high-throughput plant phenotyping and breeding. In this article, we present data-driven approaches to address the calibration challenges for utilizing near-earth hyperspectral data for agriculture. A data-driven, fully automated calibration workflow that includes a suite of robust algorithms for radiometric calibration, bidirectional reflectance distribution function (BRDF) correction and reflectance normalization, soil and shadow masking, and image quality assessments was developed. An empirical method that utilizes predetermined models between camera photon counts (digital numbers) and downwelling irradiance measurements for each spectral band was established to perform radiometric calibration. A kernel-driven semiempirical BRDF correction method based on the Ross Thick-Li Sparse (RTLS) model was used to normalize the data for both changes in solar elevation and sensor view angle differences attributed to pixel location within the field of view. Following rigorous radiometric and BRDF corrections, novel rule-based methods were developed to conduct automatic soil removal; and a newly proposed approach was used for image quality assessment; additionally, shadow masking and plot-level feature extraction were carried out. Our results show that the automated calibration, processing, storage, and analysis pipeline developed in this work can effectively handle massive amounts of hyperspectral data and address the urgent challenges related to the production of sustainable bioenergy and food crops, targeting methods to accelerate plant breeding for improving yield and biomass traits.", "pages": "1-20", "note": "", "ISSN": "1558-0644", "publicationtype": "article", "author": ["Sagan, Vasit", "Maimaitijiang, Maitiniyazi", "Paheding, Sidike", "Bhadra, Sourav", "Gosselin, Nichole", "Burnette, Max", "Demieville, Jeffrey", "Hartling, Sean", "LeBauer, David", "Newcomb, Maria", "Pauli, Duke", "Peterson, Kyle", "Shakoor, Nadia", "Stylianou, Abby", "Zender, Charles", "Mockler, Todd"]}]["9727160", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2022.3156584", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Resource management", "Big Data", "Processor scheduling", "Downlink", "Interference", "Delays", "Clustering algorithms", "Time scheduling", "resource allocation", "user selection", "cloud radio access network (CRAN)", "big data transmission"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "Globally Optimal Resource Allocation and Time Scheduling in Downlink Cognitive CRAN Favoring Big Data Requests", "url": "", "volume": "10", "year": "2022", "abstract": "This paper is concerned with a cognitive cloud radio access network (CRAN) with a special attention to efficient and reliable downlink transmission of big data for secondary users (SUs). Existing approaches either try to maximize the number of accepted SUs or the sum data rate of accepted SUs. The first approach unfairly favors users with small data requests, whereas the second approach allocates most resources to users with better channel conditions. In contrast, this paper develops a novel approach that favors big data requests while simultaneously maintaining a certain degree of fairness among SUs. To this end, we first introduce a novel objective function that allows us to jointly optimize deadline-aware time scheduling, spectrum allocation, SU selection, and remote radio head (RRH) allocation for SUs. Second, we demonstrate that finding the global optimum solution entails the enumeration of all colorful independent sets on a generalized interval graph, which is known to be NP-hard. Third, we propose a dynamic programming (DP) approach, which yields the global optimum solution at a reduced computational cost. Fourth, we analyze the complexity of the proposed DP approach and assess its performance against existing baseline algorithms. Simulation results reveal that our solution favors big data users while incurring only a small degradation in the fairness index. Our proposed solution is practical for small-to-medium size networks. Furthermore, it offers an optimum benchmark for any new sub-optimal low-complexity algorithm.", "pages": "27504-27521", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Bigdeli, Mohammad", "Farahmand, Shahrokh", "Abolhassani, Bahman", "Nguyen, Ha"]}]["8825796", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2019.2939614", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Data centers", "Monitoring", "Sensors", "Cloud computing", "Servers", "Edge computing", "Energy consumption", "Edge computing", "data centers", "monitoring", "energy efficiency", "intelligent operation"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "An Edge Computing Platform for Intelligent Operational Monitoring in Internet Data Centers", "url": "", "volume": "7", "year": "2019", "abstract": "The increasing demand for cloud-based services, such as big data analytics and online e-commerce, leads to rapid growth of large-scale internet data centers. In order to provide highly reliable, cost effective, and high quality cloud services, data centers are equipped with sensors to monitor the operational states of infrastructure hardware, such as servers, storage arrays, networking devices, and computer room air conditioning systems. However, such coarse grained monitoring cannot provide fine grained real time information for resource multiplexing and job scheduling. Moreover, the monitoring of node level power consumption plays an important role in the optimization of workload placement and energy efficiency in data centers. In this paper, we propose an edge computing platform for intelligent operational monitoring in data centers. The platform integrates wireless sensors and on-board built-in sensors to collect data during the operation and maintenance of data centers. Using logical functions, we divide the data center clusters into grids, and then deploy wireless sensors and edge servers in each grid. As such, data processing on edge servers can reduce the latency in data transmission to central clouds and thereby enhance the real time resource mapping decisions in data centers. In addition, the proposed platform also provides predictions of resource utilization, workload characteristics, and hardware health trends in data centers.", "pages": "133375-133387", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Jiang, Congfeng", "Qiu, Yeliang", "Gao, Honghao", "Fan, Tiantian", "Li, Kangkang", "Wan, Jian"]}]["9354632", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2021.3059483", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Harmonic analysis", "Lighting", "Predictive models", "LED lamps", "Prediction algorithms", "Power quality", "Neurons", "LED lamps", "THD prediction", "ensemble learning", "mind evolution algorithm (MEA)", "generalized regression neural network (GRNN)"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "Harmonic Characteristics Data-Driven THD Prediction Method for LEDs Using MEA-GRNN and Improved-AdaBoost Algorithm", "url": "", "volume": "9", "year": "2021", "abstract": "Light-emitting Diode (LED) lamps have been widely used due to versatility and energy efficiency. However, LEDs are nonlinear loads, the massive usage will inject harmonics into the lighting system, which has influenced the power quality. Total Harmonic Distortion (THD) is an important parameter to evaluate the power quality, but the prediction of THD for LEDs is a challenging task. This paper addresses this issue by designing harmonic characteristics detection experiment and using artificial intelligence algorithm. Firstly, LED lamps with different driving circuits were tested, the relevant data of each harmonic were sampled and analyzed. Then, a THD prediction method based on an improved AdaBoost algorithm is proposed. In this method, a Generalized Regression Neural Network (GRNN) model is established, and its parameters are optimized by Mind Evolution Algorithm (MEA) to improve the search ability of GRNN. On this basis, the AdaBoost algorithm is utilized to integrate multiple MEA-GRNN individuals to form a strong predictor, which improves the generalization ability of the model. To avoid the integration failure caused by improper selection of threshold value, a sigmoid adaptive factor is added to improve the accuracy of AdaBoost algorithm. Finally, the Ada-MEA-GRNN model is trained and simulated with the LED harmonic data collected by the experiment. The simulation results show that the prediction accuracy of the proposed method is better than BP and GRNN, which can reach 95.48%. Meanwhile, even if the input dimension is reduced, the error is still small.", "pages": "31297-31308", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Yang, Jingjian", "Ma, Hongyan", "Dou, Jiaming", "Guo, Rong"]}]["8811507", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2019.2937107", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Artificial intelligence", "Software", "Software testing", "Data models", "Quality assurance", "Big Data", "AI software quality validation", "AI testing", "testing AI software"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "Testing and Quality Validation for AI Software\u2013Perspectives, Issues, and Practices", "url": "", "volume": "7", "year": "2019", "abstract": "With the fast growth of artificial intelligence and big data computing technologies, more and more software service systems have been developed using diverse machine learning models and technologies to make business and intelligent decisions based on their multimedia input to achieve intelligent features, such as image recognition, recommendation, decision making, prediction, etc. Nevertheless, there are increasing quality problems resulting in erroneous testing costs in enterprises and businesses. Existing work seldom discusses how to perform testing and quality validation for AI software. This paper focuses on quality validation for AI software function features. The paper provides our understanding of AI software testing for new features and requirements. In addition, current AI software testing categories are presented and different testing approaches are discussed. Moreover, test quality assessment and criteria analysis are illustrated. Furthermore, a practical study on quality validation for an image recognition system is performed through a metamorphic testing method. Study results show the feasibility and effectiveness of the approach.", "pages": "120164-120175", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Tao, Chuanqi", "Gao, Jerry", "Wang, Tiexin"]}]["8693792", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2019.2911892", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": [""], "month": "", "number": "", "numpages": "", "publisher": "", "title": "Notice of Violation of IEEE Publication Principles: Single-Image Super-Resolution Algorithm Based on Structural Self-Similarity and Deformation Block Features", "url": "", "volume": "7", "year": "2019", "abstract": "To solve the problem of insufficient sample resources and poor noise immunity in single-image super-resolution (SR) restoration procedure, the paper has proposed the single-image SR algorithm based on structural self-similarity and deformation block features (SSDBF). First, the proposed method constructs a scale model, expands the search space as much as possible, and overcomes the shortcomings caused by the lack of a single-image SR training sample; Second, the limited internal dictionary size is increased by the geometric deformation of the sample block; Finally, in order to improve the anti-noise performance of the reconstructed picture, a group sparse learning dictionary is used to reconstruct the pending image. The experimental results show that, compared with state-of-the-art algorithms such as bicubic interpolation (BI), sparse coding (SC), deep recursive convolutional network (DRCN), multi-scale deep SR network (MDSR), super-resolution convolutional neural network (SRCNN) and second-order directional total generalized variation (DTGV). The SR images with more subjective visual effects and higher objective evaluation can be obtained through the proposed method. Compared with existing algorithms, the structural network converges more rapidly, the image edge and texture reconstruction effects are obviously improved, and the image quality evaluation, such as peak signal-noise ratio (PSNR), root mean square error (RMSE), and structural similarity (SSIM), are also superior and popular in image evaluation.", "pages": "58791-58801", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Chen, Yuantao", "Wang, Jin", "Chen, Xi", "Zhu, Mingwei", "Yang, Kai", "Wang, Zhi", "Xia, Runlong"]}]["9644473", {"address": "", "articleno": "", "doi": "10.1109/TSG.2021.3134018", "issn": "1949-3061", "issue_date": "", "journal": "IEEE Transactions on Smart Grid", "keywords": ["Support vector machines", "Data models", "Task analysis", "Training", "Radio frequency", "Computational modeling", "Smart grids", "Electricity theft detection", "big data", "preprocessing", "data classification", "smart grid"], "month": "March", "number": "2", "numpages": "", "publisher": "", "title": "A Stacked Machine and Deep Learning-Based Approach for Analysing Electricity Theft in Smart Grids", "url": "", "volume": "13", "year": "2022", "abstract": "The role of electricity theft detection (ETD) is critical to maintain cost-efficiency in smart grids. However, existing methods for theft detection can struggle to handle large electricity consumption datasets because of missing values, data variance and nonlinear data relationship problems, and there is a lack of integrated infrastructure for coordinating electricity load data analysis procedures. To help address these problems, a simple yet effective ETD model is developed. Three modules are combined into the proposed model. The first module deploys a combination of data imputation, outlier handling, normalization and class balancing algorithms, to enhance the time series characteristics and generate better quality data for improved training and learning by the classifiers. Three different machine learning (ML) methods, which are uncorrelated and skillful on the problem in different ways, are employed as the base learning model. Finally, a recently developed deep learning approach, namely a temporal convolutional network (TCN), is used to ensemble the outputs of the ML algorithms for improved classification accuracy. Experimental results confirm that the proposed framework yields a highly-accurate, robust classification performance, in comparison to other well-established machine and deep learning models and thus can be a practical tool for electricity theft detection in industrial applications.", "pages": "1633-1644", "note": "", "ISSN": "1949-3061", "publicationtype": "article", "author": ["Khan, Inam", "Javeid, Nadeem", "Taylor, C.", "Gamage, Kelum", "Ma, Xiandong"]}]["9174979", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2020.3019095", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Smart grids", "Time series analysis", "Task analysis", "Feature extraction", "Data models", "Change detection algorithms", "smart grids", "one-class learning", "neural networks", "embedding"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "ECHAD: Embedding-Based Change Detection From Multivariate Time Series in Smart Grids", "url": "", "volume": "8", "year": "2020", "abstract": "Smart grids are power grids where clients may actively participate in energy production, storage and distribution. Smart grid management raises several challenges, including the possible changes and evolutions in terms of energy consumption and production, that must be taken into account in order to properly regulate the energy distribution. In this context, machine learning methods can be fruitfully adopted to support the analysis and to predict the behavior of smart grids, by exploiting the large amount of streaming data generated by sensor networks. In this article, we propose a novel change detection method, called ECHAD (Embedding-based CHAnge Detection), that leverages embedding techniques, one-class learning, and a dynamic detection approach that incrementally updates the learned model to reflect the new data distribution. Our experiments show that ECHAD achieves optimal performances on synthetic data representing challenging scenarios. Moreover, a qualitative analysis of the results obtained on real data of a real power grid reveals the quality of the change detection of ECHAD. Specifically, a comparison with state-of-the-art approaches shows the ability of ECHAD in identifying additional relevant changes, not detected by competitors, avoiding false positive detections.", "pages": "156053-156066", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Ceci, Michelangelo", "Corizzo, Roberto", "Japkowicz, Nathalie", "Mignone, Paolo", "Pio, Gianvito"]}]["8944072", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2019.2962771", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Entropy", "Data compression", "Medical services", "Data communication", "Compression algorithms", "Software algorithms", "Batteries", "Wireless sensor networks", "data compression", "entropy", "quality of service", "energy saving", "quality prediction", "differential information entropy"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "An Entropy Evaluation Algorithm to Improve Transmission Efficiency of Compressed Data in Pervasive Healthcare Mobile Sensor Networks", "url": "", "volume": "8", "year": "2020", "abstract": "Data transmission is the most critical operation for mobile sensors networks in term of energy waste. Particularly in pervasive healthcare sensors network it is paramount to preserve the quality of service also by means of energy saving policies. Communication and data transmission are among the most critical operation for such devises in term of energy waste. In this paper we present a novel approach to increase battery life-span by means of shorter transmission due to data compression. On the other hand, since this latter operation has a non-neglectable energy cost, we developed a compression efficiency estimator based on the evaluation of the absolute and relative entropy. Such algorithm provides us with a fast mean for the evaluation of data compressibility. Since mobile wireless sensor networks are prone to battery discharge-related problems, such an evaluation can be used to improve the electrical efficiency of data communication. In facts the developed technique, due to its independence from the string or file length, is extremely robust both for small and big data files, as well as to evaluate whether or not to compress data before transmission. Since the proposed solution provides a quantitative analysis of the source's entropy and the related statistics, it has been implemented as a preprocessing step before transmission. A dynamic threshold defines whether or not to invoke a compression subroutine. Such a subroutine should be expected to greatly reduce the transmission length. On the other hand a data compression algorithm should be used only when the energy gain of the reduced transmission time is presumably greater than the energy used to run the compression software. In this paper we developed an automatic evaluation system in order to optimize the data transmission in mobile sensor networks, by compressing data only when this action is presumed to be energetically efficient. We tested the proposed algorithm by using the Canterbury Corpus as well as standard pictorial data as benchmark test. The implemented system has been proven to be time-inexpensive with respect to a compression algorithm. Finally the computational complexity of the proposed approach is virtually neglectable with respect to the compression and transmission routines themselves.", "pages": "4668-4678", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Capizzi, Giacomo", "Coco, Salvatore", "Lo, Grazia", "Napoli, Christian", "Ho\u0142ubowski, Waldemar"]}]["9467271", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2021.3093430", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Air quality", "Atmospheric modeling", "Predictive models", "Data models", "Meteorology", "Time series analysis", "Forecasting", "Air quality prediction", "integrated dual model", "LSTM model with attention mechanism", "Seq2Seq technology", "XGBoosting tree"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "Air Quality Prediction Based on Integrated Dual LSTM Model", "url": "", "volume": "9", "year": "2021", "abstract": "Air quality prediction is an important reference for meteorological forecast and air controlling, but over fitting often occurs in prediction algorithms based on a single model. Aiming at the complexity of air quality prediction, a prediction method based on integrated dual LSTM (Long Short-Term Memory) model was proposed in this paper. Firstly, the Seq2Seq (Sequence to Sequence) technology is used to establish a single-factor prediction model which can obtain the predicted value of each component in air quality data, independently. Each component of air quality is regarded as time series data in the forecasting process. Then, the LSTM model with attention mechanism is used as the multi-factor prediction model. The influencing factors of air quality, like the data of neighboring stations and weather data, are considered in the model. Finally, XGBoosting (eXtreme Gradient Boosting) tree is used to integrate two models. The final prediction results can be obtained by accumulating the predicted values of the optimal subtree nodes. Through evaluation and analysis using five evaluation methods, the proposed method has better performance in terms of error and model expression power. Compared with other various models, the precision of prediction data has been greatly improved in our model.", "pages": "93285-93297", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Chen, Hongqian", "Guan, Mengxi", "Li, Hui"]}]["9082667", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2020.2991462", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Roads", "Data models", "Predictive models", "Convolutional neural networks", "Urban areas", "Convolutional neural network", "long short-term memory", "partially convolutional neural network", "spatiotemporal feature", "traffic congestion forecasting", "transport network"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "City-Wide Traffic Congestion Prediction Based on CNN, LSTM and Transpose CNN", "url": "", "volume": "8", "year": "2020", "abstract": "Traffic congestion is a significant problem faced by large and growing cities that hurt the economy, commuters, and the environment. Forecasting the congestion level of a road network timely can prevent its formation and increase the efficiency and capacity of the road network. However, despite its importance, traffic congestion prediction is not a hot topic among the researcher and traffic engineers. It is due to the lack of high-quality city-wide traffic data and computationally efficient algorithms for traffic prediction. In this paper, we propose (i) an efficient and inexpensive city-wide data acquisition scheme by taking a snapshot of traffic congestion map from an open-source online web service; Seoul Transportation Operation and Information Service (TOPIS), and (ii) a hybrid neural network architecture formed by combing Convolutional Neural Network, Long Short-Term Memory, and Transpose Convolutional Neural Network to extract the spatial and temporal information from the input image to predict the network-wide congestion level. Our experiment shows that the proposed model can efficiently and effectively learn both spatial and temporal relationships for traffic congestion prediction. Our model outperforms two other deep neural networks (Auto-encoder and ConvLSTM) in terms of computational efficiency and prediction performance.", "pages": "81606-81620", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Ranjan, Navin", "Bhandari, Sovit", "Zhao, Hong", "Kim, Hoon", "Khan, Pervez"]}]["9110861", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2020.3000767", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Air pollution", "Urban areas", "Time series analysis", "Compressed sensing", "Correlation", "Spatiotemporal phenomena", "Granger causality analysis", "maximum correntropy criterion", "data compression", "air pollutant"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "Causal Identification Based on Compressive Sensing of Air Pollutants Using Urban Big Data", "url": "", "volume": "8", "year": "2020", "abstract": "This study addresses the causal identification of air pollutants from surrounding cities affecting Beijing's air quality. A novel compressive sensing causality analysis (CS-Causality) method, which combines Granger causality analysis (GCA) and maximum correntropy criterion (MCC), is presented for efficient identification of the air pollutant causality between Beijing and surrounding cities. Firstly, taking the spatiotemporal correlation into consideration, the original data is mapped into low-dimensional space. Valid information is then obtained based on compressive sensing (CS), which can greatly reduce the dimensions of the data, thus decreasing the amount of data analysis required. Secondly, to analyze the causal relations, GCA, represented by the prediction from one time series to another, is extended to rule out \u201cNon-Granger\u201d causes of air pollutants in Beijing originating from its surrounding cities. Thirdly, the greatest impact on Beijing's air quality is confirmed based on MCC. Finally, the accuracy of these results is verified using the transfer entropy.", "pages": "109207-109216", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Li, Mingwei", "Li, Jinpeng", "Wan, Shuangning", "Chen, Hao", "Liu, Chao"]}]["8478185", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2018.2872805", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Roads", "Data analysis", "Data privacy", "Standards", "Big Data", "Data integration", "Intelligent transportation system", "multi-source traffic data", "data analysis", "data fusion", "data privacy"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "Research on Intelligent Analysis and Depth Fusion of Multi-Source Traffic Data", "url": "", "volume": "6", "year": "2018", "abstract": "Intelligence transportation system (ITS) and vehicular networks have attracted the research community in the recent years which generate the \u201cbig data\u201d in traffic. However, the collection and application of the big traffic data is limited by the privacy of people who generate data. Besides, data-driven-based ITS only needs information that could reflect one or more types of vehicles at specific intersections, sections, and road networks, rather than that of each individual vehicle. Overall, intelligent analysis and data fusion of multi-source traffic data play an important role to reduce the phenomenon of privacy disclosure and ensure the quality of data. As a result, a complete method of multi-source traffic data analyzing and processing is proposed in this paper, including the data analysis method based on the spatio-temporal regression model and the data fusion method using evidence theory based on the confidence tensor. Finally, the practical data is used to conform the ways proposed before. And not only do the results show that the implicit privacy information has been removed but also present a higher accuracy of the proceed data.", "pages": "59329-59335", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Shen, Guojiang", "Han, Xiao", "Zhou, Junjie", "Ruan, Zhongyuan", "Pan, Qihong"]}]["9079499", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2020.2990765", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Libraries", "Data mining", "Data models", "Predictive models", "Analytical models", "Predictive analytics", "Machine learning", "Data mining", "academic libraries", "big data", "machine learning", "predictive analysis"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "Toward Effective Planning and Management Using Predictive Analytics Based on Rental Book Data of Academic Libraries", "url": "", "volume": "8", "year": "2020", "abstract": "Large scale data and predictive analytics are the most challenging tasks in the field of academic data mining. Academic libraries are a great source of information and knowledge to provide a wide range of services to meet end-user requirements. Due to the rapid changes in the educational environment and availability of huge library rental book data, it is required to utilize data mining and machine learning techniques in the context of the academic library to extract and analyze underlying knowledge from rental book data, which is important to facilitate library administration to drive better future decisions to improve and manage library resources effectively. These are the following resources, such as managing future demands of the library books, selection and arrangement of the books, operational efficiency, and also improve the quality of interaction between the library and end-users, etc. This work uses and analyzes a real dataset collected from the library of Jeju National University, the Republic of Korea. The dataset contains 2,211,413 rental book records including 173671 unique book records, 57203 unique number of the rental user, and 78 data parameters. In this paper, we propose a novel model to analyze and predict library rental book data to facilitates library administration in order to plan and manage library resources effectively and provide better services to end-users. The proposed model consists of two different modules; library data analysis and prediction modules. Firstly, we use data mining techniques to analyze and extract useful underlying patterns from library rental book data, which can lead to plan and manage library resources effectively. Secondly, a novel prediction model is proposed based on Deep Neural Network (DNN), Support Vector Regressor (SVR), and Random Forest (RF) to predict future usage of the academic libraries rental books. The performance results of the implemented regression models are evaluated in terms of MAE, MSE, and RMSE. In this paper, it is found that the DNN model performs significantly better than SVR and RF. The experimentation results show that the proposed model improves the future usage of library books to facilitate library administration to plan and manage library resources effectively. Based on the proposed model results, the academic library administration can easily plan and manage resources effectively to provide quality services to end-users.", "pages": "81978-81996", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Iqbal, Naeem", "Jamil, Faisal", "Ahmad, Shabir", "Kim, Dohyeun"]}]["9508412", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2021.3103052", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Harmonic analysis", "LED lamps", "Power system harmonics", "Lighting", "Integrated circuit modeling", "Buildings", "Entropy", "Power quality", "evaluation approach", "light-emitting diode lamp", "sequential analysis method", "entropy weighting method"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "An Improved Power Quality Evaluation for LED Lamp Based on G1-Entropy Method", "url": "", "volume": "9", "year": "2021", "abstract": "Nowadays, light emitting diode (LED) lamps have been widely utilized for lighting system due to its low-energy consumption. The harmonic emission standard is ignored by most of the manufacturers, high harmonic current will increase harmonic injection and cause fire risk. Existing research focuses on investigating harmonic emissions from several specific LED drivers, but a systematic evaluation approach is not given. The contribution of this paper proposed a LED harmonic evaluation in the management view, which can evaluate the harmonics of the LED lamps, accelerate the elimination of inferior LED lamps, and improve the power quality of distribution network. The evaluation approach combines G1 method and entropy method, which can make the weighting more scientific and rational. An evaluation model is established by collecting data, then the G1-entropy method is used to calculate the weights of harmonic characteristics in this model. Finally, we analyze and discuss the results, a specific evaluation approach is proposed, which can thoroughly and accurately represent the harmonic characteristics of LED lamps.", "pages": "111171-111180", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Dou, Jiaming", "Ma, Hongyan", "Yang, Jingjian", "Zhang, Yingda", "Guo, Rong"]}]["9104998", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2020.2999079", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Standardization", "Semantics", "Feature extraction", "Task analysis", "Data mining", "Data processing", "Data models", "Nomenclature standardization", "radiotherapy", "deep learning", "3D classification", "voting"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "Mining Domain Knowledge: Improved Framework Towards Automatically Standardizing Anatomical Structure Nomenclature in Radiotherapy", "url": "", "volume": "8", "year": "2020", "abstract": "The automatic standardization of nomenclature for anatomical structures in radiotherapy (RT) clinical data is a critical prerequisite for data curation and data-driven research in the era of big data and artificial intelligence, but it is currently an unmet need. Existing methods either cannot handle cross-institutional datasets or suffer from heavy imbalance and poor-quality delineation in clinical RT datasets. To solve these problems, we propose an automated structure nomenclature standardization framework, 3D Non-local Network with Voting (3DNNV). This framework consists of an improved data processing strategy, namely, adaptive sampling and adaptive cropping (ASAC) with voting, and an optimized feature extraction module. The framework simulates clinicians' domain knowledge and recognition mechanisms to identify small-volume organs at risk (OARs) with heavily imbalanced data better than other methods. We used partial data from an open-source head-and-neck cancer dataset to train the model, then tested the model on three cross-institutional datasets to demonstrate its generalizability. 3DNNV outperformed the baseline model, achieving higher average true positive rates (TPR) over all categories on the three test datasets (+8.27%, +2.39%, and +5.53%, respectively). More importantly, the 3DNNV outperformed the baseline on the test dataset, 28.63% to 91.17%, in terms of F1 score for a small-volume OAR with only 9 training samples. The results show that 3DNNV can be applied to identify OARs, even error-prone ones. Furthermore, we discussed the limitations and applicability of the framework in practical scenarios. The framework we developed can assist in standardizing structure nomenclature to facilitate data-driven clinical research in cancer radiotherapy.", "pages": "105286-105300", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Yang, Qiming", "Chao, Hongyang", "Nguyen, Dan", "Jiang, Steve"]}]["8423621", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2018.2861421", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Big Data", "Internet of Things", "Engines", "Authentication", "Protocols", "Real-time systems", "Demand side management", "home area network", "industrial Internet of Things", "security", "smart societies", "trust"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "A Secured Data Management Scheme for Smart Societies in Industrial Internet of Things Environment", "url": "", "volume": "6", "year": "2018", "abstract": "Smart societies have an increasing demand for quality-oriented services and infrastructure in an industrial Internet of Things (IIoT) paradigm. Smart urbanization faces numerous challenges. Among them, secured energy demand-side management (DSM) is of particular concern. The IIoT renders the industrial systems to malware, cyberattacks, and other security risks. The IIoT with the amalgamation of big data analytics can provide efficient solutions to such challenges. This paper proposes a secured and trusted multilayered DSM engine for a smart social society using IIoT-based big data analytics. The major objective is to provide a generic secured solution for smart societies in IIoT environment. The proposed engine uses a centralized approach to achieve optimum DSM over a home area network. To enhance the security of this engine, a payload-based authentication scheme is utilized that relies on a lightweight handshake mechanism. Our proposed method utilizes the lightweight features of the constrained application protocol to facilitate the clients in monitoring various resources residing over the server in an energy-efficient manner. In addition, data streams are processed using big data analytics with MapReduce parallel processing. The proposed authentication approach is evaluated using NetDuino Plus 2 boards that yield a lower connection overhead, memory consumption, response time, and a robust defense against various malicious attacks. On the other hand, our data processing approach is tested on reliable datasets using Apache Hadoop with Apache Spark to verify the proposed DMS engine. The test results reveal that the proposed architecture offers valuable insights into the smart social societies in the context of IIoT.", "pages": "43088-43099", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Babar, Muhammad", "Khan, Fazlullah", "Iqbal, Waseem", "Yahya, Abid", "Arif, Fahim", "Tan, Zhiyuan", "Chuma, Joseph"]}]["9739736", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2022.3161556", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Temperature sensors", "Temperature distribution", "Temperature measurement", "Temperature control", "Predictive models", "Data models", "Cooling", "Mass concrete temperature", "big data processing technology", "CART prediction model"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "Research on CART Model of Mass Concrete Temperature Prediction Based on Big Data Processing Technology", "url": "", "volume": "10", "year": "2022", "abstract": "Due to the influence of temperature changes or temperature gradients in the construction process of mass concrete, temperature cracks will occur in the concrete. In order to achieve a reasonable prediction of the temperature change of the mass concrete during the construction process and accurately obtain the temperature change trend, this paper attempts to construct a CART prediction model based on the big data processing technology based on the characteristics of the temperature change of the mass concrete. This paper introduces in detail how to use data processing methods such as outlier identification, missing value filling and random error elimination to improve data quality, as well as the method for constructing the CART prediction model, and combines engineering examples to demonstrate the feasibility of the model method. The results show that the model and method can better predict the temperature change of mass concrete. It has high prediction accuracy and can provide necessary guidance for practical engineering.", "pages": "32845-32854", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Baoyu, Li", "Guoxing, Li", "Guiyu, Wang", "Guofeng, Zhang", "Man, Yang"]}]["9364983", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2021.3062735", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Sports", "Privacy", "Data privacy", "Indexes", "Collaborative filtering", "Big Data", "Encoding", "Similar player clustering", "sport exercise score records", "SimHash", "privacy", "big data", "cloud platform"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "A Novel Similar Player Clustering Method With Privacy Preservation for Sport Performance Evaluation in Cloud", "url": "", "volume": "9", "year": "2021", "abstract": "With the ever-increasing popularity of sports and health ideas, people are paying more attentions to gaining high-quality healthy life through various taking various sport items or exercises. Through observing and analyzing the past sport exercise score records, we can cluster the players into different categories, each of which share the same or similar sport preferences or performances. However, the sport exercise score records are often massive and often stored in different cloud platforms, which raise a big difficulty for time-efficient player clustering. Furthermore, the sport exercise score records are a kind of privacy for most players; therefore, it is often not rational or legal to release these sensitive data to the public for similar player clustering purpose. Considering the above two issues, we use SimHash, a kind of privacy-aware approximate neighbor search technique, for similar player clustering by analyzing the sport exercise score records distributed across different cloud platforms. Thus, we can realize privacy-aware similar player clustering through SimHash. At last, we provide a set of experiments to validate the advantages of our proposed privacy-aware similar player clustering algorithm. Reported experimental results show the effectiveness of our proposal in remedying the big data volume and privacy concerns in player clustering based on sport exercise score records.", "pages": "37255-37261", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Ma, Rui", "Li, Jianqiang", "Xing, Baohui", "Zhao, Yuanyuan", "Liu, Yuwen", "Yan, Chao", "Yin, Hang"]}]["9627153", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2021.3130758", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Data mining", "Analytical models", "Data models", "Heuristic algorithms", "Clustering algorithms", "Business", "Task analysis", "Online incremental mining", "trusted behavior interval", "event stream", "clustering", "process mining"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "Online Incremental Mining Based on Trusted Behavior Interval", "url": "", "volume": "9", "year": "2021", "abstract": "Incremental mining improves the quality of process mining by analyzing the differences between event logs and a reference model to obtain valuable information to update the reference model. Existing incremental mining methods focus on offline logs by setting thresholds for analysis, which limits process mining efforts by the domain knowledge, log completeness, and business completion time. Aiming at these problems, a real-time incremental mining algorithm based on the trusted behavior interval is proposed to analyze online event streams for updating the reference model. First, a clustering technique to analyze an existing reference model selects the core structure of the model and calculates the trusted behavior interval. Then, the behavioral and structural relationships between the online event streams and the reference model are analyzed to obtain a valid candidate set. Based on this set, an incremental update algorithm is proposed to optimize the model structure to achieve an online dynamic update of the reference model. The proposed algorithm is implemented in PM4PY and Scikit-learn frameworks; a reasonable number of clusters is determined using the elbow method and validated with artificial and real data. Experimental results show that the algorithm improves the efficiency of incremental mining and enhances the quality of the model with both complete and incomplete data.", "pages": "158562-158573", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Fang, Na", "Fang, Xianwen", "Lu, Ke", "Asare, Esther"]}]["8880549", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2019.2949136", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Wireless sensor networks", "Sensor phenomena and characterization", "Big Data", "Energy consumption", "Quality of service", "Maintenance engineering", "Large-scale heterogeneous wireless sensor networks", "big data collection", "confident information coverage", "repairing confident information coverage holes"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "Repairing Confident Information Coverage Holes for Big Data Collection in Large-Scale Heterogeneous Wireless Sensor Networks", "url": "", "volume": "7", "year": "2019", "abstract": "The quality of service (QoS) and lifetime of wireless sensor networks (WSNs) are severely degraded by coverage holes generated by random deployment or battery exhaustion of sensors. This work firstly introduces a novel confident information coverage CIC) model to dramatically reduce the density of sensor nodes and accurately detect confident information coverage holes (CICHs). Then, the problem of repairing confident information coverage holes (RCICHs) for big data collection in a large-scale heterogeneous WSN (LS-HWSN) which widely spreads over a geographic area with thousands of stationary sensor nodes and mobile sensor nodes is formulated, called as RCICH problem, which is to effectively repair CICHs considering that the transmitted data velocity of sensor nodes is different. Furthermore, we prove it to be NP-completeness. The target of the problem is to find a subset of mobile sensor nodes from all mobile sensor nodes while minimizing the amount of lost throughputs LTs) of all dispatched mobile sensor nodes or maximizing the amount of repairing transmission times (RTTs) for all dispatched mobile sensor nodes, with different objectives. Finally, based on the CIC model and the data-centric perspective, two heuristic schemes including a centralized dispatch scheme and a distributed dispatch scheme are proposed to effectively solve the RCICH problem. Simulation results show that the proposed schemes effectively repair CICHs while increasing the QoS and lifetime of the LS-HWSN with the topology control of a fan-shaped clustering (FSC) protocol.", "pages": "155347-155360", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Feng, Jie", "Chen, Hongbin"]}]["9006895", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2020.2975529", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Accidents", "Smart cities", "Big Data", "Mathematical model", "Meteorology", "Roads", "Automobiles", "Big data", "traffic accident data", "intelligent transportation systems", "smart city technology", "stochastic process", "time series", "temporal impulse"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "Temporal Impulse of Traffic Accidents in South Korea", "url": "", "volume": "8", "year": "2020", "abstract": "With the emergence of urban computing technology, the development of smart cities has gained much attention as a means to improve citizens' quality of life. As traffic accidents constitute a major problem that affects the quality of life, an effective solution to address this problem can significantly increase the level of intelligence of smart cities. This paper presents the development of a mathematical model for accurate analysis of big data to promote the effectiveness of policy decisions, thereby largely advancing the intelligent transportation systems (ITS) of smart cities. Temporal impulse was designed as a novel and measurable quantity to analyze traffic accidents by identifying the hidden patterns, such as varying causes and diverging impacts of traffic accidents. Based on the big data produced by the South Korean National Police Agency, we analyzed traffic accidents over three years by applying the temporal impulse. The research results suggested that the temporal impulse not only helped in identifying the varying influence of weather and driver conditions but also facilitated the establishment of sophisticated policies in the implementation of smart cities with the use of urban computing technology. As presented in the section VII, our simulation outputs indicated that our temporal model was predictive within the parameter space comprising driver's dynamic behaviors, day of the week, and environmental factors including weather, road surface condition, and road type.", "pages": "38380-38390", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Shin, Hyunkyung", "Lee, Jaeho"]}]["9164974", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2020.3015939", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Radio frequency", "Machine learning", "Wireless communication", "Signal to noise ratio", "Robustness", "Data models", "Training", "Radio frequency learning", "signal-to-noise ratio", "training and testing strategy", "spectrum data", "convolutional neural network"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "Robust Deep Radio Frequency Spectrum Learning for Future Wireless Communications Systems", "url": "", "volume": "8", "year": "2020", "abstract": "Intelligent capabilities are of utmost importance in future wireless communication systems. For optimum resource utilization, wireless communication systems require knowledge of the prevalent situation in a frequency band through learning. To learn appropriately, it is imperative for practitioners to select the right parameters for building robust data-driven learning models as well as use the appropriate algorithms and performance evaluation methods. In this paper, we evaluate the performance of deep learning models against the performance of other machine learning methods for wireless communication systems. We explore the different wireless communication scenarios in which deep learning can be used given Radio Frequency (RF) data, and evaluate its performance in various scenarios. Furthermore, we express it as a distribution alignment problem in which deep learning models do not perform well when learning from RF data of a particular distribution and evaluating on RF data from a different distribution. We also discuss our results in the light of how signal quality affects deep learning model leveraging on the knowledge from computer vision domain. The effect of Signal-to-Noise Ratio (SNR) selection for training on the model performance as it relates to practical implementation of deep learning in communications systems is also discussed. From our analysis, we conclude that the design and use of RF spectrum learning must be tailored to each specific scenario being considered in practice.", "pages": "148528-148540", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Adesina, Damilola", "Bassey, Joshua", "Qian, Lijun"]}]["9248996", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2020.3036285", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Analytical models", "Sensitivity analysis", "Encoding", "Reliability", "Task analysis", "Public transportation", "Open source software", "Big data", "complex systems", "passengers\u2019 perspective", "simulation model", "sensitivity analysis"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "Simulation-Based Sensitivity Analysis for Evaluating Factors Affecting Bus Service Reliability: A Big and Smart Data Implementation", "url": "", "volume": "8", "year": "2020", "abstract": "Service quality is a significant concern for both providers and users of public transportation. It is crucial for transit agencies to clearly recognize the causes of unreliability before adapting any improvement strategy. However, evaluation of main causes of bus service unreliability has not been investigated well. Existing studies have three main limitations in context of recognizing causes of service unreliability. First, public transport networks and traffic condition are highly complex systems and most of the existing models are not capable to accurately determine the relationship between service irregularity and impact factors. Second, definition of \u201cBig data\u201d has been neglected and most of the studies only focused on one source of large scale data set to determine the causes of unreliability. Third, bus service unreliability can impact the users' perception toward the public transport, significantly. It has been recommended by number of studies that bus service reliability should be evaluated from both service providers' and users' perspective. However, the impact of service unreliability from passengers' perception is not well investigated, yet. Consequently, we proposed a novel simulation-based sensitivity analysis to evaluating main causes of bus service unreliability using a combination of three different sources of big data. Moreover, for the first time we developed a simulation model in R studio which is an open source and powerful coding environment. According to the results, the level of reliability in Route U32 showed the highest sensitivity to headway variations. Waiting time can be decreased by 61% if only bus operators can reduce the headway variation by 25% of the actual observed data. Big gap and bus bunching could be almost disappeared by decreasing headway variations. Moreover, the terminal departure policy could significantly improve the passenger waiting time. Waiting time can be decreased by 36% when almost all the buses depart the terminal on-time.", "pages": "201937-201955", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Moosavi, Seyed", "Yuen, Choon", "Yap, Soon", "Onn, Chiu"]}]["9503402", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2021.3101871", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Servers", "Internet of Things", "Reliability", "Computational modeling", "Cloud computing", "Quality of service", "Collaborative work", "Big data", "federated learning", "massive Internet of Things", "machine learning", "software-defined network"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "Reliable Federated Learning Systems Based on Intelligent Resource Sharing Scheme for Big Data Internet of Things", "url": "", "volume": "9", "year": "2021", "abstract": "Federated learning (FL) is the up-to-date approach for privacy constraints Internet of Things (IoT) applications in next-generation mobile network (NGMN), 5th generation (5G), and 6th generation (6G), respectively. Due to 5G/6G is based on new radio (NR) technology, the multiple-input and multiple-output (MIMO) of radio services for heterogeneous IoT devices have been performed. The autonomous resource allocation and the intelligent quality of service class identity (IQCI) in mobile networks based on FL systems are obligated to meet the requirements of privacy constraints of IoT applications. In massive FL communications, the heterogeneous local devices propagate their local models and parameters over 5G/6G networks to the aggregation servers in edge cloud areas. Therefore, the assurance of network reliability is compulsory to facilitate end-to-end (E2E) reliability of FL communications and provide the satisfaction of model decisions. This paper proposed an intelligent lightweight scheme based on the reference software-defined networking (SDN) architecture to handle the massive FL communications between clients and aggregators to meet the mentioned perspectives. The handling method adjusts the model parameters and batches size of the individual client to reflect the apparent network conditions classified by the k-nearest neighbor (KNN) algorithm. The proposed system showed notable experimented metrics, including the E2E FL communication latency, throughput, system reliability, and model accuracy.", "pages": "108091-108100", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Math, Sa", "Tam, Prohim", "Kim, Seokhoon"]}]["9052728", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2020.2984942", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Two dimensional displays", "Stacking", "Graphene", "Printing", "Substrates", "Spintronics", "Adhesives", "movable-type", "PVA transfer", "two-dimensional materials", "spin valve", "spintronics"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "Movable-Type Transfer and Stacking of van der Waals Heterostructures for Spintronics", "url": "", "volume": "8", "year": "2020", "abstract": "The key to achieving high-quality and practical van der Waals heterostructure devices made from various two-dimensional (2D) materials lies in the efficient control over clean and flexible interfaces. Inspired by the \u201cmovable-type printing\u201d, one of the four great inventions of ancient China, we demonstrate the \u201cmovable-type\u201d transfer and stacking of 2D materials, which utilizes prefabricated polyvinyl alcohol (PVA) film to engineer the interfacial adhesion to 2D materials, and provides a flexible, efficient and batchable transfer scheme for 2D materials. The experiments also verify the \u201cmovable-type\u201d transfer can preciously control the position and orientation of 2D materials, which meets the burgeoning requirements such as the preparation of twisted graphene and other heterostructures. Importantly, water-solubility of PVA film ensures an ideal interface of the materials without introducing contamination. We illustrate the superiority of this method with a WSe2 vertical spin valve device, whose performance verifies the applicability and advantages of such a method for spintronics. Our PVA-assisted \u201cmovable-type\u201d transfer process may promote the development of high-performance 2D-material-based devices.", "pages": "70488-70495", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Cao, Yuan", "Wang, Xinhe", "Lin, Xiaoyang", "Yang, Wei", "Lv, Chen", "Lu, Yuan", "Zhang, Youguang", "Zhao, Weisheng"]}]["8804358", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2019.2935907", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Image coding", "Visualization", "Transform coding", "Bit rate", "Agriculture", "Forestry", "Quantization (signal)", "Data hiding", "image compression", "MBTC", "BSOC", "image inpainting"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "Joint Data Hiding and Compression Scheme Based on Modified BTC and Image Inpainting", "url": "", "volume": "7", "year": "2019", "abstract": "Seamlessly integrating image compression technique and secret data hiding technique into a single procedure for security and efficient data transmission is a novel research issue in modern, decentralized digital communication environments. The first joint data hiding and compression (JDHC) scheme on block truncation coding (BTC) compression domain is presented in this paper. In the compressing and embedding procedure, for the complex blocks, modified block truncation coding (MBTC) is utilized to embed secret data and compress blocks simultaneously, while further controlling the visual distortion that is caused during data embedding. For the smooth blocks, according to the current embedding bit, either image inpainting or block search order coding (BSOC) is used to embed secret data and compress blocks simultaneously with maintaining acceptable compression performance. According to the image compression codes that are provided as output, image decompression and secret bits extraction procedures can be conducted simultaneously. Experimental results indicate that outstanding compression bit rate can be achieved by the proposed JDHC scheme with satisfactory visual quality and hiding capacity.", "pages": "116027-116037", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Liu, Xiaolong", "Lin, Chia-Chen", "Muhammad, Khan", "Al-Turjman, Fadi", "Yuan, Shyan-Ming"]}]["7590317", {"address": "", "articleno": "", "doi": "10.1109/TST.2016.7590317", "issn": "1007-0214", "issue_date": "", "journal": "Tsinghua Science and Technology", "keywords": ["Pricing", "Data models", "Cost accounting", "Data privacy", "Information entropy", "Indexes", "Computational modeling", "data tuple", "Big Personal Data", "positive grading", "reverse pricing", "pricing model"], "month": "Oct", "number": "5", "numpages": "", "publisher": "", "title": "A pricing model for Big Personal Data", "url": "", "volume": "21", "year": "2016", "abstract": "Big Personal Data is growing explosively. Consequently, an increasing number of internet users are drowning in a sea of data. Big Personal Data has enormous commercial value; it is a new kind of data asset. An urgent problem has thus arisen in the data market: How to price Big Personal Data fairly and reasonably. This paper proposes a pricing model for Big Personal Data based on tuple granularity, with the help of comparative analysis of existing data pricing models and strategies. This model is put forward to implement positive rating and reverse pricing for Big Personal Data by investigating data attributes that affect data value, and analyzing how the value of data tuples varies with information entropy, weight value, data reference index, cost, and other factors. The model can be adjusted dynamically according to these parameters. With increases in data scale, reductions in its cost, and improvements in its quality, Big Personal Data users can thereby obtain greater benefits.", "pages": "482-490", "note": "", "ISSN": "1007-0214", "publicationtype": "article", "author": ["Shen, Yuncheng", "Guo, Bing", "Shen, Yan", "Duan, Xuliang", "Dong, Xiangqian", "Zhang, Hong"]}]["9682686", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2022.3143609", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Data mining", "Convolutional neural networks", "Feature extraction", "Encoding", "Xenon", "Deep learning", "Process mining", "automatic event log conversion", "event data engineering", "event density embedding"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "Automatic Conversion of Event Data to Event Logs Using CNN and Event Density Embedding", "url": "", "volume": "10", "year": "2022", "abstract": "In process mining, converting event data to event logs is related to the quality of analysis results. In general, to convert event data into event logs, it is necessary to identify process entities, such as the case identifier, activity label, activity originator, and activity timestamp, from the data fields in the event data, as well as other optional attributes. Up to now, the event log conversion process has been attempted by relying on an expert\u2019s intuition or an analyst\u2019s experience. However, the conversion is a challenging procedure without sufficient prior knowledge of process mining. To automate the conversion process, an event log\u2013converting algorithm based on the convolutional neural network (CNN) was developed with a new embedding method called Event Density Embedding (EDE). To verify the performance of the proposed embedding method and the automatic event log conversion framework, a comparative experiment was performed using nine pieces of real-world event data. The experiments show that our method is 5\u201320% higher conversion accuracy than the other methods. It is expected that business experts will be able to easily apply the method to process mining technology by utilizing system-derived event data.", "pages": "15994-16009", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Sim, Sunghyun", "Sutrisnowati, Riska", "Won, Seokrae", "Lee, Sanghwa", "Bae, Hyerim"]}]["8320776", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2018.2817921", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Data collection", "Malware", "Monitoring", "Intrusion detection", "Communication networks", "Telecommunication traffic", "Network security", "security-related data", "data collection technologies", "large-scale heterogeneous networks"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "A Survey on Network Security-Related Data Collection Technologies", "url": "", "volume": "6", "year": "2018", "abstract": "Security threats and economic loss caused by network attacks, intrusions, and vulnerabilities have motivated intensive studies on network security. Normally, data collected in a network system can reflect or can be used to detect security threats. We define these data as network security-related data. Studying and analyzing security-related data can help detect network attacks and intrusions, thus making it possible to further measure the security level of the whole network system. Obviously, the first step in detecting network attacks and intrusions is to collect security-related data. However, in the context of big data and 5G, there exist a number of challenges in collecting these security-related data. In this paper, we first briefly introduce network security-related data, including its definition and characteristics, and the applications of network data collection. We then provide the requirements and objectives for security-related data collection and present a taxonomy of data collection technologies. Moreover, we review existing collection nodes, collection tools, and collection mechanisms in terms of network data collection and analyze them based on the proposed requirements and objectives toward high quality security-related data collection. Finally, we discuss open research issues and conclude with suggestions for future research directions.", "pages": "18345-18365", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Lin, Huaqing", "Yan, Zheng", "Chen, Yu", "Zhang, Lifang"]}]["9392233", {"address": "", "articleno": "", "doi": "10.1109/JSTARS.2021.3066697", "issn": "2151-1535", "issue_date": "", "journal": "IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing", "keywords": ["Biological system modeling", "Water quality", "Reservoirs", "Estimation", "Water pollution", "Genetic algorithms", "Optical sensors", "Artificial neural networks (ANN)", "case-II waters", "chlorophyll-a (Chl-a)", "coastal areas", "genetic algorithm (GA)", "machine learning", "Sentinel 2 MSI", "three-band models (TBM)"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "Remote Sensing Estimation of Chlorophyll-A in Case-II Waters of Coastal Areas: Three-Band Model Versus Genetic Algorithm\u2013Artificial Neural Networks Model", "url": "", "volume": "14", "year": "2021", "abstract": "Chlorophyll-a (Chl-a), an important indicator of phytoplankton biomass and eutrophication, is sensitive to water constitutes and optical characteristics. An integrated machine learning method of genetic algorithm and artificial neural networks (GA\u2013ANN) was developed to retrieve the concentration of Chl-a. In situ spectra and simultaneous water quality parameters of 107 samples from two reservoirs (Res) and coastal waters (CW) were used to calibrate GA\u2013ANN and three-band models (TBM) for comparison of Chl-a estimation. Both GA\u2013ANN and TBM methods perform well for the joint dataset (WGD) of Res and CW with the R2 exceeding 0.90, and the root mean square error (RMSE) of corresponding validation (N = 35) are 4.40 and 5.23 \u03bcg/L, respectively. Similarly, for independent dataset of Res (N = 45), GA\u2013ANN and TBM methods show robust performance: the R2 values are 0.87 and 0.80, respectively; and the corresponding RMSE values are 7.79 and 7.73 \u03bcg/L, respectively. For CW dataset (N = 62), the R2 values of two methods are 0.81 and 0.62, respectively; and the corresponding RMSE values are 0.79 and 1.32 \u03bcg/L, respectively. When the GA\u2013ANN and TBM models were applied to retrieve Chl-a concentration from the calibrated Sentinel 2 MSI reflectance data in two Res on October 20, 2019, however, the validated results of MSI-derived Chl-a concentrations using quasi-synchronous in situ data (N = 36) indicated that the GA\u2013ANN model outperforms TBM with higher R2 value (0.91 vs. 0.26) and smaller RMSE (4.41 vs. 13.85 \u03bcg/L) and mean absolute errors (3.40 vs. 11.87 \u03bcg/L) values. Although TBM has obvious overestimation of Chl-a concentration when applied to remote sensing image, we still thought that both GA\u2013ANN and TBM are useful methods for Chl-a estimation in case-II waters, and GA\u2013ANN performs marginally better with less deviation to measured Chl-a for multispectral remote sensing data. The ratio of TSS to Chl-a, experimental measurements, abundance of sampling points, and Chl-a concentration range are several important factors affecting the accuracy and robustness of GA\u2013ANN and TBM methods.", "pages": "3640-3658", "note": "", "ISSN": "2151-1535", "publicationtype": "article", "author": ["Chen, Jinyue", "Chen, Shuisen", "Fu, Rao", "Wang, Chongyang", "Li, Dan", "Peng, Yongshi", "Wang, Li", "Jiang, Hao", "Zheng, Qiong"]}]["8794542", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2019.2934515", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Economics", "Games", "Data models", "Quality of service", "Analytical models", "Data collection", "Task analysis", "Cyber-physical-social system", "price decision", "game theory", "economic model"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "A Game-Based Economic Model for Price Decision Making in Cyber-Physical-Social Systems", "url": "", "volume": "7", "year": "2019", "abstract": "Cyber-physical-social (CPS) systems integrate Big Data Collectors (BDCs), Service Organizers (SOs) and users to build a unified data-centric computing framework. In CPS systems, BDCs leverage a vast variety of sensing devices to collect cyber-physical-social data, and report these data to SOs to orchestrate various services provided to users, thus offering a great potential for solving complex network tasks that are far beyond the capabilities of existing networks. However, due to the lack of an economic model to describe such complex data interactions, their applications are limited. So, a game-based economic model is proposed in this paper to make smart price decisions in CPS systems. Specifically, it has the following innovations: (a) The economic model gives a dynamic game income matrix which can accurately describe the revenue changes of BDCs in the game, so as to help BDCs select appropriate game parameters and strategies, and make BDCs competitive in the game. (b) The economic model can help SOs to make optimized data purchase price and service selling price based on data collection cost and competitor price analysis, so that SOs can have a better Quality of Service (QoS) and users attraction, and maximize the profit. Experimental results demonstrate that the proposed model can help BDCs and SOs find the most suitable game strategy and price adjustment principle, which has great significance in applications.", "pages": "111559-111576", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Huang, Mingfeng", "Liu, Wei", "Wang, Tian", "Deng, Qingyong", "Liu, Anfeng", "Xie, Mande", "Ma, Ming", "Zhang, Guoping"]}]["9802898", {"address": "", "articleno": "", "doi": "10.26599/TST.2021.9010082", "issn": "1007-0214", "issue_date": "", "journal": "Tsinghua Science and Technology", "keywords": ["Fault tolerance", "Satellites", "File systems", "Fault tolerant systems", "Distributed databases", "Memory", "Geospatial analysis", "Remote Sensing (RS) data", "geospatial indexing", "multi-indexing mechanism", "Hadoop Distributed File System (HDFS)", "Multi-IndeXing-RS (MIX-RS)"], "month": "December", "number": "6", "numpages": "", "publisher": "", "title": "MIX-RS: A Multi-Indexing System Based on HDFS for Remote Sensing Data Storage", "url": "", "volume": "27", "year": "2022", "abstract": "A large volume of Remote Sensing (RS) data has been generated with the deployment of satellite technologies. The data facilitate research in ecological monitoring, land management and desertification, etc. The characteristics of RS data (e.g., enormous volume, large single-file size, and demanding requirement of fault tolerance) make the Hadoop Distributed File System (HDFS) an ideal choice for RS data storage as it is efficient, scalable, and equipped with a data replication mechanism for failure resilience. To use RS data, one of the most important techniques is geospatial indexing. However, the large data volume makes it time-consuming to efficiently construct and leverage. Considering that most modern geospatial data centres are equipped with HDFS-based big data processing infrastructures, deploying multiple geospatial indices becomes natural to optimise the efficacy. Moreover, because of the reliability introduced by high-quality hardware and the infrequently modified property of the RS data, the use of multi-indexing will not cause large overhead. Therefore, we design a framework called Multi-IndeXing-RS (MIX-RS) that unifies the multi-indexing mechanism on top of the HDFS with data replication enabled for both fault tolerance and geospatial indexing efficiency. Given the fault tolerance provided by the HDFS, RS data are structurally stored inside for faster geospatial indexing. Additionally, multi-indexing enhances efficiency. The proposed technique naturally sits on top of the HDFS to form a holistic framework without incurring severe overhead or sophisticated system implementation efforts. The MIX-RS framework is implemented and evaluated using real remote sensing data provided by the Chinese Academy of Sciences, demonstrating excellent geospatial indexing performance.", "pages": "881-893", "note": "", "ISSN": "1007-0214", "publicationtype": "article", "author": ["Wu, Jiashu", "Xiong, Jingpan", "Dai, Hao", "Wang, Yang", "Xu, Chengzhong"]}]["8786119", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2019.2932754", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Computational modeling", "Image denoising", "Image edge detection", "Computed tomography", "TV", "Noise reduction", "X-ray imaging", "Low-dose computed tomography (LDCT) image denoising", "sparse representation", "stationary sub-dictionaries", "the maximum eigenvalue of the gradient covariance matrix", "total variation (TV)", "the clipped and normalized local activity"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "Low-Dose CT Image Denoising Model Based on Sparse Representation by Stationarily Classified Sub-Dictionaries", "url": "", "volume": "7", "year": "2019", "abstract": "Low-dose computed tomography (LDCT) technique is an important imaging modality, but LDCT images are always severely degraded by mottle noise and streak artifacts. The recently proposed nonlocally centralized sparse representation (NCSR) algorithm has good performance in natural image denoising, but it suffers from residual streak artifacts and can't preserve edges structure information well when implemented in LDCT image denoising. In addition, it has high computational complexity. To address this problem, in this paper, we propose an improved model, i.e. SNCSR model, based on the stationary PCA sub-dictionaries, nonlocally centralized sparse representation and relative total variation. In the SNCSR model, in order to learn more accurate sub-dictionaries, the LDCT image is preprocessed by the improved total variation (ITV) model in which the weighted coefficient of the regularization term is constructed depending on a clipped and normalized local activity. In addition, the maximum eigenvalue of the gradient covariance matrix of the image patch is used to distinguish edge structure information from background region so that the restored image can be represented more sparsely. Moreover, unlike the NCSR model that needs to learn sub-dictionaries in each outer loop, the proposed model learns stationary sub-dictionaries only once before iteration starts, which shorten the computation time significantly. At last, the relative total variation (RTV) algorithm is applied to further reduce the residual artifacts in the recovered image more thoroughly. The experiments are performed on the simulated pelvis phantom, the actual thoracic phantom and the clinical abdominal data. Compared with several other competitive denoising algorithms, both subjective visual effect and objective evaluation criteria show that the proposed SNCSR model has lower computational complexity and can improve LDCT images quality more effectively.", "pages": "116859-116874", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Chen, Wenbin", "Shao, Yanling", "Jia, Lina", "Wang, Yanling", "Zhang, Quan", "Shang, Yu", "Liu, Yi", "Chen, Yan", "Liu, Yanli", "Gui, Zhiguo"]}]["9430134", {"address": "", "articleno": "", "doi": "10.26599/BDMA.2021.9020001", "issn": "2096-0654", "issue_date": "", "journal": "Big Data Mining and Analytics", "keywords": ["Clustering algorithms", "Data mining", "Shape", "Big Data", "Bayes methods", "Optimization", "Clustering methods", "density-based clustering", "incomplete data", "clustering algorihtm"], "month": "Sep.", "number": "3", "numpages": "", "publisher": "", "title": "Effective density-based clustering algorithms for incomplete data", "url": "", "volume": "4", "year": "2021", "abstract": "Density-based clustering is an important category among clustering algorithms. In real applications, manydatasets suffer from incompleteness. Traditional imputation technologies or other techniques for handling missingvalues are not suitable for density-based clustering and decrease clustering result quality. To avoid these problems, we develop a novel density-based clustering approach for incomplete data based on Bayesian theory, which conductsimputation and clustering concurrently and makes use of intermediate clustering results. To avoid the impact oflow-density areas inside non-convex clusters, we introduce a local imputation clustering algorithm, which aims toimpute points to high-density local areas. The performances of the proposed algorithms are evaluated using tensynthetic datasets and five real-world datasets with induced missing values. The experimental results show theeffectiveness of the proposed algorithms.", "pages": "183-194", "note": "", "ISSN": "2096-0654", "publicationtype": "article", "author": ["Xue, Zhonghao", "Wang, Hongzhi"]}]["8924671", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2019.2957881", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Market research", "Recommender systems", "Proposals", "Big Data", "Context-aware services", "Context modeling", "Semantics", "Content-based recommender system", "context-awareness", "user profile contextualization", "map-reduce", "big data"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "A Big Data Semantic Driven Context Aware Recommendation Method for Question-Answer Items", "url": "", "volume": "7", "year": "2019", "abstract": "Content-Based recommender systems (CB) filter relevant items to users in overloaded search spaces using information about their preferences. However, classical CB scheme is mainly based on matching between items descriptions and user profile, without considering that context may influence user preferences. Therefore, it cannot achieve high accuracy on user preference prediction. This paper aims to handle context-awareness (CA) to improve quality of recommendation taking contextual information as the trend in current trend interest, in which a stream of status updates can be analyzed to model the context. It proposes a novel CA-CB approach that recommends question/answer items by considering context awareness based on topic detection within current trend interest. A case study and related experiments were developed in the big data framework Spark to show that the context integration benefits recommendation performance.", "pages": "182664-182678", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Castro, Jorge", "Yera, Raciel", "Alzahrani, Ahmad", "S\u00e1nchez, Pedro", "Barranco, Manuel", "Mart\u00ednez, Luis"]}]["8643345", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2019.2899939", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Head", "Kernel", "Training", "Convolutional neural networks", "Estimation", "Task analysis", "Standards", "Crowd counting", "convolutional neural network", "scale-adaptive convolutional neural network (SaCNN)", "density map", "scale-adaptive", "absolute count loss"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "Improved Crowd Counting Method Based on Scale-Adaptive Convolutional Neural Network", "url": "", "volume": "7", "year": "2019", "abstract": "Crowd counting is a challenging task due to the influence of various factors, such as scene transformation, complex crowd distribution, uneven illumination, and occlusion. To overcome such problems, scale-adaptive convolutional neural network (SaCNN) used a convolutional neural network to obtain high-quality crowd density map estimation and integrate the density map to get the estimated headcount. To obtain better performance on crowd counting, an improved crowd counting method based on SaCNN was proposed in this paper. The spread parameter, i.e., the standard variance, of geometry-adaptive Gaussian kernel used in SaCNN was optimized to generate a higher quality ground truth density map for training. The absolute count loss with weight 4e-5 was used to jointly optimize with the density map loss to improve the network generalization ability for crowd scenes with few pedestrians. Also, a random cropping method was applied to improve the diversity of training samples to enhance network generalization ability. The experimental results upon ShanghaiTech public dataset showed that the proposed method can obtain more accurate and more robust results on crowd counting than those of SaCNN.", "pages": "24411-24419", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Sang, Jun", "Wu, Weiqun", "Luo, Hongling", "Xiang, Hong", "Zhang, Qian", "Hu, Haibo", "Xia, Xiaofeng"]}]["8868305", {"address": "", "articleno": "", "doi": "10.21629/JSEE.2019.05.14", "issn": "1004-4132", "issue_date": "", "journal": "Journal of Systems Engineering and Electronics", "keywords": ["Heuristic algorithms", "Cloud computing", "Load management", "Data centers", "Scheduling", "Protocols", "Throughput", "cloud data center", "software defined networking (SDN)", "load balancing", "multi-path transmission", "OpenFlow"], "month": "Oct", "number": "5", "numpages": "", "publisher": "", "title": "MTSS: Multi-path traffic scheduling mechanism based on SDN", "url": "", "volume": "30", "year": "2019", "abstract": "Large-scale and diverse businesses based on the cloud computing platform bring the heavy network traffic to cloud data centers. However, the unbalanced workload of cloud data center network easily leads to the network congestion, the low resource utilization rate, the long delay, the low reliability, and the low throughput. In order to improve the utilization efficiency and the quality of services (QoS) of cloud system, especially to solve the problem of network congestion, we propose MTSS, a multi-path traffic scheduling mechanism based on software defined networking (SDN). MTSS utilizes the data flow scheduling flexibility of SDN and the multi-path feature of the fat-tree structure to improve the traffic balance of the cloud data center network. A heuristic traffic balancing algorithm is presented for MTSS, which periodically monitors the network link and dynamically adjusts the traffic on the heavy link to achieve programmable data forwarding and load balancing. The experimental results show that MTSS outperforms equal-cost multi-path protocol (ECMP), by effectively reducing the packet loss rate and delay. In addition, MTSS improves the utilization efficiency, the reliability and the throughput rate of the cloud data center network.", "pages": "974-984", "note": "", "ISSN": "1004-4132", "publicationtype": "article", "author": ["Xiaolong, Xu", "Yun, Chen", "Liuyun, Hu", "Anup, Kumar"]}]["8543558", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2018.2883105", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Authentication", "Servers", "Big Data", "Protocols", "Password", "Fault tolerance", "Cloud computing", "authentication", "key agreement", "big data security", "hadoop", "formal security", "AVISPA"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "HEAP: An Efficient and Fault-Tolerant Authentication and Key Exchange Protocol for Hadoop-Assisted Big Data Platform", "url": "", "volume": "6", "year": "2018", "abstract": "Hadoop framework has been evolved to manage big data in cloud. Hadoop distributed file system and MapReduce, the vital components of this framework, provide scalable and fault-tolerant big data storage and processing services at a lower cost. However, Hadoop does not provide any robust authentication mechanism for principals\u2019 authentication. In fact, the existing state-of-the-art authentication protocols are vulnerable to various security threats, such as man-in-the-middle, replay, password guessing, stolen-verifier, privileged-insider, identity compromization, impersonation, denial-of-service, online/off-line dictionary, chosen plaintext, workstation compromization, and server-side compromisation attacks. Beside these threats, the state-of-the-art mechanisms lack to address the server-side data integrity and confidentiality issues. In addition to this, most of the existing authentication protocols follow a single-server-based user authentication strategy, which, in fact, originates single point of failure and single point of vulnerability issues. To address these limitations, in this paper, we propose a fault-tolerant authentication protocol suitable for the Hadoop framework, which is called the efficient authentication protocol for Hadoop (HEAP). HEAP alleviates the major issues of the existing state-of-the-art authentication mechanisms, namely operating-system-based authentication, password-based approach, and delegated token-based schemes, respectively, which are presently deployed in Hadoop. HEAP follows two-server-based authentication mechanism. HEAP authenticates the principal based on digital signature generation and verification strategy utilizing both advanced encryption standard and elliptic curve cryptography. The security analysis using both the formal security using the broadly accepted real-or-random (ROR) model and the informal (non-mathematical) security shows that HEAP protects several well-known attacks. In addition, the formal security verification using the widely used automated validation of Internet security protocols and applications ensures that HEAP is resilient against replay and man-in-the-middle attacks. Finally, the performance study contemplates that the overheads incurred in HEAP is reasonable and is also comparable to that of other existing state-of-the-art authentication protocols. High security along with comparable overheads makes HEAP to be robust and practical for a secure access to the big data storage and processing services.", "pages": "75342-75382", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Chattaraj, Durbadal", "Sarma, Monalisa", "Das, Ashok", "Kumar, Neeraj", "Rodrigues, Joel", "Park, Youngho"]}]["8466785", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2018.2870394", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Cameras", "Optical imaging", "Estimation", "Arrays", "Photography", "Big data in Internet of Things(IoT)", "camera array", "light field imaging", "image aliasing", "depth estimation"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "Research on Depth Estimation Method of Light Field Imaging Based on Big Data in Internet of Things From Camera Array", "url": "", "volume": "6", "year": "2018", "abstract": "In recent years, optical field imaging technology has received extensive attention in the academic circle for its novel imaging characteristics of shooting first and focusing later, variable depth of field, variable viewpoint, and so on. However, the existing optical field acquisition equipment can only acquire a limited number of discrete angle signals, so image aliasing caused by under sampling of optical field angle signals reduces the quality of optical field images. Based on the camera array system as a platform, this paper studies the optical field imaging and depth estimation method based on the Big Data in Internet of Things obtained from camera array around the angle sampling characteristics of the optical field data set, and has achieved some innovative research results in the following aspects. On the basis of analyzing the characteristics of different depth clues in the optical field data set, a depth estimation method combining parallax method and focusing method is proposed. First, this paper analyzes the disparity clues and focus clues contained in the multi-view data set and the light field refocusing image set of the camera array, respectively, and points out the differences and relationships between the two depth clues extraction methods in the light field sampling frequency domain space, that is, the disparity method focuses on the energy concentration characteristics near the frequency domain spatial angle axis, while the focus method focuses on the high frequency proportion of energy distribution on the angle axis. Then, the weighted linear fusion method based on image gradient is used to fuse the two calculation results, which improves the accuracy and robustness of depth estimation. Finally, the results of depth estimation experiments on different sets of scenes show that compared with the method based on a single depth cue, the method in this paper has higher accuracy in depth calculation in discontinuous areas of scene depth and similar texture areas.", "pages": "52308-52320", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Wu, Yue"]}]["9186592", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2020.3021832", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Wireless communication", "Relays", "Quality of service", "Resource management", "Wireless sensor networks", "Internet of Things", "Energy harvesting", "Internet of Things networks", "simultaneous wireless information and power transfer", "statistical QoS", "relay communications", "power allocation"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "Statistical QoS Aware for Wireless Powered Cooperative Communications in Internet of Things", "url": "", "volume": "8", "year": "2020", "abstract": "This article unveils the importance of statistical quality of service (QoS) for resource allocation in a two-hop network. Particularly, an access point (AP) serves multiple IoT devices for information transfer with the assistance of the energy harvesting (EH) relaying. To explore the maximum constant data arrival metric, we aim to maximize the effective capacity (EC) under specified QoS requirements. Also, the statistical QoS inspired resource allocation policies are investigated for half/full duplex (HD/FD) modes, respectively, to jointly optimize power allocation and power splitting (PS) ratio. To solve the formulated problem, we first derive the closed-form solution of the optimal power allocation at the AP and the PS ratio. To gain more insights, we further derive the boundary conditions of optimal power allocation and PS ratio. Finally, numerical results are demonstrated to validate the theoretical derivations, which highlights the proposed scheme in terms of EC performance in comparison to the benchmark scheme.", "pages": "165884-165893", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Gao, Ya", "Shi, Yongpeng", "Xia, Yujie", "Zhang, Hailin"]}]["9732429", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2022.3158348", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Brain modeling", "Data models", "Electroencephalography", "Transfer learning", "Medical services", "Detectors", "Task analysis", "Active learning", "epilepsy", "mentor-student architecture", "patient-specific", "transfer learning"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "MS4PS: A Mentor-Student Architecture for Patient-Specific Seizure Detection With Combination of Transfer Learning and Active Learning", "url": "", "volume": "10", "year": "2022", "abstract": "Privacy protection, high labeling cost, and varying characteristics of seizures among patients and at different times are the main obstacles to building seizure detection models. Considering these issues, we propose a novel Mentor-Student architecture for Patient-Specific seizure detection (MS4PS). It contains a new method of knowledge transferring called mentor-select-for-student, which exploits the knowledge of a mentor model by using this model to select data for training a student model, making it possible to avoid transferring patient data and the negative influence of transferring parameters/structures of pre-trained models. It also contains a new method of active learning, which uses both an experienced mentor model and a quick-learning student model to select high-quality samples for doctors to label. Each of the two models is coupled with a particular sample selection strategy that combines uncertainty/certainty and the distance between the unlabeled samples and labeled seizure samples. The proposed method can quickly train a suitable detector for a patient at his/her first epilepsy diagnosis with the help of: (1) an experienced mentor model that chooses the most category-certain electroencephalography (EEG) data segments; (2) a student model (detector itself) that chooses the most category-uncertain EEG data segments; (3) doctors who label these data segments selected by both the mentor model and student model. By replacing or improving the mentor model and refining the historical models of patients when they come next time, the MS4PS system can be sustainably promoted. The proposed method is tested on the CHB-MIT and NEO datasets, and the results demonstrate its effectiveness and efficiency.", "pages": "29646-29667", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Ma, Shun", "Liu, Haojie", "Zhu, Xiaogang", "Fan, Yufeng", "Su, Caixia", "Cao, Yongfeng"]}]["8476553", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2018.2872784", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Data mining", "Intrusion detection", "Systematics", "Bibliographies", "Quality assessment", "Libraries", "Search problems", "Intrusion detection system", "real-time detection", "data mining", "network security"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "Data Mining Techniques in Intrusion Detection Systems: A Systematic Literature Review", "url": "", "volume": "6", "year": "2018", "abstract": "The continued ability to detect malicious network intrusions has become an exercise in scalability, in which data mining techniques are playing an increasingly important role. We survey and categorize the fields of data mining and intrusion detection systems, providing a systematic treatment of methodologies and techniques. We apply a criterion-based approach to select 95 relevant articles from 2007 to 2017. We identified 19 separate data mining techniques used for intrusion detection, and our analysis encompasses rich information for future research based on the strengths and weaknesses of these techniques. Furthermore, we observed a research gap in establishing the effectiveness of classifiers to identify intrusions in modern network traffic when trained with aging data sets. Our review points to the need for more empirical experiments addressing real-time solutions for big data against contemporary attacks.", "pages": "56046-56058", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Salo, Fadi", "Injadat, Mohammadnoor", "Nassif, Ali", "Shami, Abdallah", "Essex, Aleksander"]}]["9629345", {"address": "", "articleno": "", "doi": "10.1109/TII.2021.3131471", "issn": "1941-0050", "issue_date": "", "journal": "IEEE Transactions on Industrial Informatics", "keywords": ["Feature extraction", "Soft sensors", "Logic gates", "Data mining", "Time series analysis", "Frequency modulation", "Data models", "Deep learning", "industrial big data", "industrial intelligence", "product quality prediction", "self-supervised learning", "soft sensor"], "month": "Sep.", "number": "9", "numpages": "", "publisher": "", "title": "A Data-Driven Self-Supervised LSTM-DeepFM Model for Industrial Soft Sensor", "url": "", "volume": "18", "year": "2022", "abstract": "Soft sensor, as an important paradigm for industrial intelligence, is widely used in industrial production to achieve efficient monitoring and prediction of production status including product quality. Data-driven soft sensor methods have attracted attention, which still have challenges because of complex industrial data with diverse characteristics, nonlinear relationships, and massive unlabeled samples. In this article, a data-driven self-supervised long short-term memory\u2013deep factorization machine (LSTM-DeepFM) model is proposed for industrial soft sensor, in which a framework mainly including pretraining and finetuning stages is proposed to explore diverse industrial data characteristics. In the pretraining stage, an LSTM-autoencoder is first unsupervised pretrained. Then, based on two self-supervised mask strategies, LSTM-deep can explore the interdependencies between features as well as the dynamic fluctuation in time series. In the finetuning stage, relying on pretrained representation, the temporal, high-dimensional, and low-dimensional features can be extracted from the LSTM, deep, and FM components, respectively. Finally, experiments on the real-world mining dataset demonstrate that the proposed method achieves state of the art comparing with stacked autoencoder-based models, variational autoencoder-based models, semisupervised parallel DeepFM, etc.", "pages": "5859-5869", "note": "", "ISSN": "1941-0050", "publicationtype": "article", "author": ["Ren, Lei", "Wang, Tao", "Laili, Yuanjun", "Zhang, Lin"]}]["9445109", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2021.3085338", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Monitoring", "Production", "Feature extraction", "Object detection", "Kernel", "Convolution", "Quality assessment", "Quality monitoring", "PP-YOLO", "pruning", "deep learning"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "Research on a Product Quality Monitoring Method Based on Multi Scale PP-YOLO", "url": "", "volume": "9", "year": "2021", "abstract": "To monitor product quality in the production process in real time, this thesis proposes a quality monitoring model based on PaddlePaddle You Only Look Once (PP-YOLO). First, in the preprocessing stage, the data enhancement method and the K-means++ method are used to improve the robustness of the algorithm, and the generated anchor box can screen more refined features earlier. Second, ResNet50-vd with the deformable convolution idea is selected as the backbone of the detection model, the feature pyramid network structure and the composition of the loss function are improved, and the feature learning ability of the model is enhanced to enable it to detect multiple scales of defects. Finally, pruning is performed on the basis of the trained model to reduce the number of model parameters so that it can be deployed in industrial scenarios with limited hardware conditions. Experimental results show that the proposed quality monitoring model can meet the requirements for detection speed and accuracy in actual production, providing a new concept for the deployment of deep learning models in the industrial field.", "pages": "80373-80387", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Li, Yiting", "Huang, Haisong", "Chen, Qipeng", "Fan, Qingsong", "Quan, Huafeng"]}]["9894678", {"address": "", "articleno": "", "doi": "10.1109/TBDATA.2022.3207521", "issn": "2332-7790", "issue_date": "", "journal": "IEEE Transactions on Big Data", "keywords": ["Data privacy", "Information integrity", "Information filtering", "Europe", "Soft sensors", "Scalability", "Big Data", "Distributed data anonymization", "Mondrian", " $\\kappa $ -Anonymity", " $\\ell $ -Diversity", "Apache Spark"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "Scalable Distributed Data Anonymization for Large Datasets", "url": "", "volume": "", "year": "2022", "abstract": "$\\kappa $-Anonymity and $\\ell $-diversity are two well-known privacy metrics that guarantee protection of the respondents of a dataset by obfuscating information that can disclose their identities and sensitive information. Existing solutions for enforcing them implicitly assume to operate in a centralized scenario, since they require complete visibility over the dataset to be anonymized, and can therefore have limited applicability in anonymizing large datasets. In this paper, we propose a solution that extends Mondrian (an efficient and effective approach designed for achieving $\\kappa $-anonymity) for enforcing both $\\kappa $-anonymity and $\\ell $-diversity over large datasets in a distributed manner, leveraging the parallel computation of multiple workers. Our approach efficiently distributes the computation among the workers, without requiring visibility over the dataset in its entirety. Our data partitioning limits the need for workers to exchange data, so that each worker can independently anonymize a portion of the dataset. We implemented our approach providing parallel execution on a dynamically chosen number of workers. The experimental evaluation shows that our solution provides scalability, while not affecting the quality of the resulting anonymization.", "pages": "1-14", "note": "", "ISSN": "2332-7790", "publicationtype": "article", "author": ["Vimercati, Sabrina", "Facchinetti, Dario", "Foresti, Sara", "Livraga, Giovanni", "Oldani, Gianluca", "Paraboschi, Stefano", "Rossi, Matthew", "Samarati, Pierangela"]}]["9211498", {"address": "", "articleno": "", "doi": "10.1109/TGRS.2020.3024744", "issn": "1558-0644", "issue_date": "", "journal": "IEEE Transactions on Geoscience and Remote Sensing", "keywords": ["Clouds", "Optical imaging", "Cloud computing", "Earth", "Optical sensors", "Data integration", "Image reconstruction", "Cloud removal", "data fusion", "deep learning", "generative adversarial network (GAN)", "optical imagery", "synthetic aperture radar (SAR)-optical"], "month": "July", "number": "7", "numpages": "", "publisher": "", "title": "Multisensor Data Fusion for Cloud Removal in Global and All-Season Sentinel-2 Imagery", "url": "", "volume": "59", "year": "2021", "abstract": "The majority of optical observations acquired via spaceborne Earth imagery are affected by clouds. While there is numerous prior work on reconstructing cloud-covered information, previous studies are, oftentimes, confined to narrowly defined regions of interest, raising the question of whether an approach can generalize to a diverse set of observations acquired at variable cloud coverage or in different regions and seasons. We target the challenge of generalization by curating a large novel data set for training new cloud removal approaches and evaluate two recently proposed performance metrics of image quality and diversity. Our data set is the first publically available to contain a global sample of coregistered radar and optical observations, cloudy and cloud-free. Based on the observation that cloud coverage varies widely between clear skies and absolute coverage, we propose a novel model that can deal with either extreme and evaluate its performance on our proposed data set. Finally, we demonstrate the superiority of training models on real over synthetic data, underlining the need for a carefully curated data set of real observations. To facilitate future research, our data set is made available online.", "pages": "5866-5878", "note": "", "ISSN": "1558-0644", "publicationtype": "article", "author": ["Ebel, Patrick", "Meraner, Andrea", "Schmitt, Michael", "Zhu, Xiao"]}]["9420742", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2021.3077069", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Security", "Internet of Things", "Data analysis", "Blockchain", "Reliability", "5G mobile communication", "Quality of service", "Internet of Things", "security attack detection", "edge computing", "fog computing", "blockchain", "software-defined networking"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "Deep Learning and Blockchain-Empowered Security Framework for Intelligent 5G-Enabled IoT", "url": "", "volume": "9", "year": "2021", "abstract": "Recently, many IoT applications, such as smart transportation, healthcare, and virtual and augmented reality experiences, have emerged with fifth-generation (5G) technology to enhance the Quality of Service (QoS) and user experience. The revolution of 5G-enabled IoT supports distinct attributes, including lower latency, higher system capacity, high data rate, and energy saving. However, such revolution also delivers considerable increment in data generation that further leads to a major requirement of intelligent and effective data analytic operation across the network. Furthermore, data growth gives rise to data security and privacy concerns, such as breach and loss of sensitive data. The conventional data analytic and security methods do not meet the requirement of 5G-enabled IoT including its unique characteristic of low latency and high throughput. In this paper, we propose a Deep Learning (DL) and blockchain-empowered security framework for intelligent 5G-enabled IoT that leverages DL competency for intelligent data analysis operation and blockchain for data security. The framework's hierarchical architecture wherein DL and blockchain operations emerge across the four layers of cloud, fog, edge, and user is presented. The framework is simulated and analyzed, employing various standard measures of latency, accuracy, and security to demonstrate its validity in practical applications.", "pages": "90075-90083", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Rathore, Shailendra", "Park, Jong", "Chang, Hangbae"]}]["8845616", {"address": "", "articleno": "", "doi": "10.1109/TPDS.2019.2942591", "issn": "1558-2183", "issue_date": "", "journal": "IEEE Transactions on Parallel and Distributed Systems", "keywords": ["Computer crime", "Cloud computing", "Containers", "Mathematical model", "Computer architecture", "Resource management", "Container", "microservice", "DDoS attack", "mitigation", "cloud computing"], "month": "March", "number": "3", "numpages": "", "publisher": "", "title": "Exploring New Opportunities to Defeat Low-Rate DDoS Attack in Container-Based Cloud Environment", "url": "", "volume": "31", "year": "2020", "abstract": "DDoS attacks are rampant in cloud environments and continually evolve into more sophisticated and intelligent modalities, such as low-rate DDoS attacks. But meanwhile, the cloud environment is also developing in constant. Now container technology and microservice architecture are widely applied in cloud environment and compose container-based cloud environment. Comparing with traditional cloud environments, the container-based cloud environment is more lightweight in virtualization and more flexible in scaling service. Naturally, a question that arises is whether these new features of container-based cloud environment will bring new possibilities to defeat DDoS attacks. In this paper, we establish a mathematical model based on queueing theory to analyze the strengths and weaknesses of the container-based cloud environment in defeating low-rate DDoS attack. Based on this, we propose a dynamic DDoS mitigation strategy, which can dynamically regulate the number of container instances serving for different users and coordinate the resource allocation for these instances to maximize the quality of service. And extensive simulations and testbed-based experiments demonstrate our strategy can make the limited system resources be utilized sufficiently to maintain the quality of service acceptable and defeat DDoS attack effectively in the container-based cloud environment.", "pages": "695-706", "note": "", "ISSN": "1558-2183", "publicationtype": "article", "author": ["Li, Zhi", "Jin, Hai", "Zou, Deqing", "Yuan, Bin"]}]["8666706", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2019.2904315", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Target tracking", "Training", "Visualization", "Reliability", "Feature extraction", "Semantics", "Convolutional neural networks", "Convolutional neural networks", "densely connected learning", "self-paced learning", "online update"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "Self-Paced Dense Connectivity Learning for Visual Tracking", "url": "", "volume": "7", "year": "2019", "abstract": "When misalignment, deformation, and tracking failures occur, the appearance of the target tends to change significantly. How to effectively learn the change of target's appearance is an essential problem in visual tracking. Recently, most recent trackers based on convolutional neural networks update the tracker online to learn the change of target's appearance. These methods collect tracking results as online training samples. Thus, the reliability of training samples is very important for online updates. We propose a self-paced selection model, which integrates the self-paced learning model into the tracking framework for the goal of distinguishing the reliable samples from the tracking results. It estimates the reliability of the tracking results by the self-paced function. We design a method that adaptively calculates the value of the pace, which determines the number of samples selected. And this method is based on the number of tracking results. At the same time, the quality of the target's features plays a key role in the performance of the tracker. We employ dense connectivity learning to enhance the flow of information throughout the network, which makes the target's features represent better. The extensive experiments demonstrate that our self-paced dense connectivity learning tracker (SPDCT) performs favorably against the state-of-the-art trackers over four benchmark datasets.", "pages": "37181-37191", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Ge, Daohui", "Song, Jianfeng", "Qi, Yutao", "Wang, Chongxiao", "Miao, Qiguang"]}]["8755856", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2019.2926882", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Computational modeling", "Decoding", "Data models", "Task analysis", "Adaptation models", "Sun", "Training data", "Deep learning", "computer vision", "image generation", "generative adversarial networks", "image translation"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "Channel Attention Networks for Image Translation", "url": "", "volume": "7", "year": "2019", "abstract": "Existing image-to-image translation methods usually adopt an encoder-decoder structure to generate images. The encoder extracts the features of input images using a sequence of convolution layers until a bottleneck, and then, the intermediate features are decoded to the target image. However, the existence of bottleneck layer in such structure may lead to blurry and bad quality of the translated images, since different domain translations may be related to the global or local region in the input image or even in an abstract level. To prevent these problems, we propose the channel attention networks for image translation in this paper. It is a novel model that supports the multi-domain image-to-image translation using one single model. Conditioning on the target domain label, an auto-encoder-like network with multiple attention connections is trained to translate the input image into the target domain. The attention connections better shuttle the low-level information in the encoder to the decoder, which helps to preserve the structure. A multi-level attention mechanism is also designed in the proposed model to further improve the performance of our model. More specially, the feature maps in the encoder are first squeezed by average pooling and used to output a channel-wise attention mask. The attention mask softly determines which channels of the feature maps are translated and which channels are kept. By enforcing the model to learn a cyclic domain transformation during training, our model does not require paired training data, which greatly improves the versatility to different kinds of data. We experimentally demonstrated the effectiveness of our proposed model on the facial and clothing image translation tasks. The extensive ablations are also conducted to further validate the contribution of the proposed attention module used in our model.", "pages": "95751-95761", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Sun, Song", "Zhao, Bo", "Chen, Xin", "Mateen, Muhammad", "Wen, Junhao"]}]["8771231", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2019.2930707", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Three-dimensional displays", "Two dimensional displays", "Measurement", "Image quality", "Feature extraction", "Task analysis", "Solid modeling", "No-reference image quality assessment", "stereoscopic image quality assessment", "Siamese convolutional neural networks", "learning to rank"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "Siamese-Network-Based Learning to Rank for No-Reference 2D and 3D Image Quality Assessment", "url": "", "volume": "7", "year": "2019", "abstract": "2D image quality assessment (IQA) and stereoscopic 3D IQA are considered as two different tasks in the literature. In this paper, we present an index for both no-reference 2D and 3D IQA. We propose to transform the IQA task into a task of quality comparison between images. By generating image pairs, the amount of training data reaches the square of the original amount of data, effectively solving the lacking of training samples. We also propose a learning to rank model using Siamese convolutional neural networks (LRSN) for quality comparison. The presented LRSN has two branches that have the same structure, share weights with each other, and take two image patches as inputs. The goal of LRSN is learning to rank the quality scores between the two input image patches. The relative quality score of a test image is obtained by first comparing its image patches with many image patches of other images and counts the number of times that its image patches are ranked superior to other patches. The experimental results on three 2D (LIVE, CSIQ, and TID2013) and three 3D (LIVE 3D Phase-I, LIVE 3D Phase-II, and NBU) IQA databases demonstrate that the proposed LRSN model works well for both 2D and 3D IQA and outperforms the state-of-the-art no-reference 2D and 3D IQA metrics.", "pages": "101583-101595", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Niu, Yuzhen", "Huang, Dong", "Shi, Yiqing", "Ke, Xiao"]}]["9186113", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2020.3021531", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Face recognition", "Gallium nitride", "Generators", "Feature extraction", "Emotion recognition", "Databases", "Generative adversarial networks", "Facial expression recognition", "data enhancement", "generative adversarial networks", "self-attention"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "LAUN Improved StarGAN for Facial Emotion Recognition", "url": "", "volume": "8", "year": "2020", "abstract": "In the field of facial expression recognition, deep learning is extensively used. However, insufficient and unbalanced facial training data in available public databases is a major challenge for improving the expression recognition rate. Generative Adversarial Networks (GANs) can produce more one-to-one faces with different expressions, which can be used to enhance databases. StarGAN can perform one-to-many translations for multiple expressions. Compared with original GANs, StarGAN can increase the efficiency of sample generation. Nevertheless, there are some defects in essential areas of the generated face, such as the mouth and the fuzzy side face image generation. To address these limitations, we improved StarGAN to alleviate the defects of images generation by modifying the reconstruction loss and adding the Contextual loss. Meanwhile, we added the Attention U-Net to StarGAN's generator, replacing StarGAN's original generator. Therefore, we proposed the Contextual loss and Attention U-Net (LAUN) improved StarGAN. The U-shape structure and skip connection in Attention U-Net can effectively integrate the details and semantic features of images. The network's attention structure can pay attention to the essential areas of the human face. The experimental results demonstrate that the improved model can alleviate some flaws in the face generated by the original StarGAN. Therefore, it can generate person images with better quality with different poses and expressions. The experiments were conducted on the Karolinska Directed Emotional Faces database, and the accuracy of facial expression recognition is 95.97%, 2.19% higher than that by using StarGAN. Meanwhile, the experiments were carried out on the MMI Facial Expression Database, and the accuracy of expression is 98.30%, 1.21% higher than that by using StarGAN. Moreover, experiment results have better performance based on the LAUN improved StarGAN enhanced databases than those without enhancement.", "pages": "161509-161518", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Wang, Xiaohua", "Gong, Jianqiao", "Hu, Min", "Gu, Yu", "Ren, Fuji"]}]["8320955", {"address": "", "articleno": "", "doi": "10.1109/TR.2018.2809866", "issn": "1558-1721", "issue_date": "", "journal": "IEEE Transactions on Reliability", "keywords": ["Testing", "Erbium", "Business", "Big Data", "Data models", "Information and communication technology", "Software", "Early testing", "entity reconciliation", "heterogeneous data sources", "model-driven engineering", "software testing", "specification-based testing"], "month": "June", "number": "2", "numpages": "", "publisher": "", "title": "Early Integration Testing for Entity Reconciliation in the Context of Heterogeneous Data Sources", "url": "", "volume": "67", "year": "2018", "abstract": "Entity reconciliation (ER) aims to combine data from different sources for a unified vision. The management of large volumes of data has given rise to significant challenges to the ER problem due to facts such as data becoming more unstructured, unclean, and incomplete or the existence of many datasets that store information about the same topic. Testing the applications that implement the ER problem is crucial to ensure both the correctness of the reconciliation process and the quality of the reconciled data. This paper presents an approach based on model-driven engineering that allows the creation of test models for the early integration testing of ER applications, contributing in three main aspects: the description of the elements of the proposed framework, the definition of the testing model, and the validation of the proposal through two real-world case studies. This validation verifies that the early integration testing of the ER application is capable of detecting a series of deficiencies, which a priori are not known and that will help to improve the final result that the ER application offers.", "pages": "538-556", "note": "", "ISSN": "1558-1721", "publicationtype": "article", "author": ["Blanco, Raquel", "Enr\u00edquez, Jos\u00e9", "Dom\u00ednguez-Mayo, Francisco", "Escalona, M.", "Tuya, Javier"]}]["8869760", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2019.2947460", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Servers", "Streaming media", "Internet", "Adaptive systems", "Quality of service", "Reinforcement learning", "Recurrent neural networks", "Content management", "deep reinforcement learning", "quality of service", "content delivery network"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "NA-Caching: An Adaptive Content Management Approach Based on Deep Reinforcement Learning", "url": "", "volume": "7", "year": "2019", "abstract": "Video streaming is a dominant application over today\u2019s Internet. The current mainstream video streaming solution is to utilize the services of a Content Delivery Network (CDN) provider. By replicating video content closer to the network edge, caching provides an effective mechanism for alleviating the demand for massive bandwidth for the Internet backbone. It reduces the network traffic and capital expense for streaming the video content, and in the meantime, enhance Internet\u2019s Quality of Service (QoS). In this paper, we propose a neural adaptive caching approach, named NA-Caching, for helping cache learn to make caching decisions from its own experiences rather than a specific mathematical model, in a way similar to how a human being learns a new skill (e.g. cycling, swimming). NA-Caching leverages the benefits of the Recurrent Neural Network (RNN) as well as the Deep Reinforcement Learning (DRL) to maximize the cache efficiency by jointly learning request features, caching space dynamics and making decisions. Specifically, we utilize Gated Recurrent Unit (GRU) to characterize the evolving features of the dynamic requests and caching space. Moreover, the above GRU-based representation network is integrated into a Deep Q-Network (DQN) framework for making adaptive caching decisions online. To evaluate the performance of the proposed approach, we conduct extensive experiments on anonymized real-world traces from a video provider. The results demonstrate that our algorithm significantly outperform several candidate methods.", "pages": "152014-152022", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Fan, Qilin", "Li, Xiuhua", "Wang, Sen", "Fu, Shu", "Zhang, Xu", "Wang, Yueyang"]}]["9082679", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2020.2990751", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Predictive models", "Correlation", "Transportation", "Load modeling", "Forecasting", "Data models", "Industries", "Freight transportation", "truckload spot rates", "lagged weighted matrix", "short-term prediction", "weighted multiple regression", "trucking economy"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "Short-Term Truckload Spot Rates\u2019 Prediction in Consideration of Temporal and Between-Route Correlations", "url": "", "volume": "8", "year": "2020", "abstract": "Truckload spot rate (TSR), defined as a price offered on the spot to transport a certain cargo by using an entire truck on a target transportation line, usually price per kilometer-ton, is a key factor in shaping the freight market. In particular, the prediction of short-term TSR is of great importance to the daily operations of the trucking industry. However, existing predictive practices have been limited largely by the availability of multilateral information, such as detailed intraday TSR information. Fortunately, the emerging online freight exchange (OFEX) platforms provide unique opportunities to access and fuse more data for probing the trucking industry. As such, this paper aims to leverage the high-resolution trucking data from an OFEX platform to forecast short-term TSR. Specifically, a lagged coefficient weighted matrix-based multiple linear regression modeling (Lag-WMR) is proposed, and exogenous variables are selected by the light gradient boosting (LGB) method. This model simultaneously incorporates the dependency between historical and current TSR (temporal correlation) and correlations between the rates on alternative routes (between-route correlation). In addition, the effects of incorporating temporal and between-route correlations, time-lagged correlation and exogenous variable selection in modeling are emphasized and assessed through a case study on short-term TSR in Southwest China. The comparative results show that the proposed Lag-WMR model outperforms autoregressive integrated moving average (ARIMA) model and LGB in terms of model fitting and the quality and stability of predictions. Further research could focus on rates' standardization, to define a practical freight index for the trucking industry. Although our results are specific to the Chinese trucking market, the method of analysis serves as a general model for similar international studies.", "pages": "81173-81189", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Xiao, Wei", "Xu, Chuan", "Liu, Hongling", "Yang, Hong", "Liu, Xiaobo"]}]["8288619", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2018.2804623", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Real-time systems", "Probabilistic logic", "Uncertainty", "Data mining", "Meteorology", "Bayes methods", "Data analysis", "Complex event processing", "data analysis", "internet of things", "real-time systems", "intelligent transportation systems"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "Real-Time Probabilistic Data Fusion for Large-Scale IoT Applications", "url": "", "volume": "6", "year": "2018", "abstract": "Internet of Things (IoT) data analytics is underpinning numerous applications, however, the task is still challenging predominantly due to heterogeneous IoT data streams, unreliable networks, and ever increasing size of the data. In this context, we propose a two-layer architecture for analyzing IoT data. The first layer provides a generic interface using a service oriented gateway to ingest data from multiple interfaces and IoT systems, store it in a scalable manner and analyze it in real-time to extract high-level events; whereas second layer is responsible for probabilistic fusion of these high-level events. In the second layer, we extend state-of-the-art event processing using Bayesian networks in order to take uncertainty into account while detecting complex events. We implement our proposed solution using open source components optimized for large-scale applications. We demonstrate our solution on real-world use-case in the domain of intelligent transportation system where we analyzed traffic, weather, and social media data streams from Madrid city in order to predict probability of congestion in real-time. The performance of the system is evaluated qualitatively using a web-interface where traffic administrators can provide the feedback about the quality of predictions and quantitatively using F-measure with an accuracy of over 80%.", "pages": "10015-10027", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Akbar, Adnan", "Kousiouris, George", "Pervaiz, Haris", "Sancho, Juan", "Ta-Shma, Paula", "Carrez, Francois", "Moessner, Klaus"]}]["9729149", {"address": "", "articleno": "", "doi": "10.23919/JSEE.2022.000016", "issn": "1004-4132", "issue_date": "", "journal": "Journal of Systems Engineering and Electronics", "keywords": ["Sociology", "Evolutionary computation", "Benchmark testing", "Systems engineering and theory", "Resource management", "Statistics", "Computational complexity", "many-objective optimization", "\u03b5-domination", "boundary protection strategy", "two-archive algorithm"], "month": "February", "number": "1", "numpages": "", "publisher": "", "title": "An \u03b5-Domination based Two-Archive 2 Algorithm for Many-Objective Optimization", "url": "", "volume": "33", "year": "2022", "abstract": "The two-archive 2 algorithm (Two_Arch2) is a many-objective evolutionary algorithm for balancing the convergence, diversity, and complexity using diversity archive (DA) and convergence archive (CA). However, the individuals in DA are selected based on the traditional Pareto dominance which decreases the selection pressure in the high-dimensional problems. The traditional algorithm even cannot converge due to the weak selection pressure. Meanwhile, Two_Arch2 adopts DA as the output of the algorithm which is hard to maintain diversity and coverage of the final solutions synchronously and increase the complexity of the algorithm. To increase the evolutionary pressure of the algorithm and improve distribution and convergence of the final solutions, an \u03b5-domination based Two_Arch2 algorithm (\u03b5-Two_Arch2) for many-objective problems (MaOPs) is proposed in this paper. In \u03b5- Two_Arch2, to decrease the computational complexity and speed up the convergence, a novel evolutionary framework with a fast update strategy is proposed; to increase the selection pressure, \u03b5-domination is assigned to update the individuals in DA; to guarantee the uniform distribution of the solution, a boundary protection strategy based on I \u03b5+ indicator is designated as two steps selection strategies to update individuals in CA. To evaluate the performance of the proposed algorithm, a series of benchmark functions with different numbers of objectives is solved. The results demonstrate that the proposed method is competitive with the state-of-the-art multi-objective evolutionary algorithms and the efficiency of the algorithm is significantly improved compared with Two_Arch2.", "pages": "156-169", "note": "", "ISSN": "1004-4132", "publicationtype": "article", "author": ["Wu, Tianwei", "An, Siguang", "Han, Jianqiang", "Shentu, Nanying"]}]["8434286", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2018.2865177", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Quality of experience", "Androids", "Humanoid robots", "Performance evaluation", "Smart phones", "Resource management", "Smart devices", "App scheduling", "low-end device", "mobile device", "Android"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "Extend Capability of Low-End Android Devices by Scheduling Apps Between Local and Cloud", "url": "", "volume": "6", "year": "2018", "abstract": "In this paper, we propose a multi-layer mobile application (app) scheduling method to extend the capability of low-end Android devices. With a quantitative analysis, we find that the increase of installed apps will negatively affect quality of experience (QoE) of the user, e.g., the action response time of a mobile device, by producing more periodically or irregularly background tasks. On the other hand, the user tends to install more apps than needed in case they could play a role someday, as indicated by the consumer app usage statistics. When the storage is running out, being forced to choose an app to uninstall due to space budget is a painful experience for the user. This contradiction is intensified for low-end devices due to limited resources. We try to reduce this dilemma by a multi-layer app scheduling (MAS) schema, along with a cloud service. For the first layer, we utilize the \u201cfreeze\u201dfeature of Android to prevent non-essential background activities. For the second layer, it is a network scheduler, which automatically schedules the available apps, together with their data, between local and cloud according to user's personal policy generated by big data analysis. By dynamically scheduling the apps among three states, QoE of a low-end Android device is improved. At the same time, with the help of an app state recovery mechanism, the user can directly access a large number of apps provided by the cloud with consistent app view. Experimental results on a low-end smartphone, i.e., Samsung Galaxy ON5, and a smart watch based on Newton2_Plus wearable development board show the benefits of the proposed MAS schema.", "pages": "45740-45754", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Li, Shaoyong", "Luo, Lei", "Liu, Yaping", "Zhang, Yaoxue"]}]["8477007", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2018.2872351", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Quality of service", "Cloud computing", "Tensile stress", "Measurement", "Recommender systems", "Scalability", "Big Data", "Big data", "FDAsy-SGD", "recommendation", "spatial", "temporal", "tensor factorization"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "Spatial-Temporal Aware Intelligent Service Recommendation Method Based on Distributed Tensor factorization for Big Data Applications", "url": "", "volume": "6", "year": "2018", "abstract": "With the dramatic growth of public cloud offerings and heterogeneous data information, how to discover potentially valuable information from big history behavior data and design intelligent recommendation techniques has become more and more important. Due to the dynamics of cloud environment, both user behaviors and QoS (Quality of Service) performance of cloud services are sensitive to contextual information, such as time and location. However, the consideration of time and location information brings the increase in the order of rating matrix and the data sparsity problem. In view of these challenges, we propose a spatial-temporal aware intelligent service recommendation method based on distributed tensor factorization to address the above problems. First, the time and location information are introduced into the recommendation models by distinguishing time-sensitive QoS metrics and region-sensitive QoS metrics from stable QoS metrics. To deal with the sparse rating data, time slots and regions are clustered respectively. Then, a high-order tensor factorization technique is applied to mine the latent factors among users, services, time information, and location information. Moreover, to improve the scalability of our recommendation models in big data environment, a fast distributed asynchronous SGD (Stochastic Gradient Descent) mechanism is employed to get a good balance between the convergence speed and prediction accuracy. Finally, experiments based on both real-world data set and big synthetic data set are conducted to validate the effectiveness and scalability of our proposal. The experimental results show that our proposal achieves a good balance between the recommendation accuracy and scalability.", "pages": "59462-59474", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Meng, Shunmei", "Wang, Huihui", "Li, Qianmu", "Luo, Yun", "Dou, Wanchun", "Wan, Shaohua"]}]["9026896", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2020.2978896", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Recommender systems", "Servers", "Edge computing", "Distributed databases", "Quality of service", "Cloud computing", "Convergence", "Recommender systems", "edge computing", "the IoT", "intelligent service"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "Convergence of Recommender Systems and Edge Computing: A Comprehensive Survey", "url": "", "volume": "8", "year": "2020", "abstract": "Under the explosive growth of information available on the Web, recommender systems have been used as an effective technology to filter useless information and attempt to recommend the most useful items. The proliferation of smart phones, smart wearable devices and other Internet of Thing (IoT) devices has gradually driven many novel emerging services which are latency-sensitive and computation-intensive with a higher quality-of-service. Under such circumstances, the data sources contain four key characteristics (i.e., sparsity, heterogeneity, mobility, volatility). The conventional recommender systems based on cloud computing are incapable of digging the information of user demands. Mobile edge computing is a novel computing paradigm via pushing computation/storage resource from the remote cloud servers to the network edge servers to provide more intelligent and personalized service. This paper comprehensively reviews the state of the art literature on the convergence of recommender systems and edge computing, and identify the future directions along this dimension. This paper can provide an array of new perspectives on the convergence for researchers, practitioners, and tap into the richness of this interdisciplinary research area.", "pages": "47118-47132", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Sun, Chuan", "Li, Hui", "Li, Xiuhua", "Wen, Junhao", "Xiong, Qingyu", "Zhou, Wei"]}]["7762116", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2016.2628820", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["5G mobile comunication", "Big data", "Green communication", "Internet of Things", "Wireless communication", "Smart devices", "Channel state estimation", "5G IoT", "channel sounding", "wireless big data", "active estimation", "green communication"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "Smart Channel Sounder for 5G IoT: From Wireless Big Data to Active Communication", "url": "", "volume": "4", "year": "2016", "abstract": "Internet-of-Things (IoT) will connect billions of smart devices and generate inundant data through prominent solutions, such as machine type communication. The Third Generation Partnership Project has launched the corresponding standards for multiple heterogeneous wireless smart devices in the long term evolution (LTE)/LTE-advanced. In the forthcoming years, the valuable information hidden in the deluge of data will be extracted and utilized in every field to improve quality and efficiency. However, the bottleneck of realizing this magnificent vista of future intelligent lives lies in how to satisfy the practical demands to transmit huge data volume through efficient wireless communication in diverse scenarios. Herein, multi-scenario wireless communication triggers critical problems in wireless channel modeling and soundings for 5G IoT, which by far, are understudied. In this paper, we introduce a general wireless channel model and its multiple up-to-date corresponding channel sounding methods for future 5G IoT green wireless communication. Through adopting the perspective of wireless big data excavation, the smart channel sounder transforms the traditional passive wireless communication scheme into an active expectation-guaranteed wireless communication scheme, which helps achieve efficient and green communication. To demonstrate the validity and efficiency of this smart sounder scheme, we make a compatible prototype testified in multiple scenarios. The multiple real-scenario experiments demonstrate that the smart sounder can function effectively, especially in those scenarios where traditional channel state information is not available or imperfect.", "pages": "8888-8899", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Chen, Xuhong", "Liu, Shanyun", "Lu, Jiaxun", "Fan, Pingyi", "Letaief, Khaled"]}]["8691437", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2019.2909548", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Quality of service", "Web services", "Prediction algorithms", "Clustering algorithms", "Collaboration", "Predictive models", "QoS", "service computing", "tensor decomposition"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "Personalized Web Service Recommendation Based on QoS Prediction and Hierarchical Tensor Decomposition", "url": "", "volume": "7", "year": "2019", "abstract": "Web service recommendation based on the quality of service (QoS) is important for users to find the exact Web service among many functionally similar Web services. Although service recommendations have been recently studied, the performance of the existing ones is unsatisfactory because: 1) the current QoS predicting algorithms still experience data sparsity and cannot predict the QoS values accurately and 2) the previous approaches fail to consider the QoS variance according to the users and services' locations carefully. A Web service recommendation method based on the QoS prediction and hierarchical tensor decomposition is proposed in this paper. The method is called QoSHTD that is based on location clustering and hierarchical tensor decomposition. First, the users and services of the QoSHTD cluster into several local groups based on their location and models local and global triadic tensors for the user-service-time relationship. The hierarchical tensor decomposition is then performed on the local and global triadic tensors. Finally, the predicted QoS value through local and global tensor decomposition is combined as the missing QoS values. The comprehensive experiment shows that the proposed method achieves a high prediction accuracy and recommending quality of Web service, and can partially address data sparsity.", "pages": "62221-62230", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Cheng, Tian", "Wen, Junhao", "Xiong, Qingyu", "Zeng, Jun", "Zhou, Wei", "Cai, Xueyuan"]}]["8392677", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2018.2849820", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Air quality", "Atmospheric modeling", "Predictive models", "Neural networks", "Wind forecasting", "Lung", "Data mining", "Dynamic time warping(DTW)", "convolutional neural network(CNN)", "long-short-term memory(LSTM)", "spatio-temporal analysis", "big data", "air quality forecast"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "Adaptive Deep Learning-Based Air Quality Prediction Model Using the Most Relevant Spatial-Temporal Relations", "url": "", "volume": "6", "year": "2018", "abstract": "Air pollution has become an extremely serious problem, with particulate matter having a significantly greater impact on human health than other contaminants. The small diameter of fine particulate matter (PM2.5) allows it to penetrate deep into the alveoli as far as the bronchioles, interfering with a gas exchange within the lungs. Long-term exposure to particulate matter has been shown to cause the cardiovascular disease, respiratory disease, and increase the risk of lung cancers. Therefore, forecasting air quality has also become important to help guide individual actions. This paper aims to forecast air quality for up to 48 h using a combination of multiple neural networks, including an artificial neural network, a convolutional neural network, and a long-short-term memory to extract spatial-temporal relations. The proposed predictive model considers various meteorology data from the previous few hours as well as information related to the elevation space to extract terrain impact on air quality. The model includes trends from multiple locations, extracted from correlations between adjacent locations, and among similar locations in the temporal domain. Experiments employing Taiwan and Beijing data sets show that the proposed model achieves excellent performance and outperforms current state-of-the-art methods.", "pages": "38186-38199", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Soh, Ping-Wei", "Chang, Jia-Wei", "Huang, Jen-Wei"]}]["8515201", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2018.2878739", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Measurement", "Visualization", "Image quality", "Computational modeling", "Mathematical model", "Indexes", "Adaptation models", "Full reference", "image quality assessment", "local contrast", "summation of deviation-based pooling strategy", "visual saliency"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "Contrast and Visual Saliency Similarity-Induced Index for Assessing Image Quality", "url": "", "volume": "6", "year": "2018", "abstract": "Image quality that is consistent with human opinion is assessed by a perceptual image quality assessment (IQA) that defines/utilizes a computational model. A good model should take effectiveness and efficiency into consideration, but most of the previously proposed IQA models do not simultaneously consider these factors. Therefore, this paper attempts to develop an effective and efficient IQA metric. Contrast is an inherent visual attribute that indicates image quality, and visual saliency (VS) is a quality that attracts the attention of human beings. The proposed model utilized these two features to characterize the image quality. After obtaining the local contrast quality map and the global VS quality map, we added the weighted standard deviation of the previous two quality maps together to yield the final quality score. The experimental results for three benchmark databases (LIVE, TID2008, and CSIQ) demonstrated that our model performs the best in terms of a correlation with the human judgment of visual quality. Furthermore, compared with competing IQA models, this proposed model is more efficient. The MATLAB source code of the proposed method is public available online at http://www.scholat.com/vpost.html?pid=98172..", "pages": "65885-65893", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Jia, Huizhen", "Zhang, Lu", "Wang, Tonghan"]}]["8835021", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2019.2941022", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Collaboration", "Engineering profession", "Social networking (online)", "XML", "Libraries", "Big Data", "Scientific collaboration", "career age", "collaborator recommendation"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "Career Age-Aware Scientific Collaborator Recommendation in Scholarly Big Data", "url": "", "volume": "7", "year": "2019", "abstract": "Seeking a collaborator is one of the important academic activities of scholars because the right collaborators will help improve the quality of scholars\u2019 research and accelerate their research process. Therefore, it is becoming more and more important to recommend scientific collaborators based on big scholarly data. However, previous works mainly consider the research topic as the key academic factor, whereas many scholars\u2019 demographic characteristics such as career age, gender, etc are overlooked. It has been studied that scientific collaboration patterns may vary with scholars\u2019 career ages. It is not surprising that scholars at different career ages may have different collaboration strategies. To this end, we aim to design a scientific collaboration recommendation model that is sensitive to scholars\u2019 career age. For this purpose, we design a career age-aware scientific collaboration model. The model is mainly consisted of three parts, including authorship extraction from the digital libraries, topic extraction based on publication titles/abstract, and career age-aware random walk for measuring scholar similarity. Experimental results on two real-world datasets demonstrate that our proposed model can achieve the best performance by comparison with six baseline methods in terms of precision and recall.", "pages": "136036-136045", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Sun, Na", "Lu, Yong", "Cao, Yongcun"]}]["8421568", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2018.2860900", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Task analysis", "Sensors", "Social network services", "Computer architecture", "Monitoring", "Computational modeling", "Crowdsourcing", "Influence diffusion", "crowdsensing", "incentive mechanism", "social network"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "Improving Both Quantity and Quality: Incentive Mechanism for Social Mobile Crowdsensing Architecture", "url": "", "volume": "6", "year": "2018", "abstract": "Mobile crowdsensing has emerged as an efficient paradigm for performing large-scale sensing tasks. Improving both the quantity and quality of users is still the pivotal problem for mobile crowdsensing system. This paper gives a comprehensive solution to improve the quantity and quality of users simultaneously through the social mobile crowdsensing architecture. To incentive the users based on the novel architecture, we first propose a universal initial diffuser selection algorithm to accommodate two widely studied diffusion models and, then a lightweight, multi-metric comprehensive, and parameter-free user quality evaluation method is presented. Finally, we propose a reverse auction to optimize the new criterion, which takes both social cost and user quality into consideration. Through both rigorous theoretical analysis and extensive simulations, we demonstrate that the proposed incentive mechanisms achieve computational efficiency, individual rationality, truthfulness, and guaranteed approximation. Meanwhile, the proposed incentive mechanisms show prominent advantage in total unit quality cost and running time.", "pages": "44992-45003", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Xu, Jia", "Bao, Weiwei", "Gu, Huayue", "Xu, Lijie", "Jiang, Guoping"]}]["8259243", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2018.2791469", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Cognitive systems", "Big Data", "Cognition", "Cloud computing", "Cognitive science", "Cyberspace", "Human computer interaction", "Cognitive computing", "big data analysis", "Internet of Things", "cloud computing"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "Cognitive Computing: Architecture, Technologies and Intelligent Applications", "url": "", "volume": "6", "year": "2018", "abstract": "With the development of network-enabled sensors and artificial intelligence algorithms, various human-centered smart systems are proposed to provide services with higher quality, such as smart healthcare, affective interaction, and autonomous driving. Considering cognitive computing is an indispensable technology to develop these smart systems, this paper proposes human-centered computing assisted by cognitive computing and cloud computing. First, we provide a comprehensive investigation of cognitive computing, including its evolution from knowledge discovery, cognitive science, and big data. Then, the system architecture of cognitive computing is proposed, which consists of three critical technologies, i.e., networking (e.g., Internet of Things), analytics (e.g., reinforcement learning and deep learning), and cloud computing. Finally, it describes the representative applications of human-centered cognitive computing, including robot technology, emotional communication system, and medical cognitive system.", "pages": "19774-19783", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Chen, Min", "Herrera, Francisco", "Hwang, Kai"]}]["8778714", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2019.2931579", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Quality assessment", "Natural languages", "Data models", "Software algorithms", "Software", "Task analysis", "Information retrieval", "Code comment", "deep learning", "information retrieval", "machine learning", "program annotation"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "A Survey of Automatic Generation of Source Code Comments: Algorithms and Techniques", "url": "", "volume": "7", "year": "2019", "abstract": "As an integral part of source code files, code comments help improve program readability and comprehension. However, developers sometimes do not comment their program code adequately due to the incurred extra efforts, lack of relevant knowledge, unawareness of the importance of code commenting or some other factors. As a result, code comments can be inadequate, absent or even mismatched with source code, which affects the understanding, reusing and the maintenance of software. To solve these problems of code comments, researchers have been concerned with generating code comments automatically. In this work, we aim at conducting a survey of automatic code commenting researches. First, we generally analyze the challenges and research framework of automatic generation of program comments. Second, we present the classification of representative algorithms, the design principles, strengths and weaknesses of each category of algorithms. Meanwhile, we also provide an overview of the quality assessment of the generated comments. Finally, we summarize some future directions for advancing the techniques of automatic generation of code comments and the quality assessment of comments.", "pages": "111411-111428", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Song, Xiaotao", "Sun, Hailong", "Wang, Xu", "Yan, Jiafei"]}]["9056512", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2020.2985501", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["VLIW", "Huffman coding", "Image coding", "Streaming media", "Parallel processing", "Decoding", "Stream processor", "Huffman coding", "horizontal compression", "vertical compression", "compression ratio"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "An Efficient and Fast VLIW Compression Scheme for Stream Processor", "url": "", "volume": "8", "year": "2020", "abstract": "Stream processor has been widely used in multimedia processing because of the high performance gained by parallelism. In order to achieve higher parallelism, the stream processor employs large width structure of VLIW (very long instruction word, VLIW) and multiple parallelizable instructions are organized into one VLIW. Because the width of VLIW is fixed, there are a large number of empty operations (non-operation, NOP) filled in VLIW, which results in serious code size expansion problem. Aiming at this issue, the horizontal code compression and vertical code compression methods are applied on the VLIW of stream processor respectively. First the VLIW is divided into several subfields according to the logic characteristics of VLIW instruction, then the horizontal code compression scheme which based on Huffman coding is applied on each subfield and this method can achieve approximately 78% code size reduction on average. However, the extra-long time required to decode the compressed VLIW before instruction execution may cause system performance penalty. In order to reduce the decompression time consumption, the vertical compression scheme is proposed. The vertical compression can reduce the code size nearly 70% by deleting the NOPs of VLIW in vertical direction. Furthermore the VLIW after vertical compression can be executed directly without decompression operation by using banked instruction memory. Specifically, the vertical compression can compress stream processor VLIW code size significantly and without any negative influence on performance.", "pages": "224817-224824", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Li, Gongli", "Hou, Yingying", "Zhu, Junzhe"]}]["9295320", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2020.3043459", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Training data", "Reliability", "Semisupervised learning", "Feature extraction", "Information retrieval", "Training", "Supervised learning", "Transductive learning", "query quality", "retrieval performance", "learning to rank"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "Improving Query Quality for Transductive Learning in Learning to Rank", "url": "", "volume": "8", "year": "2020", "abstract": "In traditional transductive learning, all queries are used in learning to rank in order to generate pseudo-labels when sufficient training data are not available. However, low quality queries may affect retrieval performance in transductive learning. We thus think that it is important to improve the quality of queries in transductive learning to train an effective ranking model. By using a small number of reliable samples and data close to the boundaries of classification, we propose building a query quality estimator by establishing a relationship between the benefits of good retrieval performance and features of the normalized query commitment that influence query quality. In our proposed transduction model, all queries available are filtered by the proposed query quality estimator and only high quality queries that enhance the effectiveness of retrieval such that they yield performance-related benefits, are used to generate pseudo-labels for learning to rank. Queries that can degrade performance benefits are discarded while creating the pseudo-labels. Pseudo-labels aggregated by high quality queries in transductive learning are then leveraged in learning to rank scenarios without sufficient training data. The results of extensive experiments on the standard LETOR 4.0 dataset showed that our proposed method can outperform strong baselines and the average normalized discounted cumulative gain is enhanced up to 7.77% in some case.", "pages": "226188-226198", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Zhang, Xin", "Cheng, Zhi"]}]["8552406", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2018.2884022", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Education", "Performance evaluation", "Big Data", "Data models", "Computer architecture", "Sensors", "Internet of Things", "Smart campus", "teaching performance evaluation", "educational big Data", "analytic hierarchy process", "grey correlation degree", "TOPSIS algorithm"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "Teaching Performance Evaluation in Smart Campus", "url": "", "volume": "6", "year": "2018", "abstract": "With the rapid development of modern teaching technology, the construction of smart campus has become the focus of modern college education reform. The application of technologies, such as the Internet of Things and big data, plays an important role in improving the teaching environment of colleges and universities, improving the utilization of teaching resources, and the flexibility of education. As an important part of campus activities, teaching performance evaluation scientifically and effectively utilizes teaching information and teacher and student interaction information to evaluate teachers' teaching performance, which helps to motivate teachers' work enthusiasm, improve teaching quality, and enhance school core competitiveness. This paper analyzes the salient features of smart campus from the perspectives of technology, business, and construction mode, and proposes a smart campus architecture model. According to the research content of teaching performance evaluation, the framework model of smart campus education data collection and storage platform is established, which provides a reference model for the construction of smart campus in colleges and universities. The evaluation of teaching performance in smart campus first analyzes the shortcomings of traditional evaluation methods and proposes the necessity of combining teaching performance evaluation with modern technology. Second, six principal components were determined using the PCA algorithm. Then, use the AHP to calculate the weights of each layer of the indicator set, avoiding the decision errors caused by subjective factors. Finally, the gray correlation degree is used to improve the TOPSIS algorithm for multi-objective decision analysis. The evaluation results of the AHP-TOPSIS teaching performance model are consistent with the actual situation. The application of the smart campus education data platform combined with the AHP and the gray correlation improvement TOPSIS algorithm is more targeted to the teacher's teaching performance evaluation and provides a new evaluation method for scientific performance evaluation, and avoid the problem of strong subjectivity of traditional teaching performance evaluation.", "pages": "77754-77766", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Xu, Xin", "Wang, Yunsheng", "Yu, Shujiang"]}]["9229062", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2020.3031963", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Bandwidth", "Long Term Evolution", "Urban areas", "Area measurement", "Sociology", "Statistics", "Resource management", "4G mobile communication", "radio spectrum management", "crowdsourcing"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "How Does Spectrum Affect Mobile Network Deployments? Empirical Analysis Using Crowdsourced Big Data", "url": "", "volume": "8", "year": "2020", "abstract": "Despite the growing trend towards the use of big data methodologies, there is still limited application of such techniques to understand how spectrum is used in mobile networks. In this paper we analyse how low (<; 1 GHz) and high (>1 GHz) frequency spectrum is used in 4G networks in urban areas, in relation to eNodeB density, available bandwidth, Reference Signal Received Power (RSRP) and Reference Signal Received Quality (RSRQ). We present a method to analyse the strategies used by Mobile Network Operators (MNOs) to deal with traffic congestion, and the degree to which they must densify their networks depending on their spectrum portfolio. Using crowdsourced data from 2017 from a popular mobile app, we apply this method to Greater London. We find that the fraction of sites that fully use all available bands to the MNO range from 2% to 20%. Additionally, MNOs with large bandwidth use 42% fewer sites on average in dense urban environments. This difference decreases in suburban areas to 23% fewer sites. The lowest frequencies in each eNodeB tend to exhibit lower RSRP values, as they are often used to serve cell-edge users. These frequencies also show lower RSRQ values because of higher interference caused by neighbouring cells. Similarly, large (high frequency) bandwidth improves RSRQ as it allows for fewer users per MHz, which reduces interference and enables larger cell sizes. We conclude that in dense urban environments, the available bandwidth, rather than propagation properties, determines the preferred band for network deployment by MNOs.", "pages": "190812-190821", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Frias, Zoraida", "Mendo, Luis", "Oughton, Edward"]}]["7974743", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2017.2725984", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Air quality", "Roads", "Pollution", "Monitoring", "Tools", "Sensors", "Urban areas", "Air quality management", "crowd-sourcing", "Dijkstra algorithm", "mobile sensors", "pollution prediction", "traffic regulation"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "An Agent Based Traffic Regulation System for the Roadside Air Quality Control", "url": "", "volume": "5", "year": "2017", "abstract": "This paper describes an on-road air quality monitoring and control approach by proposing an agent-based system for modeling the urban road network infrastructure, establishing the real-time and predicted air pollution indexes in different road segments and generating recommendations and regulation proposals for road users. This can help by reducing vehicle emissions in the most polluted road sections, optimizing the pollution levels while maximizing the vehicle flow. For this, we use data sets gathered from a set of air quality monitoring stations, embedded low-cost e-participatory pollution sensors, contextual data, and the road network available data. These data are used in the air quality indexes calculation and then the generation of a dynamic traffic network. This network is represented by a weighted graph in which the edges weights evolve according to the pollution indexes. In this paper, we propose to combine the benefits of agent technology with both machine learning and big data tools. An artificial neural networks model and the Dijkstra algorithm are used for air quality prediction and the least polluted path finding in the road network. All data processing tasks are performed over a Hadoop-based framework: HBase and MapReduce.", "pages": "13192-13201", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["El, Abdelaziz", "Benslimane, Djamal", "Sadiq, Abderrahmane", "Ouarzazi, Jamal", "Sadgal, Mohamed"]}]["9502731", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2021.3101469", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Education", "Prediction algorithms", "Matrix decomposition", "Correlation", "Sparse matrices", "Bipartite graph", "Data models", "LFM", "PersonalRank", "PRLFM", "teacher recommendation", "teaching quality"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "A Course Teacher Recommendation Algorithm Based on Improved Latent Factor Model and PersonalRank", "url": "", "volume": "9", "year": "2021", "abstract": "To scientifically and accurately recommend suitable teachers for university courses and improve teaching quality, designing an effective recommendation algorithm is necessary. Therefore, we construct quantitative models of teacher characteristics, course characteristics, and teaching evaluations under the theories and methods of education and build a sparse experimental data matrix based on the quantified data. On this basis, we propose a teacher recommendation algorithm (PRLFM) based on the improved latent factor model (LFM) and the improved PersonalRank algorithm. Firstly, the improved LFM is used to predict the evaluation scores of those courses that teachers have not taught. The scores which are higher than the specified threshold are used to fill the corresponding missing items in the sparse matrix to reduce the matrix's sparsity. Then, the bipartite graph model based on the teacher set and course set is constructed according to the filled experimental data matrix. The weight of edges in the bipartite graph is replaced by the teacher and course's evaluation score multiplied by the course difficulty, which reflects the correlation between course and evaluation score. Next, an improved probability transition matrix based on the bipartite graph is constructed. The access probability in the matrix is replaced by the node's out degree's reciprocal multiplied by the edge's weight. The correlation degree between the course and all teachers is quickly calculated using the matrix algorithm of PersonalRank. Finally, a teacher recommendation model is constructed to realize teachers' top-N recommendation by combining the correlation degree with teachers' characteristics. Experiments show that the PRLFM algorithm can effectively improve the accuracy of prediction and top-N recommendation. It solves the problem of lack of scientific basis in recommending suitable teachers for university courses and improving the teaching quality.", "pages": "108614-108627", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Yao, Dunhong", "Deng, Xiaowu"]}]["8979341", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2020.2971253", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Water quality", "Correlation", "Predictive models", "Interpolation", "Smoothing methods", "Time series analysis", "Data models", "Smart mariculture", "precision agriculture", "water quality prediction", "SRU", "deep learning"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "Accurate Prediction Scheme of Water Quality in Smart Mariculture With Deep Bi-S-SRU Learning Network", "url": "", "volume": "8", "year": "2020", "abstract": "In the smart mariculture, the timely and accurate predictions of water quality can help farmers take countermeasures before the ecological environment deteriorates seriously. However, the openness of the mariculture environment makes the variation of water quality nonlinear, dynamic and complex. Traditional methods face challenges in prediction accuracy and generalization performance. To address these problems, an accurate water quality prediction scheme is proposed for pH, water temperature and dissolved oxygen. First, we construct a new huge raw data set collected in time series consisting of 23,204 groups of data. Then, the water quality parameters are preprocessed for data cleaning successively through threshold processing, mean proximity method, wavelet filter, and improved smoothing method. Next, the correlation between the water quality to be predicted and other dynamics parameters is revealed by the Pearson correlation coefficient method. Meanwhile, the data for training is weighted by the discovered correlation coefficients. Finally, by adding a backward SRU node to the training sequence, which can be integrated into the future context information, the deep Bi-S-SRU (Bi-directional Stacked Simple Recurrent Unit) learning network is proposed. After training, the prediction model can be obtained. The experimental results demonstrate that our proposed prediction method achieve higher prediction accuracy than the method based on RNN (Recurrent Neural Network) or LSTM (Long Short-Term Memory) with similar or less time computing complexity. In our experiments, the proposed method takes 12.5ms to predict data on average, and the prediction accuracy can reach 94.42% in the next 3~8 days.", "pages": "24784-24798", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Liu, Juntao", "Yu, Chuang", "Hu, Zhuhua", "Zhao, Yaochi", "Bai, Yong", "Xie, Mingshan", "Luo, Jian"]}]["8660441", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2019.2896226", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Sensors", "Task analysis", "Wireless sensor networks", "Resource management", "Data integrity", "Wireless communication", "Mobile handsets", "Mobile crowdsensing (MCS)", "task allocation", "wireless sensor networks (WSNs)"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "A Survey of Task Allocation: Contrastive Perspectives From Wireless Sensor Networks and Mobile Crowdsensing", "url": "", "volume": "7", "year": "2019", "abstract": "Wireless sensor networks (WSNs) and mobile crowdsensing (MCS) are two important paradigms in urban dynamic sensing. In both sensing paradigms, task allocation is a significant problem, which may affect the completion quality of sensing tasks. In this paper, we give a survey of task allocation in WSNs and MCS from the contrastive perspectives in terms of data quality and sensing cost, which help to better understand related objectives and strategies. We first analyze the different characteristics of two sensing paradigms, which may lead to difference in task allocation issues or strategies. Then, we present some common issues in task allocation with objectives in data quality and sensing cost. Furthermore, we provide reviews of unique task allocation issues in MCS according to its new characteristics. Finally, we identify some potential opportunities for the future research.", "pages": "78406-78420", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Guo, Wenzhong", "Zhu, Weiping", "Yu, Zhiyong", "Wang, Jiangtao", "Guo, Bin"]}]["8778793", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2019.2931592", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Organizations", "Clustering algorithms", "Corporate acquisitions", "Feature extraction", "Graph theory", "Libraries", "Data mining", "Associative processing", "clustering algorithms", "data handling", "informatics"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "A Multi-Level Author Name Disambiguation Algorithm", "url": "", "volume": "7", "year": "2019", "abstract": "With the rapid development of information technology, the name ambiguity problem has become one of the primary issues in the fields of information retrieval, data mining, and scientific measurement. Name disambiguation is used to promote computer technology and big data information, which maps virtual relational networks to real social networks to solve the problem that the same name points to multiple entities. At present many literature search platforms launched their respective scholar system, name ambiguity problem will inevitably affect the precision of other information calculations, reduce the credibility of the system, and affect the information quality and content quality. Most work deals with this issue by using graph theory and clustering. However, the name disambiguation problem is still not well resolved. In this paper, we propose a multi-level name disambiguation algorithm. This algorithm is mainly based on the unsupervised algorithm, which combines hierarchical agglomerative clustering (HAC) and graph theory for disambiguating. The experimental results show that the proposed solution achieves clearly better performance (+17 ~ 25% in terms of F1-Measure) than several methods, including HAC and Graph.", "pages": "104250-104257", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Zhang, Siyang", "Xinhua, E.", "Pan, Tian"]}]["9715073", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2022.3152211", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Data mining", "Manufacturing", "Information management", "Production", "Manufacturing processes", "Companies", "Analytical models", "Bottleneck analysis", "manufacturing process", "process mining", "process modelling", "information management system", "value stream"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "Screening Process Mining and Value Stream Techniques on Industrial Manufacturing Processes: Process Modelling and Bottleneck Analysis", "url": "", "volume": "10", "year": "2022", "abstract": "One major result of the Industrial Digitalization is the access to a large set of digitalized data and information, i.e. Big Data. The market of analytic tools offers a huge variety of algorithms and software to exploit big datasets. Implementing their advantages into one approach brings better results and empower possibilities for process analysis. Its application in the manufacturing industry requires a high level of effort and remains to be challenging due to product complexity, human-centric processes, and data quality. In this manuscript, the authors combine process mining and value streams methods for analyzing the data from the information management system, applying the approach to the data delivered by one specific manufacturing system. The manufacturing process to be examined is the process of assembling gas meters in the manufacture. This specific and important part of the whole supply-chain process was taken as suitable for the study due to almost full-automated line with data about each process activity of the value-stream in the information system. The paper applies process mining algorithms in discovering a descriptive process model that plays the main role as a basis for further analysis. At the same time, modern techniques of the bottleneck analysis are described, and two new comprehensible methods of bottlenecks detection (TimeLag and Confidence intervals methods), as well as their advantages, will be discussed. Achieved results can be subsequently used for other sources of big data and industrial-compliant Information Management Systems.", "pages": "24203-24214", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Rudnitckaia, Julia", "Venkatachalam, Hari", "Essmann, Roland", "Hru\u0161ka, Tom\u00e1\u0161", "Colombo, Armando"]}]["8918337", {"address": "", "articleno": "", "doi": "10.1109/TCBB.2019.2956708", "issn": "1557-9964", "issue_date": "", "journal": "IEEE/ACM Transactions on Computational Biology and Bioinformatics", "keywords": ["Bioinformatics", "Genomics", "Quality assessment", "Pipelines", "Electronic mail", "RNA", "RNA-Seq", "analysis workflow", "pipeline", "R", "bioconductor", "transcriptome assembly", "differential expression analysis", "gene expression", "statistical analysis", "visualization"], "month": "Sep.", "number": "5", "numpages": "", "publisher": "", "title": "RNASeqR: An R Package for Automated Two-Group RNA-Seq Analysis Workflow", "url": "", "volume": "18", "year": "2021", "abstract": "RNA-Seq analysis has revolutionized researchers\u2019 understanding of the transcriptome in biological research. Assessing the differences in transcriptomic profiles between tissue samples or patient groups enables researchers to explore the underlying biological impact of transcription. RNA-Seq analysis requires multiple processing steps and huge computational capabilities. There are many well-developed R packages for individual steps; however, there are few R/Bioconductor packages that integrate existing software tools into a comprehensive RNA-Seq analysis and provide fundamental end-to-end results in pure R environment so that researchers can quickly and easily get fundamental information in big sequencing data. To address this need, we have developed the open source R/Bioconductor package, RNASeqR. It allows users to run an automated RNA-Seq analysis with only six steps, producing essential tabular and graphical results for further biological interpretation. The features of RNASeqR include: six-step analysis, comprehensive visualization, background execution version, and the integration of both R and command-line software. RNASeqR provides fast, light-weight, and easy-to-run RNA-Seq analysis pipeline in pure R environment. It allows users to efficiently utilize popular software tools, including both R/Bioconductor and command-line tools, without predefining the resources or environments. RNASeqR is freely available for Linux and macOS operating systems from Bioconductor (https://bioconductor.org/packages/release/bioc/html/RNASeqR.html).", "pages": "2023-2031", "note": "", "ISSN": "1557-9964", "publicationtype": "article", "author": ["Chao, Kuan-Hao", "Hsiao, Yi-Wen", "Lee, Yi-Fang", "Lee, Chien-Yueh", "Lai, Liang-Chuan", "Tsai, Mong-Hsun", "Lu, Tzu-Pin", "Chuang, Eric"]}]["8952697", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2020.2964791", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Fault diagnosis", "Rough sets", "Genetic algorithms", "Maintenance engineering", "Real-time systems", "Engines", "Data models", "Rough set", "genetic algorithms", "cellular genetic algorithm", "bus", "fault diagnosis"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "Research on Construction of Crude Set Model of Critical Fault Information for Bus Based on CAN-BUS Data", "url": "", "volume": "8", "year": "2020", "abstract": "Under the high load, high frequency and high strength operating environment, the frequent occurrence of vehicle fault gradually attracts the attention of the society. The real-time monitoring and data recording function of vehicle-mounted equipment provides data support for vehicle status assessment and fault warning. In this paper, the real-time data collected by CAN-BUS system of Beijing Bus Group are preprocessed and discretized. On the basis of the traditional rough set theory, a new coding method is set up, and the dependency between conditional attributes and decision attributes is set as an adaptive function, which is reduced by genetic algorithm and cellular genetic algorithm respectively. The calculation results show that the key fault information of public transport vehicles is instrumental speed, oil pressure, percentage of torque, timing engine speed, and coolant temperature. By comparing the results of reduction, it is found that the cellular genetic algorithm has higher applicability than the genetic algorithm in terms of algorithm efficiency, stability, and convergence quality. Although the genetic algorithm attribute reduction is slightly better than the cellular genetic algorithm attribute reduction in the rule matching, the cellular genetic algorithm has a better ability to excavate information within the acceptable compatibility range. Finally, the selected key factors will be deployed on the Beijing Bus Group's big data platform and displayed in real time. The conclusion of this paper enriches the theory of bus engine fault warning and establishes an engine failure warning system, which can effectively reduce the failure rate of bus vehicles and reduce the maintenance cost expenditure. It has certain guiding significance for the bus operation work of Beijing Bus Group.", "pages": "14875-14892", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Li, Jing", "Jia, Ruiqi", "Zhang, Ke", "Yang, Zifan", "Liu, Haixiao", "Zhu, Xin", "Li, Xueyan"]}]["8488459", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2018.2875242", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Big Data", "Writing", "Stakeholders", "Testing", "Multimedia systems", "Blockchain", "dyslexia", "auto-grading", "mass screening", "mobile multimedia health"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "Spatial Blockchain-Based Secure Mass Screening Framework for Children With Dyslexia", "url": "", "volume": "6", "year": "2018", "abstract": "In this paper, we present a novel method, process, and system for calculating dyslexic symptoms, generating metric data for an individual user, community, or group in general. We present a mobile multimedia Internet of Things (IoT)-based environment that can capture multimodal smartphone or tab-based user interaction data during dyslexia testing and share it via a mobile edge network, which employs auto-grading algorithms to find dyslexia symptoms. In addition to algorithm-based auto-grading, the captured mobile multimedia payload is stored in a decentralized repository that can be shared with a medical practitioner for replay and further manual analysis purposes. Since the framework is language-independent and based on Blockchain and a decentralized big data repository, dyslexic patterns and a massive amount of captured multimedia IoT test data can be shared for further clinical research, statistical analysis, and quality assurance. Notwithstanding, our proposed Blockchain and off-chain-based decentralized and secure dyslexia data storage, management, and sharing framework will allow security, anonymity, and multimodal visualization of the captured test data for mobile users. This paper presents the detailed design, implementation, and test results, which demonstrate the strong potential for wider adoption of the dyslexia mobile health management globally.", "pages": "61876-61885", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Rahman, Md.", "Hassanain, Elham", "Rashid, Md.", "Barnes, Stuart", "Hossain, M."]}]["9005249", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2020.2975415", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Supply chains", "Blockchain", "Information management", "Hazards", "Reliability", "Blockchain", "food safety", "grain supply chain", "hyperledger", "smart contract"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "Blockchain-Based Safety Management System for the Grain Supply Chain", "url": "", "volume": "8", "year": "2020", "abstract": "In recent years, various food-safety issues have aroused public concern regarding safety in the food supply chain. Since grains are closely linked to human life and health, it is necessary to effectively manage information in the grain supply chain. The grain supply chain is characterized by a long life cycle, complex links, various hazards, and heterogeneous information sources. Problems with traditional traceability systems include easy data tampering, difficult hazardous-material information management, the \u201cinformation isolated island\u201d problem, and low traceability efficiency in the whole supply chain. Blockchain is a distributed computing paradigm characterized by decentralization, network-wide recording, security, and reliability. As such, it can reduce administrative costs and improve the efficiency of information management. Based on literature research and a field investigation of wheat-processing enterprises in Shandong Province, We analyze the operation process of grain supply chain. This study, therefore, proposed a new system architecture in the entire grain supply chain based on blockchain technology and designed a multimode storage mechanism that combines chain storage. This prototype system was tested and verified using actual cases and application scenarios. Compared to traditional systems, the proposed system is characterized by data security and reliability, information interconnection and intercommunication, real-time sharing of hazardous-material information, and dynamic and credible whole-process tracing. As such, this system is highly significant and has reference value for guaranteeing food quality and safety-process traceability.", "pages": "36398-36410", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Zhang, Xin", "Sun, Pengcheng", "Xu, Jiping", "Wang, Xiaoyi", "Yu, Jiabin", "Zhao, Zhiyao", "Dong, Yunfeng"]}]["8567886", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2018.2885514", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Adaptation models", "Filtering algorithms", "TV", "Image edge detection", "Mathematical model", "Computational modeling", "Standards", "Low-dose CT", "image denoising", "total variation", "weighted coefficient", "edges and details preservation"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "A Novel Total Variation Model for Low-Dose CT Image Denoising", "url": "", "volume": "6", "year": "2018", "abstract": "Low-dose computed tomography (LDCT) images are polluted by mottle noise and streak artifacts. To improve LDCT images quality, this paper proposes a novel total variation (NTV) model. A weighted coefficient of the regularization term of NTV model is constructed by standard deviation, gray-level probability and gradient magnitude to smooth LDCT images adaptively, since the standard deviation and the gray-level probability of detail region are higher than that of the noisy background, and the gradient magnitude of edges is higher than that of the noisy background. Besides, to preserve details and edges effectively, the fidelity term of the proposed NTV model is constructed by the block-matching 3d filter because it performs well in details and edges preservation. The experiments are performed on the computer simulated phantom and the actual phantom. Compared with several other competitive methods, both subjective visual effect and objective evaluation criteria show that the proposed NTV model can improve LDCT images quality more effectively such as noise and artifacts suppression, details, and edges preservation.", "pages": "78892-78903", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Chen, Wenbin", "Shao, Yanling", "Wang, Yanling", "Zhang, Quan", "Liu, Yi", "Yao, Linhong", "Chen, Yan", "Yang, Guanru", "Gui, Zhiguo"]}]["8936407", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2019.2960516", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Computer architecture", "Decision making", "Real-time systems", "Big Data", "Context-aware services", "Predictive analytics", "Service-oriented architecture", "Context awareness", "context-aware services", "service-oriented architecture", "decision making", "microservice architecture"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "Real-Time Context-Aware Microservice Architecture for Predictive Analytics and Smart Decision-Making", "url": "", "volume": "7", "year": "2019", "abstract": "The impressive evolution of the Internet of Things and the great amount of data flowing through the systems provide us with an inspiring scenario for Big Data analytics and advantageous real-time context-aware predictions and smart decision-making. However, this requires a scalable system for constant streaming processing, also provided with the ability of decision-making and action taking based on the performed predictions. This paper aims at proposing a scalable architecture to provide real-time context-aware actions based on predictive streaming processing of data as an evolution of a previously provided event-driven service-oriented architecture which already permitted the context-aware detection and notification of relevant data. For this purpose, we have defined and implemented a microservice-based architecture which provides real-time context-aware actions based on predictive streaming processing of data. As a result, our architecture has been enhanced twofold: on the one hand, the architecture has been supplied with reliable predictions through the use of predictive analytics and complex event processing techniques, which permit the notification of relevant context-aware information ahead of time. On the other, it has been refactored towards a microservice architecture pattern, highly improving its maintenance and evolution. The architecture performance has been evaluated with an air quality case study.", "pages": "183177-183194", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Ortiz, Guadalupe", "Caravaca, Jos\u00e9", "Garc\u00eda-de-Prado, Alfonso", "O, Fr\u00e0ncisco", "Boubeta-Puig, Juan"]}]["8787230", {"address": "", "articleno": "", "doi": "10.26599/BDMA.2019.9020009", "issn": "2096-0654", "issue_date": "", "journal": "Big Data Mining and Analytics", "keywords": ["Prediction algorithms", "Task analysis", "Optimization", "Probability", "Neural networks", "Matrix decomposition", "Sparse matrices", "network representation learning", "network feature mining", "embedding learning", "link prediction", "matrix factorization"], "month": "Dec", "number": "4", "numpages": "", "publisher": "", "title": "Network representation based on the joint learning of three feature views", "url": "", "volume": "2", "year": "2019", "abstract": "Network representation learning plays an important role in the field of network data mining. By embedding network structures and other features into the representation vector space of low dimensions, network representation learning algorithms can provide high-quality feature input for subsequent tasks, such as network link prediction, network vertex classification, and network visualization. The existing network representation learning algorithms can be trained based on the structural features, vertex texts, vertex tags, community information, etc. However, there exists a lack of algorithm of using the future evolution results of the networks to guide the network representation learning. Therefore, this paper aims at modeling the future network evolution results of the networks based on the link prediction algorithm, introducing the future link probabilities between vertices without edges into the network representation learning tasks. In order to make the network representation vectors contain more feature factors, the text features of the vertices are also embedded into the network representation vectors. Based on the above two optimization approaches, we propose a novel network representation learning algorithm, Network Representation learning algorithm based on the joint optimization of Three Features (TFNR). Based on Inductive Matrix Completion (IMC), TFNR algorithm introduces the future probabilities between vertices without edges and text features into the procedure of modeling network structures, which can avoid the problem of the network structure sparse. Experimental results show that the proposed TFNR algorithm performs well in network vertex classification and visualization tasks on three real citation network datasets.", "pages": "248-260", "note": "", "ISSN": "2096-0654", "publicationtype": "article", "author": ["Ye, Zhonglin", "Zhao, Haixing", "Zhang, Ke", "Wang, Zhaoyang", "Zhu, Yu"]}]["8336850", {"address": "", "articleno": "", "doi": "10.26599/BDMA.2018.9020014", "issn": "2096-0654", "issue_date": "", "journal": "Big Data Mining and Analytics", "keywords": ["Heuristic algorithms", "Query processing", "Dispersion", "Parallel processing", "Data preprocessing", "Convergence", "Acceleration", "package queries", "heuristic algorithms", "parallel processing", "opposition-based learning"], "month": "June", "number": "2", "numpages": "", "publisher": "", "title": "HPPQ: A parallel package queries processing approach for large-scale data", "url": "", "volume": "1", "year": "2018", "abstract": "A lot of scholars have focused on developing effective techniques for package queries, and a lot of excellent approaches have been proposed. Unfortunately, most of the existing methods focus on a small volume of data. The rapid increase in data volume means that traditional methods of package queries find it difficult to meet the increasing requirements. To solve this problem, a novel optimization method of package queries (HPPQ) is proposed in this paper. First, the data is preprocessed into regions. Data preprocessing segments the dataset into multiple subsets and the centroid of the subsets is used for package queries, this effectively reduces the volume of candidate results. Furthermore, an efficient heuristic algorithm is proposed (namely IPOL-HS) based on the preprocessing results. This improves the quality of the candidate results in the iterative stage and improves the convergence rate of the heuristic algorithm. Finally, a strategy called HPR is proposed, which relies on a greedy algorithm and parallel processing to accelerate the rate of query. The experimental results show that our method can significantly reduce time consumption compared with existing methods.", "pages": "146-159", "note": "", "ISSN": "2096-0654", "publicationtype": "article", "author": ["Shi, Meihui", "Shen, Derong", "Nie, Tiezheng", "Kou, Yue", "Yu, Ge"]}]["9770057", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2022.3172975", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Computed tomography", "Image edge detection", "Noise reduction", "Mathematical models", "Laplace equations", "Image reconstruction", "Detectors", "Low-dose computed tomography", "anisotropic diffusion", "fourth-order PDEs", "residual local energy"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "Image Processing for Low-Dose CT via Novel Anisotropic Fourth-Order Diffusion Model", "url": "", "volume": "10", "year": "2022", "abstract": "Low-dose CT images contain severe mottle noise and streak artifacts, which seriously affect the physician\u2019s diagnosis of the disease. Hence, in this paper, we propose a novel anisotropic fourth-order diffusion model for low-dose CT image processing. The proposed diffusion model uses both image gradient magnitude and weighted residual local energy to determine the diffusion coefficient. Gradient magnitude is used to detect the image edges, while the weighted residual local energy preserves textures and details in the image. In addition, the fidelity term is introduced into the diffusion model to avoid excessive smoothing and weaken the blocky effects. Experimental results show that when compared with the anisotropic fourth-order diffusion model, the proposed algorithm protects the texture details and suppresses the blocky effects. In comparison with other state-of-the-art algorithms, the proposed model effectively suppresses mottle noise and streak artifacts while simultaneously improving the low-dose CT image quality.", "pages": "50114-50124", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Wang, Lei", "Liu, Yi", "Wu, Rui", "Liu, Yuhang", "Yan, Rongbiao", "Ren, Shilei", "Gui, Zhiguo"]}]["7809136", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2016.2647238", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Tumors", "Feature extraction", "Data mining", "Medical diagnostic imaging", "Artificial neural networks", "Brain tumor", "morphological features", "ANNIGMA", "MRMR", "feature selection", "classification"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "A Hybrid Feature Selection With Ensemble Classification for Imbalanced Healthcare Data: A Case Study for Brain Tumor Diagnosis", "url": "", "volume": "4", "year": "2016", "abstract": "Electronic health records (EHRs) are providing increased access to healthcare data that can be made available for advanced data analysis. This can be used by the healthcare professionals to make a more informed decision providing improved quality of care. However, due to the inherent heterogeneous and imbalanced characteristics of medical data from EHRs, data analysis task faces a big challenge. In this paper, we address the challenges of imbalanced medical data about a brain tumor diagnosis problem. Morphometric analysis of histopathological images is rapidly emerging as a valuable diagnostic tool for neuropathology. Oligodendroglioma is one type of brain tumor that has a good response to treatment provided the tumor subtype is recognized accurately. The genetic variant, 1p-/19q-, has recently been found to have high chemosensitivity, and has morphological attributes that may lend it to automated image analysis and histological processing and diagnosis. This paper aims to achieve a fast, affordable, and objective diagnosis of this genetic variant of oligodendroglioma with a novel data mining approach combining a feature selection and ensemble-based classification. In this paper, 63 instances of brain tumor with oligodendroglioma are obtained due to prevalence and incidence of the tumor variant. In order to minimize the effect of an imbalanced healthcare data set, a global optimization-based hybrid wrapper-filter feature selection with ensemble classification is applied. The experiment results show that the proposed approach outperforms the standard techniques used in brain tumor classification problem to overcome the imbalanced characteristics of medical data.", "pages": "9145-9154", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Huda, Shamsul", "Yearwood, John", "Jelinek, Herbert", "Hassan, Mohammad", "Fortino, Giancarlo", "Buckland, Michael"]}]["8768306", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2019.2930279", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Global Positioning System", "Stability criteria", "Trajectory", "Synchronization", "Network topology", "Public transport network", "stability of headway", "GPS trajectory data", "data-driven analysis"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "A Data-Driven Analysis for Operational Vehicle Performance of Public Transport Network", "url": "", "volume": "7", "year": "2019", "abstract": "The operational stability of public transport is significant for both passengers and operators. Affected by many stochastic factors, such as traffic congestion, traffic signals and passenger demand at stops, the headway always become uneven, which greatly reduces the service quality. This paper used the big global positioning systems (GPS) trajectory data to analyze the headway stability of bus system from the perspective of network. A statistical method is proposed to analyze the operational vehicle performance of bus network. The GPS trajectory data of Jinan is used to test the model. The results show that the average dwell time, actual headway, and headway stability index of stations follow lognormal distributions with obvious right tails. Moreover, the seriously unstable situations do not appear in the peak hours, but in the time periods before peak hours. In addition, the stations with most unstable headway are located in the suburbs and the fringe area of downtown. The outcomes suggest that operators should pay more attention to the suburbs and the fringe area of downtown, and the time periods before peak hours to efficiently improve the service quality.", "pages": "96404-96413", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Zhang, Hui", "Cui, Houdun", "Shi, Baiying"]}]["9154564", {"address": "", "articleno": "", "doi": "10.1109/TNET.2020.3008512", "issn": "1558-2566", "issue_date": "", "journal": "IEEE/ACM Transactions on Networking", "keywords": ["Topology", "Network topology", "Data centers", "Routing", "Scalability", "Fault tolerance", "Fault tolerant systems", "Data center network", "network topology", "average path length", "bisection bandwidth"], "month": "Oct", "number": "5", "numpages": "", "publisher": "", "title": "LaScaDa: A Novel Scalable Topology for Data Center Network", "url": "", "volume": "28", "year": "2020", "abstract": "The growth of cloud-based services is mainly supported by the core networking infrastructures of large-scale data centers, while the scalability of these services is influenced by the performance and dependability characteristics of data centers. Hence, the data center network must be agile and reconfigurable in order to respond quickly to the ever-changing application demands and service requirements. The network must also be able to interconnect the big number of nodes, and provide an efficient and fault-tolerant routing service to upper-layer applications. In response to these challenges, the research community began exploring novel interconnect topologies, namely: Flecube, DCell, Ficonn, HyperFlaNet and BCube. However, these topologies either scale too fast (grows exponentially in size), or too slow, and therefore suffer from performance bottlenecks. In this paper, we propose a novel data center topology called LaScaDa (Layered Scalable Data Center) as a new solution for building scalable and cost-effective data center networking infrastructures. The proposed topology organizes nodes in clusters of similar structure, then interconnect these clusters in a well-crafted pattern and system of coordinates for nodes to reduce the number of redundant connections between clusters, while maximizing connectivity. LaScaDa forwards packets between nodes using a new hierarchical row-based routing algorithm. The algorithm constructs the route to the source based on the modular difference between the source and destination coordinates. Furthermore, the proposed topology interconnects a large number of nodes using a small node degree. This strategy increases the number of directly connected clusters and avoids redundant connections. As a result, we get a good quality of nodes in terms of average path length (APL), bisection bandwidth, and aggregated bottleneck throughput. Experimental results show that LaScaDa has better performance than DCell, BCube, and HyperBcube in terms of scalability, while providing a good quality of service.", "pages": "2051-2064", "note": "", "ISSN": "1558-2566", "publicationtype": "article", "author": ["Chkirbene, Zina", "Hadjidj, Rachid", "Foufou, Sebti", "Hamila, Ridha"]}]["9200319", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2020.3024558", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Pharmaceuticals", "Machine learning", "Support vector machines", "Companies", "Training", "Semantics", "Data integration", "Big data", "data integration", "machine learning", "pattern detection", "medicine"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "Automatic Learning Framework for Pharmaceutical Record Matching", "url": "", "volume": "8", "year": "2020", "abstract": "Pharmaceutical manufacturers need to analyse a vast number of products in their daily activities. Many times, the same product can be registered several times by different systems using different attributes, and these companies require accurate and quality information regarding their products since these products are drugs. The central hypothesis of this research work is that machine learning can be applied to this domain to efficiently merge different data sources and match the records related to the same product. No human is able to do this in a reasonable way because the number of records to be matched is extremely high. This article presents a framework for pharmaceutical record matching based on machine learning techniques in a big data environment. The proposed framework aims to explode the well-known rules for the matching of records from different databases for training machine learning models. Then the trained models are evaluated by predicting matches with records that do not follow these known rules. Finally, the production environment is simulated by generating a huge amount of combinations of records and predicting the matches. The obtained results show that, despite the good results obtained with the training datasets, in the production environment, the average accuracy of the best model is around 85%. That shows that matches which do not follow the known rules can be predicted and, considering that there is not a human way to process this amount of data, the results are promising.", "pages": "171754-171770", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["L\u00f3pez-Cuadrado, Jos\u00e9", "Gonz\u00e1lez-Carrasco, Israel", "Leonardo, Jes\u00fas", "Mart\u00ednez-Fern\u00e1ndez, Paloma", "Mart\u00ednez-Fern\u00e1ndez, Jos\u00e9"]}]["9371683", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2021.3064350", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Monitoring", "Information security", "Business", "Pollution", "Internet of Things", "Big Data", "Information security", "big data", "evaluation model", "monitoring mechanism", "Internet of Things"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "Information Security Monitoring and Management Method Based on Big Data in the Internet of Things Environment", "url": "", "volume": "9", "year": "2021", "abstract": "As an important part of the new generation of information technology, the Internet of Things (IoT), with its ubiquitous connection and service characteristics, has penetrated into various fields of application and played an important role. In this paper, based on the study of the basic technology of the environmental Internet of Things, combined with the service-oriented technology architecture SOA, J2EE, multi-level system architecture MVC, real-time database and other technologies and project practice experience, summarized and proposed a kind of environmental quality monitoring integrated management platform design and implementation feasibility scheme. Firstly, the background of the era of big data is described in detail, the urgency and necessity of information security monitoring under the background of big data is clarified, and the three elements of information security monitoring mechanism, namely network monitoring personnel, environment and technology, are proposed, and the three elements as the starting point to establish the information security monitoring mechanism; Starting from the relevant monitoring strategies and safety monitoring technologies, this paper explains the basic principles of constructing the evaluation index system, and establishes the evaluation index system according to the key influencing factors of enterprise information security level in the environment of big data. AHP fuzzy comprehensive evaluation method is chosen on the basis of analyzing various comprehensive evaluation methods, and the weight of each evaluation index is determined and the comprehensive evaluation model is constructed. The establishment of information security monitoring and evaluation system, the use of information security monitoring and evaluation system, for information security monitoring work to provide reference standards. Finally, on the basis of the foregoing, relevant strategies for information security monitoring are proposed, and necessary suggestions are provided for information security work.", "pages": "39798-39812", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Liang, Wuchao", "Li, Wenning", "Feng, Lili"]}]["9309228", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2020.3047648", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Estimation", "Signal to noise ratio", "Forestry", "Heuristic algorithms", "Wireless sensor networks", "Clustering algorithms", "Wireless sensor networks", "link quality estimation", "deep forest", "stratified sampling"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "A Link Quality Estimation Method for Wireless Sensor Networks Based on Deep Forest", "url": "", "volume": "9", "year": "2021", "abstract": "In wireless sensor networks, sensor nodes, the miniature embedded devices, have limitation of energy, storage, computing, and etc. One of the tasks of the nodes is to use their limited resources to complete work efficiently. Choosing high quality link communication can effectively save energy. In this paper, we propose a link quality estimation model that is based on deep forest. To avoid a noise sample becoming a center point in the clustering, we use an improved K-medoids algorithm based on step increasing and optimizing medoids (INCK) when dividing the link quality grades. During the sample preprocessing stage, the Pauta criterion is used to delete the noise link samples, and we fill the mean value of each grade into the missing values. The feature extraction performance of deep forest is improved by combining the stratified sampling to change the unbalance distribution of link quality samples. And then the Stratified Sampling Cascade Forest link quality estimation (SCForest-LQE) is constructed by combining stratified sampling with cascade forest. The experiments are conducted in three real application scenarios. Compared with the existing six link quality estimation models, SCForest-LQE has better estimation performance and stability.", "pages": "2564-2575", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["He, Mu", "Shu, Jian"]}]["9099795", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2020.2997831", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Cloud computing", "Computer architecture", "Medical diagnostic imaging", "Edge computing", "Medical services", "Radiofrequency identification", "Internet of medical things (IoMT)", "deep learning", "edge of computing", "computation offloading"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "Edge-Cloud Computing and Artificial Intelligence in Internet of Medical Things: Architecture, Technology and Application", "url": "", "volume": "8", "year": "2020", "abstract": "With the booming development of medical informatization and the ubiquitous connections in the fifth generation mobile communication technology (5G) era, the heterogeneity and explosive growth of medical data have brought huge challenges to data access, security and privacy, as well as information processing in Internet of Medical Things (IoMT). This article provides a comprehensive review of how to realize the timely processing and analysis of medical big data and the sinking of high-quality medical resources under the constraints of the existing medical environment and medical-related equipment. We mainly focus on the advantages brought by the cloud computing, edge computing and artificial intelligence technologies to the IoMT. We also explore how to rationalize the use of medical resources and the security and privacy of medical data, so that high-quality medical services can be provided to patients. Finally, we discuss the current challenges and possible future research directions in the edge-cloud computing and artificial intelligence related IoMT.", "pages": "101079-101092", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Sun, Lanfang", "Jiang, Xin", "Ren, Huixia", "Guo, Yi"]}]["8360420", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2018.2837498", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Quality of service", "Global Positioning System", "Roads", "Data mining", "Neural networks", "Companies", "Reliability", "Road-based mass transit systems", "travel time", "intelligent transportation systems", "data mining", "pattern clustering", "global positioning system"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "Systematic Approach to Analyze Travel Time in Road-Based Mass Transit Systems Based on Data Mining", "url": "", "volume": "6", "year": "2018", "abstract": "Road-based mass transit systems are an effective means to combat the negative impact of transport that is based on private vehicles. Providing quality of service in this type of transit system is a priority for transport authorities. In these systems, travel time (TT) is a basic factor in quality of service. This paper presents a methodology, based on data mining, for analyzing TT in a mass transit system that is planned by timetable. The objective of the methodology is to understand the behavior patterns of TTs on the different routes of the transport network, as well as the factors that influence these patterns. To achieve this objective, the methodology uses clustering techniques to process the GPS data provided by the vehicles of the public transport fleet. The results that were obtained when implementing this methodology in a public transport company are presented as a use case, demonstrating its validity.", "pages": "32861-32873", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Crist\u00f3bal, Teresa", "Padr\u00f3n, Gabino", "Quesada-Arencibia, Alexis", "Alay\u00f3n, Francisco", "Garc\u00eda, Carmelo"]}]["9416689", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2021.3075953", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Medical diagnostic imaging", "Computed tomography", "Gabor filters", "Image edge detection", "Feature extraction", "Filter banks", "Medical image fusion", "G-CNNs", "Gabor representation", "convolutional neural network", "fuzzy neural network"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "Multimodal Medical Image Fusion Based on Gabor Representation Combination of Multi-CNN and Fuzzy Neural Network", "url": "", "volume": "9", "year": "2021", "abstract": "Aiming at the current multimodal medical image fusion methods that cannot fully characterize the complex textures and edge information of the lesion in the fused image, a method based on Gabor representation of multi-CNN combination and fuzzy neural network is proposed. This method first filters the CT and MR image sets through a set of Gabor filter banks with different proportions and directions to obtain different Gabor representations pairs of CT and MR, each pair of different Gabor representations is used to train the corresponding CNN to generate a G- CNN and multiple G- CNN form a G- CNN group, namely G- CNNs; then when fusing CT and MR images, CT and MR are represented by Gabors to get Gabor representation pairs firstly, each Gabor representation pair is put into the corresponding trained G- CNN for preliminary fusion, then use the fuzzy neural network to fuse multiple outputs of the G- CNNs to obtain the final fused image. Compared with the nine recent state-of-the-art multimodal fusion methods, the average mutual information of the three groups of experiments has increased by 13%, 10.3%, and 10% respectively; the average spatial frequency has increased by 10.3%, 20%, and 10.7%; the average standard deviation has increased respectively 12.4%, 10.8%, 14.4%; the average edge retention information increased by 33.5%, 22%, and 43%. The experimental results show that the proposed fusion method is significantly better than the other comparative fusion methods in objective evaluation and visual quality. It has the best performance on the four indicators and can better integrate the rich texture features and the clear edge information of the source images into the final fused image, which improves the quality of multimodal medical image fusion, and effectively assists doctors in disease diagnosis.", "pages": "67634-67647", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Wang, Lifang", "Zhang, Jin", "Liu, Yang", "Mi, Jia", "Zhang, Jiong"]}]["8787738", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2019.2933147", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Crowdsourcing", "Task analysis", "Indexes", "Machine learning", "Data models", "Analytical models", "Game theory", "Crowdsourcing participants", "reputation evaluation", "machine learning", "random forest", "data dimension reduction"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "Improve Reputation Evaluation of Crowdsourcing Participants Using Multidimensional Index and Machine Learning Techniques", "url": "", "volume": "7", "year": "2019", "abstract": "Building a scientific and reasonable reputation evaluation mechanism for crowdsourcing participants is an effective way to solve the problem of transaction fraud, to establish the trust of traders and ensure the quality of task completion. Under the big data environment, machine learning methods have been applied in the domain of e-commerce of physical goods to improve the traditional reputation evaluation methods, and achieved good results. However, few studies have applied machine learning methods to crowdsourcing, a form of service e-commerce, to evaluate the reputation of participants. This paper proposes a reputation evaluation model (i.e. LDA-RF) for crowdsourcing participants of Random Forest based on Linear Discriminant Analysis. The model consists of five steps: firstly, building a multidimensional reputation evaluation index system for crowdsourcing participants, collecting real data sets, and preprocessing data; secondly, data dimensionality reduction methods, including Linear Discriminant Analysis, Principal Component Analysis, Mean Impact Value method and ReliefF feature selection method, are used to eliminate redundant variables; thirdly, data normalization; fourthly, with selected feature subset, five machine learning techniques, Random Forest, Decision Tree, Back propagation Neural Network, Radial Basis Function Neural Network and Support Vector Machine are used to train the model; Fifthly, the validity of the model is tested by four evaluation measures: 10 fold cross validation, confusion matrix, Kruskal-wallis test and dispersion degree. The results show that the LDA-RF model on accuracy, F1-measure, generalization ability and robustness are better than those of other models, and it has better performance and effectiveness. This study represents a new contribution to establish reputation evaluation of crowdsourcing participants under big data environment.", "pages": "118055-118067", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Huang, Yanrong", "Chen, Min"]}]["9143114", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2020.3009977", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Microwave radiometry", "Meteorology", "Satellite broadcasting", "Sensors", "Soil moisture", "Synthetic aperture radar", "Soil measurements", "Gap-filling", "satellite retrieved soil moisture", "the essential climate variable soil moisture", "the soil moisture active passive soil moisture"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "Potential Applicability of SMAP in ECV Soil Moisture Gap-Filling: A Case Study in Europe", "url": "", "volume": "8", "year": "2020", "abstract": "The Essential Climate Variable (ECV) soil moisture (SM) datasets, originated from the European Space Agency, have revealed great potential for application in hydrology and agriculture. Hence, it is essential to continuously enhance the data quality and spatial completeness to satisfy the increasing scientific research requirements. In this study, we explore the potential possibility of Soil Moisture Active Passive (SMAP) datasets in filling the gaps of ECV SM. The comprehensive assessment results show that: (1) The data missing percent of gap-filled ECV decreases 20% on average, which can be one step closer to generate a seamlessly covered global land surface SM product with favorable quality. (2) Compared to the original ECV, the gap-filled ECV products express similar good response to the in-situ measurements, suggesting that the SMAP SM products could be taken to efficiently fill the gaps and consistently maintain favorable accuracy at the same time. (3) Compared to the in-situ measurements, the original ECV SM products demonstrate extremely high probability density peak percentages. Fortunately, this eminent high value could be effectively rectified through gap-filling progress using SMAP. Overall, this study conducts objective and detailed evaluation on the performance of applying SMAP to fill the gaps of ECV, and is expected to act as a valuable reference in ECV SM gap-filling method.", "pages": "133114-133127", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Liu, Yangxiaoyue", "Yang, Yaping", "Jing, Wenlong"]}]["8901214", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2019.2953560", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Apertures", "Semantics", "Labeling", "Cameras", "Visualization", "Image reconstruction", "Synthetic aperture imaging", "data-driven variable synthetic aperture", "semantic feedback imaging", "multi-camera array"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "Data-Driven Variable Synthetic Aperture Imaging Based on Semantic Feedback", "url": "", "volume": "7", "year": "2019", "abstract": "Synthetic aperture imaging, which has been proved to be an effective approach for occluded object imaging, is one of the challenging problems in the field of computational imaging. Currently most of the related researches focus on fixed synthetic aperture which usually accompanies with mixed observation angle and foreground de-focus blur. But the existence of them is frequently a source of perspective effect decrease and occluded object imaging quality degradation. In order to solve this problem, we propose a novel data-driven variable synthetic aperture imaging based on semantic feedback. The semantic content we concerned for better de-occluded imaging is the foreground occlusions rather than the whole scene. Therefore, unlike other methods worked on pixel-level, we start from semantic layer and present a semantic labeling method based on feedback. Semantic labeling map deeply mines visual data in synthetic image and preserves the semantic information of foreground occluder. On the basis of semantic feedback strategy, semantic labeling map will conversely pass to synthetic imaging process. The proposed data-driven variable synthetic aperture imaging contains two levels: one is adaptive changeable imaging aperture driven by synthetic depth and perspective angle, the other is light ray screening driven by visual information in semantic labeling map. On this basis, the hybrid camera view and superimposition of foreground occlusion can be removed. Evaluations on several complex indoor scenes and real outdoor environments demonstrate the superiority and robustness performance of our proposed approach.", "pages": "166021-166042", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Li, Congcong", "Li, Jing", "Dai, Yanran", "Yang, Tao", "Xie, Yuguang", "Lu, Zhaoyang"]}]["9745947", {"address": "", "articleno": "", "doi": "10.1109/TGRS.2022.3164193", "issn": "1558-0644", "issue_date": "", "journal": "IEEE Transactions on Geoscience and Remote Sensing", "keywords": ["Synthetic aperture radar", "Superresolution", "Deep learning", "Neural networks", "Reflectivity", "Imaging", "Image reconstruction", "Complex-valued learned iterative shrinkage thresholding algorithm (LISTA)", "compressive sensing (CS)", "synthetic aperture radar~(SAR) tomography (TomoSAR)", "super-resolution"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "\u03b3-Net: Superresolving SAR Tomographic Inversion via Deep Learning", "url": "", "volume": "60", "year": "2022", "abstract": "Synthetic aperture radar tomography (TomoSAR) has been extensively employed in 3-D reconstruction in dense urban areas using high-resolution SAR acquisitions. Compressive sensing (CS)-based algorithms are generally considered as the state-of-the art in super-resolving TomoSAR, in particular in the single look case. This superior performance comes at the cost of extra computational burdens, because of the sparse reconstruction, which cannot be solved analytically, and we need to employ computationally expensive iterative solvers. In this article, we propose a novel deep learning-based super-resolving TomoSAR inversion approach, $\\boldsymbol {\\gamma }$ -Net, to tackle this challenge. $\\boldsymbol {\\gamma }$ -Net adopts advanced complex-valued learned iterative shrinkage thresholding algorithm (CV-LISTA) to mimic the iterative optimization step in sparse reconstruction. Simulations show the height estimate from a well-trained $\\boldsymbol {\\gamma }$ -Net approaches the Cram\u00e9r-Rao lower bound (CRLB) while improving the computational efficiency by one to two orders of magnitude comparing to the first-order CS-based methods. It also shows no degradation in the super-resolution power comparing to the state-of-the-art second-order TomoSAR solvers, which are much more computationally expensive than the first-order methods. Specifically, $\\boldsymbol {\\gamma }$ -Net reaches more than 90% detection rate in moderate super-resolving cases at 25 measurements at 6 dB SNR. Moreover, simulation at limited baselines demonstrates that the proposed algorithm outperforms the second-order CS-based method by a fair margin. Test on real TanDEM-X data with just six interferograms also shows high-quality 3-D reconstruction with high-density detected double scatterers.", "pages": "1-16", "note": "", "ISSN": "1558-0644", "publicationtype": "article", "author": ["Qian, Kun", "Wang, Yuanyuan", "Shi, Yilei", "Zhu, Xiao"]}]["8827494", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2019.2940137", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Classification algorithms", "Corner detection", "Complexity theory", "Nanoscale devices", "Clustering algorithms", "Feature extraction", "Microsoft Windows", "Harris", "texture classification", "interval categorization", "classification matching"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "Multi-Threshold Corner Detection and Region Matching Algorithm Based on Texture Classification", "url": "", "volume": "7", "year": "2019", "abstract": "In order to address the unreasonable distributed corners in single threshold Harris detection and expensive computation cost incurred from image region matching performed by normalized cross correlation (NCC) algorithm, multi-threshold corner detection and region matching algorithm based on texture classification are proposed. Firstly, the input image is split into sub-blocks which are classified into four different categories based on the specific texture: flat, weak, middle texture and strong regions. Subsequently, an algorithm is suggested to decide threshold values for different texture type, and interval calculation for the sub-blocks is performed to improve operation efficiency in the algorithm implementation. Finally, based on different texture characteristics, Census, interval-sampled NCC, and complete NCC are employed to perform image matching. As demonstrated by the experimental results, corner detection based on texture classification is capable to obtain a reasonable corner number as well as a more uniform spatial distribution, when compared to the traditional Harris algorithm. If combined with the interval classification, speedup for texture classification is approximately 30%. In addition, the matching algorithm based on texture classification is capable to improve the speed of 26.9%~29.9% while maintaining the comparable accuracy of NCC. In general, for better splicing quality, the overall stitching speed is increased by 14.1%~18.4%. Alternatively, for faster speed consideration, the weak texture region which accounts for a large proportion of an image and provides less effective information can be ignored, for which 23.9%~28.4% speedup can be achieved at the cost of a 1.9%~3.9% reduction in corner points. Therefore, the proposed algorithm is made potentially suited to uniformly distributed corner point calculation and high computation efficiency requirement scenarios.", "pages": "128372-128383", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Tang, Zetian", "Ding, Zhao", "Zeng, Ruimin", "Wang, Yang", "Wen, Jun", "Bian, Lifeng", "Yang, Chen"]}]["9516007", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2021.3105796", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Random forests", "Vegetation", "Decision trees", "Artificial bee colony algorithm", "Forestry", "Training", "Radio frequency", "Random forest", "Kappa measure", "artificial bee colony algorithm", "haze prediction"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "A Modified Random Forest Based on Kappa Measure and Binary Artificial Bee Colony Algorithm", "url": "", "volume": "9", "year": "2021", "abstract": "Random forest (RF) is an ensemble classifier method, all decision trees participate in voting, some low-quality decision trees will reduce the accuracy of random forest. To improve the accuracy of random forest, decision trees with larger degree of diversity and higher classification accuracy are selected for voting. In this paper, the RF based on Kappa measure and the improved binary artificial bee colony algorithm (IBABC) are proposed. Firstly, Kappa measure is used for pre-pruning, and the decision trees with larger degree of diversity are selected from the forest. Then, the crossover operator and leaping operator are applied in ABC, and the improved binary ABC is used for secondary pruning, and the decision trees with better performance are selected for voting. The proposed method (Kappa+IBABC) are tested on a quantity of UCI datasets. Computational results demonstrate that Kappa+IBABC improves the performance on most datasets with fewer decision trees. The Wilcoxon signed-rank test is used to verify the significant difference between the Kappa+IBABC method and other pruning methods. In addition, Chinese haze pollution is becoming more and more serious. This proposed method is used to predict haze weather and has achieved good results.", "pages": "117679-117690", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Zhang, Chen", "Wang, Xiaofeng", "Chen, Shengbing", "Li, Hong", "Wu, Xiaoxuan", "Zhang, Xin"]}]["9270014", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2020.3040416", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Databases", "Image coding", "Distortion", "Image quality", "Measurement", "Feature extraction", "Training", "Image quality assessment", "coding distortion", "image-patch quality assessment", "compression artifacts"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "End-to-End Image Patch Quality Assessment for Image/Video With Compression Artifacts", "url": "", "volume": "8", "year": "2020", "abstract": "In this paper, we present an experimental image quality assessment (IQA) method for image/video patches with compression artifacts. Using the High Efficiency Video Coding (HEVC) standard, we create a new database of image patches with compression artifacts. Then, we conduct a completed subjective testing process to obtain the `ground truth' quality scores for the mentioned database. Finally, we employ an end-to-end learning method to estimate the IQA model for the patches with HEVC compression artifacts. In such proposed method, a modified convolutional neural network (CNN) architecture is exploited for feature extraction while an adaptive moment estimation optimizer solution is used to perform the training process. Experimental results show that the proposed end-to-end IQA method significantly outperforms the relevant IQA benchmarks, especially when the compression artifacts are strongly realized in image/video patches. The proposed IQA method is expected to drive a new set of image/video compression solutions in future image/video coding and transmissions.", "pages": "215157-215172", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Pham, Tung", "Hoang, Xiem", "Nguyen, Nghia", "Dinh, Duong", "Ha, Le"]}]["8938802", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2019.2961392", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Mobile computing", "YouTube", "Deep learning", "Search engines", "Big Data", "Mobile handsets", "Mobile computing", "social media", "big data", "video tagging", "video popularity", "artificial intelligence", "deep learning"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "An Intelligent Video Tag Recommendation Method for Improving Video Popularity in Mobile Computing Environment", "url": "", "volume": "8", "year": "2020", "abstract": "Big data generated from social media and smart mobile devices has been regarded as a key to obtain insights into human behavior and been extensively utilized for launching marketing activities. A successful marketing activity requires attracting high social popularity to their contents, since higher popularity usually indicates stronger influence, more fame and higher revenue. In this paper, we focus on the question of how to improve popularity of videos sharing on websites like YouTube in mobile computing environment. Obviously, composing high quality titles and tags is beneficial for viewers to discover videos of their interests and increase their tendency to watch more videos. However, it is not an easy task for uploaders, which is especially true since the screen is tight for most mobile devices. To this end, this paper proposes a novel hybrid method based on multi-modal content analysis that recommends keywords for video uploaders to compose titles and tags of their videos and then to gain higher popularity. The method generates candidate keywords by integrating techniques of textual semantic analysis of original tags and recognition of video content. On one hand, taking the original keywords of a video as input, the method obtains most relevant words from WordNet and related video titles gathered from the three top video sharing sites (YouTube, Yahoo Video, Bing Video). On the other hand, through recognizing video content with deep learning technology, the method extracts the entity name of video content as candidate keywords. Finally, a TF-SIM algorithm is proposed to rank the candidate keywords and the most relevant keywords are recommended to uploaders for optimizing the titles and tags of their videos. The experimental results show that the proposed method can effectively improve the social popularity of the videos as well as extend the length of video viewing time per playback.", "pages": "6954-6967", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Zhou, Renjie", "Xia, Dongchen", "Wan, Jian", "Zhang, Sanyuan"]}]["8943962", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2019.2962724", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Computational modeling", "Mathematical model", "Analytical models", "Optimization", "Estimation", "Uncertainty", "Throughput", "Distributed file system", "HDFS", "performance modeling", "randomness", "ordinal optimization"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "Ordinal Optimization-Based Performance Model Estimation Method for HDFS", "url": "", "volume": "8", "year": "2020", "abstract": "Modeling and analyzing the performance of distributed file systems (DFSs) benefit the reliability and quality of data processing in data-intensive applications. Hadoop Distributed File System (HDFS) is a typical representative of DFSs. Its internal heterogeneity and complexity as well as external disturbance contribute to HDFS's built-in features of nonlinearity as well as randomness in system level, which raises a great challenge in modeling these features. Particularly, the randomness results in the uncertainty of HDFS performance model. Due to the complex mathematical structure and parameters hardly estimated of analytical models, it is highly complicated and computationally impossible to build an explicit and precise analytical model of the randomness. The measurement-based methodology is a promising way to model HDFS performance in terms of randomness since it requires no knowledge of system's internal behaviors. In this paper, the estimation of HDFS performance models on account of the randomness is transformed to an optimization problem of finding out the real best design of performance model structure with large design space. Core ideas of ordinal optimization (OO) are introduced to solve this problem with a limited computing budget. Piecewise linear (PL) model is applied to approximate the nonlinear characteristics and randomness of HDFS performance. The experimental results show that the proposed method is effective and practical to estimate the optimal design of the PL-based performance model structure for HDFS. It not only provides a globally consistent evaluation of the design space but also guarantees the goodness of the solution with high probability. Moreover, it improves the accuracy of system model-based HDFS performance models.", "pages": "889-899", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Ma, Tian", "Tian, Feng", "Dong, Bo"]}]["8382164", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2018.2846609", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Medical services", "Cloud computing", "Edge computing", "Quality of service", "Machine learning", "Smart cities", "Internet of Things", "Cloudlets", "deep learning", "Internet of Things (IoT)", "mobile edge computing", "mobile healthcare", "preventive healthcare", "traffic classification", "traffic prediction", "survey", "fog computing", "cloud computing", "multimedia applications", "smart cities"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "UbeHealth: A Personalized Ubiquitous Cloud and Edge-Enabled Networked Healthcare System for Smart Cities", "url": "", "volume": "6", "year": "2018", "abstract": "Smart city advancements are driving massive transformations of healthcare, the largest global industry. The drivers include increasing demands for ubiquitous, preventive, and personalized healthcare, to be provided to the public at reduced risks and costs. Mobile cloud computing could potentially meet the future healthcare demands by enabling anytime, anywhere capture and analyses of patients' data. However, network latency, bandwidth, and reliability are among the many challenges hindering the realization of next-generation healthcare. This paper proposes a ubiquitous healthcare framework, UbeHealth, that leverages edge computing, deep learning, big data, high-performance computing (HPC), and the Internet of Things (IoT) to address the aforementioned challenges. The framework enables an enhanced network quality of service using its three main components and four layers. Deep learning, big data, and HPC are used to predict network traffic, which in turn are used by the Cloudlet and network layers to optimize data rates, data caching, and routing decisions. Application protocols of the traffic flows are classified, enabling the network layer to meet applications' communication requirements better and to detect malicious traffic and anomalous data. Clustering is used to identify the different kinds of data originating from the same application protocols. A proof of concept UbeHealth system has been developed based on the framework. A detailed literature review is used to capture the design requirements for the proposed system. The system is described in detail including the algorithmic implementation of the three components and four layers. Three widely used data sets are used to evaluate the UbeHealth system.", "pages": "32258-32285", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Muhammed, Thaha", "Mehmood, Rashid", "Albeshri, Aiiad", "Katib, Iyad"]}]["9519539", {"address": "", "articleno": "", "doi": "10.1109/JSTARS.2021.3106038", "issn": "2151-1535", "issue_date": "", "journal": "IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing", "keywords": ["Water quality", "Rivers", "Land surface", "Remote sensing", "Belts", "Monitoring", "Manganese", "Impervious surface", "rapid urbanization", "remote sensing", "segmented regression", "threshold", "water quality", "Yangtze River economic belt"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "The Impacts of Impervious Surface on Water Quality in the Urban Agglomerations of Middle and Lower Reaches of the Yangtze River Economic Belt From Remotely Sensed Data", "url": "", "volume": "14", "year": "2021", "abstract": "The Urban Agglomerations of Middle and Lower Reaches of the Yangtze River Economic Belt (UAMLYREB) have experienced rapid and intense urbanization over the past decades with natural ecosystems being converted to impervious surfaces. Thus, impervious surfaces are recognized as critical parameters when considering the effect of urbanization on water quality. While understanding how the threshold of impervious surfaces affects water quality has been a hot topic, there has been little quantitative analysis on how such thresholds change during rapid urbanization periods across large urban areas. To remedy this deficiency, this article made use of remotely-sensed impervious surface area data and in situ water quality monitoring observations for the period 2000 to 2018 to quantitively derive the temporal variation in the thresholds of the percentage of the impervious surface area (PISA) when inferring the relationship between PISA and a set of water quality indicators for a selection of watersheds within the UAMLYREB. We employed segmented regression model to derive the nonlinear relationship between PISA, the water quality indicators, and the PISA-related thresholds. Our results indicate that PISA may be considered a useful water quality indicator over watershed spatial scales. We also found that the threshold effects differed between water quality indicators (DO, CODMn, NH3-N), where, except for NH3-N, the indicators showed a PISA threshold of 30.08 to 42.34%, with slight variations over the study period. These results imply that maintaining PISA to be around 30% in watershed areas may be sufficient to mitigate against water quality degradation during the urbanization process.", "pages": "8398-8406", "note": "", "ISSN": "2151-1535", "publicationtype": "article", "author": ["Li, Zhihui", "Peng, Lu", "Wu, Feng"]}]["9027867", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2020.2979255", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Collaboration", "Sparse matrices", "Time complexity", "Matrix decomposition", "Machine learning", "Neurons", "Training", "Recommendation system", "matrix factorization", "semi-autoencoder"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "Hybrid Collaborative Recommendation via Dual-Autoencoder", "url": "", "volume": "8", "year": "2020", "abstract": "With the rapid increase of internet information, personalized recommendation systems are an effective way to alleviate the information overload problem, which has attracted extensive attention in recent years. The traditional collaborative filtering utilizes matrix factorization methods to learn hidden feature representations of users and/or items. With deep learning achieved good performance in representation learning, the autoencoder model is widely applied in recommendation systems for the advantages of fast convergence and no label requirement. However, the previous recommendation systems may take the reconstruction output of an autoencoder as the prediction of missing values directly, which may deteriorate their performance and cause unsatisfactory results of recommendation. In addition, the parameters of an autoencoder need to be pre-trained ahead, which greatly increases the time complexity. To address these problems, in this paper, we propose a Hybrid Collaborative Recommendation method via Dual-Autoencoder (HCRDa). More specifically, firstly, a novel dual-autoencoder is utilized to simultaneously learn the feature representations of users and items in our HCRDa, which obviously reduces time complexity. Secondly, embedding matrix factorization into the training process of the autoencoder further improves the quality of hidden features for users and items. Finally, additional attributes of users and items are utilized to alleviate the cold start problem and to make hybrid recommendations. Comprehensive experiments on several real-world data sets demonstrate the effectiveness of our proposed method in comparison with several state-of-the-art methods.", "pages": "46030-46040", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Dong, Bingbing", "Zhu, Yi", "Li, Lei", "Wu, Xindong"]}]["8491315", {"address": "", "articleno": "", "doi": "10.1109/JIOT.2018.2876088", "issn": "2327-4662", "issue_date": "", "journal": "IEEE Internet of Things Journal", "keywords": ["Medical services", "Cloud computing", "Internet of Things", "Edge computing", "Quality of service", "Monitoring", "Data fusion", "healthcare application", "healthcare big data", "load balancing", "quality of service (QoS)", "real-time computing", "resource allocation"], "month": "June", "number": "3", "numpages": "", "publisher": "", "title": "Toward a Heterogeneous Mist, Fog, and Cloud-Based Framework for the Internet of Healthcare Things", "url": "", "volume": "6", "year": "2019", "abstract": "Rapid developments in the fields of information and communication technology and microelectronics allowed seamless interconnection among various devices letting them to communicate with each other. This technological integration opened up new possibilities in many disciplines including healthcare and well-being. With the aim of reducing healthcare costs and providing improved and reliable services, several healthcare frameworks based on Internet of Healthcare Things (IoHT) have been developed. However, due to the critical and heterogeneous nature of healthcare data, maintaining high quality of service (QoS)-in terms of faster responsiveness and data-specific complex analytics-has always been the main challenge in designing such systems. Addressing these issues, this paper proposes a five-layered heterogeneous mist, fog, and cloud-based IoHT framework capable of efficiently handling and routing (near-)real-time as well as offline/batch mode data. Also, by employing software defined networking and link adaptation-based load balancing, the framework ensures optimal resource allocation and efficient resource utilization. The results, obtained by simulating the framework, indicate that the designed network via its various components can achieve high QoS, with reduced end-to-end latency and packet drop rate, which is essential for developing next generatione-healthcare systems.", "pages": "4049-4062", "note": "", "ISSN": "2327-4662", "publicationtype": "article", "author": ["Asif-Ur-Rahman, Md.", "Afsana, Fariha", "Mahmud, Mufti", "Kaiser, M.", "Ahmed, Muhammad", "Kaiwartya, Omprakash", "James-Taylor, Anne"]}]["9292991", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2020.3044367", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Task analysis", "Clustering algorithms", "Matrix decomposition", "Complex networks", "Topology", "Symmetric matrices", "Network representation learning", "semi-supervised", "pairwise constraints", "community structure", "random walk"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "The Network Representation Learning Algorithm Based on Semi-Supervised Random Walk", "url": "", "volume": "8", "year": "2020", "abstract": "As an important tool of social network analysis, network representation learning also called network embedding maps the network to a latent space and learns low-dimensional and dense real vectors of nodes, while preserving the structure and internal attributes of network. The learned representations or embedding vectors can be used for node clustering, link prediction, network visualization and other tasks for network analysis. Most of the existing network representation learning algorithms mainly focus on the preservation of micro or macro network structure, ignoring the mesoscopic community structure information. Although a few network embedding methods are proposed to preserve the community structure, they all ignore the prior information about communities. Inspired by the semi-supervised community detection in complex networks, in this article, a novel Semi-Supervised DeepWalk method(SSDW) is proposed for network representation learning, which successfully preserves the community structure of network in the embedding space. Specifically, a semi-supervised random walk sampling method which effectively integrates the pairwise constraints is proposed. By doing so, the SSDW model can guide the transition probability in the random walk process and obtain the node context sequence in line with the prior knowledge. The experimental results on eight real networks show that comparing with the popular network embedding methods, the node representation vectors integrating pairwise constraints into the random walk process can obtain higher accuracy on node clustering task, and the results of link prediction, network visualization tasks indicate that the semi-supervised model SSDW is more discriminative than unsupervised ones.", "pages": "222956-222965", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Liu, Dong", "Li, Qinpeng", "Ru, Yan", "Zhang, Jun"]}]["9514599", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2021.3105189", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Random forests", "Production", "Predictive models", "Logic gates", "Computer architecture", "Radio frequency", "Indexes", "Long short-term memory (LSTM)", "random forests (RF)", "egg laying rate", "feature importance selection"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "A Novel Method to Predict Laying Rate Based on Multiple Environment Variables", "url": "", "volume": "9", "year": "2021", "abstract": "Realizing an accurate laying rate prediction based on environmental factors plays a vital role in livestock and poultry breeding. In this paper, multiple environmental factors were considered to improve the accuracy of egg production rate prediction. A method was proposed by combining the Random Forest (RF) and Long Short-Term Memory (LSTM) to analyze the impact of the external environmental factors on the laying rate. Firstly, using RF, feature importance selection was implemented on environmental factors affecting laying rate. Secondly, the extreme Gradient Boosting (XGBoost) was introduced as a comparison to evaluate the accuracy and reliability of the RF feature importance selection. Finally, by discarding the features with low importance one by one, the multi-variable RF-LSTM laying rate prediction was conducted. Experiment results showed that the proposed RF-LSTM method significantly improved the prediction accuracy on laying rate.", "pages": "115488-115496", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Yin, Hang", "Liu, Chuanyun", "Gao, Yacui", "Fan, Wenting", "Xiao, Bin", "Cao, Liang", "Hassan, Shahbaz", "Liu, Shuangyin"]}]["9042330", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2020.2981972", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Task analysis", "Particle swarm optimization", "Cloud computing", "Scheduling", "Quality of service", "Sociology", "Statistics", "Big data processing", "high-performance data processing", "networked data centre", "opposition-based learning", "tentative perception"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "Improved PSO Algorithm Integrated With Opposition-Based Learning and Tentative Perception in Networked Data Centres", "url": "", "volume": "8", "year": "2020", "abstract": "Particle swarm optimization (PSO) algorithms have low-quality initial particle swarm, which is generated by a random method when handling the problem of task scheduling in networked data centres. Such algorithms also fall easily into local optimum when searching for the optimal solution. To address these problems, this study proposes combining opposition-based learning (OBL) and tentative perception (TP) with PSO; the proposed method is called OBL-TP-PSO. This algorithm uses reverse learning to generate the initial population, such that the quality of the initial particle swarm can be improved. Before the particle speed and location are updated, the TP method is used to search for the individual optimum around each particle, thereby reducing the possibility of missing the potential optimal solution during the process of searching. In this manner, the problem in which the PSO algorithm easily falls into the local optimal is effectively solved. To evaluate the performance of the proposed algorithm, simulation experiments are performed on CloudSim toolkit. Experimental results show that in comparison with other algorithms (namely, Min-Min, Max-Min and PSO algorithm), the proposed OBL-TP-PSO algorithm has better performance in terms of the total execution time, load balancing and quality of service.", "pages": "55872-55880", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Zhou, Zhou", "Li, Fangmin", "Abawajy, Jemal", "Gao, Chaochao"]}]["8698790", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2019.2913215", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Quality control", "Biological system modeling", "Economics", "Indexes", "Process control", "Optimization", "Response surface methodology", "Parameter", "tolerance", "economic design", "multivariate quality characteristics", "process capability index", "individual observations"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "Parameter and Tolerance Economic Design for Multivariate Quality Characteristics Based on the Modified Process Capability Index With Individual Observations", "url": "", "volume": "7", "year": "2019", "abstract": "The process capability index (PCI) is widely used in an on-line quality control stage for measuring and controlling the quality level of a production process. The calculation of PCI requires a large number of samples, but in the off-line quality control stage, a certain production process in off-line quality control stage only has a few individual observations. From the perspective of quality loss and tolerance cost, this paper proposes a parameter and tolerance economic design approach for multivariate quality characteristics based on the modified PCI with individual observations. The response surface models of mean and variance are constructed using individual observations, and exponential models are fitted according to the tolerance cost data of design variables. A modified PCI is proposed with the consideration of three types of quality characteristics. The optimal design variables and tolerances are obtained by a comprehensive optimization model that is constructed based on the proposed PCI. An example of an isobutylene-isoprene rubber (IIR) inner tube is used to (i) demonstrate the implementation of our proposed approach, (ii) improve the PCI value and reflect the sensitivity of the deviation between process mean and specification, and (iii) reduce the risk of increasing cost of quality caused by replicated experimental design and some other unknown reasons.", "pages": "59249-59262", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Gu, Xiao-Guang", "Tong, Wen-Tao", "Han, Meng-Meng", "Wang, Yan", "Wang, Zhong-Yang"]}]["9364686", {"address": "", "articleno": "", "doi": "10.1109/JSTARS.2021.3062411", "issn": "2151-1535", "issue_date": "", "journal": "IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing", "keywords": ["Remote sensing", "Indexes", "Vegetation mapping", "Optical sensors", "Satellites", "Earth Observing System", "Reflectivity", "Generic optical remote sensing products (ORSPs)", "high-resolution earth observation system", "inversion technology", "quality evaluation", "remote sensing (RS) application", "RS product category", "validation"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "Research on Generic Optical Remote Sensing Products: A Review of Scientific Exploration, Technology Research, and Engineering Application", "url": "", "volume": "14", "year": "2021", "abstract": "With the initial establishment of global earth observation system in various countries, more and more high-resolution remote sensing data of multisource, multitemporal, multiscale, and different types of satellites are obtained. It is urgent to explore the advanced basic theory of remote sensing information science, design high-performance generic key technologies of remote sensing information system and global positioning system, and study complex engineering system of remote sensing applications and geographic information system. In this article, the basic theory exploration, inversion technology research, and engineering application design and development of generic optical remote sensing product (ORSP) are systematically reviewed. We classify the ORSP scientifically, review the main algorithms and application scope of 16 kinds of generic ORSP, and expound the validation and quality evaluation methods of ORSP in engineering application. Furthermore, we analyze the current core problems and solutions, and prospects for the state-of-the-art research and the future development trend of generic ORSP. This will provide valuable reference for scientific research and construction of high-resolution earth observation system.", "pages": "3937-3953", "note": "", "ISSN": "2151-1535", "publicationtype": "article", "author": ["Liu, Yang", "Zuo, Xianyu", "Tian, Junfeng", "Li, Shenshen", "Cai, Kun", "Zhang, Wanjun"]}]["9040585", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2020.2981669", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Stochastic processes", "Airports", "Uncertainty", "Atmospheric modeling", "Distribution networks", "Numerical models", "Hierarchical multimodal hub location", "cargo delivery systems", "stochastic programming", "mixed-integer linear programming", "memetic algorithm", "Monte Carlo simulation"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "Stochastic Hierarchical Multimodal Hub Location Problem for Cargo Delivery Systems: Formulation and Algorithm", "url": "", "volume": "8", "year": "2020", "abstract": "This study aims at developing a stochastic hierarchical multimodal hub location modeling framework for cargo delivery systems to capture uncertainty in hub construction cost and travel time at the strategic level. From a ring-star-star type network design perspective, a stochastic model is established to formulate this problem formally via the expected value and chance-constrained programming techniques. In particular, three types of chance constraints are proposed to ensure that the on-time delivery with pre-specified confidence levels in their respective layer networks. For normal distributions, the original stochastic model can be reformulated as a crisp equivalent mixed-integer linear programming (MILP) model by invoking the central limit theorem. Since the number of constraints and variables increases drastically with the size of cargo delivery distribution network, a memetic algorithm (MA) is designed. This algorithm incorporates genetic search and local intensification to obtain optimal/near-optimal solutions for realistic instance size within a reasonable time limit. For general distributions, it is difficult to convert the stochastic model into its deterministic counterpart. Hence, a hybrid methodology is further designed by combining the MA and Monte Carlo (MC) simulation to solve the proposed stochastic model. To demonstrate the properties of the proposed model and the performance of the designed algorithm, a series of numerical experiments are set up based on the Civil Aeronautics Board (CAB) and Turkish network data sets. Computational results indicate as the confidence level increases, the airport hubs are located further apart in the cargo delivery distribution network for gaining a greater time advantage. In addition, comparative results demonstrate that the MA algorithm proposed herein performs better than the genetic algorithm (GA) in terms of computing speed and quality of the solution.", "pages": "55076-55090", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Shang, Xiaoting", "Yang, Kai", "Wang, Weiqiao", "Wang, Weiping", "Zhang, Haifeng", "Celic, Selena"]}]["6574675", {"address": "", "articleno": "", "doi": "10.1109/TST.2013.6574675", "issn": "1007-0214", "issue_date": "", "journal": "Tsinghua Science and Technology", "keywords": ["Partitioning algorithms", "Data integration", "Internet", "Set theory", "Data mining", "Information management", "Internet of Things", "data fusion", "multidimensional data", "partitioning", "rough set theory"], "month": "August", "number": "4", "numpages": "", "publisher": "", "title": "An efficient multidimensional fusion algorithm for IoT data based on partitioning", "url": "", "volume": "18", "year": "2013", "abstract": "The Internet of Things (IoT) implies a worldwide network of interconnected objects uniquely addressable, via standard communication protocols. The prevalence of IoT is bound to generate large amounts of multisource, heterogeneous, dynamic, and sparse data. However, IoT offers inconsequential practical benefits without the ability to integrate, fuse, and glean useful information from such massive amounts of data. Accordingly, preparing us for the imminent invasion of things, a tool called data fusion can be used to manipulate and manage such data in order to improve process efficiency and provide advanced intelligence. In order to determine an acceptable quality of intelligence, diverse and voluminous data have to be combined and fused. Therefore, it is imperative to improve the computational efficiency for fusing and mining multidimensional data. In this paper, we propose an efficient multidimensional fusion algorithm for IoT data based on partitioning. The basic concept involves the partitioning of dimensions (attributes), i.e., a big data set with higher dimensions can be transformed into certain number of relatively smaller data subsets that can be easily processed. Then, based on the partitioning of dimensions, the discernible matrixes of all data subsets in rough set theory are computed to obtain their core attribute sets. Furthermore, a global core attribute set can be determined. Finally, the attribute reduction and rule extraction methods are used to obtain the fusion results. By means of proving a few theorems and simulation, the correctness and effectiveness of this algorithm is illustrated.", "pages": "369-378", "note": "", "ISSN": "1007-0214", "publicationtype": "article", "author": ["Zhou, Jin", "Hu, Liang", "Wang, Feng", "Lu, Huimin", "Zhao, Kuo"]}]["8976082", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2020.2970496", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Depression", "Estimation", "Generative adversarial networks", "Gallium nitride", "Data models", "Visualization", "Frequency measurement", "Depression estimation", "audio features", "data augmentation", "deep convolutional generative adversarial network", "spatial domain", "frequency domain", "deep learning aspect"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "Feature Augmenting Networks for Improving Depression Severity Estimation From Speech Signals", "url": "", "volume": "8", "year": "2020", "abstract": "Depression disorder has become one of the major psychological diseases endangering human health. Researcher in the affective computing community is supporting the development of reliable depression severity estimation system, from multiple modalities (speech, face, text), to assist doctors in their diagnosis. However, the limited amount of annotated data has become the main bottleneck restricting the study on depression screening, especially when deep learning models are used. To alleviate this issue, in this work we propose to use Deep Convolutional Generative Adversarial Network (DCGAN) for features augmentation to improve depression severity estimation from speech. To the best of our knowledge, this approach is the first attempt to apply the Generative Adversarial Network for depression severity estimation from speech. Besides, to measure the quality of the augmented features, we propose three different measurement criteria, characterizing the spatial, frequency and representation learning of the augmented features. Finally, the augmented features are used to train depression estimation models. Experiments are carried out on speech signals from the Audio Visual Emotion Challenge (AVEC2016) depression dataset, and the relationship between the model performance and data size is explored. Our experimental results show that: 1) The combination of the three proposed evaluation criteria can effectively and comprehensively evaluate the quality of the augmented features. 2) When increasing the size of the augmented data, the performance of depression severity estimation gradually improves and the model converges to a certain stable state. 3) The proposed DCGAN based data augmentation approach effectively improves the performance of depression severity estimation, with the root mean square error (RMSE) reduced to 5.520 and mean absolute error (MAE) reduced to 4.634, which is better than most of the state of the art results on AVEC 2016.", "pages": "24033-24045", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Yang, Le", "Jiang, Dongmei", "Sahli, Hichem"]}]["8314682", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2018.2815039", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Data centers", "Sensors", "Edge computing", "Routing", "Cloud computing", "Big Data", "Network architecture", "Big data", "edge computing", "services joint data routing", "in-network caching", "services fusion"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "A Services Routing Based Caching Scheme for Cloud Assisted CRNs", "url": "", "volume": "6", "year": "2018", "abstract": "With the emergence of Internet of Things, the number of connected devices has been dramatically increasing, causing severe spectrum shortage problem. To fully explore the spectrum resources, big data, and cloud computing can be employed by cognitive radio networks, to make efficient use of various sensing results from different sensing sources. However, the massive growth of sensing data brings tremendous load pressure on the data center, resulting in long service response time and poor Quality of Experience. Edge computing and fog computing deal with these issues by placing computation resources at the network edge. However, compared with the data center, the capabilities at edge servers are limited. Therefore, a services routing-based caching scheme (SRCS) is proposed, which can greatly lighten the load on the data center and maintain the advantages of global intelligent computing of traditional cloud computing. Specifically, SRCS first introduces the concept of transmitting service flow. At the edge layer, data are converted to service flow by network hardware and software, thus achieving the network architecture centered on service computing. Then, SRCS proposes a service routing based on service similarity, transmits similar services through the same path, and service data are fused on the path to minimize transmission load. Moreover, SRCS caches services in content routers (CRs). When the service is requested again, CRs are used as service providers to return data, thus achieving the nearest access to the content. Both theoretical analysis and experiment results demonstrate that comparing existing schemes, SRCS improves service response time by 13.67%\u201351.15%, reduces transmitting data amount by 23.62%\u201330.3%, and makes energy consumption more balanced.", "pages": "15787-15805", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Huang, Mingfeng", "Liu, Yuxin", "Zhang, Ning", "Xiong, Neal", "Liu, Anfeng", "Zeng, Zhiwen", "Song, Houbing"]}]["8883242", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2019.2949409", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Task analysis", "Privacy", "Quality control", "Crowdsourcing", "Uncertainty", "Entropy", "Data privacy", "Spatial crowdsourcing", "obfuscation location privacy of workers", "quality control", "EM algorithm"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "Preserving Location Privacy in Spatial Crowdsourcing Under Quality Control", "url": "", "volume": "7", "year": "2019", "abstract": "Emerging spatial crowdsourcing (SC) provides an approach for collecting and analyzing spatiotemporal information from intelligent transportation systems. However, the exposure of massive location privacy to potential adversaries for the purpose of quality control makes workers more vulnerable. To protect workers\u2019 location privacy, an obfuscation scheme is proposed to incorporate uncertainties into the SC quality control problem through obfuscating the standard location data in terms of both space and time. Two measures, location entropy and results accuracy, are used to evaluate the performance of location privacy protection. We theoretically and experimentally confirm the security and accuracy of the obfuscation approach. The results of experiments show that: a) hiding workers\u2019 location from the requester reduces the quality of SC; and b) obfuscation arithmetic with appropriate obfuscation coefficients protects workers\u2019 location privacy with little effect on SC quality. Under the protection of this obfuscation scheme, the new system provides better security and similar quality compared to the existing SC system.", "pages": "155851-155859", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Chu, Xiang", "Liu, Jun", "Gong, Daqing", "Wang, Rui"]}]["9018381", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2020.2976881", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Social network services", "Urban areas", "Analytical models", "Spatiotemporal phenomena", "Economic indicators", "Sociology", "Social media", "insomnia", "geographically weighted regression model", "influencing factors"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "Social Media Big Data-Based Research on the Influencing Factors of Insomnia and Spatiotemporal Evolution", "url": "", "volume": "8", "year": "2020", "abstract": "Insomnia is a prevalent sleep disorder that causes serious harm to individuals and society. It is closely linked to not only personal factors but also social, economic and other factors. This study explores the influencing factors and spatial differentiation of insomnia from the perspective of social media. This paper chose China's largest social media platform, Sina Weibo, as its data source. Then, based on the collected relevant data of 288 Chinese cities from 2013 to 2017, it explored the impact of economic, social, and environmental factors and an educated population on insomnia. Additionally, the importance and interaction of each influencing factor were analyzed. According to the results, the gross domestic product (GDP), proportion of households connected to the Internet and number of students in regular institutions of higher education are the major factors that influence insomnia, and their influences show obvious spatial nonstationarity. Rapid GDP growth has increased the probability of insomnia, and the positive correlation between the proportion of households connected to the internet and insomnia has strengthened annually. Although the impact of insomnia on college students decreased in some regions, the overall impact was still increasing annually, and spatial nonstationarity was obvious. Properly controlling GDP growth and unnecessary time spent online and guiding people to develop healthy Internet surfing habits and lifestyles will help improve their sleep quality. Our research results will help relevant professionals better understand the distribution of regional insomnia and provide a reference for related departments to formulate regional insomnia prevention and treatment policies.", "pages": "41516-41529", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Liu, Yu", "Luo, Qinyao", "Shen, Hang", "Zhuang, Sida", "Xu, Chen", "Dong, Yihe", "Sun, Yukai", "Wang, Shaochen", "Deng, Hao"]}]["9378518", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2021.3066041", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Data models", "Deep learning", "Image reconstruction", "Training", "Computational modeling", "Feature extraction", "Transfer learning", "Computer vision", "deep learning", "domain adaptation", "domain generalization", "transfer learning"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "Adversarial Reconstruction Loss for Domain Generalization", "url": "", "volume": "9", "year": "2021", "abstract": "The biggest fear when deploying machine learning models to the real world is their ability to handle the new data. This problem is significant especially in medicine, where models trained on rich high-quality data extracted from large hospitals do not scale to small regional hospitals. One of the clinical challenges addressed in this work is magnetic resonance image generalization for improved visualization and diagnosis of hip abnormalities such as femoroacetabular impingement and dysplasia. Domain Generalization (DG) is a field in machine learning that tries to solve the model's dependency on the training data by leveraging many related but different data sources. We present a new method for DG that is both efficient and fast, unlike the most current state of art methods, which add a substantial computational burden making it hard to fine-tune. Our model trains an autoencoder setting on top of the classifier, but the encoder is trained on the adversarial reconstruction loss forcing it to forget style information while extracting features useful for classification. Our approach aims to force the encoder to generate domain-invariant representations that are still category informative by pushing it in both directions. Our method has proven universal and was validated on four different benchmarks for domain generalization, outperforming state of the art on RMNIST, VLCS and IXMAS with a 0.70% increase in accuracy and providing comparable results on PACS with a 0.02% difference. Our method was also evaluated for unsupervised domain adaptation and has shown to be quite an effective method against over-fitting.", "pages": "42424-42437", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Bekkouch, Imad", "Nicolae, Drago\u015f", "Khan, Adil", "Kazmi, S.", "Khattak, Asad", "Ibragimov, Bulat"]}]["9040614", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2020.2981738", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Feature extraction", "Signal to noise ratio", "Media", "Reliability", "Training", "Dispersion", "Fast feature selection", "GFR", "steganalytic features", "multi-scale"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "A Multi-Scale Feature Selection Method for Steganalytic Feature GFR", "url": "", "volume": "8", "year": "2020", "abstract": "The Rich Model of the Gabor filter (referred to as the GFR steganalytic feature) can detect JPEG-adaptive steganography objects. However, feature dimensionality that is too high will lead to too much computation and will correspondingly reduce the detection efficiency. To reduce the dimensionality and the operating time of GFR steganalytic features and to improve the stego image detection accuracy, this paper proposes a multi-scale feature selection method for steganalytic feature GFR. First, we use the SNR criterion to measure the uselessness of each feature component and to provide a basis for the removal of useless steganalytic feature components. Second, we improve the Relief algorithm to measure the importance of feature components in detecting stego images, which provides a basis for the selection of important feature components. Then, we set the threshold value for deleting the useless feature components, and we select the important feature components as the final feature. Finally, we conduct experiments on feature selection for GFR with high-dimensional steganalytic features, and we compared the proposed method with the Fisher-based method, the PCA-based method, the SSFC method, and the Steganalysis-\u03b1 method. The results show that the method proposed in this paper is effective and fast.", "pages": "55063-55075", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Yu, Xinquan", "Ma, Yuanyuan", "Jin, Ruixia", "Xu, Lige", "Duan, Xintao"]}]["9086449", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2020.2992582", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Uncertainty", "Measurement uncertainty", "Rough sets", "Entropy", "Extraterrestrial measurements", "Information entropy", "Mathematical model", "Accuracy measure", "approximation accuracy", "approximation quality", "rough sets", "uncertainty measure"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "On Uncertainty Measure Issues in Rough Set Theory", "url": "", "volume": "8", "year": "2020", "abstract": "Rough set theory is a tool for dealing with uncertainty problems. How to measure the uncertainty of a knowledge is an important issue in the theory. However, the existing uncertainty measures may not accurately reflect the uncertainty degree. This study analyzes the causes of it and explores a reasonable solution to it. Firstly, the existing accuracy models only focuses on some factors related to the target set while neglecting its own important influence on the model. Secondly, since no one gives a clear definition of knowledge uncertainty in approximation space, it is difficult to evaluate the accuracy and rationality of a knowledge uncertainty measure. Thirdly, most uncertain measures of knowledge are constructed based on the structure of knowledge itself, while neglecting other factors in the approximation model. In view of these, we first propose a new accuracy model which fully considers the important role of the target set itself. Second, two definitions of accuracy measure of knowledge are proposed to explain what the uncertainty of a knowledge is. And then, two uncertainty measures of knowledge are proposed and a method for quickly calculating them is designed. At last, an uncertain entropy is constructed for more conveniently calculating of knowledge uncertainty.", "pages": "91089-91102", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Tang, Jianguo", "Wang, Jianghua", "Wu, Chunling", "Ou, Guojian"]}]["8936442", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2019.2958119", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Task analysis", "Radar imaging", "Quality assessment", "Image color analysis", "Feature extraction", "Computer vision", "Neural network", "multitasking", "computer vision", "incremental learning"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "Incremental Learning of Multi-Tasking Networks for Aesthetic Radar Map Prediction", "url": "", "volume": "7", "year": "2019", "abstract": "It is difficult and challenging to evaluate the aesthetics quality of images from multiple angles. Since humans' perception of images comes from many factors, the integrated image aesthetic quality assessment cannot be easily summarized by few attributes. A comprehensive evaluation is supposed to predict many aesthetic attributes across not only one dataset. This requires the model to have not only high accuracy, but also strong generalization ability, resulting in a better prediction on multiple models and datasets. Recent work shows that deep convolution neural network can be used to extract image features and further evaluate the total score of images, and the method of evaluation are lacking of sufficient detailed features. In this paper, we propose a multi-task convolution neural network with more incremental features. We show the results in the way of a hexagon map, which is called aesthetic radar map. This allows the network model to fit different attributes in various datasets better.", "pages": "183647-183655", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Jin, Xin", "Zhou, Xinghui", "Li, Xiaodong", "Zhang, Xiaokun", "Sun, Hongbo", "Li, Xiqiao", "Liu, Ruijun"]}]["7508921", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2016.2585468", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Internet of things", "Computer architecture", "3GPP", "Cloud computing", "Data analysis", "Smart devices", "5G mobile commuinication", "Big data", "Machine learning", "Wireless communication", "Internet of things (IoT)", "big data", "network coding", "network function computation", "machine learning", "wireless communications"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "CONDENSE: A Reconfigurable Knowledge Acquisition Architecture for Future 5G IoT", "url": "", "volume": "4", "year": "2016", "abstract": "In forthcoming years, the Internet of Things (IoT) will connect billions of smart devices generating and uploading a deluge of data to the cloud. If successfully extracted, the knowledge buried in the data can significantly improve the quality of life and foster economic growth. However, a critical bottleneck for realizing the efficient IoT is the pressure it puts on the existing communication infrastructures, requiring transfer of enormous data volumes. Aiming at addressing this problem, we propose a novel architecture dubbed Condense which integrates the IoT-communication infrastructure into the data analysis. This is achieved via the generic concept of network function computation. Instead of merely transferring data from the IoT sources to the cloud, the communication infrastructure should actively participate in the data analysis by carefully designed en-route processing. We define the Condense architecture, its basic layers, and the interactions among its constituent modules. Furthermore, from the implementation side, we describe how Condense can be integrated into the Third Generation Partnership Project (3GPP) machine type communications (MTCs) architecture, as well as the prospects of making it a practically viable technology in a short time frame, relying on network function virtualization and software-defined networking. Finally, from the theoretical side, we survey the relevant literature on computing atomic functions in both analog and digital domains, as well as on function decomposition over networks, highlighting challenges, insights, and future directions for exploiting these techniques within practical 3GPP MTC architecture.", "pages": "3360-3378", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Vukobratovic, Dejan", "Jakovetic, Dusan", "Skachek, Vitaly", "Bajovic, Dragana", "Sejdinovic, Dino", "Karabulut, G\u00fcne\u015f", "Hollanti, Camilla", "Fischer, Ingo"]}]["9893181", {"address": "", "articleno": "", "doi": "10.1109/TGRS.2022.3207098", "issn": "1558-0644", "issue_date": "", "journal": "IEEE Transactions on Geoscience and Remote Sensing", "keywords": ["Neurons", "Feature extraction", "Computational modeling", "Training", "Convolutional neural networks", "Hyperspectral imaging", "Brain modeling", "Approximate derivative algorithm", "brain-inspired computing", "hyperspectral image~(HSI) classification", "neuromorphic computing", "spiking neural network (SNN)"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "Hyperspectral Image Classification of Brain-Inspired Spiking Neural Network Based on Approximate Derivative Algorithm", "url": "", "volume": "60", "year": "2022", "abstract": "Recently, deep learning methods have made significant progress in solving hyperspectral image (HSI) classification problems of high-dimensional features, band redundancy, and spectral mixture. However, the deep neural network is too complex, with a long training time and high energy consumption, making it difficult to deploy on edge computing devices. In order to solve the above problems, this article proposes a brain-inspired computing framework based on the spiking leaky integrate-and-fire neuron model for HSIs\u2019 classification. Then, we design an approximate derivative algorithm to solve the nondifferentiable spike activity of the spiking neuron. The framework uses direct coding to generate spatiotemporal spikes for input HSI and achieves efficient extraction of spatial\u2013spectral features through spiking standard convolution and spiking depthwise separable convolution. Extensive experiments are performed on four benchmark hyperspectral datasets and two public unmanned aerial vehicle-borne hyperspectral datasets. Experiments show that the proposed model has the advantages of high classification accuracy and fewer spiking time steps. The proposed model can save about ten times computational energy consumption compared with the CNN of the same architecture. This research has great significance for overcoming the technical bottleneck of HSI classification based on brain-inspired computing, solving the critical problems of mobile computing in unmanned autonomous systems, and realizing the engineering application of unmanned aerial vehicles and software-defined satellites. The source code will be made available at https://github.com/Katherine-Cao/HSI_SNN.", "pages": "1-16", "note": "", "ISSN": "1558-0644", "publicationtype": "article", "author": ["Liu, Yang", "Cao, Kejing", "Li, Rui", "Zhang, Hongxia", "Zhou, Liming"]}]["9265432", {"address": "", "articleno": "", "doi": "10.35833/MPCE.2020.000041", "issn": "2196-5420", "issue_date": "", "journal": "Journal of Modern Power Systems and Clean Energy", "keywords": ["Monitoring", "Indexes", "Current measurement", "Power measurement", "Energy measurement", "Standards", "Voltage measurement", "Continuous statistical monitoring", "big data", "data compression", "higher-order statistics (HOSs)", "power quality (PQ)"], "month": "January", "number": "1", "numpages": "", "publisher": "", "title": "Site Characterization Index for Continuous Power Quality Monitoring Based on Higher-order Statistics", "url": "", "volume": "10", "year": "2022", "abstract": "The high penetration of distributed generation (DG) has set up a challenge for energy management and consequently for the monitoring and assessment of power quality (PQ). Besides, there are new types of disturbances owing to the uncontrolled connections of non-linear loads. The stochastic behaviour triggers the need for new holistic indicators which also deal with big data of PQ in terms of compression and scalability so as to extract the useful information regarding different network states and the prevailing PQ disturbances for future risk assessment and energy management systems. Permanent and continuous monitoring would guarantee the report to claim for damages and to assess the risk of PQ distortions. In this context, we propose a measurement method that postulates the use of two-dimensional (2D) diagrams based on higher-order statistics (HOSs) and a previous voltage quality index that assesses the voltage supply waveform in a continous monitoring campaign. Being suitable for both PQ and reliability applications, the results conclude that the inclusion of HOS measurements in the industrial metrological reports helps characterize the deviations of the voltage supply waveform, extracting the individual customers' pattern fingerprint, and compressing the data from both time and spatial aspects. The method allows a continuous and robust performance needed in the SG framework. Consequently, the method can be used by an average consumer as a probabilistic method to assess the risk of PQ deviations in site characterization.", "pages": "222-231", "note": "", "ISSN": "2196-5420", "publicationtype": "article", "author": ["Florencias-Oliveros, Olivia", "Gonz\u00e1lez-de-la-Rosa, Juan-Jos\u00e9", "Sierra-Fern\u00e1ndez, Jose-Mar\u00eda", "Ag\u00fcera-P\u00e9rez, Agust\u00edn", "Espinosa-Gavira, Manuel-Jes\u00fas", "Palomares-Salas, Jos\u00e9-Carlos"]}]["9286384", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2020.3043240", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Encryption", "Chaotic communication", "Matching pursuit algorithms", "Compressed sensing", "Image coding", "Sparse matrices", "Logistics", "M sequence", "compressive sensing", "image encryption", "improved 1D chaotic map"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "An Image Encryption Algorithm Based on Compressive Sensing and M Sequence", "url": "", "volume": "8", "year": "2020", "abstract": "In this article, a new image encryption algorithm based on compressive sensing (CS) and M sequence is proposed to decrease the image communication load and improve the security of image communication in the internet of things. Most of the available image encryption schemes are based on chaotic systems to shuffle the image pixels. Before shuffling the image pixels, the random sequence, which is produced by a chaotic system, need to be sorted. This sorting operation is avoided by utilizing a modified linear feedback shift register (LFSR) state sequence. Then, the security of the proposed scheme is improved by combining CS with an improved 1D chaotic system, which is used to construct a measurement matrix. The computational complexity is reduced by the use of the improved 1D chaotic system. Simultaneously, the amount of image data is reduced. Simulation results and performance analyses demonstrate that the proposed encryption scheme can greatly reduce the amount of image data and has good security and robustness.", "pages": "220646-220657", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Dou, Yuqiang", "Li, Ming"]}]["9229407", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2020.3032169", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Three-dimensional displays", "Image reconstruction", "Surface reconstruction", "Solid modeling", "Computational modeling", "Cameras", "Structure from motion", "3D reconstruction", "feature tracking", "structure from motion", "multi-view stereo", "surface reconstruction"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "An End-to-End Approach to Reconstructing 3D Model From Image Set", "url": "", "volume": "8", "year": "2020", "abstract": "Large-scale 3D reconstruction from imagery has received much attention from the computer vision community. However, recovering 3D structures from 2D images is a notoriously complex process that requires expertise with often limited results. This paper presents an end-to-end 3D reconstruction system that can produce high-quality 3D models from a set of unordered image collections. Our workflow is a typical 3D reconstruction architecture that consists of structure from motion (SFM), multi-view stereo (MVS), and surface reconstruction, and can automatically recover desirable 3D models without any interactive operations. Finally, a comprehensive experiment is conducted on several benchmark datasets to assess the presented system. Experimental results show that the presented system achieves significant improvements in reconstruction accuracy and completeness over the existing state-of-the-art approach.", "pages": "193268-193284", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Cai, Youcheng", "Cao, Mingwei", "Li, Lin", "Liu, Xiaoping"]}]["8993765", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2020.2973209", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Imaging", "Discrete cosine transforms", "Blood", "Correlation", "Finite element analysis", "US Department of Transportation", "Photonics", "Diffuse correlation tomography", "blood flow", "image reconstruction", "finite element method", "Nth-order linear approach"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "Impact of Reconstruction Algorithms on Diffuse Correlation Tomography Blood Flow Imaging", "url": "", "volume": "8", "year": "2020", "abstract": "Near-infrared diffuse correlation tomography (DCT) is an emerging technology for non-invasive imaging of the tissue blood flow. The flow imaging quality relies on the image reconstruction algorithm, which, however, is little studied thus far. In this study, we conducted the first investigation of reconstruction algorithm impact on DCT blood flow imaging. Two reconstruction algorithms, i.e., the finite element method (FEM) representing the imaging framework of partial differential equation, and the Nth-order linear (NL) approach, representing the imaging framework of integral equation that was recently proposed by us to incorporate the tissue morphological information, were compared. Both computer simulations and phantom experiment outcomes show that the NL approach performs much better in image accuracy and homogeneity over anomaly or background, when compared with the FEM at the same source-detector configuration and spatial resolution. This study demonstrates that the DCT blood flow imaging is substantially influenced by the reconstruction algorithm, thus it has great potential in future algorithm design and optimization.", "pages": "31882-31891", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Zuo, Jia", "Zhang, Xiaojuan", "Lu, Jianju", "Gui, Zhiguo", "Shang, Yu"]}]["9826585", {"address": "", "articleno": "", "doi": "10.23919/JCN.2022.000016", "issn": "1976-5541", "issue_date": "", "journal": "Journal of Communications and Networks", "keywords": ["Drones", "Task analysis", "Internet of Things", "Resource management", "Optimization", "Servers", "Computational efficiency", "Bandwidth allocation", "inter-server computation offloading", "multi-drone", "space-air-ground integrated IoT network"], "month": "June", "number": "3", "numpages": "", "publisher": "", "title": "Inter-server computation offloading and resource allocation in multi-drone aided space-air-ground integrated IoT networks", "url": "", "volume": "24", "year": "2022", "abstract": "Combining mobile edge computing (MEC), the multi-drone aided space-air-ground integrated Internet of things (SAG-IoT) networks can provide ground IoT devices (GIDs) high-quality wireless access and computing services. However, the diverse tasks, moving drones, and limited network resources reveal great challenges for the task offloading and resource allocation scheme exploitation. Especially, given the restricted computation resources, how to make full use of available applications deployed on MEC servers (MECSs) to compute various types of tasks, is even an important issue. To the best of our knowledge, it is an entirely new problem since most existing works in this line assume that all types of applications can be deployed on one MECS so as to process various offloaded tasks. Toward this end, we present this paper to investigate inter-server computation offloading, resource allocation, and drone deployment to minimize the overall computation overhead of all GIDs. An iteratively optimization algorithm is proposed which alternately utilizes heuristic greedy and successive convex approximation methods. Simulation results verify that, for different GID numbers, optimization schemes, and computing models, our devised schemes can not only significantly reduce the overall computation overhead but also achieve optimal decisions of computation offloading, spectrum allocation, and drone deployment.", "pages": "324-335", "note": "", "ISSN": "1976-5541", "publicationtype": "article", "author": ["Shi, Yongpeng", "Zhang, Junjie", "Gao, Ya", "Xia, Yujie"]}]["9792257", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2022.3181815", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Rail transportation", "Microscopy", "Optimization", "Resource management", "Maintenance engineering", "Heuristic algorithms", "Numerical models", "Multi-station high-speed railway hub", "train routing problem", "EMUs depot", "Lagrangian relaxation", "passenger demand"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "Optimizing Train Routing Problem in a Multistation High-Speed Railway Hub by a Lagrangian Relaxation Approach", "url": "", "volume": "10", "year": "2022", "abstract": "As the intersection of multiple high-speed railway lines, the multi-station high-speed railway hub is the key to improve the transport efficiency of the high-speed railway network. This paper focuses on the optimization of the multi-station high-speed railway hub and models it as a train routing problem (TRP). Considering the capacity of railway infrastructures and the demand of passengers, a mixed integer linear programming model is proposed to minimize the total cost of train routes and passenger routes. The optimized train routes include the macroscopic routes between stations and the microscopic track allocation inside stations and Electric Multiple Units (EMUs) depots. A Lagrangian relaxation (LR) approach is developed to dualize the hard constraints and decompose the origin model into train and passenger subproblems, then a shortest path algorithm is designed to solve the subproblems independently. Numerical experiments based on an illustrative railway hub network and a real-world network are implemented to demonstrate the effectiveness of the model and algorithm. The solution results prove that the LR approach can obtain high-quality solutions within an acceptable computational time. Compared with the existing fixed scheme, the optimization scheme can reduce the total cost by 37.18% and utilize the railway lines and tracks more reasonably.", "pages": "61992-62010", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Wang, Yidong", "Song, Rui", "He, Shiwei", "Song, Zilong", "Chi, Jushang"]}]["9319232", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2021.3050404", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Databases", "Itemsets", "Clustering algorithms", "Data models", "Prototypes", "Computer science", "Computational modeling", "Multi-database mining", "graph clustering", "dual gradient descent", "quasi-convex optimization", "similarity measure"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "A Gradient-Based Clustering for Multi-Database Mining", "url": "", "volume": "9", "year": "2021", "abstract": "Multinational corporations have multiple databases distributed throughout their branches, which store millions of transactions per day. For business applications, identifying disjoint clusters of similar and relevant databases contributes to learning the common buying patterns among customers and also increases the profits by targeting potential clients in the future. This process is called clustering, which is an important unsupervised technique for big data mining. In this article, we present an effective approach to search for the optimal clustering of multiple transaction databases in a weighted undirected similarity graph. To assess the clustering quality, we use dual gradient descent to minimize a constrained quasi-convex loss function whose parameters will determine the edges needed to form the optimal database clusters in the graph. Therefore, finding the global minimum is guaranteed in a finite and short time compared with the existing non-convex objectives where all possible candidate clusterings are generated to find the ideal clustering. Moreover, our algorithm does not require specifying the number of clusters a priori and uses a disjoint-set forest data structure to maintain and keep track of the clusters as they are updated. Through a series of experiments on public data samples and precomputed similarity matrices, we show that our algorithm is more accurate and faster in practice than the existing clustering algorithms for multi-database mining.", "pages": "11144-11172", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Miloudi, Salim", "Wang, Yulin", "Ding, Wenjia"]}]["9431193", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2021.3080288", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Semantics", "Trajectory", "Location awareness", "Hidden Markov models", "Deep learning", "Noise measurement", "Indoor environment", "Deep learning", "indoor localization", "the Internet of Things", "rule-based refinement", "semantic trajectories"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "A Stacked Denoising Autoencoder and Long Short-Term Memory Approach With Rule-Based Refinement to Extract Valid Semantic Trajectories", "url": "", "volume": "9", "year": "2021", "abstract": "Indoor location-based services have been widely investigated to take advantage of semantic trajectories for providing user oriented services in indoor environments. Although indoor semantic trajectories can provide seamless understanding to users regarding the provided location-based services, studies on the application of deep learning approaches for robust and valid semantic indoor localization are lacking. In this study, we combined a stacked denoising autoencoder and long short term memory technique with a rule-based refinement method applying a rule-based hidden Markov model (HMM) to perform robust and valid semantic trajectory extraction. In particular, our rule-based HMM approach incorporates a direct set of rules into HMM to resolve invalid movements of the extracted semantic trajectories and is extensible to various deep learning techniques. We compared the performance of our proposed approach with that of other cutting-edge deep learning approaches on two different real-world data sets. The experimental results demonstrate the feasibility of our proposed approach to produce more robust and valid semantic trajectories.", "pages": "73152-73168", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Yustiawan, Yoga", "Ramadhan, Hani", "Kwon, Joonho"]}]["9235583", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2020.3033200", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Recurrent neural networks", "Knowledge engineering", "Deep learning", "Predictive models", "Education", "Task analysis", "Student performance prediction", "knowledge tracing", "recurrent neural network", "attention mechanism"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "Multiple Features Fusion Attention Mechanism Enhanced Deep Knowledge Tracing for Student Performance Prediction", "url": "", "volume": "8", "year": "2020", "abstract": "Student performance prediction is a fundamental task in online learning systems, which aims to provide students with access to active learning. Generally, student performance prediction is achieved by tracing the evolution of each student's knowledge states via a series of learning activities. Every learning activity record has two types of feature data: student behavior and exercise features. However, most methods use features that are related to exercises, such as correctness and concepts, while other student behavior features are usually ignored. The few studies that have focused on student behavior features through subjective manual selection argue that different student behavior features can be used in an equivalent manner to predict student performance. In this paper, we assume that the integration of student behavior features and exercise features is crucial to improve the precision of prediction, and each feature has a different impact on student performance. Therefore, this paper proposes a novel framework for student performance prediction by making full use of both student behavior features and exercise features and combining the attention mechanism with the knowledge tracing model. Specifically, we first exploit machine learning to capture feature representation automatically. Then, a fusion attention mechanism based on recurrent neural network architecture is used for student performance prediction. Extensive experiments on a real-world dataset show the effectiveness and practicability of our approach. The accuracy of our method is up to 98%, which is superior to previous methods.", "pages": "194894-194903", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Liu, Dong", "Zhang, Yunping", "Zhang, Jun", "Li, Qinpeng", "Zhang, Congpin", "Yin, Yu"]}]["9454474", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2021.3089357", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Security", "Particle swarm optimization", "Encryption", "Image edge detection", "Payloads", "Optimization", "Medical diagnostic imaging", "Steganography", "encryption", "embedding process", "salp swarm optimization", "hybrid fuzzy neural network"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "SSII: Secured and High-Quality Steganography Using Intelligent Hybrid Optimization Algorithms for IoT", "url": "", "volume": "9", "year": "2021", "abstract": "Internet of Things (IoT) is a domain where the transfer of big data is taking place every single second. The security of these data is a challenging task; however, security challenges can be mitigated with cryptography and steganography techniques. These techniques are crucial when dealing with user authentication and data privacy. In the proposed work, a highly secured technique is proposed using IoT protocol and steganography. This work proposes an image steganography procedure by utilizing the combination of various algorithms that build the security of the secret data by utilizing Binary bit-plane decomposition (BBPD) based image encryption technique. Thereafter a Salp Swarm Optimization Algorithm (SSOA) based adaptive embedding process is proposed to increase the payload capacity by setting different parameters in the steganographic embedding function for edge and smooth blocks. Here the SSOA algorithm is used to localize the edge and smooth blocks efficiently. Then, the hybrid Fuzzy Neural Network with a backpropagation learning algorithm is used to enhance the quality of the stego images. Then these stego images are transferred to the destination in the highly secured protocol of IoT. The proposed steganography technique shows better results in terms of security, image quality, and payload capacity in comparison with the existing state of art methods.", "pages": "87563-87578", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Dhawan, Sachin", "Chakraborty, Chinmay", "Frnda, Jaroslav", "Gupta, Rashmi", "Rana, Arun", "Pani, Subhendu"]}]["8844663", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2019.2942485", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Anomaly detection", "Microprocessors", "Computer architecture", "Quality of service", "Quality of experience", "Servers", "Cellular networks", "Cellular network", "anomaly detection", "call detail record", "deep learning", "big data analytics", "sleeping cell", "congestion detection"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "Mobile Edge Computing-Based Data-Driven Deep Learning Framework for Anomaly Detection", "url": "", "volume": "7", "year": "2019", "abstract": "5G is anticipated to embed an artificial intelligence (AI)-empowerment to adroitly plan, optimize and manage the highly complex network by leveraging data generated at different positions of the network architecture. Outages and situation leading to congestion in a cell pose severe hazard for the network. High false alarms and inadequate accuracy are the major limitations of modern approaches for the anomaly\u2014outage and sudden hype in traffic activity that may result in congestion\u2014detection in mobile cellular networks. This indicates wasting limited resources that ultimately leads to an elevated operational expenditure (OPEX) and also interrupting quality of service (QoS) and quality of experience (QoE). Motivated by the outstanding success of deep learning (DL) technology, our study applies it for detection of the above-mentioned anomalies and also supports mobile edge computing (MEC) paradigm in which core network (CN)\u2019s computations are divided across the cellular infrastructure among different MEC servers (co-located with base stations), to relief the CN. Each server monitors user activities of multiple cells and utilizes $L$ -layer feedforward deep neural network (DNN) fueled by real call detail record (CDR) dataset for anomaly detection. Our framework achieved 98.8% accuracy with 0.44% false positive rate (FPR)\u2014notable improvements that surmount the deficiencies of the old studies. The numerical results explicate the usefulness and dominance of our proposed detector.", "pages": "137656-137667", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Hussain, Bilal", "Du, Qinghe", "Zhang, Sihai", "Imran, Ali", "Imran, Muhammad"]}]["9703317", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2022.3148294", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Respiratory system", "Atmospheric modeling", "Predictive models", "Pediatrics", "Deep learning", "Data models", "Atmospheric measurements", "Asthma", "big data", "machine learning", "recurrent neural network", "peak expiratory flow rates (PEFR)"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "Forecasting the Effects of Real-Time Indoor PM2.5 on Peak Expiratory Flow Rates (PEFR) of Asthmatic Children in Korea: A Deep Learning Approach", "url": "", "volume": "10", "year": "2022", "abstract": "We built a deep learning algorithm to predict the deterioration of health symptoms among asthmatic children between 8\u201312 years of age. It is based on Peak Expiratory Flow Rates (PEFR) and indoor air pollution data, as well as meteorological data collected at their indoor residences every 2 minutes using portable monitoring devices with a low-cost sensor between November 2018 and March 2019. The PEFR results collected twice a day were matched with daily PM2.5. A personalized model has been developed to predict the peak expiratory flow rate of the next day, considering indoor air quality data including PM2.5, humidity, temperature, and CO2 level in previous days. Two models were developed incorporating Indoor Air Quality (IAQ) with the PEFR-only model. The IAQ uses the daily IAQ, and 10-minute basis IAQ in predicting the future PEFR. Recurrent Neural Networks (RNN) and Deep Neural Networks (DNN) models were trained using 4 months of linked data to predict PEFR for the next days during the study period. The 10-minute RNN model was found to predict better PEFR with a Root Mean Square Error (RMSE) of 42.5 and a Mean Absolute Percentage Error (MAPE) of 14.0, as it consolidates the cumulative effects of PM2.5 concentrations over time. The highly accurate estimation showed that indoor air quality significantly affects PEFR.", "pages": "19391-19400", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Woo, Jiyoung", "Lee, Ji-Hyun", "Kim, Yeonjin", "Rudasingwa, Guillaume", "Lim, Dae", "Kim, Sungroul"]}]["9530389", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2021.3110709", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Convolution", "Training", "Logic gates", "Feature extraction", "Computational modeling", "Adaptive systems", "Relays", "Convolution neural network", "deep learning", "dynamic neural structure", "micronetwork", "multilayer perceptron"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "Pluggable Micronetwork for Layer Configuration Relay in a Dynamic Deep Neural Surface", "url": "", "volume": "9", "year": "2021", "abstract": "The classical convolution neural network architecture adheres to static declaration procedures, which means that the shape of computation is usually predefined and the computation graph is fixed. In this research, the concept of a pluggable micronetwork, which relaxes the static declaration constraint by dynamic layer configuration relay, is proposed. The micronetwork consists of several parallel convolutional layer configurations and relays only the layer settings, incurring a minimum loss. The configuration selection logic is based on the conditional computation method, which is implemented as an output layer of the proposed micronetwork. The proposed micronetwork is implemented as an independent pluggable unit and can be used anywhere on the deep learning decision surface with no or minimal configuration changes. The MNIST, FMNIST, CIFAR-10 and STL-10 datasets have been used to validate the proposed research. The proposed technique is proven to be efficient and achieves appropriate validity of the research by obtaining state-of-the-art performance in fewer iterations with wider and compact convolution models. We also naively attempt to discuss the involved computational complexities in these advanced deep neural structures.", "pages": "124831-124846", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Khan, Farhat", "Aziz, Izzatdin", "Akhir, Emilia"]}]["9580651", {"address": "", "articleno": "", "doi": "10.1109/JSTARS.2021.3120724", "issn": "2151-1535", "issue_date": "", "journal": "IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing", "keywords": ["Computational modeling", "Sensors", "Data models", "Servers", "Real-time systems", "Adaptation models", "Resource management", "Convolutional neural networks (CNN)", "deep q-learning (DQL)", "federated learning (FL)", "quality of service (QoS)", "real-time image classifications"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "Adaptive Resource Optimized Edge Federated Learning in Real-Time Image Sensing Classifications", "url": "", "volume": "14", "year": "2021", "abstract": "With the exponential growth of the Internet of things (IoT) in remote sensing image applications, network resource orchestration and data privacy are significant aspects to handle in bigdata cellular networks. The image data sharing procedure toward central cloud servers in order to perform real-time classifications has leaked client personalization and heavily burdened the communication networks. Thus, the deployment of IoT image sensors in privacy-constrained sectors requires an optimized federated learning (FL) scheme to efficiently consider both aspects of securing data privacy and maximizing the model accuracy with sufficient communication and computation resources. In this article, an adaptive model communication scheme with virtual resource optimization for edge FL is proposed by converging a deep q-learning algorithm to enforce a self-learning agent interacting with network functions virtualization orchestrator and software-defined networking based architecture. The agent targets to optimize the resource control policy of virtual multi-access edge computing entities in virtualized infrastructure manager. The proposed scheme trains the learning model and weighs the optimal actions for particular network states by using an epsilon-greedy strategy. In the exploitation phase, the scheme considers multiple spatial-resolution sensing conditions and allocates computation offloading resources for global multiconvolutional neural networks model aggregation based on the congestion states. In the simulation results, the quality of service and global collaborative model performance metrics were evaluated in terms of delay, packet drop ratios, packet delivery ratios, loss values, and overall accuracy.", "pages": "10929-10940", "note": "", "ISSN": "2151-1535", "publicationtype": "article", "author": ["Tam, Prohim", "Math, Sa", "Nam, Chaebeen", "Kim, Seokhoon"]}]["9767588", {"address": "", "articleno": "", "doi": "10.1109/TETC.2022.3170544", "issn": "2168-6750", "issue_date": "", "journal": "IEEE Transactions on Emerging Topics in Computing", "keywords": ["Malware", "Generative adversarial networks", "Generators", "Training", "Training data", "Big Data", "Linear programming", "Zero-day Malware", "Analogous Malware Detection", "Malware Augmentation", "Malware Data", "Generative Adversarial Networks"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "PlausMal-GAN: Plausible Malware Training Based on Generative Adversarial Networks for Analogous Zero-day Malware Detection", "url": "", "volume": "", "year": "2022", "abstract": "Zero-day malicious software (malware) refers to a previously unknown or newly discovered software vulnerability. The fundamental objective of this paper is to enhance detection for analogous zero-day malware by efficient learning to plausible generated data. To detect zero-day malware, we proposed a malware training framework based on the generated analogous malware data using generative adversarial networks (PlausMal-GAN). Thus, the PlausMal-GAN can suitably produce analogous zero-day malware images with high quality and high diversity from the existing malware data. The discriminator, as a detector, learns various malware features using both real and generated malware images. In terms of performance, the proposed framework showed higher and more stable performances for the analogous zero-day malware images, which can be assumed to be analogous zero-day malware data. We obtained reliable accuracy performances in the proposed PlausMal-GAN framework with representative GAN models (i.e., deep convolutional GAN, least-squares GAN, Wasserstein GAN with gradient penalty, and evolutionary GAN). These results indicate that the use of the proposed framework is beneficial for the detection and prediction of numerous and analogous zero-day malware data from noted malware when developing and updating malware detection systems.", "pages": "1-1", "note": "", "ISSN": "2168-6750", "publicationtype": "article", "author": ["Won, Dong-Ok", "Jang, Yong-Nam", "Lee, Seong-Whan"]}]["9905561", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2022.3210578", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["High-speed rail transportation", "Freight handling", "Rail transportation", "Optimization", "Resource management", "Genetic algorithms", "Heuristic algorithms", "Generation algorithm", "genetic algorithm", "high-speed railway", "passenger and freight transport coordination", "the line planning", "train candidate set"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "Optimization of High-Speed Railway Line Planning With Passenger and Freight Transport Coordination", "url": "", "volume": "10", "year": "2022", "abstract": "This paper studies the line planning optimization problem based on the coordinate mode of high-speed railway (HSR) passenger trains and freight trains. The multi objective nonlinear mixed integer programming model of HSR passenger train and freight train line planning with passengers and freight is designed on the basis of comprehensive consideration of passenger and freight transport demand. Then, in order to simultaneously determine the types, origin and destination stations, operation sections, stop schemes, operation frequencies, and demand allocation of HSR passenger trains and freight trains, the model is solved iteratively using a hybrid heuristic algorithm combining a column generation algorithm and a genetic algorithm. Finally, a numerical experiment based on the operation data of China\u2019s Dalian-Harbin HSR line is implemented to verify the effectiveness of the proposed model and algorithm, and the solution performance of the CPLEX solver and the hybrid heuristic algorithm is compared. The results show that both the CPLEX solver and the hybrid heuristic algorithm can obtain the global optimal solution set. With the expansion of the scale of the problem, the solution quality and convergence efficiency of the hybrid heuristic algorithm have significantly improved, and it can solve large-scale problems and obtain satisfactory solutions within a shorter time.", "pages": "110217-110247", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Li, Shiqi", "Lang, Maoxiang", "Li, Siyu", "Chen, Xinghan", "Yu, Xueqiao", "Geng, Yixuan"]}]["8528409", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2018.2878761", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Sensors", "Task analysis", "Analytic hierarchy process", "Training", "Technological innovation", "Data integrity", "Simulation", "Mobile crowdsensing", "incentive mechanism", "analytic hierarchy process", "multi-attribute user selection", "participation intention analysis"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "MAIM: A Novel Incentive Mechanism Based on Multi-Attribute User Selection in Mobile Crowdsensing", "url": "", "volume": "6", "year": "2018", "abstract": "In the user selection phase of mobile crowdsensing, most existing incentive mechanisms focus on either single-attribute selection or random selection, which possibly lead to serious consequences such as low user enthusiasm, decreased task completion rate, and increased cost of platform consumption. To tackle these issues, in this paper, we propose a novel incentive mechanism MAIM, which is based on multi-attribute user selection and participation intention analysis function in mobile crowdsensing. In this mechanism, the sensing platform employs the analytic hierarchy process to determine the weights of three attributes: participation threshold, cost, and reputation. The weight calculation results of each sensing user with respect to each attribute are then integrated to obtain the sorted weight of each user, with which the sensing platform will then obtain the optimal user set. From the users' perspective, they can autonomously decide whether to accept task processing requests, as enabled by the participation intention analysis function, thereby voiding the absolute authority and control of the sensing platform over users and achieving a two-way selection between the sensing platform and the sensing users. Furthermore, the sensing platform establishes a score-based reputation reward to inspire active performers and utilizes a punishment mechanism to overawe malicious vandals, which substantially helps activize enthusiasm of user participation and improve sensing data quality. Simulation results indicate that the proposed MAIM has significantly improved the sensing task completion ratio and the budget surplus ratio compared with the existing incentive mechanisms in mobile crowdsensing.", "pages": "65384-65396", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Xiong, Jinbo", "Chen, Xiuhua", "Tian, Youliang", "Ma, Rong", "Chen, Lei", "Yao, Zhiqiang"]}]["9594816", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2021.3124339", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Rough sets", "Information systems", "Data models", "Approximation algorithms", "Granular computing", "Computational modeling", "Big Data", "Tolerance relation", "local rough sets", "attribute reduction", "set-valued information systems"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "Local Generalized Multigranulation Variable Precision Tolerance Rough Sets and its Attribute Reduction", "url": "", "volume": "9", "year": "2021", "abstract": "In the era of big data, as for an important granular computing model, rough set model is an important tool for us to deal with data. As a kind of extension of classical rough sets, multigranulation rough sets have two forms, including optimistic and pessimistic cases. However, these two models have their shortcomings, one is too loose, and the other is too strict. To overcome the above shortcomings, based on the concept of local multigranulation tolerance rough sets in set-valued information systems, the local generalized multigranulation variable precision tolerance rough sets model by introducing characteristic function is established. Then the related properties are studied and proved. In addition, we define the concepts of lower approximate quality, inner and outer importance of attribute according to different granularity structures in set-valued decision information systems because different granularity structures have different effectives on the decision classes. Finally, the local attribute reduction algorithm and the global attribute reduction algorithm of local generalized multigranulation variable precision tolerance rough sets in set-valued decision information systems are given, and the effectiveness of the algorithms is proved by using UCI data sets.", "pages": "147237-147249", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Zhou, Yueli", "Lin, Guoping"]}]["8782454", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2019.2928441", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Access control", "Encryption", "Medical services", "Keyword search", "Internet of Things", "Internet of Things", "fine-grained and ranked", "multi-keyword retrieval", "hierarchical data"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "Fine-Grained Ranked Multi-Keyword Search Over Hierarchical Data for IoT-Oriented Health System", "url": "", "volume": "7", "year": "2019", "abstract": "With the rapid advance of the Internet of Things (IoT) and cloud computing technologies, the IoT-oriented health is expected to greatly improve the quality of healthcare service. However, data security and privacy concerns have become one of the biggest issues in smart health applications. As a potential and promising solution, attribute-based keyword search (ABKS) can provide fine-grained keyword search and access control over the encrypted data at the same time. Nevertheless, prior ABKS schemes cannot simultaneously support fine-grained, effective, and accurate data retrieval over hierarchical data. In this paper, to tackle these issues, we propose a fine-grained ranked multi-keyword search scheme over hierarchical data by leveraging ciphertext-policy hierarchical attribute-based encryption (CP-HABE) and ranked multi-keyword search (RMKS) technologies. Then, we prove that our proposed scheme is selectively secure through security analysis and we also show the practicability and feasibility of the proposed scheme by performance evaluation.", "pages": "101969-101980", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Sun, Jianfei", "Hu, Shengnan", "Nie, Xuyun"]}]["8482117", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2018.2873634", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Feature extraction", "Cancer", "Mutual information", "Redundancy", "Time complexity", "Predictive models", "Prediction algorithms", "Dimension reduction", "feature selection", "high-dimensional data", "criteria fusion", "cancer prediction"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "A New Filter Feature Selection Based on Criteria Fusion for Gene Microarray Data", "url": "", "volume": "6", "year": "2018", "abstract": "In machine learning and data mining, feature selection aims to seek a compact and discriminant feature subset from the original feature space. It is usually used as a preprocessing step to improve the prediction performance, understandability, scalability, and generalization capability of classifiers. A typical gene microarray data set has the characteristics of high dimensionality, limited samples, and most irrelevant features, and these characteristics make it difficult to discover a compact set of features that really contribute to the response of the model. In this paper, a score-based criteria fusion feature selection method (SCF) is proposed for cancer prediction, and this method aims at improving the prediction performance of the classification model. The SCF method is evaluated on five open gene microarray data sets and three low-dimensional data sets, and it shows superior performance over many well-known feature selection methods when employing two classifiers SVM and KNN to measure the quality of selected features. Experiments verify that SCF is able to find more discriminative features than the competing methods and can be used as a preprocessing algorithm to combine with other methods effectively.", "pages": "61065-61076", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Ke, Wenjun", "Wu, Chunxue", "Wu, Yan", "Xiong, Neal"]}]["9103202", {"address": "", "articleno": "", "doi": "10.1109/JSTARS.2020.2997239", "issn": "2151-1535", "issue_date": "", "journal": "IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing", "keywords": ["Three-dimensional displays", "Solid modeling", "Geology", "Software", "Earth", "Germanium", "Data models", "Digital outcrop model", "geosciences", "Google Earth (GE)", "photogrammetry"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "Printgrammetry\u20143-D Model Acquisition Methodology From Google Earth Imagery Data", "url": "", "volume": "13", "year": "2020", "abstract": "This article proposes a technique named Printgrammetry, a structured workflow that allows the extraction of 3-D models from Google Earth platform through the combination of image captures from the screen monitor with structure from motion algorithms. This technique was developed to help geologists and other geoscientists in acquiring 3-D photo-realistic models of outcrops and natural landscapes of big proportions without the need of field mapping and expensive equipment. The methodology is detailed aiming to permit easy reproducibility and focused on achieving the highest resolution possible by working with the best images that the platform can provide. The results have shown that it is possible to obtain visually high-quality models from natural landscapes from Google Earth by acquiring images at high level of detail regions of the software, using a 4K monitor, multidirectional screenshots, and by marking homogeneously spaced targets for georeferencing and scaling. The geometric quality assessment performed using light detection and ranging ground truth data as comparison shows that the Printgrammetry dense point clouds have reached 98.1% of the total covered area under 5 m of distance for the Half Dome case study and 96.7% for the Raplee Ridge case study. The generated 3-D models were then visualized and interacted through an immersive virtual reality software that allowed geologists to manipulate this virtual field environment in different scales. This technique is considered by the authors to have a promising potential for research, industrial, and educational projects that do not require high-precision models.", "pages": "2819-2830", "note": "", "ISSN": "2151-1535", "publicationtype": "article", "author": ["Horota, Rafael", "Aires, Alysson", "Marques, Ademir", "Rossa, Pedro", "Souza, Eniuce", "Gonzaga, Luiz", "Veronez, Mauricio"]}]["8063957", {"address": "", "articleno": "", "doi": "10.1109/TIP.2017.2760518", "issn": "1941-0042", "issue_date": "", "journal": "IEEE Transactions on Image Processing", "keywords": ["Feature extraction", "Image quality", "Distortion", "Databases", "Optimization", "Computational modeling", "Full-reference image quality assessment", "no-reference image quality assessment", "neural networks", "quality pooling", "deep learning", "feature extraction", "regression"], "month": "Jan", "number": "1", "numpages": "", "publisher": "", "title": "Deep Neural Networks for No-Reference and Full-Reference Image Quality Assessment", "url": "", "volume": "27", "year": "2018", "abstract": "We present a deep neural network-based approach to image quality assessment (IQA). The network is trained end-to-end and comprises ten convolutional layers and five pooling layers for feature extraction, and two fully connected layers for regression, which makes it significantly deeper than related IQA models. Unique features of the proposed architecture are that: 1) with slight adaptations it can be used in a no-reference (NR) as well as in a full-reference (FR) IQA setting and 2) it allows for joint learning of local quality and local weights, i.e., relative importance of local quality to the global quality estimate, in an unified framework. Our approach is purely data-driven and does not rely on hand-crafted features or other types of prior domain knowledge about the human visual system or image statistics. We evaluate the proposed approach on the LIVE, CISQ, and TID2013 databases as well as the LIVE In the wild image quality challenge database and show superior performance to state-of-the-art NR and FR IQA methods. Finally, cross-database evaluation shows a high ability to generalize between different databases, indicating a high robustness of the learned features.", "pages": "206-219", "note": "", "ISSN": "1941-0042", "publicationtype": "article", "author": ["Bosse, Sebastian", "Maniry, Dominique", "M\u00fcller, Klaus-Robert", "Wiegand, Thomas", "Samek, Wojciech"]}]["8576506", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2018.2886551", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Power system reliability", "Electricity supply industry", "Data mining", "Economics", "Decision trees", "Security", "Blackout sensitivity", "big data", "decision tree", "electricity market", "Internet of Things", "long-tailed", "Pareto effect"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "Analysis and Identification of Power Blackout-Sensitive Users by Using Big Data in the Energy System", "url": "", "volume": "7", "year": "2019", "abstract": "With the further liberalization of the electricity market of China, customers\u2019 requirements, characteristics, and distribution, as well as the quality, security, and reliability of power supplies without interruption, have received considerable attention from power companies, policymakers, and researchers. How to deeply explore the distribution characteristics of electricity customers and analyze their sensitivities to electricity blackouts has become an especially important problem. This paper takes over 0.1 billion data, collected by various smart devices of the Internet of Things in the power system of China, such as smart meters, intelligent power consumption interactive terminals, data concentrators, and other cross-platform data, for example, 95 598 telephone records, complaint information, user bills, user information, and maintenance records, as study objects, to analyze the consumption characteristics of power users. It has been found that there is a wide range of power users who pay different electricity bills; a long-tail distribution following a power law lies in the number of users versus their paid electricity bills. Meanwhile, there are two Pareto effects (2-8 rule): the number of residents and non-residents versus their electricity bills, and the number of large industrial users and general industry (business users) versus in their electricity consumption and bills. Then, a decision tree algorithm is proposed to capture the characteristics of electricity consumers and to recognize the crowd who is power blackout sensitive. The evaluation indexes and parameters of the decision tree are discussed in detail, and a comparison with other intelligent algorithms shows that the decision tree has a good recognition performance over that of others, and the characteristics used to identify the blackout-sensitive crowd is various. All the results state that except for economic factors, positive social effects should also be considered. Various marketing strategies to satisfy different requirements of power users should be provided to promote long-term relationships between the power companies and power customers.", "pages": "19488-19501", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Shuai, Chunyan", "Yang, Hengcheng", "Ouyang, Xin", "He, Mingwei", "Gong, Zeweiyi", "Shu, Wanneng"]}]["9238029", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2020.3033279", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Information services", "Itemsets", "Financial services", "Data mining", "Software as a service", "Technological innovation", "Market research", "Smart sensor", "smart financial environment", "financial information service platform", "big data integration"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "Rural Financial Information Service Platform Under Smart Financial Environment", "url": "", "volume": "8", "year": "2020", "abstract": "The development and improvement of agricultural financial information service system is of great significance to the development of rural modernization, the improvement of rural comprehensive competitiveness and the construction of new socialist countryside. The construction of rural financial information service platform is directly related to the quality of rural financial information service, and directly affects the construction of new socialist countryside. In order to solve the problems of information collection, processing and integration of rural financial information service platform in China, the diversification, personalization, timeliness and accuracy of information demand are difficult to be met, and the organization and operation mode of the platform are not perfect. In this paper, based on the intelligent sensor, the whole digital transformation is realized through the reference of big data. Based on this, this paper establishes the research model of rural financial information service platform under the smart financial environment. From the current situation of the construction and application of rural financial information service platform in China, it studies the basic situation of the construction of rural financial information service platform in China from three aspects of functional scope, service mode and operation mode, and draws the improvement conclusion. The results show that the efficiency of rural financial information service is increased by 20% after using the improved method in this paper, which has certain use value.", "pages": "199944-199952", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Wensheng, Dai"]}]["8911489", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2019.2955735", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Delays", "Airports", "Pattern recognition", "Atmospheric modeling", "Meteorology", "Air traffic control", "Air traffic", "flight delay", "non-negative tensor factorization", "pattern recognition"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "Application of Non-Negative Tensor Factorization for Airport Flight Delay Pattern Recognition", "url": "", "volume": "7", "year": "2019", "abstract": "With the rapid development of civil aviation transportation in China, huge demand growth has broken the balance between supply and demand, resulting in airspace congestion and increasing flight delays. The delays of large airports have been increasing year by year, which has seriously affected the air travel experience of passengers. Obtaining their flight delay patterns can help identify defects in flight scheduling and airspace utilization. The investigation based on the actual flight operation data of Tianjin Binhai International Airport (TSN) is conducted, in order to capture the relationship and impact between the factors such as traffic flow direction, airline attributes and hourly average delay distribution. Furthermore, Non-negative Tensor Factorization (NTF) is applied to pattern recognition by introducing CP (CANDECOMP/PARAFAC) decomposition and Block Coordinate Descent (BCD) algorithm for selected data set. Numerical experiments show that the designed method has good performance in terms of computation speed and solution quality. Recognition results indicate the significant pattern characteristics of the Tianjin airport delay are extracted, which can provide some new perspectives for air traffic management unit to alleviate airspace congestion and improve service quality.", "pages": "171724-171737", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Zhong, Han", "Qi, Geqi", "Guan, Wei", "Hua, Xiaochen"]}]["8920046", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2019.2957437", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Delays", "Rails", "Optimization methods", "Roads", "Process control", "Tram", "timetable", "bidirectional signal priority strategy", "multiobjective optimization", "fuzzy mathematical programming"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "Timetable Optimization for a Two-Way Tram Line With an Active Signal Priority Strategy", "url": "", "volume": "7", "year": "2019", "abstract": "Modern trams typically run along semi-exclusive right-of-way. Although tram lanes isolate trams from other traffic in the running sections, the operation process will be affected by signal control. To improve the service quality of trams and reduce the negative impact on intersections caused by bidirectional priority requests, we propose a timetable optimization method for a single two-way tram line based on active transit signal priority strategy. Combining with the characteristics of bidirectional signal priority strategy, trams can pass through the intersections without stopping by adjusting the running times and dwell times. A multiobjective optimization model of a tram timetable is established to minimize the total travel time, dwell time increment, and negative effect of the signal priority strategy. For obtaining a timetable with equal satisfaction for the three objectives, we adopt the fuzzy mathematical programming approach to transform the problems into mixed integer linear programming (MILP) problems, which can be solved by using standard solvers. The case study of Nanjing Qilin Tram Line 1 shows that the timetable optimization method designed in this paper can effectively improve the service efficiency of trams, and reduce the negative impact of the signal priority strategy on social vehicles. These empirical findings can give us some useful insights on the optimum design of tram timetable.", "pages": "176896-176911", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Zhang, Tong", "Mao, Baohua", "Xu, Qi", "Feng, Jia"]}]["8865058", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2019.2946683", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Servers", "Task analysis", "Edge computing", "Delays", "Computational modeling", "Energy consumption", "Optimization", "Latency", "energy", "offloading", "edge computing"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "An Energy-Efficient Off-Loading Scheme for Low Latency in Collaborative Edge Computing", "url": "", "volume": "7", "year": "2019", "abstract": "Mobile terminal users applications, such as smartphones or laptops, have frequent computational task demanding but limited battery power. Edge computing is introduced to offload terminals' tasks to meet the quality of service requirements such as low delay and energy consumption. By offloading computation tasks, edge servers can enable terminals to collaboratively run the highly demanding applications in acceptable delay requirements. However, existing schemes barely consider the characteristics of the edge server, which leads to random assignment of tasks among servers and big tasks with high computational intensity (named as \u201cbig task\u201d) may be assigned to servers with low ability. In this paper, a task is divided into several subtasks and subtasks are offloaded according to characteristics of edge servers, such as transmission distance and central processing unit (CPU) capacity. With this multi-subtasks-to-multi-servers model, an adaptive offloading scheme based on Hungarian algorithm is proposed with low complexity. Extensive simulations are conducted to show the efficiency of the scheme on reducing the offloading latency with low energy consumption.", "pages": "149182-149190", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Wang, Jin", "Wu, Wenbing", "Liao, Zhuofan", "Sangaiah, Arun", "Simon, R."]}]["9310346", {"address": "", "articleno": "", "doi": "10.1109/TIM.2020.3047954", "issn": "1557-9662", "issue_date": "", "journal": "IEEE Transactions on Instrumentation and Measurement", "keywords": ["Syntactics", "Semantics", "Pragmatics", "Measurement uncertainty", "Decision making", "Pollution measurement", "Big Data", "Decision-making", "measurement", "measurement information (MI)", "quality management", "semiotic criteria", "semiotics"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "Quality of Measurement Information in Decision-Making", "url": "", "volume": "70", "year": "2021", "abstract": "This article introduces a general-purpose framework aimed at capturing the elusive concept of quality of measurement information (MI), a critical issue for both researchers and practitioners when dealing with MI-enabled decision-making. The framework is a blueprint for the definition, assessment, communication, and improvement of MI quality, as analyzed through a set of general criteria, classified according to the syntactic, semantic, and pragmatic layers of semiotics, as suggested in the ISO 8000-8:2015 technical standard. The top-down analysis, where each criterion is specified in terms of characteristics and each characteristic in terms of domain-related indicators, is complemented with a bottom-up synthesis and operationalized by means of a flowchart. An application example, about the quality of information provided by the networks of measurement instruments reporting pollutants in the air, is presented to test the usefulness and the limitations of the framework.", "pages": "1-16", "note": "", "ISSN": "1557-9662", "publicationtype": "article", "author": ["Petri, Dario", "Carbone, Paolo", "Mari, Luca"]}]["9353479", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2021.3059187", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Predictive models", "Forecasting", "Data models", "Machine learning algorithms", "Heuristic algorithms", "Prediction algorithms", "Portfolios", "Stock market forecast", "statistical arbitrage", "machine learning", "ensemble learning"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "Ensembling and Dynamic Asset Selection for Risk-Controlled Statistical Arbitrage", "url": "", "volume": "9", "year": "2021", "abstract": "In recent years, machine learning algorithms have been successfully employed to leverage the potential of identifying hidden patterns of financial market behavior and, consequently, have become a land of opportunities for financial applications such as algorithmic trading. In this paper, we propose a statistical arbitrage trading strategy with two key elements: an ensemble of regression algorithms for asset return prediction, followed by a dynamic asset selection. More specifically, we construct an extremely heterogeneous ensemble ensuring model diversity by using state-of-the-art machine learning algorithms, data diversity by using a feature selection process, and method diversity by using individual models for each asset, as well models that learn cross-sectional across multiple assets. Then, their predictive results are fed into a quality assurance mechanism that prunes assets with poor forecasting performance in the previous periods. We evaluate the approach on historical data of component stocks of the S&P500 index. By performing an in-depth risk-return analysis, we show that this setup outperforms highly competitive trading strategies considered as baselines. Experimentally, we show that the dynamic asset selection enhances overall trading performance both in terms of return and risk. Moreover, the proposed approach proved to yield superior results during both financial turmoil and massive market growth periods, and it showed to have general application for any risk-balanced trading strategy aiming to exploit different asset classes.", "pages": "29942-29959", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Carta, Salvatore", "Consoli, Sergio", "Podda, Alessandro", "Recupero, Diego", "Stanciu, Maria"]}]["9120049", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2020.3003375", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Feature extraction", "Three-dimensional displays", "Convolution", "Data mining", "Image edge detection", "Convolutional neural networks", "Neural network", "stereo matching", "multi-scale attention module", "feature refinement module", "3D attention aggregation module"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "Multi-Attention Network for Stereo Matching", "url": "", "volume": "8", "year": "2020", "abstract": "In recent years, convolutional neural network (CNN) algorithms promote the development of stereo matching and make great progress, but some mismatches still occur in textureless, occluded and reflective regions. In feature extraction and cost aggregation, CNNs will greatly improve the accuracy of stereo matching by utilizing global context information and high-quality feature representations. In this paper, we design a novel end-to-end stereo matching algorithm named Multi-Attention Network (MAN). To obtain the global context information in detail at the pixel-level, we propose a Multi-Scale Attention Module (MSAM), combining a spatial pyramid module with an attention mechanism, when we extract the image features. In addition, we introduce a feature refinement module (FRM) and a 3D attention aggregation module (3D AAM) during cost aggregation so that the network can extract informative features with high representational ability and high-quality channel attention vectors. Finally, we obtain the final disparity through bilinear interpolation and disparity regression. We evaluate our method on the Scene Flow, KITTI 2012 and KITTI 2015 stereo datasets. The experimental results show that our method achieves state-of-the-art performance and that every component of our network is effective.", "pages": "113371-113382", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Yang, Xiaowei", "He, Lin", "Zhao, Yong", "Sang, Haiwei", "Yang, Zu", "Cheng, Xian"]}]["8730359", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2019.2920776", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Smart contracts", "Blockchain", "Encoding", "Programming", "Visualization", "Security", "Smart contract", "Char-RNN", "LSTM", "automatic coding"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "Visual and User-Defined Smart Contract Designing System Based on Automatic Coding", "url": "", "volume": "7", "year": "2019", "abstract": "Smart contract applications based on Ethereum blockchain have been widely used in many fields. They are developed by professional developers using specialized programming languages like solidity. It requires high requirements on knowledge of the specialized field and the proficiency in contract programming. Thus, it is hard for normal users to design a usable smart contract based on their own demands. Most current studies about smart contracts focus on the security of coding while lack of friendly tools for users to design the specialized templates of contracts coding. This paper provides a visual and user-defined smart contract designing systems. It makes the development of domain-specific smart contracts simpler and visualization for contract users. The system implements the domain-specific features extraction about the crawled data sets of smart contract programs by TF-IDF and K-means++ clustering algorithm. Then, it achieves the automatic generation of unified basic function codes by Char-RNN (improved by LSTM) based on the domain-specific features. The system adopts Google Blockly and links the generated codes with UI controls. Finally, it provides a set of specialized templates of basic functions for users to design smart contracts by the friendly interface. It reduces the difficulty and costs of contract programming. The paper offers a case study to design contracts by users. The designed contracts were validated on the existing system to implement the food trading and traders' credit evaluation. The experimental results show that the designed smart contracts achieve good integration with the existing system and they can be deployed and compiled successfully.", "pages": "73131-73143", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Mao, Dianhui", "Wang, Fan", "Wang, Yalei", "Hao, Zhihao"]}]["8936445", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2019.2960551", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Respiratory system", "Real-time systems", "Monitoring", "Deep learning", "Predictive models", "Air pollution", "Atmospheric measurements", "Asthma", "indoor particulate matter", "deep learning", "peak expiratory flow rates", "real-time monitoring"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "Predicting Asthma Attacks: Effects of Indoor PM Concentrations on Peak Expiratory Flow Rates of Asthmatic Children", "url": "", "volume": "8", "year": "2020", "abstract": "Despite ample research on the association between indoor air pollution and allergic disease prevalence, public health and environmental policies still lack predictive evidence for developing a preventive guideline for patients or vulnerable populations mostly due to limitation of real-time big data and model predictability. Recent popularity of IoT and machine learning techniques could provide enabling technologies for collecting real-time big data and analyzing them for more accurate prediction of allergic disease risks for evidence-based intervention, but the effort is still in its infancy. This pilot study explored and evaluated the feasibility of a deep learning algorithm for predicting asthma risk. It is based on peak expiratory flow rates (PEFR) of 14 pediatric asthma patients visiting the Korea University Medical Center and indoor particulate matter PM10 and PM2.5 concentration data collected at their residence every 10 minutes using a PM monitoring device with a low-cost sensor between September 1, 2017 and August 31, 2018. We interpolated the PEFR results collected twice a day for each patient throughout the day so that it can be matched to the PM and other weather data. The PEFR results were classified into three categories such as `Green' (normal), `Yellow' (mild to moderate exacerbation) and `Red' (severe exacerbation) with reference to their best peak flow value. Long Short-Term Memory (LSTM) model was trained using the first 10 months of the linked data and predicted asthma risk categories for the next 2 months during the study period. LSTM model is found to predict the asthma risk categories better than multinomial logistic (MNL) regression as it incorporates the cumulative effects of PM concentrations over time. Upon successful modifications of the algorithm based on a larger sample, this approach could potentially play a groundbreaking role for the scientific data-driven medical decision making.", "pages": "8791-8797", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Kim, Dohyeong", "Cho, Sunghwan", "Tamil, Lakshman", "Song, Dae", "Seo, Sungchul"]}]["9583290", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2021.3121997", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Data mining", "Task analysis", "Solid modeling", "Roads", "Reactive power", "Power capacitors", "Process mining", "process discovery", "filtering infrequent behaviors", "event log preprocessing", "process model enhancement"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "Process Model Enhancement Through Capturing Important Behaviors and Rating Trace Variants", "url": "", "volume": "9", "year": "2021", "abstract": "In the field of process discovery, it is worth noting that most process discovery algorithms assume that event logs are clean, i.e., event logs should not contain infrequent behaviors. However, real-life event logs often contain infrequent behaviors (i.e., outliers) and lead to quality issues of the discovered process model. On the other hand, driven by recent trends such as big data and process automation, the volume of event data is rapidly increasing: an event log may contain billions of event data. Unfortunately, some process mining algorithms and platforms may have difficulties handling such event logs. The ever-increasing size of event data and infrequent behaviors in the event log are two main challenges in the field of process discovery nowadays. However, little research has been conducted on simultaneously filtering infrequent behaviors and decreasing the size of the event log: Various filtering methods can filter infrequent behaviors, whereas the volume of the filtered log is still considerable. On the other hand, sampling methods can reduce the size of the event log, but the processed event log may still contain infrequent behaviors. Therefore, this paper proposes a technique to simultaneously filter infrequent behaviors and control the volume of input logs by capturing important behaviors and rating trace variants. Our experiments show that our approach can significantly improve the quality of the discovered process models. Furthermore, our approach can obtain a better process model from 0.001% trace variants than the complete event log and significantly improves the runtime of discovery algorithms.", "pages": "143634-143660", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Wang, Mimi", "He, Xudong", "Zhao, Peihai"]}]["9381206", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2021.3067138", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Differential privacy", "Internet of Things", "Data privacy", "Privacy", "Sensors", "Incentive schemes", "Databases", "Data protection", "Internet of Things", "differential privacy", "crowd sensing IoT system"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "Bi-Tier Differential Privacy for Precise Auction-Based People-Centric IoT Service", "url": "", "volume": "9", "year": "2021", "abstract": "With the fast proliferation of device sensing and computing, crowed sensing has become the building block of the Internet of things. Consequently, various data collection and incentive mechanisms are investigated for people-centric services. In this paper, we have investigated the problem of privacy-aware people-centric IoT service based on a tailored auction approach. We applied a bi-tier differential privacy methodology on the data collected from crowdsensing IoT devices. A corresponding pricing scheme is also proposed to ensure the property of incentive compatibility, precise service data, and anonymized query results. Comparing to traditional privacy-aware auction schemes which only focus on the cost, our corresponding precise privacy-aware auction scheme provides a tailored IoT service based on the customers' request. The proposed trial query technique is able to provide a precise assessment of service quality, thus improves the efficiency of the people-centric IoT service. The customer could enjoy the convenience of service evaluation before making a bid, while the actual service data is anonymized to guarantee the service providers' interests. We evaluate the proposed bi-tier differential privacy schema for auction-based service by conducting extensive simulations. The experimental results show that our proposed method yields higher data utility and accuracy for the IoT service customers with privacy concerns.", "pages": "55036-55044", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Tian, Yuan", "Song, Biao", "Ma, Tinghuai", "Al-Dhelaan, Abdullah", "Al-Dhelaan, Mohammed"]}]["8552343", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2018.2883939", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Quality of service", "Cloud computing", "Sparse matrices", "Smoothing methods", "Adaptation models", "Predictive models", "Time series analysis", "Could service", "recommender system", "QoS prediction", "time-aware", "matrix factorization"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "Time-Aware QoS Prediction for Cloud Service Recommendation Based on Matrix Factorization", "url": "", "volume": "6", "year": "2018", "abstract": "Prediction of quality of service (QoS) is a critical area of research for cloud service recommendation. The disadvantage of QoS values is that they are directly related to time series of service status and network condition and thus instantly vary over time. The main contribution of this paper is to consider service invocation time as a dynamic factor in the collaborative filtering model and recommend high-quality services for target user. In particular, this paper proposes a time-aware matrix factorization (TMF) model that integrates QoS time series to provide two-phase QoS predictions for cloud service recommendation. The TMF model uses an adaptive matrix factorization model on a sparse QoS dataset to predict the missing QoS values. A temporal smoothing method is then developed and applied to the predicted result to perform the time-varying QoS prediction that accounts for the dependence of QoS values at different time intervals. The numerical experiments presented are conducted to validate the accuracy of the proposed method on a public QoS dataset.", "pages": "77716-77724", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Li, Shun", "Wen, Junhao", "Luo, Fengji", "Ranzi, Gianluca"]}]["8822928", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2019.2939211", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Decision making", "Linguistics", "Indexes", "Hospitals", "Surgery", "Entropy", "Computational modeling", "Cholecystitis", "best-worst method (BWM)", "entropy weight method", "2-tuple linguistic", "group decision-making (MCGDM)", "TODIM"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "A Multi-Criteria 2-Tuple Linguistic Group Decision-Making Method Based on TODIM for Cholecystitis Treatments Selection", "url": "", "volume": "7", "year": "2019", "abstract": "Cholecystitis is a common disease with a high incidence, and attracts much attention. It not only harms human health, but also affects quality of work and life. Therefore, the choice of a suitable treatment is badly important for patients. In this paper, a novel selection model of treatments for cholecystitis based on hybrid multiple-criteria group decision-making (MCGDM), which is helpful to choose the most suitable treatment in the case of asymmetric information between doctors and patients. Subsequently, subjective and objective criteria are comprehensively taken into account in the index system of the selection model for cholecystitis, and combines 2-tuple linguistic with quantitative data analysis. Besides, the evaluation information obtained from the patient's conditions, the treatment and the hospital's medical status, etc., including real numbers, interval numbers, and linguistic labels with multi-granularity, is more complete and real. And the 2-tuple linguistic model is used to unify the non-homogeneous information, so the treatment selection is accurate and reliable. Simultaneously, for the unknown index and criteria weight, the improved entropy weight method and the BWM (best-worst-method) are utilized to figure out the index weight and criteria weight, respectively. Further, TODIM (an acronym in Portuguese for interactive and multi-criteria decision-making model) method based on the prospect theory is applied to solve the prioritization of cholecystitis treatments, and give full consideration to the decision maker of risk aversion. Eventually, an empirical study of treatment selection for cholecystitis is conducted. Sensitivity analysis and comparative analysis indicate that the proposed selection model of treatments for cholecystitis patients is reliable and effective.", "pages": "127967-127986", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Xie, Li", "He, Jiqun", "Cheng, Pengfei", "Xiao, Runsha", "Zhou, Xianghong"]}]["8573120", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2018.2885818", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Measurement", "Image quality", "Two dimensional displays", "Visualization", "Three-dimensional displays", "Distortion", "Indexes", "Image quality assessment", "image aesthetics assessment", "visual comfort", "image quality enhancement"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "2D and 3D Image Quality Assessment: A Survey of Metrics and Challenges", "url": "", "volume": "7", "year": "2019", "abstract": "Image quality is important not only for the viewing experience, but also for the performance of image processing algorithms. Image quality assessment (IQA) has been a topic of intense research in the fields of image processing and computer vision. In this paper, we first analyze the factors that affect two-dimensional (2D) and three-dimensional (3D) image quality, and then provide an up-to-date overview on IQA for each main factor. The main factors that affect 2D image quality are fidelity and aesthetics. Another main factor that affects stereoscopic 3D image quality is visual comfort. We also describe the IQA databases and give the experimental results on representative IQA metrics. Finally, we discuss the challenges for IQA, including the influence of different factors on each other, the performance of IQA metrics in real applications, and the combination of quality assessment, restoration, and enhancement.", "pages": "782-801", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Niu, Yuzhen", "Zhong, Yini", "Guo, Wenzhong", "Shi, Yiqing", "Chen, Peikun"]}]["9253992", {"address": "", "articleno": "", "doi": "10.1109/JPHOT.2020.3036873", "issn": "1943-0655", "issue_date": "", "journal": "IEEE Photonics Journal", "keywords": ["Scattering", "Real-time systems", "Image restoration", "Additive haze model", "global optimization dehazing", "haze thickness map", "low-frequency component", "propagation and scattering"], "month": "Dec", "number": "6", "numpages": "", "publisher": "", "title": "VROHI: Visibility Recovery for Outdoor Hazy Image in Scattering Media", "url": "", "volume": "12", "year": "2020", "abstract": "Additive haze model (AHM), due to its high simplicity, has a potential to increase the efficiency of the restoration procedure of images degraded by scattering media. However, AHM is designed for hazy remote sensing data and is not suitable to be used on outdoor images. In this paper, according to the low-frequency feature (LFC) of haze, AHM is modified via gamma correction technique to make it suitable for modeling outdoor images. Benefitting from the modified AHM (MAHM), a simple yet effective method called VROHI is proposed to enhance the visibility of an outdoor hazy image. In specific, a low complexity LFC extraction method is designed by utilizing characteristic of the discrete cosine transform. Subsequently, by constructing the linear function of unknown parameters and imposing the saturation prior on MAHM, the image dehazing problem can be derived into a global optimization function. Experiments reveal that the proposed VROHI is superior to the other state-of-the-art techniques in terms of both the processing efficiency and recovery quality.", "pages": "1-15", "note": "", "ISSN": "1943-0655", "publicationtype": "article", "author": ["Ju, Mingye", "Ding, Can", "Guo, Y."]}]["9458302", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2021.3090165", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Pattern recognition", "Electronic noses", "Olfactory", "Taxonomy", "Food industry", "Sensor arrays", "Quality assessment", "Artificial olfaction", "electronic nose", "feature classification", "food quality", "machine learning", "pattern recognition"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "A Review on Electronic Nose: Coherent Taxonomy, Classification, Motivations, Challenges, Recommendations and Datasets", "url": "", "volume": "9", "year": "2021", "abstract": "Context: Quality Control (QC) has been constantly an essential concern in many fields like food industry production, medical drugs, environmental protection, and so on. An odor or flavor, as a global fingerprint, can be implemented as a non-invasive mechanism for quality assurance. This computer-based approach can assure accurate detection and precise identification of the product quality or manufactured goods. Objective: This paper aims to achieve a systematic review about e-nose by introducing the achievements made by researchers in this area, to summarize their findings, to provide motivations and challenges to new researchers in the field of e-nose. Methods: The articles that were being utilized in the e-nose field were systematically achieved using three search engines: The online library of IEEE Explore, Web of Science and Science Direct for time span of 7 years (from 2013 to 2020). Both medical literature reviews and technical reviews were considered in the criteria of the research for wider understanding in the field of e-nose. The articles were categorized according to the objective of the research and projected into four classes. Upon completion of screening process 333 research papers using the exclusion and inclusion conditions, as the final set 54 articles were selected. Results: The taxonomy of this research was classified into four categories. The first one included the suggested methods that introduced the utilization of the e-nose for classification purposes (9/54 papers). The second category comprises the methods related to the development of e-nose (24/54 papers). The third one included the review studies about the e-nose (8/54 papers). The fourth group comprises comparative studies and evaluation (13/54 papers). Discussion: This systematic review contributes for a clearer understanding and a full insight in the e- nose research field by surveying and categorizing pertinent research efforts. Conclusion: This review paper will help to address the up-to-date research opportunities, challenges, problems, motivations and recommendations related to the utilization of e-nose in all fields of sciences and industries.", "pages": "88535-88551", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Al-Dayyeni, Wisam", "Al-Yousif, Shahad", "Taher, Mayada", "Al-Faouri, Ahmad", "Tahir, Nooritawati", "Jaber, Mustafa", "Ghabban, Fahad", "Najm, Ihab", "Alfadli, Ibrahim", "Ameerbakhsh, Omair", "Mnati, Mohannad", "Al-Shareefi, Nael", "Saleh, Abbadullah"]}]["9256314", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2020.3037258", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Deep learning", "Analytical models", "Image recognition", "Skin", "Data models", "Medical diagnosis", "Diseases", "Deep learning", "image recognition", "review", "skin disease"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "Deep Learning in Skin Disease Image Recognition: A Review", "url": "", "volume": "8", "year": "2020", "abstract": "The application of deep learning methods to diagnose diseases has become a new research topic in the medical field. In the field of medicine, skin disease is one of the most common diseases, and its visual representation is more prominent compared with the other types of diseases. Accordingly, the use of deep learning methods for skin disease image recognition is of great significance and has attracted the attention of researchers. In this study, we review 45 research efforts on the identification of skin disease by using deep learning technology since 2016. We analyze these studies from the aspects of disease type, data set, data processing technology, data augmentation technology, model for skin disease image recognition, deep learning framework, evaluation indicators, and model performance. Moreover, we summarize the traditional and machine learning-based skin disease diagnosis and treatment methods. We also analyze the current progress in this field and predict four directions that may become the research topic in the future. Our results show that the skin disease image recognition method based on deep learning is better than those of dermatologists and other computer-aided treatment methods in skin disease diagnosis, especially the multi deep learning model fusion method has the best recognition effect.", "pages": "208264-208280", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Li, Ling-Fang", "Wang, Xu", "Hu, Wei-Jian", "Xiong, Neal", "Du, Yong-Xing", "Li, Bao-Shan"]}]["8835895", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2019.2940961", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Acoustics", "Training", "Hidden Markov models", "Data models", "Task analysis", "Neural networks", "Deep learning", "Semi-supervised learning", "data preprocessing", "acoustic modeling", "speech recognition"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "Large-Scale Semi-Supervised Training in Deep Learning Acoustic Model for ASR", "url": "", "volume": "7", "year": "2019", "abstract": "This study investigated large-scale semi-supervised training (SST) to improve acoustic models for automatic speech recognition. The conventional self-training, the recently proposed committee-based SST using heterogeneous neural networks and the lattice-based SST were examined and compared. The large-scale SST was studied in deep neural network acoustic modeling with respect to the automatic transcription quality, the importance data filtering, the training data quantity and other data attributes of a large quantity of multi-genre unsupervised live data. We found that the SST behavior on large-scale ASR tasks was very different from the behavior obtained on small-scale SST: 1) big data can tolerate a certain degree of mislabeling in the automatic transcription for SST. It is possible to achieve further performance gains with more unsupervised fresh data, and even the automatic transcriptions have a certain degree of errors; 2) the audio attributes, transcription quality and importance of the fresh data are more important than the increased data quantity for large-scale SST; and 3) there are large differences in performance gains on different recognition tasks, such that the benefits highly depend on the selected data attributes of unsupervised data and the data scale of the baseline ASR system. Furthermore, we proposed a novel utterance filtering approach based on active learning to improve the data selection in large-scale SST. The experimental results showed that the SST with the proposed data filtering yields a 2-11% relative word error rate reduction on five multi-genre recognition tasks, even with the baseline acoustic model that was already well trained on a 10000-hr supervised dataset.", "pages": "133615-133627", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Long, Yanhua", "Li, Yijie", "Wei, Shuang", "Zhang, Qiaozheng", "Yang, Chunxia"]}]["9793359", {"address": "", "articleno": "", "doi": "10.26599/BDMA.2022.9020002", "issn": "2096-0654", "issue_date": "", "journal": "Big Data Mining and Analytics", "keywords": ["Estimation", "Psychology", "Reinforcement learning", "Big Data", "Data mining", "Task analysis", "Testing", "stylometry", "IQ estimation", "authorship attribution", "intelligence", "IQ", "author profiling", "machine learning"], "month": "Sep.", "number": "3", "numpages": "", "publisher": "", "title": "Estimating Intelligence Quotient Using Stylometry and Machine Learning Techniques: A Review", "url": "", "volume": "5", "year": "2022", "abstract": "The task of trying to quantify a person's intelligence has been a goal of psychologists for over a century. The area of estimating IQ using stylometry has been a developing area of research and the effectiveness of using machine learning in stylometry analysis for the estimation of IQ has been demonstrated in literature whose conclusions suggest that using a large dataset could improve the quality of estimation. The unavailability of large datasets in this area of research has led to very few publications in IQ estimation from written text. In this paper, we review studies that have been done in IQ estimation and also that have been done in author profiling using stylometry and we conclude that based on the success of IQ estimation and author profiling with stylometry, a study on IQ estimation from written text using stylometry will yield good results if the right dataset is used.", "pages": "163-191", "note": "", "ISSN": "2096-0654", "publicationtype": "article", "author": ["Adebayo, Glory", "Yampolskiy, Roman"]}]["7352306", {"address": "", "articleno": "", "doi": "10.1109/JPROC.2015.2494218", "issn": "1558-2256", "issue_date": "", "journal": "Proceedings of the IEEE", "keywords": ["Big data", "Bayes methods", "Linear programming", "Decision making", "Design of experiments", "Optimization", "Genomes", "Statistical analysis", "decision making", "design of experiments", "optimization", "response surface methodology", "statistical learning", "genomic medicine", "Decision making", "design of experiments", "optimization", "response surface methodology", "statistical learning"], "month": "Jan", "number": "1", "numpages": "", "publisher": "", "title": "Taking the Human Out of the Loop: A Review of Bayesian Optimization", "url": "", "volume": "104", "year": "2016", "abstract": "Big Data applications are typically associated with systems involving large numbers of users, massive complex software systems, and large-scale heterogeneous computing and storage architectures. The construction of such systems involves many distributed design choices. The end products (e.g., recommendation systems, medical analysis tools, real-time game engines, speech recognizers) thus involve many tunable configuration parameters. These parameters are often specified and hard-coded into the software by various developers or teams. If optimized jointly, these parameters can result in significant improvements. Bayesian optimization is a powerful tool for the joint optimization of design choices that is gaining great popularity in recent years. It promises greater automation so as to increase both product quality and human productivity. This review paper introduces Bayesian optimization, highlights some of its methodological aspects, and showcases a wide range of applications.", "pages": "148-175", "note": "", "ISSN": "1558-2256", "publicationtype": "article", "author": ["Shahriari, Bobak", "Swersky, Kevin", "Wang, Ziyu", "Adams, Ryan", "Freitas, Nando"]}]["9427182", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2021.3078773", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Task analysis", "Containers", "Job shop scheduling", "Cloud computing", "Optimal scheduling", "Learning automata", "Dynamic scheduling", "Container cloud", "learning automata", "self-adapting scheduling", "reward-penalty strategy"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "A Self-Adapting Task Scheduling Algorithm for Container Cloud Using Learning Automata", "url": "", "volume": "9", "year": "2021", "abstract": "With the rapid development of cloud computing and container technology, more and more applications are deployed to the cloud, and the scale of cloud platform is expanding. Due to the large number of container instances running in the platform, complex dependency relationship, fast version iteration and other characteristics, the update of business can often cause the change of the whole cloud resource environment, which triggers the repetitive scheduling problem of related tasks and affects stability of the business. In this paper, we propose a self-adapting task scheduling algorithm (ADATSA) using learning automata to solve these problems. Firstly, we design a learning automata model and objective function for the system on task scheduling problem. Then, we realize an effective reward-penalty mechanism for scheduling actions in combination with the idle state of resources and the running state of tasks in the current environment. Meanwhile, the environment is modeled by cluster, node and task, and the probability of action selected is optimized by scheduling execution, thus enhancing the adaptability to the cloud environment of the scheduling and accelerating convergence. Finally, we construct a framework of task load monitoring with buffer queue to achieve dynamic scheduling based on priority. The experimental part verifies the effectiveness of proposed algorithm with different angles such as resource imbalance degree, resource residual degree and QoS. Compared with other learning automata scheduling models such as LAEAS, non-automata technology based algorithms such as PSOS and K8S scheduling engine, ADATSA shows the better performance of environment adaptability, resource optimization efficiency and QoS in dynamic scheduling. The theoretical analysis was consistent with the experimental results.", "pages": "81236-81252", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Zhu, Lilu", "Huang, Kai", "Hu, Yanfeng", "Tai, Xianqing"]}]["7109106", {"address": "", "articleno": "", "doi": "10.1109/JSTARS.2015.2421879", "issn": "2151-1535", "issue_date": "", "journal": "IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing", "keywords": ["Reliability", "Orbits", "Synthetic aperture radar", "Calibration", "Accuracy", "Coherence", "Earth", "Digital elevation models (DEMs)", "image fusion", "interferometric synthetic aperture radar (InSAR)", "mosaicking", "TanDEM-X", "Digital elevation models (DEMs)", "image fusion", "interferometric synthetic aperture radar (InSAR)", "mosaicking", "TanDEM-X"], "month": "March", "number": "3", "numpages": "", "publisher": "", "title": "The TanDEM-X DEM Mosaicking: Fusion of Multiple Acquisitions Using InSAR Quality Parameters", "url": "", "volume": "9", "year": "2016", "abstract": "Since 2010, TanDEM-X and its twin satellite TerraSAR-X fly in a close orbit formation and form a single-pass synthetic aperture radar (SAR) interferometer. The formation was established to acquire a global high-precision digital elevation model (DEM) using SAR interferometry (InSAR). In order to achieve the required height accuracy of the TanDEM-X DEM, at least two global coverages have to be acquired. However, in difficult and mountainous terrain, up to five coverages are present. Here, acquisitions from ascending and descending orbits are needed to fill gaps and to overcome geometric limitations. Therefore, a strategy to properly combine the available height estimates is mandatory. The objective of this paper is the presentation of the operational TanDEM-X DEM mosaicking approach. In general, multiple InSAR DEM heights are combined by means of a weighted average with the height error as weight. Apart from this widely used mosaicking approach, one big challenge remains with the handling of larger height discrepancies between the input data, which are mainly caused by phase unwrapping errors, but also by temporal changes between acquisitions. In the case of inconsistencies, the TanDEM-X mosaicking approach performs a grouping into height levels. A priority concept is set up to evaluate the different groups of heights considering the number of DEMs and several InSAR quality parameters: the height error, the phase unwrapping method, and the height of ambiguity. This allows the identification of the most reliable height level for mosaicking. This fusion concept is verified on different test areas affected by phase unwrapping errors in flat and mountainous terrain as well as by height discrepancies in forests. The results show that the quality of the final TanDEM-X DEM mosaic benefits a lot from this mosaicking approach.", "pages": "1047-1057", "note": "", "ISSN": "2151-1535", "publicationtype": "article", "author": ["Gruber, Astrid", "Wessel, Birgit", "Martone, Michele", "Roth, Achim"]}]["8501920", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2018.2877153", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Data models", "Predictive models", "Training data", "Load modeling", "Computational modeling", "Sensors", "Distributed deep neural networks", "spatio-temporal analysis", "ensemble deep learning", "bloom filter", "Internet of Things", "smart city"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "Exploiting the Spatio-Temporal Patterns in IoT Data to Establish a Dynamic Ensemble of Distributed Learners", "url": "", "volume": "6", "year": "2018", "abstract": "Internet of Things applications can greatly benefit from accurate prediction models. The performance of prediction models is highly dependent on the quantity and quality of their training data. In this paper, we investigate the creation of a dynamic ensemble from distributed deep learning models by considering the spatiotemporal patterns embedded in the training data. Our dynamic ensemble does not depend on offline configurations. Instead, it exploits the spatiotemporal patterns embedded in the training data to generate dynamic weights for the underlying weak distributed deep learners to create a stronger learner. Our evaluation experiments using three real-world datasets in the context of the smart city show that our proposed dynamic ensemble strategy leads to an improved error rate of up to 33% compared to the baseline strategy even when using31of the training data. Moreover, using only 20% of the training data, the error rate of the model slightly increased by up to 2 in terms of mean square error. This increase is 82% less than the 11.3 increase seen in the baseline model. Therefore, our approach contributes to the reduced network traffic while not hindering the accuracy significantly.", "pages": "63316-63328", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Mohammadi, Mehdi", "Al-Fuqaha, Ala"]}]["8787775", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2019.2933169", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Feature extraction", "Training", "Visualization", "Neural networks", "Task analysis", "Learning systems", "Streaming media", "Deep neural network", "weakly-supervised learning", "multi-steam feature extractor", "probability fusion module"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "Probability Fusion Decision Framework of Multiple Deep Neural Networks for Fine-Grained Visual Classification", "url": "", "volume": "7", "year": "2019", "abstract": "Fine-grained visual classification tasks often suffer from that the subordinate categories within a basic-level category have low inter-class discrepancy and high intra-class variances, which is still challenging research for traditional deep neural networks (DNNs). However, different models extract local parts' features in isolation and neglect the inherent correlations and distribution in high-dimensional space, which limit the single model to achieve better accuracy. In this paper, we propose a novel probability fusion decision framework (named as PFDM-Net) for fine-grained visual classification. More specifically, it first employs data-augmented tricks to enlarge the dataset and pretrain the basic VGG19 and ResNet networks on high-quality images datasets to learn common and domain knowledge simultaneously while fine-tuning with professional skill. Next, refined multiple DNNs with transfer learning are applied to design a multi-stream feature extractor, which utilizes the mixture-granularity information to exploit high-dimensionality features for distinguishing interclass discrepancy and tolerating intra-class variances. Finally, a probability fusion module equipped with gating network and probability fusion layer is developed to fuse different components model with Gaussian distribution as a unified probability representation for the ultimate fine-grained recognition. The input of this module is the various features of multi-models and the output is the fused classification probability. The end-to-end implementation of our framework contain an inner loop about the EM algorithm within an outer loop with the gradient back-propagation optimization of the whole network. Experimental results demonstrate the outperforming performance of PFDM-Net with higher classification accuracy on different fine-grained datasets compared with the state-of-the-arts methods. More discussions are provided to indicate the potential applications in combination with other work.", "pages": "122740-122757", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Zheng, Yang-Yang", "Kong, Jian-Lei", "Jin, Xue-Bo", "Wang, Xiao-Yi", "Su, Ting-Li", "Wang, Jian-Li"]}]["8242357", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2017.2788417", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Smoothing methods", "Semantics", "Computational modeling", "Correlation", "Information retrieval", "Knowledge based systems", "Probability distribution", "Language model smoothing", "entity", "knowledge base", "semantic relevance"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "Entity-Based Language Model Smoothing Approach for Smart Search", "url": "", "volume": "6", "year": "2018", "abstract": "Smart search plays an important role in all walks of life, for example, according to business needs, accurate search of required knowledge from massive resources is an important way to enhance industrial intelligence. Smoothing of the language model is essential for obtaining high-quality search results because it helps to reduce mismatching and overfitting problems caused by data sparseness. Traditional smoothing methods lexically focus on the global corpus and locally cluster documents information without semantic analysis, which leads to deficiency of the semantic correlations between query statements and documents. In this paper, we propose an entity-based language model smoothing approach for smart search that uses semantic correlation and takes entities as bridges to build the entity semantic language model using a knowledge base. In this approach, entities in the documents are linked to an external knowledge base, such as Wikipedia. Then, the entity semantic language model is generated by using soft-fused and hardfused methods. A two-level merging strategy is also presented to smooth the language model according to whether a given word is semantically relevant to the document or not, which integrates the Dir-smoothing and JM-smoothing methods. Experimental results show that the smoothed language model more closely approximates the word probability distribution under the document semantic theme and more accurately estimates the relevance between query and document.", "pages": "9991-10002", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Zhao, Feng", "Tian, Zeliang", "Jin, Hai"]}]["8884208", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2019.2949871", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Apertures", "Shape", "Optimization", "Pricing", "Linear programming", "Acceleration", "Convergence", "Aperture shape", "column generation", "direct aperture optimization", "gradient descent direction", "gradient descent with momentum"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "Aperture Shape Generation Based on Gradient Descent With Momentum", "url": "", "volume": "7", "year": "2019", "abstract": "Direct aperture optimization (DAO) is an effective method to generate high-quality intensity-modulated radiation therapy treatment plans. In generic DAO, the direction of negative gradient descent is generally used to determine the aperture shape. However, this strategy can reduce the convergence rate, especially near the optimal value. We propose aperture shape generation based on the direction of gradient descent with momentum, where column generation is implemented as carrier. During aperture shape generation of column generation, the current aperture gradient map is first calculated. Then, the gradient with momentum is calculated based on the existing gradient information. Finally, the direction of gradient descent with momentum is constructed for obtaining the deliverable aperture shape by solving the pricing problem. To verify the effectiveness of the proposed method, we conducted comparative experiments on two head and neck and two prostate tumor cases. Compared with generic column generation, the proposed method can effectively protect the organs at risk while ensuring the required dose distribution to the target. Using the proposed method, the number of apertures and optimization time can be reduced by up to 30.95 and 32.96%, respectively, compared to the conventional approach. The experimental results suggest that the proposed method can accelerate the search speed and improve the quality of treatment plans.", "pages": "157623-157632", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Zhang, Liyuan", "Zhang, Pengcheng", "Yang, Jie", "Li, Jie", "Gui, Zhiguo"]}]["8630825", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2019.2896175", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Apertures", "Optimization", "Pricing", "Linear programming", "Shape", "Biomedical applications of radiation", "Sequential analysis", "Column generation", "direct aperture optimization", "image processing", "intensity-modulated radiation therapy", "region growth"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "A Column Generation Approach Based on Region Growth", "url": "", "volume": "7", "year": "2019", "abstract": "In intensity-modulated radiation therapy (IMRT), a network flow is adopted to solve the pricing problem of the generic column generation approach in order to obtain a deliverable aperture. However, excessive computation results from the direct use of a network flow. In addition, a decline in plan quality may result from the direct determination of the leaf position using the gradient information. To overcome these problems, a column generation approach based on region growth is proposed. The proposed method is designed to reduce the computational cost of solving the pricing problem and improve the IMRT plan quality. First, the gradients of the beamlets are obtained by an objective function constructed under the constraint conditions of the organs. Second, the gradients are transformed nonlinearly. Third, the positions of the continuous negative gradient regions in each row of the aperture are determined and stored. Fourth, these gradients are taken as a whole and added to the aperture network flow, which is solved as a shortest-path problem. Finally, the deliverable aperture is obtained and added to the treatment plan. To verify the effectiveness of the proposed method, experiments involving five five-field prostate cancer cases and five nine-field head and neck cancer cases were conducted. Compared with the generic column generation method, the dose distribution of the target is ensured by the proposed method, which also effectively protects organs at risk and reduces the running time. Specifically, in ten groups of comparative experiments, the normal tissue complication probability of the proposed method is reduced by up to 3.37%, and the maximum acceleration rate is 20.44%. According to the experimental results, the proposed method is more consistent with clinical requirements compared with the generic column generation method.", "pages": "31123-31139", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Zhang, Liyuan", "Gui, Zhiguo", "Yang, Jie", "Zhang, Pengcheng"]}]["7484255", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2016.2576286", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Internet of things", "Spectrum management", "Data analytics", "Resource management", "Quality of experience", "Cognitive radio", "Wireless networks", "Cloud computing", "Systematics", "Internet of spectrum devices (IoSD)", "cognitive radio", "data analytics", "resource optimization", "quality of experience (QoE)", "Internet of spectrum devices (IoSD)", "cognitive radio", "data analytics", "resource optimization", "quality of experience (QoE)"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "A Cloud-Based Architecture for the Internet of Spectrum Devices Over Future Wireless Networks", "url": "", "volume": "4", "year": "2016", "abstract": "The dramatic increase in data rates in wireless networks has caused radio spectrum usage to be an essential and critical issue. Spectrum sharing is widely recognized as an affordable, near-term method to address this issue. This paper first characterizes the new features of spectrum sharing in future wireless networks, including heterogeneity in sharing bands, diversity in sharing patterns, crowd intelligence in sharing devices, and hyperdensification in sharing networks. Then, to harness the benefits of these unique features and promote a vision of spectrum without bounds and networks without borders, this paper introduces a new concept of the Internet of spectrum devices (IoSDs) and develops a cloud-based architecture for IoSD over future wireless networks, with the prime aim of building a bridging network among various spectrum monitoring devices and massive spectrum utilization devices, and enabling a highly efficient spectrum sharing and management paradigm for future wireless networks. Furthermore, this paper presents a systematic tutorial on the key enabling techniques of the IoSD, including big spectrum data analytics, hierarchal spectrum resource optimization, and quality of experience-oriented spectrum service evaluation. In addition, the unresolved research issues are also presented.", "pages": "2854-2862", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Wu, Qihui", "Ding, Guoru", "Du, Zhiyong", "Sun, Youming", "Jo, Minho", "Vasilakos, Athanasios"]}]["9530686", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2021.3110947", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Circuit faults", "Feature extraction", "Tools", "Iron", "Data models", "Photovoltaic systems", "Deep learning", "Fault diagnosis", "deep learning", "photovoltaic systems"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "Deep Learning-Based Fault Diagnosis of Photovoltaic Systems: A Comprehensive Review and Enhancement Prospects", "url": "", "volume": "9", "year": "2021", "abstract": "Photovoltaic (PV) systems are subject to failures during their operation due to the aging effects and external/environmental conditions. These faults may affect the different system components such as PV modules, connection lines, converters/inverters, which can lead to a decrease in the efficiency, performance, and further system collapse. Thus, a key factor to be taken into consideration in high-efficiency grid-connected PV systems is the fault detection and diagnosis (FDD). The performance of the FDD method depends mainly on the quality of the extracted features including real-time changes, phase changes, trend changes, and faulty modes. Thus, the data representation learning is the core stage of intelligent FDD techniques. Recently, due to the enhancement of computing capabilities, the increase of the big data use, and the development of effective algorithms, the deep learning (DL) tool has witnessed a great success in data science. Therefore, this paper proposes an extensive review on deep learning based FDD methods for PV systems. After a brief description of the DL-based strategies, techniques for diagnosing PV systems proposed in recent literature are overviewed and analyzed to point out their differences, advantages and limits. Future research directions towards the improvement of the performance of the DL-based FDD techniques are also discussed. This review paper aims to systematically present the development of DL-based FDD for PV systems and provide guidelines for future research in the field.", "pages": "126286-126306", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Mansouri, Majdi", "Trabelsi, Mohamed", "Nounou, Hazem", "Nounou, Mohamed"]}]["9775658", {"address": "", "articleno": "", "doi": "10.1109/JSTARS.2022.3173676", "issn": "2151-1535", "issue_date": "", "journal": "IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing", "keywords": ["Hyperspectral imaging", "Feature extraction", "Transfer learning", "Sensors", "Neural networks", "Convolution", "Image classification", "Classification", "convolutional neural network (CNN)", "generalized feature extraction network (GFEN)", "hyperspectral imaging", "spectral projection", "transfer learning"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "Multisource Domain Transfer Learning Based on Spectral Projections for Hyperspectral Image Classification", "url": "", "volume": "15", "year": "2022", "abstract": "Hyperspectral image classification is an important topic for hyperspectral remote sensing with various applications. Hyperspectral image classification accuracy. It has been greatly improved with the introduction of deep neural networks, while the idea of transfer learning provides an opportunity to solve the problem even with the lack of training samples. In this article, we propose an effective transfer learning approach for hyperspectral images, projecting hyperspectral images with different sensors and different number of bands into a general spectral space, preserving the relative positions of each band for spectral alignment, and designing a hierarchical depth neural network for shallow feature transfer and deep feature classification. The experiments show that the proposed method can effectively preserve the source domain features, especially for the scenarios with very few samples in the target domain, which can significantly improve the classification accuracy and reduce the risk of model overfitting. Meanwhile, this strategy greatly reduces the requirement of source domain data, using multisensor data to jointly train a more robust general feature model. The proposed method can achieve high accuracies even with few training samples compared to currently many state-of-the-art classification methods.", "pages": "3730-3739", "note": "", "ISSN": "2151-1535", "publicationtype": "article", "author": ["Yang, Bin", "Hu, Shunshi", "Guo, Qiandong", "Hong, Danfeng"]}]["9291393", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2020.3044069", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Clustering algorithms", "Classification algorithms", "Machine learning algorithms", "Partitioning algorithms", "Optimization", "Euclidean distance", "Data mining", "K-means", "local optimum", "Initial center", "UCI"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "A Novel Effective Distance Measure and a Relevant Algorithm for Optimizing the Initial Cluster Centroids of K-means", "url": "", "volume": "", "year": "2020", "abstract": "The traditional K-means algorithm is very sensitive to the selection of the initial clustering point and the calculation of the distance measure, which is likely to result in the convergence of only partly optimal solutions. An improved k-means algorithm is proposed to solve the problem of unbalanced clustering effect caused by the fact that the first initial clustering centre falls in the non-dense region of the boundary in the initial clustering centre optimisation process. An improved k-means algorithm for initial clustering centres is proposed, namely, the optimal matching algorithm for K-means clustering, and related experimental analysis of the algorithm is carried out. The improved algorithm first selects the initial points of the traditional K-means clustering algorithm and analyses the clustering results. Then, the initial clustering centre selection and distance determination were tested and the clustering effect was evaluated by introducing the contour coefficient. Experiments on both artificial data sets and UCI data sets show that the algorithm can achieve better clustering results. The experimental results indicate that the improved algorithm has a much higher clustering quality than the traditional K-means algorithm and other improved algorithms.", "pages": "1-1", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Liu, Yang", "Ma, Shuaifeng", "Du, Xinxin"]}]["6797876", {"address": "", "articleno": "", "doi": "10.1109/TETC.2014.2316524", "issn": "2168-6750", "issue_date": "", "journal": "IEEE Transactions on Emerging Topics in Computing", "keywords": ["Quality of service", "Logistics", "Algorithm design and analysis", "Heuristic algorithms", "Optimization", "Urban areas", "Social network services", "Logistics path planning", "service composition", "social network", "QoS", "big data"], "month": "Dec", "number": "4", "numpages": "", "publisher": "", "title": "A Dynamic QoS-Aware Logistics Service Composition Algorithm Based on Social Network", "url": "", "volume": "2", "year": "2014", "abstract": "The public logistics platform aims to provide customers with end-to-end logistics services by finding and composing a huge quantity of web services from logistics service providers. But, traditional service composition required predefined business process so that its flexibility is far from satisfactory in the problem. Path planning can be a solution of finding a suitable business path during service composition, but the search space will increase dramatically with the growth of service quantity and is hard to get a result within a tolerable interaction time. In the context of big data, to quickly build a service path with the optimal global QoS has become a problem demanding prompt solution. Sociologists point out that companies prefer familiar partners in the commercial environment. Using this principle, a concept of partner circle is defined, which can significantly reduce the search space in path planning. Combining path planning with service composition, a PartnerFirst algorithm is presented based on the social network, which is the cooperation network of service providers here. Simulation experiment shows that the PartnerFirst algorithm outperforms current approaches over 10 times in efficiency, with just about 10% loss in QoS. The relationship between efficiency and service quantity of the PartnerFirst algorithm is nearly linear. It proves that using social network in dynamic service composition is efficient and effective.", "pages": "399-410", "note": "", "ISSN": "2168-6750", "publicationtype": "article", "author": ["Yu, Yang", "Chen, Jian", "Lin, Shangquan", "Wang, Ying"]}]["8793128", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2019.2934166", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Predictive models", "Hospitals", "Prediction algorithms", "Red blood cells", "Data analysis", "Length of stay", "intensive care unit", "exploratory data analysis", "least absolute shrinkage and selection operator"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "Prediction of Length of Stay on the Intensive Care Unit Based on Least Absolute Shrinkage and Selection Operator", "url": "", "volume": "7", "year": "2019", "abstract": "Length of stay (LoS) in the intensive care unit (ICU) is a common outcome measure used as an indicator of both quality of care and resource use. However, the existing analysis methods of LoS are poorly interpretable and extensible, and there is controversial for the predictive performance of LoS. In this paper, the study includes data from 1,214 unplanned ICU admissions to participate in the ICU of Sichuan Provincial People\u2019s Hospital between Dec. 11, 2015 and Dec. 6, 2018. On the basis of these data, this study creates a highly accurate and predictive model using advanced preprocessing techniques, exploratory data analysis (EDA) and least absolute shrinkage and selection operator (LASSO) algorithm. Next, this study evaluates the predictive performance of the proposed model by 10-fold cross validation and external validation method using the root mean square prediction error (RMSPE), mean absolute error (MAE), and coefficient of determination ( $R^{2}$ ). The predictive performance of the proposed model is 0.88\u00b10.13 day for RMSPE, 0.87\u00b10.07 day for MAE and 0.35\u00b10.09 for $R^{2}$ . Experimental results show that the performance of the proposed method are competitive with the state-of-the-art methods and results. Furthermore, this study explores the risk factors for ICU LoS in survivors and non-survivors and compare their predictive performance.", "pages": "110710-110721", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Li, Chunling", "Chen, Longyi", "Feng, Jie", "Wu, Duanpo", "Wang, Zimeng", "Liu, Junbiao", "Xu, Weifeng"]}]["8390920", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2018.2849208", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Apertures", "Shape", "Optimization", "Linear programming", "Shortest path problem", "Sequential analysis", "Search methods", "Optimization", "intensity modulated radiation therapy", "fuzzy enhancement", "aperture shape optimization"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "The Aperture Shape Optimization Based on Fuzzy Enhancement", "url": "", "volume": "6", "year": "2018", "abstract": "The aperture shape optimization (ASO) is a critical step in the direct aperture optimization (DAO) method. During ASO, the gradient of objective function is calculated with respect to the beamlet weight. These gradient components are directly utilized to generate the new aperture shape. In this way, the beamlet of the large positive gradient value may be grouped into the generated aperture shape. The treatment quality may be deteriorated by adding this aperture into the treatment plan. In order to overcome this drawback, a novel method based on the fuzzy enhancement was proposed to generate the aperture shape. We apply the fuzzy enhancement method to enhance the gradient map that is composed of the gradients of objective function in a beam. The enhanced gradient map was then employed to form a network flow, which was solved to generate the new aperture shape. The optimal aperture shape was generated by removing the beamlet of the large positive gradient value from the new generated aperture shape. To verify the effectiveness, the proposed method was compared with the conventional column generation (CG) method on a prostate cancer case and on a head-and-neck cancer case. Experimental results demonstrate that the new algorithm has a better performance than the CG algorithm. The proposed method can further reduce the dose delivered to the critical structures, when the similar dose coverage is delivered on the targets.", "pages": "35979-35987", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Zhang, Pengcheng", "Zhang, Liyuan", "Yang, Jie", "Gui, Zhiguo"]}]["9328097", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2021.3052429", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Atmospheric modeling", "Predictive models", "Air pollution", "Forecasting", "Vehicle dynamics", "Uncertainty", "Deep learning", "Conv-LSTM", "spatio-temporel prediction", "highly dynamic air quality", "accidental pollutant release", "uncertainty", "FFT-07", "WSN"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "Uncertainty-Aware Deep Learning Architectures for Highly Dynamic Air Quality Prediction", "url": "", "volume": "9", "year": "2021", "abstract": "Forecasting air pollution is considered as an essential key for early warning and control management of air pollution, especially in emergency situations, where big amounts of pollutants are quickly released in the air, causing considerable damages. Predicting pollution in such situations is particularly challenging due to the strong dynamic of the phenomenon and the various spatio-temporal factors affecting air pollution dispersion. In addition, providing uncertainty estimates of prediction makes the forecasting model more trustworthy, which helps decision-makers to take appropriate actions with more confidence regarding the pollution crisis. In this study, we propose a multi-point deep learning model based on convolutional long short term memory (ConvLSTM) for highly dynamic air quality forecasting. ConvLSTM architectures combines long short term memory (LSTM) and convolutional neural network (CNN), which allows to mine both temporal and spatial data features. In addition, uncertainty quantification methods were implemented on top of our model's architecture and their performances were further excavated. We conduct extensive experimental evaluations using a real and highly dynamic air pollution data set called Fusion Field Trial 2007 (FFT07). The results demonstrate the superiority of our proposed deep learning model in comparison to state-of-the-art methods including machine and deep learning techniques. Finally, we discuss the results of the uncertainty techniques and we derive insights.", "pages": "14765-14778", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Mokhtari, Ichrak", "Bechkit, Walid", "Rivano, Herv\u00e9", "Yaici, Mouloud"]}]["8506340", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2018.2877666", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Diversity reception", "Clustering algorithms", "Partitioning algorithms", "Measurement", "Classification algorithms", "Geology", "Data structures", "Diversity", "multi-modal metrics", "quality", "selective clustering ensemble"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "Two-Level-Oriented Selective Clustering Ensemble Based on Hybrid Multi-Modal Metrics", "url": "", "volume": "6", "year": "2018", "abstract": "The purpose of selective clustering ensemble is to select a subset of base clustering partitions with predictive performance and combine these partitions into more accurate and stable final results. Traditional approaches tend to utilize the well-known validity criteria such as NMI to evaluate the quality and diversity of base clustering partitions in the selection process. However, the characteristics of the original data and the data structure itself are commonly neglected. Furthermore, the generation process of base clustering partitions is more concerned with diversity and less consideration of quality. To tackle these problems, we propose a new selective clustering ensemble scheme. In the process of generating base clustering partitions, k-means and hierarchical clustering algorithm alternately combined with random projection method are employed to generate diverse base partitions. Meanwhile, in order to improve the quality of base clustering partitions, we propose a new selection strategy for the number of clusters k in k-means algorithm. In the clustering selection process, both diversity and quality of the base clustering partitions are evaluated by multi-modal metrics from two levels: clustering labels and data structure. Based on five UCI benchmark datasets, experimental results demonstrate that the proposed method not only can generate but also select base clustering partitions with both diversity and quality. Experimental analyses show the validity and stability of the proposed scheme.", "pages": "64159-64168", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Wang, Hongling", "Liu, Gang"]}]["8125076", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2017.2779045", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Cloud computing", "Quality of service", "Bayes methods", "Cognition", "Computational modeling", "Predictive models", "Time factors", "Web service", "quality of service", "cloud service", "Bayesian network model"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "A Novel QoS Prediction Approach for Cloud Services Using Bayesian Network Model", "url": "", "volume": "6", "year": "2018", "abstract": "Cloud computing is the next generation computing model, which has a significant position in the field of scientific and business computing. By predicting cloud service's QoS in next period, it is helpful for end users to choose the most suitable cloud service that meets their needs. The underlying hardware/software resources of cloud architecture may have a certain influence on cloud service QoS. However, existing cloud service QoS prediction approaches do not take this influence into account. As these effects are real during the process of cloud service QoS prediction, ignoring the impact of these effects may create a big gap between the prediction results and the actual results. Therefore, in this paper interactive information is first used to describe the correlation between the hardware/software resources and the QoS attributes of the cloud service. Then, a Bayesian network model is established to predict cloud QoS. Bayesian network prediction reasoning algorithm is used to predict and reason about the future QoS values. A set of dedicated experiments is conducted to validate that our approach can accurately predict QoS of cloud service and the accuracy rate is better than state-of-the-art approaches.", "pages": "1391-1406", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Li, Wenrui", "Zhang, Pengcheng", "Leung, Hareton", "Ji, Shunhui"]}]["9400395", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2021.3072314", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Watermarking", "Image coding", "Feature extraction", "Authentication", "Discrete cosine transforms", "Data mining", "Tampered image", "image recovery", "image authentication", "feature extraction", "watermarking", "image compression"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "Detection and Recovery of Higher Tampered Images Using Novel Feature and Compression Strategy", "url": "", "volume": "9", "year": "2021", "abstract": "Due to the availability of powerful image-editing software and the growing amount of multimedia data that is transmitted via the Internet, integrity verifications and confidentiality of the data are becoming critical issues. However, currently, the accuracy of detecting and the recovery capability of the tampered images by the existing methods through watermarking strategy is still not at the required level, especially at a higher tampered rate. This paper proposes a new blind and fragile watermarking method to detect tampering and better recovery of tampered images. To improve the quality of both the watermarked and the recovered images, a new feature extraction scheme is introduced which will produce a short but comprehensive recovery code using a new compression strategy. If a block in the image tampers, the proposed embedded feature allows the original data to be extracted for recovery. To overcome tamper coincidence, every block\u2019s watermarked data contains not only the recovery code belonging to the block itself but also its neighbor\u2019s data as a second layer of recovery. Various size blocks were investigated to see the performance and compare their efficiency for recovering an image after different tampering rates. The test showed the smaller block sizes may be more suitable for locating tampering, where the bigger ones are more suitable when the tampering rate is higher. The bigger block sizes in the proposed method can recover an image even after a 60% tampering rate with high quality (more than 31 dB). The experimental results prove that the proposed method can have better efficiency for detecting tampering, and recovery of the original image, compared to the relevant existing methods.", "pages": "57510-57528", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Tohidi, Faranak", "Paul, Manoranjan", "Hooshmandasl, Mohammad"]}]["9815595", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2022.3188862", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Job shop scheduling", "Schedules", "Industrial Internet of Things", "Reliability", "IEEE 802.15 Standard", "Quality of service", "Standards", "6TiSCH", "autonomous scheduling", "industrial IoT", "latency", "RPL", "TSCH"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "6TiSCH Low Latency Autonomous Scheduling for Industrial Internet of Things", "url": "", "volume": "10", "year": "2022", "abstract": "With the advancements in the Internet of Things (IoT), machine-to-machine communication, big-data, and the associated environment, a new model of the Industrial Internet of Things (IIoT) has emerged. The IIoT brings sensors, intelligent machines and tools, instruments, and analytics together for applications like manufacturing, robotics, and many others. Most of these applications require adaptive and autonomous behavior, Quality of Service (QoS), efficient resource allocation, and reservation. One of these crucial challenges is to build and maintain a data communication schedule. Time Slotted Channel Hopping (TSCH) based network operation promises required QoS for low-power applications and enables high reliability. 6TiSCH layer is being developed by standardizing the protocol stack to achieve industrial performance requirements by using IPv6 over IEEE802.15.4e TSCH MAC. 6TiSCH aims to manage the schedule and configure it with the topology and traffic requirements in the industrial environment. This paper proposes a novel low latency autonomous scheduling scheme for the 6TiSCH networks. It generates a segmented schedule for the network where all source nodes can send application data packets to the root node in a single slotframe. The performance of the proposed technique is compared with existing scheduling techniques. Our scheme outperforms the other techniques. The result shows that the latency is reduced up to 41% in comparison with the other scheduling schemes. The proposed scheme has a lower radio duty cycle as the node\u2019s ON time is reduced, making it more energy efficient and reliable.", "pages": "71566-71575", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Pradhan, Nilam", "Chaudhari, Bharat", "Zennaro, Marco"]}]["9104693", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2020.2998767", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Containers", "Heuristic algorithms", "Edge computing", "Computational modeling", "Virtualization", "Artificial intelligence", "Quality of service", "Edge computing system", "microservices", "elastic deployment", "system cost", "container"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "Towards Cost-Efficient Edge Intelligent Computing With Elastic Deployment of Container-Based Microservices", "url": "", "volume": "8", "year": "2020", "abstract": "With the tremendous growth of the Internet of Things (IoT), big data, and artificial intelligence (AI), the edge computing-based service paradigm has been introduced to meet the increasing demand of applications. To provide efficient computing services at the network edge, the algorithms and applications are generally deployed based on the container-based microservice strategy, which significantly impacts the system efficiency and QoS. Considering the fundamental system uncertainties, including the dynamic workload and service rate, we investigate how to minimize the long-term system cost through the elastic microservice deployment in this paper. To this end, we formulate the container-based microservice deployment as a stochastic optimization problem to minimize the system cost while maintaining the system QoS and stability. We develop a cost-aware elastic microservice deployment algorithm to solve the formulated problem, which balances the tradeoff between system cost and QoS. Our algorithm makes the real-time decisions based on current queue backlogs and system states without predicting the future knowledge. Finally, we conduct the theoretical analysis and extensive simulations based on data traces from the ResNet-50 model-based visual recognition application. The results demonstrate that our algorithm outperforms the baseline strategies with respect to the system cost, queue backlogs, and the number of Pod replicas.", "pages": "102947-102957", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Zhao, Peng", "Wang, Peizhe", "Yang, Xinyu", "Lin, Jie"]}]["9875288", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2022.3204066", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Farming", "Green products", "Sensors", "Monitoring", "Wireless sensor networks", "Taxonomy", "Protocols", "Security", "Internet of Things", "Communication protocols", "Big Data", "Data analysis", "Cloud computing", "Internet of Things (IoT)", "greenhouse", "applications", "sensors", "communication protocols", "cloud computing", "big data analytics", "security attacks"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "IoT Based Smart Greenhouse Framework and Control Strategies for Sustainable Agriculture", "url": "", "volume": "10", "year": "2022", "abstract": "In recent years, the Internet of Things (IoT) has become one of the most familiar names creating a benchmark and scaling new heights. IoT an indeed future of the communication that has transformed the objects (things) of the real world into smarter devices. With the advent of IoT technology, this decade is witnessing a transformation from traditional agriculture approaches to the most advanced ones. In perspective to the current standing of IoT in agriculture, identification of the most prominent application of IoT-based smart farming i.e. greenhouse has been highlighted and presented a systematic analysis and investigated the high quality research work for the implementation of greenhouse farming. The primary objective of this study is to propose an IoT-based network framework for a sustainable greenhouse environment and implement control strategies for efficient resources management. A rigorous discussion on IoT-based greenhouse applications, sensors/devices, and communication protocols have been presented. Furthermore, this research also presents an inclusive review of IoT-based greenhouse sensors/devices and communication protocols. Moreover, we have also presented a rigorous discussion on smart greenhouse farming challenges and security issues as well as identified future research directions to overcome these challenges. This research has explained many aspects of the technologies involved in IoT-based greenhouse and proposed network architecture, topology, and platforms. In the end, research results have been summarized by developing an IoT-based greenhouse farm management taxonomy and attacks taxonomy.", "pages": "99394-99420", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Farooq, Muhammad", "Javid, Rizwan", "Riaz, Shamyla", "Atal, Zabihullah"]}]["7349100", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2015.2506648", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Streaming media", "Multimedia communication", "Broadcasting", "Quality of service", "Encoding", "Computer architecture", "Next generation networking", "Quality of service", "video coding", "broadcast technology", "communications technology"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "QoE-Enabled Big Video Streaming for Large-Scale Heterogeneous Clients and Networks in Smart Cities", "url": "", "volume": "4", "year": "2016", "abstract": "The rapid growth of the next-generation communication and networks is bringing video services into more pervasive environments. More and more users access and interact with video content using different devices, such as smart televisions, personal computers, tablets, smartphones, and wearable equipments. Providing heterogeneous Quality of Experience (QoE) that supports a wide variety of multimedia devices is critical to video broadcasting over the next-generation wireless network. This paper reviews practical video broadcasting technologies and examines current requirements ranging from heterogeneous devices to transmission technologies. Meanwhile, various coding methodologies, including QoE modeling, scalable compression efficiency, and flexible transmission, are also discussed. Moreover, this paper presents a typical paradigm as an example for video broadcasting with large-scale heterogeneity support, which enables QoE mapping, joint coding, flexible forward error coding, and cross-layer transmission, as well as optimal and dynamic adaptation to improve the overall receiving quality of heterogeneous devices. Finally, a brief summary of the key ideas and a discussion of interesting open areas are summarized at the end of this paper along with a future recommendation.", "pages": "97-107", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Chen, Bo-Wei", "Ji, Wen", "Jiang, Feng", "Rho, Seungmin"]}]["8720195", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2019.2918409", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Feature extraction", "Convolution", "Noise reduction", "Kernel", "Classification algorithms", "Neural networks", "Data mining", "Unsupervised learning", "feature extraction", "denoising auto-encoder", "convolutional neural network"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "A Stacked Multi-Granularity Convolution Denoising Auto-Encoder", "url": "", "volume": "7", "year": "2019", "abstract": "With the development of big data, artificial intelligence has provided many intelligent solutions to urban life. For instance, an image-based intelligent technology, such as image classification of diseases, is widely used in daily life. However, the image in real life is mostly unlabeled, so the performance of many image-based intelligent models shows limitations. Therefore, how to use a large amount of unlabeled image data to build an efficient and high-quality model for better urban life has been an urgent research topic. In this paper, we propose an unsupervised image feature extraction method that is referred to as a stacked multi-granularity convolution denoising auto-encoder (SMGCDAE). The algorithm is based on a convolutional neural network (CNN), yet it introduces a multi-granularity kernel. This approach resolved issues with image unicity by extracting a diverse category of high-level features. In addition, the denoising auto-encoder ensures stability and improves the classification accuracy by extracting more robust features. The algorithm was assessed using three image benchmark datasets and a series of meningitis images, achieving higher average accuracy than other methods. These results suggest that the algorithm is capable of extracting more discriminative high-level features and thus offers superior performance compared with the existing methodologies.", "pages": "83888-83899", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Yang, Yun", "Cao, Lijuan", "Liu, Qing", "Yang, Po"]}]["8588980", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2018.2889766", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Atmospheric modeling", "Image restoration", "Image color analysis", "Remote sensing", "Scattering", "Meteorology", "Lighting", "Heavy haze", "image dehazing", "implementation efficiency", "non-uniform haze", "non-uniform illumination", "remote sensing"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "Remote Sensing Image Haze Removal Using Gamma-Correction-Based Dehazing Model", "url": "", "volume": "7", "year": "2019", "abstract": "Haze is evident in most remote sensing (RS) images, particularly for the RS scenes captured in inclement weather, which severely hinders image interpretation. In this paper, two simple yet effective visibility restoration formulas are proposed for RGB-channel RS (RRS) images and multi-spectral RS (MSRS) images, respectively. More specifically, a robust gamma-correction-based dehazing model (RGDM) is first defined, which can better address the non-uniform illumination problem in hazy images. Then, the scene albedo restoration formula (SARF) used for the RRS images is obtained by imposing the existing prior knowledge on this RGDM, which enables us to simultaneously eliminate the interferences of haze and non-uniform illumination. In subsequence, according to Rayleigh\u2019s law, an expanded restoration formula (E-SARF) is further developed for MSRS data. Using the proposed E-SARF, the spatially varying haze in each band can be thoroughly removed without using any extra information. The experiments are conducted on the challenging RRS and MSRS images, including images with non-uniform illumination, non-uniform haze distribution, and heavy haze. The results reveal that the SARF and the E-SARF are superior to most other state-of-the-art techniques in terms of both the recover quality and the implementation efficiency.", "pages": "5250-5261", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Ju, Mingye", "Ding, Can", "Guo, Y.", "Zhang, Dengyin"]}]["9200608", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2020.3024979", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Learning (artificial intelligence)", "Urban areas", "Heuristic algorithms", "Optimization", "Roads", "Adaptation models", "Transportation dynamics", "human mobility data", "reinforcement learning", "partially observable discrete event decision process", "MATSim"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "Optimizing Transportation Dynamics at a City-Scale Using a Reinforcement Learning Framework", "url": "", "volume": "8", "year": "2020", "abstract": "Urban planners, authorities, and numerous additional players have to deal with challenges related to the rapid urbanization process and its effect on human mobility and transport dynamics. Hence, optimize transportation systems represents a unique occasion for municipalities. Indeed, the quality of transport is linked to economic growth, and by decreasing traffic congestion, the life quality of the inhabitants is drastically enhanced. Most state-of-the-art solutions optimize traffic in specific and small zones of cities (e.g., single intersections) and cannot be used to gather insights for an entire city. Moreover, evaluating such optimized policies in a realistic way that is convincing for policy-makers can be extremely expensive. In our work, we propose a reinforcement learning frameworks to overtake these two limitations. In particular, we use human mobility data to optimize the transport dynamics of three real-world cities (i.e., Berlin, Santiago de Chile, Dakar) and a synthesized one (i.e., SynthTown). To this end, we transform the transportation dynamics' simulator MATSim into a realistic reinforcement learning environment able to optimize and evaluate transportation policies using agents that perform realistic daily activities and trips. In this way, we can assess transportation policies in a manner that is convincing for policy-makers. Finally, we develop a model-based reinforcement learning algorithm that approximates MATSim dynamics with a Partially Observable Discrete Event Decision Process (PODEDP) and, with respect to other state-of-art policy optimization techniques, can scale on big transportation data and find optimal policies also on a city-scale.", "pages": "171528-171541", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Khaidem, Luckyson", "Luca, Massimiliano", "Yang, Fan", "Anand, Ankit", "Lepri, Bruno", "Dong, Wen"]}]["9422830", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2021.3077567", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Support vector machines", "Image segmentation", "Data models", "Feature extraction", "Training", "Mathematical model", "Image recognition", "Apple quality classification", "SVM", "DCGAN", "ResNet50"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "An Improved CNN-Based Apple Appearance Quality Classification Method With Small Samples", "url": "", "volume": "9", "year": "2021", "abstract": "Apple quality classification is an important means to refine apple sales market and promote apple sales. At present, most of classification methods based on a convolutional neural network (CNN) depend on the quantity of training samples to get good performance. But due to the lack of large-scale public apple appearance dataset, it is a big challenge to obtain high accuracy of apple appearance quality classification with small samples. Therefore, we propose an improved method based on CNN for apple appearance, quality classification with small samples. Firstly, support vector machine (SVM) is used for image segmentation to avoid the decrease of recognition accuracy caused by environmental noise. Secondly, the segmented image data are input into deep convolutional generative adversarial networks (DCGAN) model, which is used for data expansion. Thirdly, the improved ResNet50 (Imp-ResNet50) is proposed as follows: Replace the fully-connected layer with global average pooling layer; Add the dropout algorithm and batch normalization algorithm at the fully-connected layer; Replace the activation function ReLU with Swish. Through comparative experiments with 360 apple images, we verify the performance of the proposed method including the training image quality, the running time, and classification accuracy. The result shows that the proposed method can obtain high quality training samples and reduce the running time of the method effectively. At the same time, it can realize higher classification accuracy that is up to 96.5%, which is higher than the previous classification method.", "pages": "68054-68065", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Sun, Li", "Liang, Kaibo", "Song, Yanxing", "Wang, Yuzhi"]}]["7935490", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2017.2706019", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Cloud computing", "Resource management", "Measurement", "Monitoring", "Servers", "Time factors", "Schedules", "Network", "resource management", "big data", "turnaround time", "service management"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "Cloud Resource Management With Turnaround Time Driven Auto-Scaling", "url": "", "volume": "5", "year": "2017", "abstract": "Cloud resource management research and techniques have received relevant attention in the last years. In particular, recently numerous studies have focused on determining the relationship between server-side system information and performance experience for reducing resource wastage. However, the genuine experiences of clients cannot be readily understood only by using the collected server-side information. In this paper, a cloud resource management framework with two novel turnaround time driven auto-scaling mechanisms is proposed for ensuring the stability of service performance. In the first mechanism, turnaround time monitors are deployed in the client-side instead of the more traditional server-side, and the information collected outside the server is used for driving a dynamic auto-scaling operation. In the second mechanism, a schedule-based auto scaling preconfiguration maker is designed to test and identify the amount of resources required in the cloud. The reported experimental results demonstrate that using our original framework for cloud resource management, stable service quality can be ensured and, moreover, a certain amount of quality variation can be handled in order to allow the stability of the service performance to be increased.", "pages": "9831-9841", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Liu, Xiaolong", "Yuan, Shyan-Ming", "Luo, Guo-Heng", "Huang, Hao-Yu", "Bellavista, Paolo"]}]["8326701", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2018.2818682", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Feature extraction", "Genetic algorithms", "Optimization", "Genetics", "Focusing", "Data models", "Correlation", "Feature selection", "wrapper\u2212embedded method", "memetic framework", "genetic algorithm", "L\u00bd + L\u2082 regularization"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "A Hybrid Genetic Algorithm With Wrapper-Embedded Approaches for Feature Selection", "url": "", "volume": "6", "year": "2018", "abstract": "Feature selection is an important research area for big data analysis. In recent years, various feature selection approaches have been developed, which can be divided into four categories: filter, wrapper, embedded, and combined methods. In the combined category, many hybrid genetic approaches from evolutionary computations combine filter and wrapper measures of feature evaluation to implement a population-based global optimization with efficient local search. However, there are limitations to existing combined methods, such as the two-stage and inconsistent feature evaluation measures, difficulties in analyzing data with high feature interaction, and challenges in handling large-scale features and instances. Focusing on these three limitations, we proposed a hybrid genetic algorithm with wrapper-embedded feature approach for selection approach (HGAWE), which combines genetic algorithm (global search) with embedded regularization approaches (local search) together. We also proposed a novel chromosome representation (intron+exon) for global and local optimization procedures in HGAWE. Based on this \u201cintron+exon\u201d encoding, the regularization method can select the relevant features and construct the learning model simultaneously, and genetic operations aim to globally optimize the control parameters in the above non-convex regularization. We mention that any efficient regularization approach can serve as the embedded method in HGAWE, and a hybrid L1/2 + L2 regularization approach is investigated as an example in this paper. Empirical study of the HGAWE approach on some simulation data and five gene microarray data sets indicates that it outperforms the existing combined methods in terms of feature selection and classification accuracy.", "pages": "22863-22874", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Liu, Xiao-Ying", "Liang, Yong", "Wang, Sai", "Yang, Zi-Yi", "Ye, Han-Shuo"]}]["9262875", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2020.3039011", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Education", "Matrix decomposition", "Sparse matrices", "Predictive models", "Task analysis", "Prediction algorithms", "Biological system modeling", "Teacher characteristics", "course characteristics", "sparse evaluation matrix", "FCTR-LFM", "TOP-N recommendation", "teaching quality"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "Teaching Teacher Recommendation Method Based on Fuzzy Clustering and Latent Factor Model", "url": "", "volume": "8", "year": "2020", "abstract": "Colleges and universities attach great importance to the quality of undergraduate teaching. To virtually guarantee the course's teaching quality, the key lies in recommending suitable teachers for the course scientifically. It is a seemingly simple but very complicated problem. Moreover, with the development of colleges and universities, new courses are continually set up, and new teachers are introduced, which further complicates the problem. The problem has not been solved well for many years. Therefore, we propose a course teacher recommendation model (FCTR-LFM) based on fuzzy clustering and the latent factor model (LFM) to solve this problem. Firstly, under the guidance of pedagogy theories and methods, we conduct quantitative modeling for teachers and courses' relevant characteristics and combine the quantitative results with historical teaching scores to establish a large-scale sparse course teaching evaluation matrix as the recommendation dataset. Next, we adopt the improved fuzzy clustering model to realize teachers' automatic clustering according to their characteristics and use the teacher cluster to reconstruct the teaching evaluation matrix, significantly reducing the dataset's size and reducing the sparsity. Then, we used the improved LFM to predict the score items in the evaluation matrix, including the missing score items. Finally, the prediction evaluation scores are sorted according to the course, and the TOP-N recommendation of the course teachers is realized. The experimental results show that FCTR-LFM can realize the prediction and recommendation well using the optimized parameters. It effectively solves the problem that there is no scientific basis for recommending suitable teachers for the course for a long time.", "pages": "210868-210885", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Yao, Dunhong", "Deng, Xiaowu"]}]["9189762", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2020.3022891", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Quality of service", "Context modeling", "Predictive models", "Context-aware services", "Internet", "Organizations", "Software", "Service-oriented computing", "Web API", "quality of service prediction", "context aware", "deep factorization machine"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "Contexts Enhance Accuracy: On Modeling Context Aware Deep Factorization Machine for Web API QoS Prediction", "url": "", "volume": "8", "year": "2020", "abstract": "Service-oriented computing (SOC) promises a world of cooperating services loosely connected, constructing agile Web applications in heterogeneous environments conveniently. Web application interface (API) as an emerging technique attracts more and more enterprises and organizations to publish their deep computing functionalities and big data on the Internet, Web API has become the backbone to promote the development of SOC, thus forming the prosperous Web API economy. However, the number of available Web APIs on the Internet is massive and growing constantly, which causes the Web API overload problem. Quality of service (QoS) as an indicator is able to well differentiate the quality of Web APIs and has been widely applied for high quality Web API selection. Since testing QoS for massive Web APIs is resource-consuming, and the QoS performance depends on contextual information such as network and location, hence accurate QoS prediction has become very crucial for personalized Web API recommendation and high quality Web application construction. To address the above issue, this paper presents a context aware deep factorization machine model (CADFM for short) for accurate Web API QoS prediction. Specifically, we first carry out detailed data analysis using real-world QoS dataset and discover a positive relationship between QoS and contextual information, which motivates us to incorporate beneficial contexts for enhancing QoS prediction accuracy. Then, we treat QoS prediction as a regression problem and propose a context aware CADFM framework that integrates the contextual information via embedding technique. Particularly, we adopt MF and MLP for high-order and nonlinear interaction modeling, so as to learn the complex interaction between users and Web APIs accurately. Finally, the experimental results on real-world QoS dataset demonstrate that CADFM outperforms the classic and the state-of-the-art baselines, thereby generating the most accurate QoS predictions and increasing the revenue of Web APIs recommendation.", "pages": "165551-165569", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Shen, Limin", "Pan, Maosheng", "Liu, Linlin", "You, Dianlong", "Li, Feng", "Chen, Zhen"]}]["8819913", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2019.2938349", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Semantics", "Predictive models", "Training", "Testing", "Image reconstruction", "Feature extraction", "Learning systems", "Zero-shot image classification", "broad learning", "elastic net constraint", "enhanced feature", "enhanced attribute"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "Broad Attribute Prediction Model With Enhanced Attribute and Feature", "url": "", "volume": "7", "year": "2019", "abstract": "For the zero-shot image classification without intersection between training and testing sets, the high-quality representation of image attributes and features plays a key role to improve the classification performance. In order to overcome the limitations related to insufficient attribute and feature expression in zero-shot image classification, we propose a broad attribute prediction model with enhanced attribute and feature (EAF-BAP) based on broad learning and elastic net constraint. Firstly, the EAF-BAP enhances pre-defined attributes by elastic net constraint to obtain hybrid attributes, which effectively improves the finiteness of semantic attributes. Secondly, the enhanced features are constructed by broad learning to increase the discrimination ability of features in different classes. Meanwhile, the broad learning is employed to train multiple attribute classifiers synchronously, which is more efficient compared to traditional support vector machines. Finally, the similarity between predicted attributes and hybrid attributes in testing classes is calculated by Manhattan distance, which is further used to implement image classification. Experiments on both AwA and Shoes datasets show that the proposed EAF-BAP model is capable of improving the accuracy of zero-shot image classification efficiently.", "pages": "124606-124620", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Zhang, Jiarui", "Wang, Xuesong", "Cheng, Yuhu"]}]["8733786", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2019.2921880", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Streaming media", "Forward error correction", "Wireless communication", "Encoding", "Distortion", "Error correction codes", "Trajectory", "Panoramic video", "wireless video", "unequal error protection (UEP)", "equi-rectangular projection (ERP)", "region of interest (ROI)"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "Unequal Error Protection Aided Region of Interest Aware Wireless Panoramic Video", "url": "", "volume": "7", "year": "2019", "abstract": "Panoramic video with its flawless immersive tele-presence is considered to be the near-future video format of choice since they carry 360 degree coverage of the designated scenes. However, the viewers may focus their specific attention on perfectly lip-synchronized video as part of the panoramic video scene, hence only have a peripheral vision of the remaining parts of a frame. Therefore, it is intuitive to allocate stronger protection to the panoramic video region of interest. As a solution, we propose Region of Interest Aware Unequal Error Protection (ROI-UEP) for wireless transmission of high-efficiency video code (HEVC) sequences. Specifically, the ROI of a panoramic frame may be deemed to be within the 120\u00b0 angular range of the viewing center, which can be estimated from the viewing trajectory of a head mounted display. Then, the most appropriate unequal forward error correction (FEC) coding rates will be found for the ROI signals by minimizing the expected video distortion. Moreover, the so-called weighted peak signal-to-noise ratio (WPSNR) is proposed for evaluating the quality of the reconstructed panoramic video, where the weights of pixels are taken into account for calculating the distortion caused by the related pixels. Our simulation results show that the ROI based equal error protection (ROI-EEP) scheme substantially outperforms the EEP by a WPSNR of more than 10 dB, while the ROI-UEP scheme further improves its ROI-EEP counterpart by a WPSNR of 9.4 dB at a channel Eb/N0 of 6 dB.", "pages": "80262-80276", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Huo, Yongkai", "Wang, Xu", "Zhang, Peichang", "Jiang, Jianmin", "Hanzo, Lajos"]}]["9122518", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2020.3004274", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Optimization", "Companies", "Delays", "Scheduling", "Energy consumption", "Stakeholders", "Mathematical model", "Public transportation", "urban railway", "train scheduling", "heuristic algorithms", "operating cost", "passenger travel time"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "Metro Scheduling to Minimize Travel Time and Operating Cost Considering Spatial and Temporal Constraints on Passenger Boarding", "url": "", "volume": "8", "year": "2020", "abstract": "Passengers on metro platforms can board a train only when the train has surplus capacity and the dwell time is sufficient, while the latter condition is omitted in previous studies. Taking into account the impacts of train capacity and dwell time on passengers boarding, this study develops a model on optimizing metro timetable to reduce passenger travel time and metro operating cost, through regulating trains' inter-station run-time, dwell time and headway. The NSGA-II algorithm is employed to obtain the near-optimal Pareto Frontier of the proposed model. To address insufficient dwell time scheduled in the timetable, three operating strategies are proposed and compared: a. sticking to nominal timetable; b. extending dwell time only; c. extending dwell time and recovering delay as soon as possible by compressing train inter-station run-time. Case studies on real-life metro line prove that some passengers cannot board the train during peak hours due to insufficient dwell time. In this context, strategy a brings low-quality service because passengers are stranded at platform even though the train has surplus capacity. In contrast, more passengers can board the train with strategies b and c because dwell time is extended for passengers' boarding when train has surplus capacity. Compared to strategy b, strategy c reduces the average in-vehicle time of passengers by 2.5% through compressing inter-station run-time to recover the delay. The timetable optimized based on strategy c saves total travel time of passengers by 3.1% without increasing operating cost when compared to the practical timetable.", "pages": "114190-114210", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Zhou, Yuhe", "Bai, Yun", "Guo, Haiyang", "Li, Tang", "Qiu, Yu", "Zhang, Zhao"]}]["9079860", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2020.2990992", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Lenses", "Liquids", "Image reconstruction", "Holography", "Holographic optical components", "Optical imaging", "Substrates", "Liquid crystal on silicon", "holographic projection", "lens"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "Holographic Zoom System With Large Focal Depth Based on Adjustable Lens", "url": "", "volume": "8", "year": "2020", "abstract": "In this paper, we propose a holographic zoom system based on two adjustable lenses. Different from the traditional holographic system, a digital conical lens and a liquid lens are used as the zoomable lenses. The liquid lens with large effective image aperture is designed and produced by a 3D printer. By mechanically controlling the curvature of the liquid-liquid surface, the focal length of the liquid lens can be changed easily. Compared with the other lenses, the conical lens has a larger focal depth. By encoding the phase information of the conical lens on the liquid crystal on silicon, the focal length and focal depth of the conical lens can be adjusted easily. The liquid lens and conical lens cooperate with each other so as to realize the high quality of holographic zoom projection. With such a system, the size and depth of the reconstructed image can be changed easily according to the requirement. Experimental results verify the feasibility of the proposed system.", "pages": "85784-85792", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Wang, Di", "Liu, Chao", "Li, Nan-Nan", "Wang, Qiong-Hua"]}]["9767708", {"address": "", "articleno": "", "doi": "10.1109/JSTARS.2022.3171771", "issn": "2151-1535", "issue_date": "", "journal": "IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing", "keywords": ["Vegetation", "Point cloud compression", "Forestry", "Three-dimensional displays", "Benchmark testing", "Licenses", "Laser modes", "Airborne laser scanning (ALS) point clouds", "model-driven", "tree crown", "tree instances", "tree localization"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "A Crown Guess and Selection Framework for Individual Tree Detection From ALS Point Clouds", "url": "", "volume": "15", "year": "2022", "abstract": "Individual tree detection from airborne laser scanning (ALS) point clouds is the basis for forestry inventory and further applications. In the past decade, many methods have been developed to localize tree instances in ALS point clouds. These methods rely on empirical rules and field measurements that may change from plot to plot. Besides, most existing methods cannot consider multiple clues (e.g., shape priors and neighboring trees) under the same framework, which makes them not flexible and extensible. In this letter, we devise a new point-based and model-driven framework named \u201ccrown guess and selection\u201d. This framework first generates crown candidates automatically, and then the qualities of candidates and their neighboring information are both considered. Finally, expected crowns are selected from candidates simultaneously. The proposed framework is tested and evaluated in a benchmark dataset. We also compare the new framework with several existing methods, and it turns out that the proposed framework outperforms others in terms of model flexibility and detection accuracy.", "pages": "3533-3538", "note": "", "ISSN": "2151-1535", "publicationtype": "article", "author": ["Wang, Pu", "Cao, Di", "Xia, Shaobo", "Wang, Cheng"]}]["9873922", {"address": "", "articleno": "", "doi": "10.1109/JSTARS.2022.3203508", "issn": "2151-1535", "issue_date": "", "journal": "IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing", "keywords": ["Remote sensing", "Clouds", "Optical imaging", "Optical sensors", "Convolutional neural networks", "Radar polarimetry", "Image reconstruction", "Cloud removal", "data fusion", "deep learning", "optical", "remote sensing", "synthetic aperture radar (SAR)"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "Cloud Removal Based on SAR-Optical Remote Sensing Data Fusion via a Two-Flow Network", "url": "", "volume": "15", "year": "2022", "abstract": "Optical remote sensing imagery plays an important role in observing the Earth's surface today. However, it is not easy to obtain complete multitemporal optical remote sensing images because of the cloud cover, how reconstructing cloud-free optical images has become a big challenge task in recent years. Inspired by the remote sensing fusion methods based on the convolutional neural network model, we propose a two-flow network to remove clouds from optical images. In the proposed method, synthetic aperture radar images are used as auxiliary data to guide optical image reconstruction, which is not influenced by cloud cover. In addition, a novel loss function called content loss is introduced to improve image quality. The ablation experiment of the loss function also proves that content loss is indeed effective. To be more in line with a real situation, the network is trained, tested, and validated on the SEN12MS-CR dataset, which is a global real cloud-removal dataset. The experimental results show that the proposed method is better than other state-of-the-art methods in many indicators (RMSE, SSIM, SAM, and PSNR).", "pages": "7677-7686", "note": "", "ISSN": "2151-1535", "publicationtype": "article", "author": ["Mao, Ruihan", "Li, Hua", "Ren, Gaofeng", "Yin, Zhangcai"]}]["9095336", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2020.2995478", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Rail transportation", "Base stations", "Long Term Evolution", "Feature extraction", "Maintenance engineering", "Time series analysis", "Prediction methods", "Time series forecasting", "deep learning", "LTE-R", "communication quality prediction"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "Temporal-Spatial Collaborative Prediction for LTE-R Communication Quality Based on Deep Learning", "url": "", "volume": "8", "year": "2020", "abstract": "In recent years, long term evolution for railway (LTE-R) has been a promising technology to meet the growing demand for railway wireless communication. To realize the active maintenance of LTE-R base station, it is of great significance to precisely predict the communication quality (CQ) of LTE-R base station. Given that the existing LTE CQ prediction methods can not support the active maintenance of LTE-R base station. Furthermore, the LTE-R base station has its unique characteristics in time relationship and regional impact, one of the most challenging problems is to effectively integrate the temporal and spatial information to improve the effect of CQ prediction. To solve the above problems, we choose daily evolved radio access bearer (E-RAB) abnormal release ratio as the CQ indicator, and propose a new deep learning-based CQ prediction approach for LTE-R. Considering the influence of adjacent base stations, this method conducts temporal-spatial collaborative prediction on multivariate time series collected from the CQ data of these stations. First, to eliminate the negative effect of redundant variables, a new variable filter method based on max-relevance, and min-redundancy (MRMR) criterion and binary particle swarm optimization (BPSO) is proposed to select a variable set from the CQ data of related base stations. Second, a new recurrent convolutional neural network (RCNN) model with a self-attention mechanism is proposed to extract temporal-spatial features from the selected variable set. With these features, we build a collaborative prediction model for CQ prediction. Experimental results on real-world LTE-R CQ datasets demonstrate the superiority of the proposed method in CQ prediction.", "pages": "94817-94832", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Qu, Jiantao", "Liu, Feng", "Ma, Yuxiang", "Fan, Jiaming"]}]["9777962", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2022.3176373", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Shafts", "Indexes", "Costs", "Rocks", "Excavation", "Safety", "Fuel processing industries", "Particle swarm optimization (PSO)", "analytic hierarchy process (AHP)", "fuzzy mathematics", "shaft blasting", "result assessment"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "Intelligent Quality Evaluation System for Vertical Shaft Blasting and its Application", "url": "", "volume": "10", "year": "2022", "abstract": "Blasting quality is a key factor in determining the productivity and total cost of the shaft blasting excavation construction, so it is of great engineering and theoretical importance to evaluate blasting quality rationally. The existing evaluation methods rely more on previous experience and the knowledge level of technicians, which are more subjective and cannot be judged by quantitative or unified standards, so the evaluation results have limitations. This paper proposes the Analytic Hierarchy Process (AHP) based on Particle Swarm Optimization (PSO) to obtain the weights of each index for evaluating the blasting quality of shafts, then combine expert knowledge, field engineering experience and statistical data for a comprehensive analysis to determine the quantitative interval of blasting quality evaluation index levels and construct a blasting quality evaluation index system, which makes the evaluation indexes more accurate and more in line with reality. The PSO-AHP combined with fuzzy comprehensive evaluation technique has constructed a blasting quality evaluation matrix more in line with the engineering reality and established a shaft blasting quality evaluation model adapted to different geological conditions. Finally, the established blasting quality evaluation model is combined with computer programming and artificial intelligence technology to develop a visualized shaft blasting quality intelligent evaluation system, which meets the practical needs of front-line operators in the field to evaluate the blasting quality objectively and reasonably, and achieves the accuracy, objectivity and intelligence of shaft blasting quality evaluation.", "pages": "61175-61191", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Ma, Xinmin", "Chen, Zhenyu", "Chen, Pan", "Jin, Yuan", "Wang, Yue", "Yang, Liyun", "Zhang, Zhaoran"]}]["9254004", {"address": "", "articleno": "", "doi": "10.1109/JSTARS.2020.3036802", "issn": "2151-1535", "issue_date": "", "journal": "IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing", "keywords": ["Wetlands", "Remote sensing", "Earth", "Monitoring", "Artificial satellites", "Biodiversity", "Synthetic aperture radar", "Big data", "Canada", "Google Earth Engine", "Landsat", "remote sensing (RS)", "wetlands"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "Evaluation of the Landsat-Based Canadian Wetland Inventory Map Using Multiple Sources: Challenges of Large-Scale Wetland Classification Using Remote Sensing", "url": "", "volume": "14", "year": "2021", "abstract": "The first Canadian wetland inventory (CWI) map, which was based on Landsat data, was produced in 2019 using the Google Earth Engine (GEE) big data processing platform. The proposed GEE-based method to create the preliminary CWI map proved to be a cost, time, and computationally efficient approach. Although the initial effort to produce the CWI map was valuable with a 71% overall accuracy (OA), there were several inevitable limitations (e.g., low-quality samples for the training and validation of the map). Therefore, it was important to comprehensively investigate those limitations and develop effective solutions to improve the accuracy of the Landsat-based CWI (L-CWI) map. Over the past year, the L-CWI map was shared with several governmental, academic, environmental nonprofit, and industrial organizations. Subsequently, valuable feedback was received on the accuracy of this product by comparing it with various in situ data, photo-interpreted reference samples, land cover/land use maps, and high-resolution aerial images. It was generally observed that the accuracy of the L-CWI map was lower relative to the other available products. For example, the average OA in four Canadian provinces using in situ data was 60%. Moreover, including reliable in situ data, using an object-based classification method, and adding more optical and synthetic aperture radar datasets were identified as the main practical solutions to improve the CWI map in the future. Finally, limitations and solutions discussed in this study are applicable to any large-scale wetland mapping using remote sensing methods, especially to CWI generation using optical satellite data in GEE.", "pages": "32-52", "note": "", "ISSN": "2151-1535", "publicationtype": "article", "author": ["Amani, Meisam", "Brisco, Brian", "Mahdavi, Sahel", "Ghorbanian, Arsalan", "Moghimi, Armin", "DeLancey, Evan", "Merchant, Michael", "Jahncke, Raymond", "Fedorchuk, Lee", "Mui, Amy", "Fisette, Thierry", "Kakooei, Mohammad", "Ahmadi, Seyed", "Leblon, Brigitte", "LaRocque, Armand"]}]["8626088", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2019.2895327", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Supply chains", "Blockchain", "Risk management", "Authentication", "Contracts", "Blockchain", "big production enterprise", "supply chain", "endogenous risk management"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "Big Production Enterprise Supply Chain Endogenous Risk Management Based on Blockchain", "url": "", "volume": "7", "year": "2019", "abstract": "In view of the influence of information's \u201cincompleteness\u201d and \u201casymmetry\u201d to supply chain operation efficiency, we make big production enterprise as the object and apply blockchain to its supply chain endogenous risk management, to research the specific operation mechanism and application value. In the operation process of big production enterprise supply chain, because of the information's asymmetry, the fraud problem will produce among the business subjects; blockchain is a decentralized distributed accounting and data storage technology, and with blockchain technology, we can resolve the business subjects' fraud problem and can provide more accurate decision information basis for each business section, and realize group decision. This paper has described the system structure and intelligent contract operation mechanism under consensus authentication of blockchain applying in big production enterprise supply chain and analyzed by the case. In view of the limitation of classical blockchain technology applying in big production enterprise supply chain, we constructed the corresponding blockchain data storage mechanism and data access mechanism. Analyzed the economic value of this paper researching from the aspects of response speed, supply accuracy, cooperation integrity, business interaction economic cost, supply quality, and supply price. This paper research will provide ideas and model structure for developing supply chain area's blockchain system and will promote the application research development of blockchain in specific area.", "pages": "15310-15319", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Fu, Yonggui", "Zhu, Jianming"]}]["8538878", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2018.2881535", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Diseases", "Water heating", "Medical diagnostic imaging", "Knowledge based systems", "Electronic mail", "Software algorithms", "Cause", "location", "characteristics", "conditions of diseases", "syndrome differentiation and treatment", "algorithm", "traditional Chinese medicine"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "Syndrome Differentiation and Treatment Algorithm Model in Traditional Chinese Medicine Based on Disease Cause, Location, Characteristics and Conditions", "url": "", "volume": "6", "year": "2018", "abstract": "Traditional Chinese medicine (TCM) is based on a unique disease diagnosis and treatment system that has been developed over the last 2,300 years. In the TCM, \u201csyndrome differentiation and treatment\u201d(SDAT) is a core method for doctors to deal with diseases. This diagnostic and therapeutic technique that infer the occurrence and the development of diseases by observing symptoms as a whole, not only has its own uniqueness but also has been recognized by the public in oriented medical fields for its clinical efficacy. With recent developments in computer science, the Internet, big data, and artificial intelligence, a study based on the SDAT algorithm has aroused much attention. This paper encompasses three stages spanning 30 years to accomplish the following: 1) the TCM data and the modern SDAT system were collated and summarized based on 35,706 reference data on the TCM, starting from the syndrome differentiation of four aspects, such as the cause, location, characteristics, and conditions of the disease (CLCC), we constructed a quantitative model of the TCM SDAT regarding the CLCC of the disease, collected the symptom information on the diagnosed subject, and transferred them to the SDAT assistant algorithm for calculation and analysis, to determine the CLCC, Based on the therapy recommended by the differentiation results in the knowledge base and the prescription and traditional Chinese medicines recommended by the therapy, any stage of all diseases could determine a syndrome type by differentiating the CLCC, we constructed the basic SDAT algorithm integrating theory, method, prescription, and medicine and realized the calculability in the TCM diagnosis and treatment process; 2) based on the SDAT algorithm, we developed the TCM doctor's workstation software and introduced it to more than 80 TCM institutions in Sichuan province, China, we collated a large-scale trove of samples of the TCM data platform that was established with more than 2.9 million TCM electronic medical records (EMRs) and reference data, and had the compliance tested and algorithm verified on the 9,300 EMRs of the common diseases in the TCM; and 3) based on the dimension reduction and degree elevation optimization of the technology with a directed graph to the basic algorithm, the algorithm complexity was reduced and the accuracy of the algorithm was improved. It was demonstrated that the coincidence rate of the basic model was 80.47% and the basic coincidence rate was 96.19%. After optimizing the basic algorithm (for example, for gastric abscess), the coincidence rate increased by 7.04%. The test results demonstrated the efficacy of the model study. This model realized a computable SDAT to specify and assist in the differentiation diagnosis and in the treatment processes of the TCM and improve the service quality of the TCM diagnosis and treatment.", "pages": "71801-71813", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Chen, Ju", "Yang, Dianxing", "Cao, Yue", "Ma, Yiyi", "Wen, Chuanbiao", "Huang, Xiwei", "Guo, Jinhong"]}]["9144301", {"address": "", "articleno": "", "doi": "10.1109/OJCOMS.2020.3010270", "issn": "2644-125X", "issue_date": "", "journal": "IEEE Open Journal of the Communications Society", "keywords": ["5G mobile communication", "Wireless communication", "Artificial intelligence", "Quality of service", "Market research", "Sensors", "5G", "6G", "artificial intelligence", "automation", "beyond 5G", "data rate", "massive connectivity", "virtual reality", "terahertz"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "6G Wireless Communication Systems: Applications, Requirements, Technologies, Challenges, and Research Directions", "url": "", "volume": "1", "year": "2020", "abstract": "The demand for wireless connectivity has grown exponentially over the last few decades. Fifth-generation (5G) communications, with far more features than fourth-generation communications, will soon be deployed worldwide. A new paradigm of wireless communication, the sixth-generation (6G) system, with the full support of artificial intelligence, is expected to be implemented between 2027 and 2030. Beyond 5G, some fundamental issues that need to be addressed are higher system capacity, higher data rate, lower latency, higher security, and improved quality of service (QoS) compared to the 5G system. This paper presents the vision of future 6G wireless communication and its network architecture. This article describes emerging technologies such as artificial intelligence, terahertz communications, wireless optical technology, free-space optical network, blockchain, three-dimensional networking, quantum communications, unmanned aerial vehicles, cell-free communications, integration of wireless information and energy transfer, integrated sensing and communication, integrated access-backhaul networks, dynamic network slicing, holographic beamforming, backscatter communication, intelligent reflecting surface, proactive caching, and big data analytics that can assist the 6G architecture development in guaranteeing the QoS. Besides, expected applications with 6G communication requirements and possible technologies are presented. We also describe potential challenges and research directions for achieving this goal.", "pages": "957-975", "note": "", "ISSN": "2644-125X", "publicationtype": "article", "author": ["Chowdhury, Mostafa", "Shahjalal, Md.", "Ahmed, Shakil", "Jang, Yeong"]}]["8979339", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2020.2971269", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Databases", "Training", "Generators", "Measurement", "Neural networks", "Generative adversarial networks", "Gallium nitride", "CGAN", "sketch", "quantitative evaluation metrics", "RSS"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "Progressive RSS Data Augmenter With Conditional Adversarial Networks", "url": "", "volume": "8", "year": "2020", "abstract": "Accuracies of most fingerprinting approaches for WiFi-based indoor localization applications are affected by the qualities of fingerprint databases, which are time-consuming and labor-intensive. Recently, many methods have been proposed to reduce the localization accuracy reliance on the qualities of the established fingerprint databases. However, studies on establishing fingerprint databases are relatively rare under the condition of sparse reference points. In this paper, we propose a novel data augmenter based on the adversarial networks to build fingerprint databases with sparse reference points. Additionally, two conditions of these networks are designed to generate data effectively and stably, which are 0-1 sketch and Gaussian sketch. Based on the networks, we design two augmenters with different cyclic training strategies to evaluate the augmenting effects comparatively. Meanwhile, five quantitative evaluation metrics of the augmenters are proposed from two perspectives of the artificial experiences and the data features, and some of them are also used as the gradient penalties for generators. Finally, experiments corresponding to these metrics and localization accuracies demonstrate that the data augmenter with the 0-1 sketch adversarial network is more efficient, effective and stable totally.", "pages": "26975-26983", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Chen, Liangfeng", "Zhang, Shutao", "Tan, Haibo", "Lv, Bo"]}]["9430113", {"address": "", "articleno": "", "doi": "10.23919/JSEE.2021.000032", "issn": "1004-4132", "issue_date": "", "journal": "Journal of Systems Engineering and Electronics", "keywords": ["Software", "Software algorithms", "Prediction algorithms", "Feature extraction", "Partitioning algorithms", "Correlation", "Clustering algorithms", "software defect prediction (SDP)", "feature selection", "cluster"], "month": "April", "number": "2", "numpages": "", "publisher": "", "title": "RFC: A feature selection algorithm for software defect prediction", "url": "", "volume": "32", "year": "2021", "abstract": "Software defect prediction (SDP) is used to perform the statistical analysis of historical defect data to find out the distribution rule of historical defects, so as to effectively predictdefects in the new software. However, there are redundant and irrelevant features in the software defect datasets affecting the performance of defect predictors. In order to identify and remove the redundant and irrelevant features in software defectdatasets, we propose Relief F-based clustering (RFC), a cluster-based feature selection algorithm. Then, the correlation between features is calculated based on the symmetric uncertainty. According to the correlation degree, RFC partitions features into kclusters based on the k-medoids algorithm, and finally selects the representative features from each cluster to form the final feature subset. In the experiments, we compare the proposed RFC with classical feature selection algorithms on nine National Aeronautics and Space Administration (NASA) software defectprediction datasets in terms of area under curve (AUC) and F-value. The experimental results show that RFC can effectively improve the performance of SDP.", "pages": "389-398", "note": "", "ISSN": "1004-4132", "publicationtype": "article", "author": ["Xiaolong, Xu", "Wen, Chen", "Xinheng, Wang"]}]["8675275", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2019.2907809", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Mechatronics", "Automation", "Industries", "Production", "Cloud computing", "Wireless communication", "Sensors", "Advanced mechatronics systems", "Wisdom as a Service (WaaS)", "Information as a Service (InaaS)", "Industry 4.0 (4IR)", "cyber-physical domains", "cloud and edge/fog platforms", "Automation of Everything (AoE)"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "Transformation to Advanced Mechatronics Systems Within New Industrial Revolution: A Novel Framework in Automation of Everything (AoE)", "url": "", "volume": "7", "year": "2019", "abstract": "The recent advances in cyber-physical domains, cloud, cloudlet, and edge platforms along with the evolving Artificial Intelligence (AI) techniques, big data analytics, and cutting-edge wireless communication technologies within the Industry 4.0 (4IR) are urging mechatronics designers, practitioners, and educators to further review the ways in which mechatronics systems are perceived, designed, manufactured, and advanced. Within this scope, we introduce the service-oriented cyber-physical advanced mechatronics systems (AMSs) along with current and future challenges. The objective in AMSs is to create remarkably intelligent autonomous products by 1) forging effective sensing, self-learning, Wisdom as a Service (WaaS), Information as a Service (InaaS), precise decision making, and actuation using effective location-independent monitoring, control and management techniques with products and 2) maintaining a competitive edge through better product performances via immediate and continuous learning, while the products are being used by customers and are being produced in factories within the cycle of Automation of Everything (AoE). With the advanced wireless communication techniques and improved battery technologies, the AMSs are capable of getting independent and working with other massive AMSs to construct robust, customizable, energy-efficient, autonomous, intelligent, and immersive platforms. In this regard, rather than providing technological details, this paper implements philosophical insights into 1) how mechatronics systems are being transformed into AMSs; 2) how robust AMSs can be developed by both exploiting the wisdom created within cyber-physical smart domains in the edge and cloud platforms and incorporating all the stakeholders with diverse objectives into all phases of the product life-cycle; and 3) what essential common features AMSs should acquire to increase the efficacy of products and prolong their product life. Against this background, an AMS development framework is proposed in order to contextualize all the necessary phases of AMS development and direct all stakeholders to rivet high-quality products and services within AoE.", "pages": "41395-41415", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Kuru, Kaya", "Yetgin, Halil"]}]["9254003", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2020.3037164", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Content distribution networks", "Computational modeling", "Systems architecture", "Computer architecture", "Quality of service", "Bandwidth", "Real-time systems", "Real time CDN cache update", "Internet of Things (IoT)", "channel heterogeneous CDN (CHCDN)", "network QoS", "distributed computing"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "An Efficient Content Distribution Network Architecture Using Heterogeneous Channels", "url": "", "volume": "8", "year": "2020", "abstract": "With the popularization and development of the IoT(Internet of Things), more and more data needs to be transmitted over the Internet, which leads to the deterioration of network quality. The CDN (Content Distribution Network) technology is an important theoretical model to ensure network QoS (Quality of Service). To improve the QoS, we extend current CDN system to the hierarchical CDN system, that traditional CDN system is just a special case of hierarchical CDN system. In the traditional CDN system and the hierarchical CDN system, by analyzing the bandwidth between subsystems, we found the inter-system bandwidth is the bottleneck that impedes CDN system expansion. To address this problem, a new kind of distributed system architecture is proposed in this paper. This new architecture uses broadcast channels to distribute broadcast type data and still using bidirection uni-cast channels for other type of data, so we call the new architecture as CHCDN (Channel Heterogeneous CDN) in this paper. The new architecture is analyzed and compared with the traditional CDN system architecture and hierarchical CDN system architecture. Moreover, the experimental simulation result has shown that the CHCDN system features better in real-time cache updating and features with higher data transmission efficiency than the hierarchical CDN system, which indicating that the new architecture has great potential for being widely used in distributed computing.", "pages": "210988-211006", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Wen, Yuqiang", "Chen, Yuqiang", "Shao, Meng-Liang", "Guo, Jian-Lan", "Liu, Jia"]}]["9481217", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2021.3096739", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Task analysis", "Layout", "Organizations", "Artificial intelligence", "Data mining", "Standards organizations", "Text analysis", "Artificial Intelligence (AI)", "information extraction", "key field extraction", "named entity recognition (NER)", "template-free invoice processing", "unstructured data"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "Multi-Layout Unstructured Invoice Documents Dataset: A Dataset for Template-Free Invoice Processing and Its Evaluation Using AI Approaches", "url": "", "volume": "9", "year": "2021", "abstract": "The daily transaction of an organization generates a vast amount of unstructured data such as invoices and purchase orders. Managing and analyzing unstructured data is a costly affair for the organization. Unstructured data has a wealth of hidden valuable information. Extracting such insights automatically from unstructured documents can significantly increase the productivity of an organization. Thus, there is a huge demand to develop a tool that can automate the extraction of key fields from unstructured documents. Researchers have used different approaches for extracting key fields, but the lack of annotated and high-quality datasets is the biggest challenge. Existing work in this area has used standard and custom datasets for extracting key fields from unstructured documents. Still, the existing datasets face some serious challenges, such as poor-quality images, domain-related datasets, and a lack of data validation approaches to evaluate data quality. This work highlights the detailed process flow for end-to-end key fields extraction from unstructured documents. This work presents a high-quality, multi-layout unstructured invoice documents dataset assessed with a statistical data validation technique. The proposed multi-layout unstructured invoice documents dataset is highly diverse in invoice layouts to generalize key field extraction tasks for unstructured documents. The proposed multi-layout unstructured invoice documents dataset is evaluated with various feature extraction techniques such as Glove, Word2Vec, FastText, and AI approaches such as BiLSTM and BiLSTM-CRF. We also present the comparative analysis of feature extraction techniques and AI approaches on the proposed multi-layout unstructured invoice document dataset. We attained the best results with BiLSTM-CRF model.", "pages": "101494-101512", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Baviskar, Dipali", "Ahirrao, Swati", "Kotecha, Ketan"]}]["6894550", {"address": "", "articleno": "", "doi": "10.1109/TETC.2014.2356505", "issn": "2168-6750", "issue_date": "", "journal": "IEEE Transactions on Emerging Topics in Computing", "keywords": ["Collaboration", "Social network services", "Vectors", "Educational institutions", "Recommender systems", "Context", "Data models", "Most valuable collaborators", "academic recommendation", "big scholarly data", "random walk", "link prediction"], "month": "Sep.", "number": "3", "numpages": "", "publisher": "", "title": "MVCWalker: Random Walk-Based Most Valuable Collaborators Recommendation Exploiting Academic Factors", "url": "", "volume": "2", "year": "2014", "abstract": "In academia, scientific research achievements would be inconceivable without academic collaboration and cooperation among researchers. Previous studies have discovered that productive scholars tend to be more collaborative. However, it is often difficult and time-consuming for researchers to find the most valuable collaborators (MVCs) from a large volume of big scholarly data. In this paper, we present MVCWalker, an innovative method that stands on the shoulders of random walk with restart (RWR) for recommending collaborators to scholars. Three academic factors, i.e., coauthor order, latest collaboration time, and times of collaboration, are exploited to define link importance in academic social networks for the sake of recommendation quality. We conducted extensive experiments on DBLP data set in order to compare MVCWalker to the basic model of RWR and the common neighbor-based model friend of friends in various aspects, including, e.g., the impact of critical parameters and academic factors. Our experimental results show that incorporating the above factors into random walk model can improve the precision, recall rate, and coverage rate of academic collaboration recommendations.", "pages": "364-375", "note": "", "ISSN": "2168-6750", "publicationtype": "article", "author": ["Xia, Feng", "Chen, Zhen", "Wang, Wei", "Li, Jing", "Yang, Laurence"]}]["8662571", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2019.2903720", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Cloud computing", "Internet of Things", "Logic gates", "Monitoring", "Production", "Agricultural products", "Management system", "modern agriculture", "big data", "cloud computing", "Internet of Things (IoT)"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "Internet of Things Monitoring System of Modern Eco-Agriculture Based on Cloud Computing", "url": "", "volume": "7", "year": "2019", "abstract": "In order to enhance the efficiency and safety of production and management of modern agriculture in China, problems, such as the quality and safety of agricultural products and the pollution of the environment from agricultural activities, should be unraveled. Based on the new generation of information technology (IT), an integrated framework system platform incorporating the Internet of Things (IoT), cloud computing, data mining, and other technologies is investigated and a new proposal for its application in the field of modern agriculture is offered. The experimental framework and simulation design suggest that the basic functions of the monitoring system of the IoT for agriculture can be realized. In addition, the innovation derived from integrating different technologies plays an important role in reducing the cost of system development and ensuring its reliability as well as security.", "pages": "37050-37058", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Liu, Shubo", "Guo, Liqing", "Webb, Heather", "Ya, Xiao", "Chang, Xiao"]}]["9145574", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2020.3010281", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["5G mobile communication", "Device-to-device communication", "Quality of service", "Path planning", "Base stations", "Unmanned aerial vehicles", "Particle swarm optimization", "UAV", "5G networks", "QoS", "coverage path planning", "device-to-device communication", "particle swarm optimization"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "UAV Path Planning With QoS Constraint in Device-to-Device 5G Networks Using Particle Swarm Optimization", "url": "", "volume": "8", "year": "2020", "abstract": "Unmanned Ariel Vehicles (UAVs) are tasked to collect sensory data which are typically retrieved after the flight. The emergence of 5G and Device-to-Device (D2D) networks enables high speed network communication for UAVs to transfer data during a flight mission instead of post flight. UAVs are now subject to constraints of area coverage, battery capacity and network quality of service, making their path planning more challenging. In this paper, we formulate the issue as a combinatorial optimization problem which minimizes the flight cost of multiple UAVs covering the entire area. We show this problem is NP-hard, therefore we propose a Particle Swarm Optimization heuristic along with path encoding and local search techniques to solve the problem. Our numerical simulations demonstrate the effectiveness of the approach and how the size of the area and D2D link affect the number of UAVs needed and their flight time.", "pages": "137884-137896", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Shi, Lin", "Xu, Shoukun"]}]["8794523", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2019.2934153", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Cloud computing", "Peer-to-peer computing", "Entropy", "Quality of service", "Clustering algorithms", "Computational modeling", "Face", "Cloud service", "selection method", "user preference clustering", "cloud model", "trust"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "A Cloud Service Selection Method Based on Trust and User Preference Clustering", "url": "", "volume": "7", "year": "2019", "abstract": "In order to achieve universal and personalized cloud service choices in a social network environment, we propose a cloud service selection method based on trust and user preference clustering. The method performs a comprehensive trust evaluation, which is used to evaluate and select cloud services. Meanwhile, we propose an improved condensed hierarchical clustering method based on user preference similarity to further improve the accuracy of recommendation trust. A cloud model-based approach is used to measure similarities between users, and then a hierarchical clustering method is used to divide users into different domains according to user similarity. The final recommendation trust will be obtained, which includes the intra-domain recommendation trust and the extra-domain recommendation trust. The comprehensive trust of cloud services, which consists of direct trust and recommended trust. Simulation experiments verify the accuracy and superiority of the clustering algorithm. Experimental results show that the cloud service selection method improves the transaction success rate and enables users to select more satisfactory cloud services.", "pages": "110279-110292", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Wang, Yubiao", "Wen, Junhao", "Zhou, Wei", "Tao, Bamei", "Wu, Quanwang", "Tao, Zhiyong"]}]["9131799", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2020.3006733", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["5G mobile communication", "Base stations", "Optimization", "Investment", "Quality of service", "Performance evaluation", "5G network optimization", "computational intelligence", "cost benefit analysis", "heterogeneous network", "algorithm design and analysis", "decision making"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "A Coverage-Based Location Approach and Performance Evaluation for the Deployment of 5G Base Stations", "url": "", "volume": "8", "year": "2020", "abstract": "It has become a strategic consensus of the international community for accelerating the deployment of 5G network. This paper presents an approach for the deployment of 5G base stations under the considerations of both the cost and the signal coverage. We formulate an optimization problem for the site selection and location of 5G macro and micro base stations. An implementation procedure is proposed in the paper for the cooperative operation and deployment scheme of optimizing the location of 5G heterogeneous base stations, which aims to optimally reduce the setup cost and strengthen the signal coverage while deploying 5G base stations. A series of numerical examples are solved in the paper to demonstrate the proposed approach, and a cost-benefit analysis is also conducted to determine the optimal deployment plan for the number of macro and micro base stations. In the conclusion, a balanced executable solution is presented to make the signal strength of all demand points in the studied 5G network reach the strongest under the budget constraint.", "pages": "123320-123333", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Wang, Chia-Hung", "Lee, Chia-Jung", "Wu, Xiaojing"]}]["9511451", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2021.3104154", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Image reconstruction", "Absorption", "Mathematical model", "Detectors", "Biological tissues", "TV", "Photoacoustic imaging", "Photoacoustic imaging", "alternating direction method of multipliers", "total variation", "image reconstruction"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "Research on ADMM Reconstruction Algorithm of Photoacoustic Tomography With Limited Sampling Data", "url": "", "volume": "9", "year": "2021", "abstract": "Photoacoustic imaging is a new non-destructive biomedical imaging method. When limited independent data is available, the restoration of the initial pressure rise distribution is often an ill-posed problem. In this paper, based on the study of photoacoustic effects, the sparse prior information of photoacoustic images is integrated into the reconstruction process by using the compressed sensing (CS) theory and the L2 norm optimization technique, combining the augmented Langrange weighting of the alternating direction method of multipliers (ADMM) with the total variation (TV) minimization problem, and the reconstruction artifacts are effectively eliminated. The simulation data from the real numerical model show that compared with the common time reversal algorithm, interpolation algorithm and truncated back projection algorithm, the total variational regularization method based on ADMM can effectively improve the quality of reconstructed images under the condition of limited viewing angles and incomplete projection data.", "pages": "113631-113641", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Wang, Zhao-Xu", "Wang, Hao-Quan", "Ren, Shi-Lei"]}]["8528834", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2018.2880198", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Logistics", "Feature extraction", "Cancer", "Gene expression", "Optimization", "Genetic algorithms", "Machine learning", "Robust logistic regression", "feature selection", "Lq regularization", "genetic algorithm"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "Robust Sparse Logistic Regression With the $L_{q}$ ( $0 < \\text{q} < 1$ ) Regularization for Feature Selection Using Gene Expression Data", "url": "", "volume": "6", "year": "2018", "abstract": "Microarray technology is a popular technique that has been extensively applied in cancer diagnosis. Many studies have used high-dimensional microarray data to identify informative features to classify the types of cancer, yet numerous irrelevant features that exist in microarray data may introduce the noise and decrease classification accuracy. Regularization techniques are common methods for feature selection, which can be used to reduce irrelevant features and avoid overfitting. In recent years, different regularization methods have been proposed. Theoretically, the Lq (0 <; q <; 1) type penalty function with the lower value of q would acquire better sparse solutions. In addition, the loss function in most regression models is based on least-squares minimization. However, the least-square method is sensitive to noise and has poor robustness, especially when the error has a heavy-tailed distribution. It is well known that the least absolute deviation regression is the most common method for the robust regression, which can overcome the big noise problem. In general, there is a high level of noise in microarray data, which deter the development of microarray technology. To solve the above-mentioned problems, we propose a robust logistic regression based on the Lq (0 <; q <; 1) regularization approach, which is a feasible and effective approach for feature selection in microarray classification. The Lq (0 <; q <; 1) regularization leads to a non-convex optimization problem that is difficult to be solved. In this paper, we utilize a genetic algorithm based on the global search strategy to obtain an optimal solution.", "pages": "68586-68595", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Yang, Zi-Yi", "Liang, Yong", "Zhang, Hui", "Chai, Hua", "Zhang, Bowen", "Peng, Cheng"]}]["9262939", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2020.3038858", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Medical services", "Industries", "Middleware", "Cloud computing", "Medical diagnostic imaging", "Reliability", "Information and communication technology", "Health 40", "Industry 40", "healthcare systems", "service-oriented middleware", "service integration", "health cloud", "health fog", "Internet of Health Things", "medical cyber-physical systems", "COVID-19"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "Health 4.0: On the Way to Realizing the Healthcare of the Future", "url": "", "volume": "8", "year": "2020", "abstract": "Health 4.0 establishes a new promising vision for the healthcare industry. It creatively integrates and employ innovative technologies such as the Internet of Health Things (IoHT), medical Cyber-Physical Systems (medical CPS), health cloud, health fog, big data analytics, machine learning, blockchain, and smart algorithms. The goal is to deliver improved, value-added and cost-effective healthcare services to patients and enhance the effectiveness and efficiency or the healthcare industry. Health 4.0 (adapted from the Industry 4.0 principles) changes the healthcare business model to enhance the interactions across the healthcare clients (the patients), stakeholders, infrastructure, and value chain. This effectively will improve the quality, flexibility, productivity, cost-effectiveness, and reliability of healthcare services in addition to increasing patients\u2019 satisfaction. However, building and utilizing healthcare applications that follow the Health 4.0 concept is a non-trivial and complex endeavor. In addition, advanced potential applications based on Health 4.0 capabilities are not yet being investigated. In this paper we define the main objectives of Health 4.0 and discuss advanced potential Health 4.0 applications. To have a clear understanding of these applications, we categorize them in 4 groups based on the primary beneficiary of these applications. Thus we have patient targeted applications, applications supporting healthcare professionals, resource management applications and high-level healthcare systems management applications. In addition, as we studied the different applications, we realized that these is a certain collection of services that these most of them need regardless of their goals or business context. Services supporting data collection and transfer, security and privacy, reliable operations are some examples. As a result we propose creating a service-oriented middleware framework to offers the common services to the applications developers and facilitate the integration of different services to build applications under the Health 4.0 umbrella.", "pages": "211189-211210", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Al-Jaroodi, Jameela", "Mohamed, Nader", "Abukhousa, Eman"]}]["9797696", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2022.3183605", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Vehicular ad hoc networks", "Cloud computing", "Edge computing", "Big Data", "Global Positioning System", "Quality of service", "Protocols", "Vehicular ad hoc network", "social Internet of Things", "Internet of Vehicle", "big data", "fog computing"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "A Review on VANET Research: Perspective of Recent Emerging Technologies", "url": "", "volume": "10", "year": "2022", "abstract": "Recent technology has modeled VANET (vehicular adhoc network) communication well in terms of privileges to derive vehicular communication technologically to save time, energy, and money. Due to the increase in powerful technology in modern times, VANETs play a vital role in uplifting daily concerns across vehicles and vehicular identities. Hence, to tune VANETs to become compatible with traditional technologies and increase demand, VANETs require upgrading. The severity and frequency of unwanted occurrences have become a considerable concern for our day-to-day lives relating to vehicular position. Thus, verily updated methodologies or working procedures are needed for the future VANET interplay to eradicate such problems occurring through vehicular identities. This article outlines in technology related to VANETS, future developments, and coping issues by deriving comprehensive frameworks, workflow patterns, upgrading procedures including big data, fog computing, SDN (software defined networking), and SIoT (social Internet of Things). This article provides a high-level overview of a complete VANET upgrade solution to address future problem management issues under a range of acceptable scientific themes, indicators, and combinations.", "pages": "65760-65783", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Mahi, Md.", "Chaki, Sudipto", "Ahmed, Shamim", "Biswas, Milon", "Kaiser, M.", "Islam, Mohammad", "Sookhak, Mehdi", "Barros, Alistair", "Whaiduzzaman, Md"]}]["9383229", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2021.3068094", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Training", "Convolution", "Generators", "Image reconstruction", "Image recognition", "Generative adversarial networks", "Diseases", "Image generation", "style transferring", "attribute registration", "image registration", "improved CycleGAN"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "ArCycleGAN: Improved CycleGAN for Style Transferring of Fruit Images", "url": "", "volume": "9", "year": "2021", "abstract": "CycleGAN can realize image translation and style transferring among unpaired images. However, it will easily generate inappropriate image results when the number and shapes of objects in the style offering image and the source image are greatly different. The paper proposed an improved network, named arCycleGAN, which introduced the mechanism of attribute registration into CycleGAN to solve the problem. The arCycleGAN can transfer the freshness styles from the style offering images to the unpaired input source images. The generated target images will have the freshness attributes of the style offering images, while maintaining the shapes and key features of the input source images. The realization of mechanism of attribute registration consists of three modules. The first module is attribute recognition module, which can identify and label the attributes of objects in images. The second module is image pre-screening module, which selects appropriate image subset as screened training set from raw image set according to the attributes of the input source images. The third module is similarity matching module, which matches the images in screened training set based on the similarity. The generator and discriminator in the new network are similar to that in the CycleGAN network. Experimental results demonstrate the effectiveness and better performance of the arCycleGAN. Compared with the CycleGAN, the new network can generate more convincing images. It can generate the target images of similar quality based on a smaller training set and less training time than the original CycleGAN. For generating images of similar quality, the number of images in the required training set can be reduced by 50%, while training time is reduced by 5.8%.", "pages": "46776-46787", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Chen, Hongqian", "Guan, Mengxi", "Li, Hui"]}]["8304571", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2018.2810295", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Software", "Data visualization", "Analytical models", "Visualization", "Labeling", "Computer bugs", "Sentiment analysis", "Open-source software community", "project development model", "visual analysis", "issue commit", "software management"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "Exploring the Characteristics of Issue-Related Behaviors in GitHub Using Visualization Techniques", "url": "", "volume": "6", "year": "2018", "abstract": "Feedback from software users, such as bug reports, is vital in the management of software projects. In GitHub, the feedback is typically expressed as new issues. Through filing issue reports, users may help identify and fix bugs, document software code, and enhance software quality via feature requests. In this paper, we aim at investigating some characteristics of issues to facilitate issue management and software management. We investigate the important degrees of behaviors that are related to issues in popular projects to assess the importance of issues in GitHub and analyze the effectiveness of issue labeling for issue handling. Then, we explore the patterns of issue commits over time in popular projects based on visual analysis and obtain the following results: we find that the behaviors that are related to issues play important roles in the GitHub. We also find that the time distribution of issue commits follows a three-period development model, which approximately corresponds to the project life cycle. These results may provide a new knowledge about issues that can help managers manage and allocate project resources more effectively and even reduce software failures.", "pages": "24003-24015", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Liao, Zhifang", "He, Dayu", "Chen, Zhijie", "Fan, Xiaoping", "Zhang, Yan", "Liu, Shengzong"]}]["8642321", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2019.2899402", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Delays", "Data centers", "Service computing", "Mobile handsets", "Propagation delay", "Relays", "Routing", "Internet of things", "big data", "queuing delay", "service aggregation"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "A Queuing Delay Utilization Scheme for On-Path Service Aggregation in Services-Oriented Computing Networks", "url": "", "volume": "7", "year": "2019", "abstract": "In services-oriented computing networks, packets in the process of routing to a data center must wait for a sufficient amount of data before service aggregation to reduce the network transmission load. However, packets must be uploaded to the data center as soon as possible to reduce delay. With the exponential growth in the number of IoT connected devices, the wait time for packets is longer at routers due to massive amounts of data, which causes a large queuing delay. If this queuing time can be utilized for service aggregation in a service-oriented computing network, the network performance will be substantially improved. Therefore, a queuing delay utilization scheme for on-path service aggregation (SAQD) is proposed in this paper. This scheme has the following innovations: 1) SAQD fully utilizes the queuing delay of packets for service aggregation, which can effectively reduce the transmission volume and communication overhead. Based on the proposed service aggregation algorithm, packets are divided into forwarding packets and aggregating packets, and the service aggregation of aggregating packets is completed by utilizing the transmission time of forwarding packets to ensure that the transmission volume and communication overhead are effectively reduced without additional latency. 2) SAQD can effectively alleviate the traffic pressure of the data center and balance the workload of routers. By the service aggregation and intranet cache of routers, some requests for the data center can be handled by routers, which reduces the traffic pressure of the data center, especially in the peak period. Compared with conventional schemes, the experimental results demonstrate that SAQD reduces the workload of the data center by 55.8%-66.26% and provides users with a better quality of experience by reducing the request response delay by 31.33%~51.41%.", "pages": "23816-23833", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Huang, Mingfeng", "Liu, Wei", "Wang, Tian", "Song, Houbing", "Li, Xiong", "Liu, Anfeng"]}]["9042874", {"address": "", "articleno": "", "doi": "10.1109/TIP.2020.2980130", "issn": "1941-0042", "issue_date": "", "journal": "IEEE Transactions on Image Processing", "keywords": ["Three-dimensional displays", "Solid modeling", "Gallium nitride", "Generative adversarial networks", "Licenses", "Cameras", "Task analysis", "Novel view synthesis", "generative adversarial nets", "perspective transformation", "generative model", "vehicle pose"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "Pose-Based View Synthesis for Vehicles: A Perspective Aware Method", "url": "", "volume": "29", "year": "2020", "abstract": "In this paper, we focus on the problem of novel view synthesis for vehicles. Some previous works solve the problem of novel view synthesis in a controlled 3D environment by exploiting additional 3D details (i.e., camera viewpoints and underlying 3D models). However, in real scenarios, the 3D details are difficult to obtain. In this case, we find that introducing vehicle pose to represent the views of vehicles is an alternative paradigm to solve the lack of 3D details. In novel view synthesis, preserving local details is one of the most challenging problems. To address this problem, we propose a perspective-aware generative model (PAGM). We are motivated by the prior that vehicles are made of quadrilateral planes. Preserving these rigid planes during image generation ensures that image details are kept. To this end, a classic image transformation method is leveraged, i.e., perspective transformation. In our GAN-based system, the perspective transformation is applied to the encoder feature maps, and the resulting maps are regarded as new conditions for the decoder. This strategy preserves the quadrilateral planes all the way through the network, thus shuttling the texture details from the input image to the generated image. In the experiments, we show that PAGM can generate high-quality vehicle images with fine details. Quantitatively, our method is superior to several competing approaches employing either GAN or the perspective transformation. Code is available at: https://github.com/ilvkai/view-synthesis-for-vehicles", "pages": "5163-5174", "note": "", "ISSN": "1941-0042", "publicationtype": "article", "author": ["Lv, Kai", "Sheng, Hao", "Xiong, Zhang", "Li, Wei", "Zheng, Liang"]}]["8936960", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2019.2960868", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Pattern matching", "Topology", "Partitioning algorithms", "Clustering algorithms", "Complexity theory", "Computational modeling", "Search problems", "Graph computing", "graph partitioning", "pattern match", "workload mining"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "Query-Sensitive Graph Partitioner for Pattern Matching Applications", "url": "", "volume": "7", "year": "2019", "abstract": "Searching and mining in large graphs is critical to a variety of applications, at the core of which is the pattern matching activity. The scalable processing of large graphs requires careful distribution of graphs across clusters. Graph partitioning is the technique that divides a big graph into several non-overlapped subgraphs and assigns each subgraph to a compute node. Traditional workload agnostic partitioners aim to minimize the number of inter-partition edges using only graph topology, which, however, may not obtain the best solution if the workload exhibits skew. Some workload-aware partitioners choose to mine information from a specific workload and use it to minimize the number of inter-partition traversals during execution; however, their methods are not suitable for pattern matching applications. In this work, we propose a query-sensitive graph partitioner that aims to improve existing partitioning for a given pattern matching workload. The partitioner takes any initial partitioning as a starting point and iteratively adjusts it by exchanging chosen clusters across partitions, heuristically reducing the probability of inter-partition traversals. We determine a few implementation-irrelative factors that may increase the traversal probability of an edge and quantify them into a calculable indicator with information from query patterns and graph topology. Then, we propose an efficient algorithm to calculate the indicator and implement a graph repartitioner by combining the indicator with a greedy cluster-exchanging mechanism. Finally, we generate a large heterogeneous labeled graph with real-world data crawled from the Netease Music website and evaluate the partitioning quality of our repartitioner with a few meaningful query patterns of common topologies including line, loop and branching. Compared with a hash-based partitioning, our system can reduce the inter-partition traversals by at least 70%. Compared with the state-of-the-art graph partitioner Metis, our repartitioner can reduce the inter-partition traversals by at least 50%.", "pages": "184668-184675", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Lu, Li", "Hua, Bei"]}]["9525388", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2021.3109216", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Sea measurements", "Water quality", "Ecosystems", "Aquaculture", "Forecasting", "Feature extraction", "Europe", "Sustainable coastal management", "sustainable aquaculture", "remote sensing", "artificial intelligence", "machine learning", "water quality", "blue economy"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "Sustainable Marine Ecosystems: Deep Learning for Water Quality Assessment and Forecasting", "url": "", "volume": "9", "year": "2021", "abstract": "An appropriate management of the available resources within oceans and coastal regions is vital to guarantee their sustainable development and preservation, where water quality is a key element. Leveraging on a combination of cross-disciplinary technologies including Remote Sensing (RS), Internet of Things (IoT), Big Data, cloud computing, and Artificial Intelligence (AI) is essential to attain this aim. In this paper, we review methodologies and technologies for water quality assessment that contribute to a sustainable management of marine environments. Specifically, we focus on Deep Leaning (DL) strategies for water quality estimation and forecasting. The analyzed literature is classified depending on the type of task, scenario and architecture. Moreover, several applications including coastal management and aquaculture are surveyed. Finally, we discuss open issues still to be addressed and potential research lines where transfer learning, knowledge fusion, reinforcement learning, edge computing and decision-making policies are expected to be the main involved agents.", "pages": "121344-121365", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Gamb\u00edn, \u00c1ngel", "Angelats, Eduard", "Gonz\u00e1lez, Jes\u00fas", "Miozzo, Marco", "Dini, Paolo"]}]["8704713", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2019.2914429", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Software", "Market research", "Data mining", "Application programming interfaces", "Mobile communication", "Mobile applications", "Technological innovation", "Non-functional requirements (NFRs)", "quality requirements", "iOS", "Latent Dirichlet allocation (LDA)", "Stack Overflow"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "Toward Empirically Investigating Non-Functional Requirements of iOS Developers on Stack Overflow", "url": "", "volume": "7", "year": "2019", "abstract": "Context: Mobile application developers are getting more concerned due to the importance of quality requirements or non-functional requirements (NFR) in software quality. Developers around the globe are actively asking a question(s) and sharing solutions to the problems related to software development on Stack Overflow (SO). The knowledge shared by developers on SO contains useful information related to software development such as feature requests (functional/non-functional), code snippets, reporting bugs or sentiments. Extracting the NFRs shared by iOS developers on programming Q&A website SO has become a challenge and a less researched area. Objective: To identify and understand the real problems, needs, trends, and the critical NFRs or quality requirements discussed on Stack Overflow related to iOS mobile application development. Method: We extracted and used only the iOS posts data of SO. We applied the well-known statistical topical model Latent Dirichlet Allocation (LDA) to identify the main topics in iOS posts on SO. Then, we labeled the extracted topics with quality requirements or NFRs by using the wordlists to assess the trend, evolution, hot and unresolved NFRs in all iOS discussions. Results: Our findings revealed that the highly frequent topics the iOS developers discussed are related to usability, reliability, and functionality followed by efficiency. Interestingly, the most problematic areas unresolved are also usability, reliability, and functionality though followed by portability. Besides, the evolution trend of each of the six different quality requirements or NFRs over time is depicted through comprehensive visualization. Conclusion: Our first empirical investigation on approximately 1.5 million iOS posts and comments of SO gives insight on comprehending the NFRs in iOS application development through the lens of real-world practitioners.", "pages": "61145-61169", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Ahmad, Arshad", "Feng, Chong", "Li, Kan", "Asim, Syed", "Sun, Tingting"]}]["8701671", {"address": "", "articleno": "", "doi": "10.1109/TBME.2019.2908345", "issn": "1558-2531", "issue_date": "", "journal": "IEEE Transactions on Biomedical Engineering", "keywords": ["Microscopy", "Lasers", "Signal to noise ratio", "Laser noise", "Biology", "Tumors", "Image restoration", "image processing", "laser scanning microscopy"], "month": "Jan", "number": "1", "numpages": "", "publisher": "", "title": "Image-Based Artefact Removal in Laser Scanning Microscopy", "url": "", "volume": "67", "year": "2020", "abstract": "Recent developments in laser scanning microscopy have greatly extended its applicability in cancer imaging beyond the visualization of complex biology, and opened up the possibility of quantitative analysis of inherently dynamic biological processes. However, the physics of image acquisition intrinsically means that image quality is subject to a tradeoff between a number of imaging parameters, including resolution, signal-to-noise ratio, and acquisition speed. We address the problem of geometric distortion, in particular, jaggedness artefacts that are caused by the variable motion of the microscope laser, by using a combination of image processing techniques. Image restoration methods have already shown great potential for post-acquisition image analysis. The performance of our proposed image restoration technique was first quantitatively evaluated using phantom data with different textures, and then qualitatively assessed using in vivo biological imaging data. In both cases, the presented method, comprising a combination of image registration and filtering, is demonstrated to have substantial improvement over state-of-the-art microscopy acquisition methods.", "pages": "79-87", "note": "", "ISSN": "1558-2531", "publicationtype": "article", "author": ["Papie\u017c, Bart\u0142omiej", "Markelc, Bo\u0161tjan", "Brown, Graham", "Muschel, Ruth", "Brady, Sir", "Schnabel, Julia"]}]["7314854", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2015.2496959", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Bandwidth allocation", "Multimedia communication", "Quality of service", "Real-time systems", "Memory management", "Memory bandwidth", "multimedia application", "memory access", "quality of service", "soft real-time", "Memory bandwidth", "multimedia application", "memory access", "quality of service", "soft real-time"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "An Adaptive Framework for Improving Quality of Service in Industrial Systems", "url": "", "volume": "3", "year": "2015", "abstract": "Limited memory bandwidth is considered as the major bottleneck in multimedia cloud computing for more and more virtual machines (VMs) of multimedia processing requiring high memory bandwidth simultaneously. Moreover, contending memory bandwidth among parallel running VMs leads to poor quality of service (QoS) of the multimedia applications, missing the deadlines of these soft real-time multimedia applications. In this paper, we present an adaptive framework, Service Maximization Optimization (SMO), which is designed to improve the QoS of the soft real-time multimedia applications in multimedia cloud computing. The framework consists of an automatic detection mechanism and an adaptive memory bandwidth control mechanism. With the automatic detection mechanism, the critical section to the multimedia application performance in the VMs is detected. Then, our adaptive memory bandwidth control mechanism adjusts the memory access rates of all the parallel running VMs to protect the QoS of the soft real-time multimedia applications. From the case studies with real-world multimedia applications, our SMO significantly improves the QoS of the soft real-time multimedia applications with a negligible penalty on system throughput.", "pages": "2129-2139", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Jia, Gangyong", "Han, Guangjie", "Zhang, Daqiang", "Liu, Li", "Shu, Lei"]}]["9606887", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2021.3126790", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Cotton", "Fabrics", "Clustering algorithms", "Yarn", "Fuzzy logic", "Manuals", "Warehousing", "Bale management system", "cotton lay-down", "cotton warehousing", "neutrosophic clustering"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "Cotton Warehousing Improvement for Bale Management System Based on Neutrosophic Classifier", "url": "", "volume": "9", "year": "2021", "abstract": "One of the big factors affecting yarn quality is the cotton mix. There is always a considerable variation in the fiber characteristics from one bale to another, even within the same lot. This variation will result in the yarn quality difference, which leads to many fabric defects if the bales are mixed in an uncontrolled manner. The bale management system is based on the categorization of cotton bales according to their fiber quality characteristics. It includes the measurement of the fiber characteristics concerning each bale by using a High Volume Instrument (HVI). The separation of bales into categories for cotton lay-down to achieve balanced bale mixes must be based on a robust clustering algorithm. This paper discusses the utilization of the neutrosophic classifier, for the first time, to categorize the cotton in the warehouse. Although the traditional categorizing method using fuzzy logic came out with some satisfying results, it was missing the way of excluding the outlier\u2019s data points (off-quality bales) which can affect the fabric quality. Neutrosophic classifier deals with cotton bale\u2019s data type by excluding some bale data points that affect the fabric quality through falsity and indeterminacy membership functions to increase the accuracy of the bale management system. Our proposed method has been tested on mill cotton data. The results have been compared with the results of the traditional fuzzy logic algorithms and revealed higher accuracy.", "pages": "159413-159420", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Halawa, Waleed", "Darwish, Saad", "Elzoghabi, Adel"]}]["8974217", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2020.2970223", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Feature extraction", "Image retrieval", "Hash functions", "Binary codes", "Sun", "Quantization (signal)", "Training data", "Deep hashing", "fine-grained images", "feature switch layer", "image retrieval"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "A Novel Semantics-Preserving Hashing for Fine-Grained Image Retrieval", "url": "", "volume": "8", "year": "2020", "abstract": "With the advent of the era of big data, the storage and retrieval of data have become a research hotspot. Hashing methods that transform high-dimensional data into compact binary codes have received increasing attention. Recently, with the successful application of convolutional neural networks in computer vision, deep hashing methods utilize an end-to-end framework to learn feature representations and hash codes mutually, which achieve better retrieval performance than conventional hashing methods. However, deep hashing methods still face some challenges in image retrieval. Firstly, most existing deep hashing methods preserve similarity between original data space and hash coding space using loss functions with high time complexity, which cannot get a win-win situation in time and accuracy. Secondly, few existing deep hashing methods are designed for fine-grained image retrieval, which is necessary in practice. In this study, we propose a novel semantics-preserving hashing method which solves the above problems. We add a hash layer before the classification layer as a feature switch layer to guide the classification. At the same time, we replace the complicated loss with the simple classification loss, combining with quantization loss and bit balance loss to generate high-quality hash codes. Besides, we incorporate feature extractor designed for fine-grained image classification into our network for better representation learning. The results on three widely-used fine-grained image datasets show that our method is superior to other state-of-the-art image retrieval methods.", "pages": "26199-26209", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Sun, Han", "Fan, Yejia", "Shen, Jiaquan", "Liu, Ningzhong", "Liang, Dong", "Zhou, Huiyu"]}]["8995472", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2020.2973411", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Image segmentation", "Optimization", "Entropy", "Particle swarm optimization", "Indexes", "Convergence", "Genetic algorithms", "Meta-heuristic optimization", "parallel multi-verse optimizer", "multilevel image segmentation", "minimum cross entropy thresholding"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "A Parallel Multi-Verse Optimizer for Application in Multilevel Image Segmentation", "url": "", "volume": "8", "year": "2020", "abstract": "Multi-version optimizer (MVO) inspired by the multi-verse theory is a new optimization algorithm for challenging multiple parameter optimization problems in the real world. In this paper, a novel parallel multi-verse optimizer (PMVO) with the communication strategy is proposed. The parallel mechanism is implemented to randomly divide the initial solutions into several groups, and share the information of different groups after each fixed iteration. This can significantly promote the cooperation individual of MVO algorithm, and reduce the deficiencies that the original MVO is premature convergence, search stagnation and easily trap into local optimal search space. To confirm the performance of the proposed scheme, the PMVO algorithm was compared with the other well-known optimization algorithms, such as gray wolf optimizer (GWO), particle swarm optimization (PSO), multi-version optimizer (MVO), and parallel particle swarm optimization (PPSO) under CEC2013 test suite. The experimental results prove that the PMVO is superior to the other compared algorithms. In addition, PMVO is also applied to solve complex multilevel image segmentation problems based on minimum cross entropy thresholding. The application results appear that the proposed PMVO algorithm can achieve higher quality image segmentation compared to other similar algorithms.", "pages": "32018-32030", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Wang, Xiaopeng", "Pan, Jeng-Shyang", "Chu, Shu-Chuan"]}]["8528424", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2018.2879648", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Heuristic algorithms", "Partitioning algorithms", "Bridges", "Steady-state", "Convergence", "Social network services", "Complex networks", "Complex network", "diffusion", "information dynamics", "overlapping community detection"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "Overlapping Community Detection Based on Information Dynamics", "url": "", "volume": "6", "year": "2018", "abstract": "Identifying overlapping communities is essential for analyzing network structures, exploring the interactions of groups, studying network functions, and obtaining insight into the dynamics of networks. Many algorithms have been proposed for detecting overlapping communities but identifying the intrinsic communities is still a non-trivial problem because of the difficulties with parameter tuning, user bias criteria, and the lack of ground truth information. In this paper, we propose a new model called OCDID (Overlapping Community Detection based on Information Dynamics) to uncover the overlapping communities, which treats the network as a dynamical system that allows an individual to communicate and share information with its neighbors. The information flow in the network is controlled by the underlying topology structure (e.g., the community structure), and the community structure is also reflected by the information dynamics. Overlapping nodes act as bridges between multiple communities and the information from multiple communities flows through these nodes. Thus, the overlapping nodes can be identified by analyzing the information flow among communities. In addition, we use the monotone convergence theorem to confirm the convergence of our model. Experiments based on synthetic and real-world networks demonstrate that in most cases, our proposed approach is superior to other representative algorithms in terms of the quality of overlapping community detection.", "pages": "70919-70934", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Sun, Zejun", "Wang, Bin", "Sheng, Jinfang", "Yu, Zhongjing", "Shao, Junming"]}]["9056814", {"address": "", "articleno": "", "doi": "10.1109/TMI.2020.2984557", "issn": "1558-254X", "issue_date": "", "journal": "IEEE Transactions on Medical Imaging", "keywords": ["Image reconstruction", "Photonics", "Fluorescence", "Inverse problems", "Surface reconstruction", "In vivo", "Surface morphology", "Fluorescence tomography", "machine learning", "brain"], "month": "Oct", "number": "10", "numpages": "", "publisher": "", "title": "K-Nearest Neighbor Based Locally Connected Network for Fast Morphological Reconstruction in Fluorescence Molecular Tomography", "url": "", "volume": "39", "year": "2020", "abstract": "Fluorescence molecular tomography (FMT) is a highly sensitive and noninvasive imaging modality for three-dimensional visualization of fluorescence probe distribution in small animals. However, the simplified photon propagation model and ill-posed inverse problem limit the improvement of FMT reconstruction. In this work, we proposed a novel K-nearest neighbor based locally connected (KNN-LC) network to improve the performance of morphological reconstruction in FMT. It directly builds the inverse process of photon transmission by learning the mapping relation between the surface photon intensity and the distribution of fluorescent source. KNN-LC network cascades a fully connected (FC) sub-network with a locally connected (LC) sub-network, where the FC part provides a coarse reconstruction result and LC part fine-tunes the morphological quality of reconstructed result. To assess the performance of our proposed network, we implemented both numerical simulation and in vivo studies. Furthermore, split Bregman-resolved total variation (SBRTV) regularization method and inverse problem simulation (IPS) method were utilized as baselines in all comparisons. The results demonstrated that KNN-LC network achieved accurate reconstruction in both source localization and morphology recovery in a short time. This promoted the in vivo application of FMT for visualizing the distribution of biomarkers inside biological tissue.", "pages": "3019-3028", "note": "", "ISSN": "1558-254X", "publicationtype": "article", "author": ["Meng, Hui", "Gao, Yuan", "Yang, Xin", "Wang, Kun", "Tian, Jie"]}]["9178818", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2020.3019806", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Routing", "Load modeling", "Vehicle routing", "Search problems", "Approximation algorithms", "Microsoft Windows", "Indexes", "School bus routing problem", "mixed load", "pickup and delivery problem with time windows", "record to record travel"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "A Metaheuristic Algorithm for Routing School Buses With Mixed Load", "url": "", "volume": "8", "year": "2020", "abstract": "Designing a system to solve school bus routing problems (SBRP), especially in a large school district, is very complex and expensive. One of the challenges resides in designing routes for the school buses when they have mixed loads, where each bus transports students for one or more schools at the same time to save the total number of buses required. This article aims to explore whether the algorithm originally developed for pickup and delivery problem with time windows (PDPTW) can be employed for solving mixed load SBRP. We present a PDPTW-based algorithm to address the mixed load SBRP focusing on minimizing the number of buses required. Our algorithm combines a record-to-record travel framework with three neighborhood operators, single paired insertion, swapping pairs between routes, and within route insertion, to improve solution iteratively. Results from implementing this algorithm show that our PDPTW-based algorithm is feasible for mixed load SBRP. Moreover, we found that guided strategy is better than random for permuting nodes that would be selected to relocate positions in a route or among routes. In addition, the results also show that the spatiotemporal connectivity index used in our algorithm can reduce the computation time needed for searching for solutions without affecting the quality of the solutions.", "pages": "158293-158305", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Hou, Yan-E", "Dang, Lanxue", "Dong, Weichuan", "Kong, Yunfeng"]}]["9108246", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2020.3000062", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Image reconstruction", "Ultrasonic imaging", "Imaging", "Transducers", "Image resolution", "Breast cancer", "Ultrasound computed tomography", "sound speed image", "combined regularization", "Tikhonov regularization", "total variation regularization"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "A Combined Regularization Method Using Prior Structural Information for Sound-Speed Image Reconstruction of Ultrasound Computed Tomography", "url": "", "volume": "8", "year": "2020", "abstract": "Ultrasound computed tomography (USCT) is a promising technique for breast imaging. It provides three modalities: echo image, sound speed image (SSI), and attenuation image. The echo image reveals structural details of the breast with high resolution but is not quantitative. The SSI is quantitative, but its resolution is generally lower than that of the echo image. The SSI reconstruction is an ill-posed problem, thus the Tikhonov or total variation (TV) regularization methods are generally used. Tikhonov regularization is stable, but it tends to smooth the SSI and results in low resolution. TV regularization can preserve the sharp edges and provide higher resolution, but it may result in staircase artifacts. To combine the advantages of Tikhonov and TV regularizations, this article proposes a combined regularization method using prior structural information from the echo image. The proposed method mainly includes three steps. First, the echo image is reconstructed using the synthetic aperture (SA) technique and then segmented into breast and water regions. The segmentation result is used as prior structural information. Second, the USCT sound propagation forward model was built. Finally, with a penalty term according to the prior structural information, the combined regularization method is used to reconstruct the SSI. Both simulation and ex vivo experiments were conducted for evaluation. In the simulation, the proposed method has a low root-mean-square-error (13.78 m/s), a high correlation coefficient (0.812), a high structural similarity (0.755), and a low standard deviation of water sound speed (2.33 m/s). In the ex vivo experiment, the method has a low standard deviation of water sound speed (1.44 and 3.13 m/s for small and large objects, respectively), a low contrast to noise ratio (3.09 and 5.08), and a high Dice coefficient (0.92 and 0.97). Thus, the proposed combined regularization method using prior structural information can improve reconstruction quality.", "pages": "106832-106842", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Yao, Zisheng", "Fan, Shangchun", "Nakajima, Yoshikazu", "Qu, Xiaolei"]}]["9373305", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2021.3064867", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Software", "Testing", "Artificial intelligence", "Computer architecture", "Tools", "Production", "Software architecture", "DevOps", "CI/CD", "infrastructure as code", "testing", "artificial intelligence", "verification"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "Quality-Aware DevOps Research: Where Do We Stand?", "url": "", "volume": "9", "year": "2021", "abstract": "DevOps is an emerging paradigm that reduces the barriers between developers and operations teams to offer continuous fast delivery and enable quick responses to changing requirements within the software life cycle. A significant volume of activity has been carried out in recent years with the aim of coupling DevOps stages with tools and methods to improve the quality of the produced software and the underpinning delivery methodology. While the research community has produced a sustained effort by conducting numerous studies and innovative development tools to support quality analyses within DevOps, there is still a limited cohesion between the research themes in this domain and a shortage of surveys that holistically examine quality engineering work within DevOps. In this paper, we address the gap by comprehensively surveying existing efforts in this area, categorizing them according to the stage of the DevOps lifecycle to which they primarily contribute. The survey holistically spans across all the DevOps stages, identify research efforts to improve architectural design, modeling and infrastructure-as-code, continuous-integration/continuous-delivery (CI/CD), testing and verification, and runtime management. Our analysis also outlines possible directions for future work in quality-aware DevOps, looking in particular at AI for DevOps and DevOps for AI software.", "pages": "44476-44489", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Alnafessah, Ahmad", "Gias, Alim", "Wang, Runan", "Zhu, Lulai", "Casale, Giuliano", "Filieri, Antonio"]}]["8844722", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2019.2942449", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Image coding", "Watermarking", "Histograms", "Visualization", "Distortion", "Airplanes", "Image quality", "Complementary embedding strategy", "prediction error", "reversible watermarking"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "Novel Scheme of Reversible Watermarking With a Complementary Embedding Strategy", "url": "", "volume": "7", "year": "2019", "abstract": "In this paper, a new scheme of reversible watermarking is proposed using a complementary embedding strategy in the spatial domain. The proposed scheme consists of two stages: horizontal direction embedding and vertical direction embedding. A complementary embedding strategy is designed to increase the embedding capacity and decrease the distortion of the watermarked image in the vertical direction embedding. Specifically, in the horizontal direction, the proposed scheme embeds one secret data bit by increasing values of pixels in even rows and decreasing values of pixels in odd rows by one. In the vertical direction, it embeds another secret data bit by decreasing values of pixels in even rows and increasing values of pixels in odd rows by one. In addition, a histogram shrinkage technique is adopted to prevent overflow and underflow problems. Experimental results demonstrate that the proposed reversible watermarking scheme outperforms state-of-the-art methods in terms of both embedding capacity and watermarked image quality.", "pages": "136592-136603", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Xiong, Xiangguang"]}]["8732318", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2019.2921320", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Clustering algorithms", "Clustering methods", "Partitioning algorithms", "Data mining", "Heuristic algorithms", "Benchmark testing", "Convergence", "Clustering", "data mining", "initial cluster centers", "stellar spectra"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "A Novel Algorithm for Initial Cluster Center Selection", "url": "", "volume": "7", "year": "2019", "abstract": "As one of the most important techniques in data mining, clustering has always been highly concerned. Most clustering algorithms have encountered challenges, such as the difficulty of cluster centers selection, the artificial determination of the number of clusters K, low accuracy of clustering, and uneven clustering efficiency of different data sets. Considering the difficulty of cluster centers chosen, a new algorithm of fast selecting the initial cluster centers is proposed in this paper. Generally, cluster centers are those data points with higher density, smaller radius threshold and far away from each other, this method uses MNN (M nearest neighbors), density and distance to determine the initial cluster centers. First, the neighborhood radius r of each point is measured by MNN based on distance, and the average value of all r is marked as r\u0305; second, the densities \u03c1 of each point in the region within r\u0305 are calculated; and then, factor f is defined to describe the probability that points become cluster centers, based on which, the initial cluster centers are determined by the candidates with bigger f . In the end, the method proposed in this paper is tested by using 12 groups of typical benchmark data sets and applied in the stellar spectral data of LAMOST survey. The experiment results compared with the other six algorithms indicate that the initial cluster centers obtained by this method are of higher quality than that of the six algorithms. Meanwhile, the initial cluster centers of spectral data are of good agreement with the actual stellar classifications.", "pages": "74683-74693", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Li, Yating", "Cai, Jianghui", "Yang, Haifeng", "Zhang, Jifu", "Zhao, Xujun"]}]["9139220", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2020.3008644", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Cryptography", "Videos", "Streaming media", "Chaotic communication", "DNA", "Multimedia communication", "Video cryptography", "H265-HEVC", "DNA", "Mandelbrot sets", "IoMT", "Arnold chaotic map"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "A Novel Hybrid Cryptosystem for Secure Streaming of High Efficiency H.265 Compressed Videos in IoT Multimedia Applications", "url": "", "volume": "8", "year": "2020", "abstract": "In this modernistic age of innovative technologies like big data processing, cloud computing, and Internet of things, the utilization of multimedia information is growing daily. In contrast to other forms of multimedia, videos are extensively utilized and streamed over the Internet and communication networks in numerous Internet of Multimedia Things (IoMT) applications. Consequently, there is an immense necessity to achieve secure video transmission over modern communication networks due to the third-party exploitation and falsification of transmitted and stored digital multimedia data. The present methods for secure communication of multimedia content between clouds and mobile devices have constraints in terms of processing load, memory support, data size, and battery power. These methods are not the optimum solutions for large-sized multimedia content and are not appropriate for the restricted resources of mobile devices and clouds. The High-Efficiency Video Coding (HEVC) is the latest and modern video codec standard introduced for efficiently storing and streaming of high-resolution videos with suitable size and higher quality. In this paper, a novel hybrid cryptosystem combining DNA (Deoxyribonucleic Acid) sequences, Arnold chaotic map, and Mandelbrot sets is suggested for secure streaming of compressed HEVC streams. Firstly, the high-resolution videos are encoded using the H.265/HEVC codec to achieve efficient compression performance. Subsequently, the suggested Arnold chaotic map ciphering process is employed individually on three channels (Y, U, and V) of the compressed HEVC frame. Then, the DNA encoding sequences are established on the primary encrypted frames resulted from the previous chaotic ciphering process. After that, a modified Mandelbrot set-based conditional shift process is presented to effectively introduce confusion features on the Y, U, and V channels of the resulted ciphered frames. Massive simulation results and security analysis are performed to substantiate that the suggested HEVC cryptosystem reveals astonishing robustness and security accomplishment in contrast to the literature cryptosystems.", "pages": "128548-128573", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Alarifi, Abdulaziz", "Sankar, Syam", "Altameem, Torki", "Jithin, K.", "Amoon, Mohammed", "El-Shafai, Walid"]}]["9166510", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2020.3016303", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Monitoring", "Internet of Things", "Indexes", "Economics", "Sustainable development", "Soil", "Sensors", "Internet of Things Technology", "land resources", "sustainable utilization", "Internet of Things monitoring system"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "Internet of Things Technology in Monitoring System of Sustainable Use of Soil and Land Resources", "url": "", "volume": "8", "year": "2020", "abstract": "With the development of global economy, the sustainable use of land resources has become a key research topic in the current social and economic development. To do a good job in the sustainable use of land resources can solve such problems as waste of land resources, quality degradation, environmental damage, ecological imbalance and so on. How to realize the sustainable utilization of resources, the first thing to be solved is the sustainable monitoring of land resources. At present, many researches have been done in this field at home and abroad, but in conclusion, there are some problems, that is, the basic theory is not deep and the practical application is less. Therefore, this paper will study the application of the monitoring system of the sustainable use of soil and land resources under the support of the Internet of things big data technology, and establish a practical monitoring system of the sustainable use of land resources. The monitoring system in this paper makes full use of modern science and technology. On the basis of intelligent technology, combined with the actual situation of land resources in China, according to the practical problems, the construction principles of evaluation index system and index construction are established. In the aspect of monitoring system construction, we have greatly optimized the traditional construction method, which makes the system in this paper have better adaptability, scalability and accuracy. In order to further verify the reliability of the system, this paper carries out the performance test, accuracy test, and monitoring test in extremely harsh environment. Finally, the data show that the system can be applied to most of the land environment to monitor the sustainability of land resources.", "pages": "152932-152940", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Chen, Zhaoya", "Wang, Daobo", "Ma, Lijun", "Chen, Yaheng"]}]["8892516", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2019.2950355", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Web services", "Tagging", "Search engines", "Clustering algorithms", "Task analysis", "Feature extraction", "Web service", "WSDL documents clustering", "co-clustering", "tag recommendation"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "Exploiting User Tagging for Web Service Co-Clustering", "url": "", "volume": "7", "year": "2019", "abstract": "We propose a novel Web services clustering framework by considering the word distribution of WSDL documents and tags. Typically, tags are annotated to Web services by users for organization. In this paper, four strategies are proposed to integrate the tagging data and WSDL documents in the process of service clustering. Tagging data is inherently uncontrolled, ambiguous, and overly personalized. Two tag recommendation approaches are proposed to improve the tagging data quality and service clustering performance. Comprehensive experiments demonstrate the effectiveness of the proposed framework using a real-world dataset.", "pages": "168981-168993", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Liang, Tingting", "Chen, Yishan", "Gao, Wei", "Chen, Ming", "Zheng, Meilian", "Wu, Jian"]}]["9783176", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2022.3178434", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Clustering algorithms", "Wireless sensor networks", "Quality of service", "Energy consumption", "Routing protocols", "Throughput", "Voting", "Energy consumption", "large-scale wireless sensor networks", "optimal path", "QoS"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "Enhancing QoS and Residual Energy by Using of Grid-Size Clustering, K-Means, and TSP Algorithms With MDC in LEACH Protocol", "url": "", "volume": "10", "year": "2022", "abstract": "Some recent researches have shown that the energy consumption problem caused by data collection in a wireless sensor network (WSN) based on a static data collector is a main threat to the network lifetime. However, with the progress of the mobile terminal technology, the implementation of mobile data collectors (MDCs) has become more popular in large-scale WSNs, but it remains a big problem to improve the Quality of Service (QoS) criteria and minimize the energy consumption at the same time. However, most existing systems based on MDCs do not successfully strike a balance between routing energy consumption and QoS. In addition, most WSN protocols fail to maintain their impact when the network topology changes. Thus, for a dynamic WSN, it is important to support an intelligent MDC to continue data propagation despite the inevitable changes in the WSN topology. Considering all the above challenges, we propose a new intelligent MDC based on the traveling salesman problem (TSP) to determine the optimal path traveled by the MDC for energy efficiency and latency. Specifically, our proposed Mobile Data Collectors-Traveling Salesman Problem-Low Energy Adaptive Clustering Hierarchy-K-Means (MDC-TSP-LEACH-K) protocol uses K-Means and Grid clustering algorithm to decrease energy consumption in the cluster head (CH) election phase. Additionally, MDC is utilized as an intermediate between CH and the sink to further enhance the QoS of WSNs, to reduce delays while collecting data, and improve the transmission phase of the LEACH protocol.", "pages": "58199-58211", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Gantassi, Rahma", "Masood, Zaki", "Choi, Yonghoon"]}]["9138396", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2020.3008514", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Collaboration", "Motion pictures", "Sparse matrices", "Recommender systems", "Object recognition", "Prediction algorithms", "Task analysis", "Association-rule mining", "collaborative filtering", "data sparsity", "recommendation system", "unfavourable items"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "Rule-Based Effective Collaborative Recommendation Using Unfavorable Preference", "url": "", "volume": "8", "year": "2020", "abstract": "The biggest challenge in collaborative filtering recommendation system research is data sparsity; it mainly occurs as user rates very few items from widely available options. Data Imputation techniques address the data sparsity problem by filling the missing values and then predicting the likeliness of the user. Most of the existing imputation systems assign high ratings to the items or incorporate additional information to enhance the performance of collaborative filtering recommendations. This paper proposes an association rule-based imputation method (RUBLE) to improve the top-N prediction performance of the collaborative filtering recommendation. The proposed method identifies the unfavorable items of each user using the association rule mining technique and imputes them with low values. The proposed method not only addresses the sparsity problem but also provides a better quality of recommendation by eliminating the unfavorable items in top-N predictions. Existing collaborative methods can quickly adapt to the proposed method as it is method agnostic. The experimental results show that the proposed method enhances the accuracy of the traditional recommender methods by two times on average and significantly outperforms existing imputation based approaches.", "pages": "128116-128123", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Suganeshwari, G.", "Ibrahim, S."]}]["8600738", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2018.2890432", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Security", "Cloud computing", "Quality of service", "Internet of Things", "Measurement", "Computer architecture", "Big Data", "Cloud-based IoT", "cloud service trust assessment", "security and reputation assessment", "trustworthy cloud service selection"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "Enhancing Cloud-Based IoT Security Through Trustworthy Cloud Service: An Integration of Security and Reputation Approach", "url": "", "volume": "7", "year": "2019", "abstract": "The Internet of Things (IoT) provides a new paradigm for the development of heterogeneous and distributed systems, and it has increasingly become a ubiquitous computing service platform. However, due to the lack of sufficient computing and storage resources dedicated to the processing and storage of huge volumes of the IoT data, it tends to adopt a cloud-based architecture to address the issues of resource constraints. Hence, a series of challenging security and trust concerns have arisen in the cloud-based IoT context. To this end, a novel trust assessment framework for the security and reputation of cloud services is proposed. This framework enables the trust evaluation of cloud services in order to ensure the security of the cloud-based IoT context via integrating security- and reputation-based trust assessment methods. The security-based trust assessment method employs the cloud-specific security metrics to evaluate the security of a cloud service. Furthermore, the feedback ratings on the quality of cloud service are exploited in the reputation-based trust assessment method in order to evaluate the reputation of a cloud service. The experiments conducted using a synthesized dataset of security metrics and a real-world web service dataset show that our proposed trust assessment framework can efficiently and effectively assess the trustworthiness of a cloud service while outperforming other trust assessment methods.", "pages": "9368-9383", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Li, Xiang", "Wang, Qixu", "Lan, Xiao", "Chen, Xingshu", "Zhang, Ning", "Chen, Dajiang"]}]["9878068", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2022.3204696", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Perturbation methods", "Optimization", "Deep learning", "Feature extraction", "Training data", "Security", "Data models", "Adversarial machine learning", "Deep learning", "adversarial samples", "black-box attack", "transferability", "intermediate layer", "attention-guided"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "Intermediate-Layer Transferable Adversarial Attack With DNN Attention", "url": "", "volume": "10", "year": "2022", "abstract": "The widespread deployment of deep learning models in practice necessitates an assessment of their vulnerability, particularly in security-sensitive areas. As a result, transfer-based adversarial attacks have elicited increasing interest in assessing the security of deep learning models. However, adversarial samples usually exhibit poor transferability over different models because of overfitting of the particular architecture and feature representation of a source model. To address this problem, the Intermediate Layer Attack with Attention guidance (IAA) is proposed to alleviate overfitting and enhance the black-box transferability. The IAA works on an intermediate layer $l$ of the source model. Guided by the model\u2019s attention (i.e., gradients) to the features of layer $l$ , the attack algorithm seeks and undermines the key features that are likely to be adopted by diverse architectures. Significantly, IAA focuses on improving existing white-box attacks without introducing significant visual perceptual quality degradation. Namely, IAA maintains the white-box attack performance of the original algorithm while significantly enhancing its black-box transferability. Extensive experiments on ImageNet classifiers confirmed the effectiveness of our method. The proposed IAA outperformed all state-of-the-art benchmarks in various white-box and black-box settings, i.e., improving the success rate of BIM by 29.65% against normally trained models and 27.16% against defense models.", "pages": "95451-95461", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Yang, Shanshan", "Yang, Yu", "Zhou, Linna", "Zhan, Rui", "Man, Yufei"]}]["9718103", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2022.3153075", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Computer bugs", "Feature extraction", "Task analysis", "Deep learning", "Convolutional neural networks", "Convolution", "Social networking (online)", "Bug triage", "bug report", "software maintenance", "defect triage", "bug assignment", "bug report", "bug fixer recommendation"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "A Graph Convolution Network-Based Bug Triage System to Learn Heterogeneous Graph Representation of Bug Reports", "url": "", "volume": "10", "year": "2022", "abstract": "Many bugs and defects occur during software testing and maintenance. These bugs should be resolved as soon as possible, to improve software quality. However, bug triage aims to solve these bugs by assigning the reported bugs to an appropriate developer or list of developers. It is an arduous task for a human triager to assign an appropriate developer to a bug report, when there are several developers with different skills, and several automated and semi-automated triage systems have been proposed in the last decade. Some recent techniques have suggested possibilities for the development of an effective triage system. However, these techniques require improvement. In previous work, we proposed a heterogeneous graph representation for bug triage, using word\u2013word edges and word-bug document co-occurrences to build a heterogeneous graph of bug data. Cosine similarity is used to weight the word\u2013word edges. Then, a graph convolution network is used to learn a heterogeneous graph representation. This paper extends our previous work by adopting different similarity metrics and correlation metrics for weighting word\u2013word edges. The method was validated using different small and large datasets obtained from large-scale open-source projects. The top-k accuracy metric was used to evaluate the performance of the bug triage system. The experimental results showed that the point-wise mutual information of the proposed model was better than that of other word\u2013word weighting methods, and our method had better accuracy for large datasets than other recent state-of-the-art methods. The proposed method with point-wise mutual information showed 3% to 6% higher top-1 accuracy than state-of-the-art methods for large datasets.", "pages": "20677-20689", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Zaidi, Syed", "Woo, Honguk", "Lee, Chan-Gun"]}]["8966357", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2020.2968728", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Cryptography", "Privacy", "Protocols", "Resists", "Telecommunications", "Information security", "Privacy protection", "verifiable", "threshold", "multi-secret"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "Information Privacy Protection Based on Verifiable (t, n)-Threshold Multi-Secret Sharing Scheme", "url": "", "volume": "8", "year": "2020", "abstract": "General secret sharing schemes comprise a secret dealer and a number of participants. The dealer can randomly generate a secret and then distribute shares of the secret to the participants. Verifiable multi-secret sharing enables a dealer to share multiple secrets among a group of participants such that the deceptive behaviors of the dealer and the participants can be detected. However, in the absence of secure channels, few verifiable secret sharing schemes can simultaneously share multiple secrets at one time. In this paper, we present an information privacy protection and verifiable multi-secret sharing scheme with a simple structure and high security. Each participant can verify correctness of the share distribution process based on public information published by the dealer and guarantee validity of the received share to avoid offering fake information in the process of restoring the original secret. Our performance and security analysis indicate that the newly proposed scheme is more efficient and practical while maintaining the same level of security compared with similar protocols available.", "pages": "20799-20804", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Wang, Na", "Cai, Yuanyuan", "Fu, Junsong", "Chen, Xiqi"]}]["9770051", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2022.3172962", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Encyclopedias", "Internet", "Online services", "Collaboration", "Electronic publishing", "Feature extraction", "Bibliographies", "Authority", "expertise", "Google\u2019s E-A-T", "hybrid approach", "information quality", "trustworthiness", "web 2.0", "Wikipedia"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "Assessing Information Quality of Wikipedia Articles Through Google\u2019s E-A-T Model", "url": "", "volume": "10", "year": "2022", "abstract": "Along with the emergence of Web 2.0, User Generated Content (UGC) is becoming increasingly important for knowledge sharing. Wikipedia being the world\u2019s largest-ever community-based collaborative encyclopedia, is also one of the biggest UGC databases in the world. Wikipedia is dealing with a significant problem of Information Quality (IQ) because of its open-source and collaborative nature. When carrying out attacks such as link spamming, malicious users take advantage of Wikipedia\u2019s popularity on the World Wide Web (WWW). As a result, Wikipedia is generally not recommended for academic-related work. There are, however, some articles that are both rich in information and quality. Existing approaches for assessing Wikipedia\u2019s IQ involve statistical models and machine learning algorithms; however, the existing models do not produce satisfactory results. In this study, a novel theoretical model based on Google\u2019s E-A-T framework is introduced to assess Wikipedia\u2019s IQ. The model comprises three IQ constructs Expertise, Authority and Trustworthiness. Based on the empirical findings and study results, a set of IQ dimensions that influence the above three IQ constructs, as well as 45 IQ attributes to measure the IQ dimensions, were identified. The IQ attributes were automatically and inexpensively extracted from the content and meta-data statistics of Wikipedia articles using a Selenium 3.14 web automation script. A sample of 2000 articles comprising 1000 Featured Articles (FA) and 1000 non-FA articles from six WikiProjects was used for the data analysis. The proposed model was compared with three previously published models in terms of classification and clustering accuracy. It received classification and clustering accuracies of 95% and 93% respectively, which is a drastic improvement over the existing models. Furthermore, an average inter-rater agreement of 84% was observed. Thus, the proposed model\u2019s effectiveness is fairly validated by this extensive experiment. This study contributes to the related knowledge area by introducing a novel framework to assess Wikipedia articles\u2019 IQ.", "pages": "52196-52209", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Sugandhika, Chinthani", "Ahangama, Supunmali"]}]["9220139", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2020.3030069", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Measurement", "Quality of service", "Software", "Cloud computing", "Service robots", "Reliability", "Service robot", "cloud robotic platform", "quality of service", "software", "network", "metrics framework"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "Overview and Framework of Quality Service Metrics for Cloud-Based Robotics Platforms", "url": "", "volume": "8", "year": "2020", "abstract": "With the rapid development of big data, cloud computing and other technologies, Cloud-based robotic has become one of the key research directions for service robot, such as used in hospitals. A framework and set of metrics for evaluating the quality of service (QOS) of a cloud robotic platform would be greatly facilitate research into and actual practice of service robots. In this paper, a QOS metrics framework of cloud robotic computing is summarized and the research of components and metrics of a cloud robotic platform is reviewed. QOS metrics are organized into software, network, and robotic services. By summarizing and analyzing the above three groups of metrics, a QOS framework or index system is proposed. Finally, future research towards open source and standardization of components of robotic cloud platform is discussed.", "pages": "185885-185898", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Feng, Jie", "Si, Guannan", "Zhou, Fengyu"]}]["9907002", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2022.3211288", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["SCADA systems", "Production processes", "Data acquisition", "Supervisory control", "Manufacturing processes", "Smart manufacturing", "Steel industry", "Process control", "Cyber-physical systems", "Fourth Industrial Revolution", "Fifth Industrial Revolution", "Supervisory control and data acquisition", "SCADA", "supervisory control", "data acquisition", "industrial process control", "cyber-physical", "continuous flow production", "manufacturing", "steel industry", "industry 4.0", "industry 5.0", "smart factory"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "SCADA Systems With Focus on Continuous Manufacturing and Steel Industry: A Survey on Architectures, Standards, Challenges and Industry 5.0", "url": "", "volume": "10", "year": "2022", "abstract": "Recent technological advances encompassed by the smart factory concept have fundamentally changed industrial control systems in the way they are structured and how they operate. Majority of these changes affect Supervisory Control And Data Acquisition (SCADA) systems, shifting them to a higher level of interoperability, heterogeneous networks, big data and toward internet technologies and services in general. However, this transformation does not affect all SCADA systems equally. The immediate industrial environment and controlled processes have a significant impact as well. This paper presents a holistic approach to SCADA systems implemented in continuous flow production control within the steel industry production environment. We outline the multi-layer architecture of the SCADA control framework and the aspects of interoperability and interconnectivity within the architecture reference models, together with the research challenges and opportunities arising from the recent rapid increasement of the industrial control systems complexity and digital transformation under the Industry 4.0 paradigm, resulting in disrupting levels of the traditional automation pyramid based on Purdue model toward a higher level of integration and interoperability enabling cross-level data exchange empowered by the Industrial Internet of Things. Furthermore, the paper addresses the problem of proprietary SCADA systems and elaborates the causal correlation between SCADA quality requirements and adoption of new technologies in relation to the specific industrial environment of the steel manufacturing process.", "pages": "109395-109430", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Sverko, Mladen", "Grbac, Tihana", "Mikuc, Miljenko"]}]["9051670", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2020.2984648", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Optimization", "Web services", "Quality of service", "Reliability", "Atomic measurements", "Service composition", "service optimization", "trustworthiness", "interface matching", "optimal binding scheme", "composite template"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "A Framework for Trustworthy Web Service Composition and Optimization", "url": "", "volume": "8", "year": "2020", "abstract": "Recently, Web service composition and optimization have received an increasing attention from both academia and industrial community. Most current methods have not paid enough attention to the user specific trust requirement for composite services. However, Trust is an important metric to judge whether a composite service can behave as user expected. In this work, firstly, a multifactor concept of trust of composite service is defined based on the trust of component services, interface compatibility and optimal binding schemes. Secondly, a trustworthy Web service composition and optimization framework called TWSCO is proposed to guarantee the trust of composite service and efficiency of Web service composition process. The interface-matching problem among component services and user preference are considered in TWSCO, which firstly uses component services filter to remove untrusted component services. Secondly, the concrete services, based on interface similarity, are organized as clusters. Thirdly, a composite template among component services is formed at the cluster level to guarantee the trust and efficiency of composite service. finally, the best binding scheme is discovered by an optimization method based on user specific trust metrics. In the end, experiments based on real Web services are presented to illustrate the proposed framework TWSCO can effectively guarantee the user preference trust of the composite service.", "pages": "73508-73522", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Hu, Chunling", "Wu, Xiaona", "Li, Bixin"]}]["8089328", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2017.2767561", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Privacy", "Medical services", "Electronic mail", "Wireless sensor networks", "Access control", "Data privacy", "e-Healthcare", "privacy", "anonymity", "access control"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "Privacy Preservation in e-Healthcare Environments: State of the Art and Future Directions", "url": "", "volume": "6", "year": "2018", "abstract": "e-Healthcare promises to be the next big wave in healthcare. It offers all the advantages and benefits imaginable by both the patient and the user. However, current e-Healthcare systems are not yet fully developed and mature, and thus lack the degree of confidentiality, integrity, privacy, and user trust necessary to be widely implemented. Two primary aspects of any operational healthcare enterprise are the quality of healthcare services and patient trust over the healthcare enterprise. Trust is intertwined with issues like confidentiality, integrity, accountability, authenticity, identity, and data management, to name a few. Privacy remains one of the biggest obstacles to ensuring the success of e-Healthcare solutions in winning patient trust as it indirectly covers most security concerns. Addressing privacy concerns requires addressing security issues like access control, authentication, non-repudiation, and accountability, without which end-to-end privacy cannot be ensured. Achieving privacy from the point of data collection in wireless sensor networks, to incorporating the Internet of Things, to communication links, and to data storage and access, is a huge undertaking and requires extensive work. Privacy requirements are further compounded by the fact that the data handled in an enterprise are of an extremely personal and private nature, and its mismanagement, either intentionally or unintentionally, could seriously hurt both the patient and future prospects of an e-Healthcare enterprise. Research carried out in order to address privacy concerns is not homogenous in nature. It focuses on the failure of certain parts of the e-Healthcare enterprise to fully address all aspects of privacy. In the middle of this ongoing research and implementation, a gradual shift has occurred, moving e-Healthcare enterprise controls away from an organizational level toward the level of patients. This is intended to give patients more control and authority over decision making regarding their protected health information/electronic health record. A lot of works and efforts are necessary in order to better assess the feasibility of this major shift in e-Healthcare enterprises. Existing research can be naturally divided on the basis of techniques used. These include data anonymization/pseudonymization and access control mechanisms primarily for stored data privacy. This, however, results in giving a back seat to certain privacy requirements (accountability, integrity, non-repudiation, and identity management). This paper reviews research carried out in this regard and explores whether this research offers any possible solutions to either patient privacy requirements for e-Healthcare or possibilities for addressing the (technical as well as psychological) privacy concerns of the users.", "pages": "464-478", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Sahi, Muneeb", "Abbas, Haider", "Saleem, Kashif", "Yang, Xiaodong", "Derhab, Abdelouahid", "Orgun, Mehmet", "Iqbal, Waseem", "Rashid, Imran", "Yaseen, Asif"]}]["9481139", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2021.3096194", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Training", "Manifolds", "Independent component analysis", "Data models", "Correlation", "Linear programming", "Multi-label classification", "dual autoencoder", "RICA", "manifold regularization", "representation learning"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "Representation Learning With Dual Autoencoder for Multi-Label Classification", "url": "", "volume": "9", "year": "2021", "abstract": "Multi-label classification aims to deal with the problem that an object may be associated with one or more labels, which is a more difficult task due to the complex nature of multi-label data. The crucial problem of multi-label classification is the more robust and higher-level feature representation learning, which can reduce non-helpful feature attributes from the input space prior to training. In recent years, deep learning methods based on autoencoders have achieved excellent performance in multi-label classification for the advantages of powerful representations learning ability and fast convergence speed. However, most existing autoencoder-based methods only rely on the single autoencoder model, which pose challenges for multi-label feature representations learning and fail to measure similarities between data spaces. To address this problem, in this paper, we propose a novel representation learning method with dual autoencoder for multi-label classification. Compared to the existing autoencoder-based methods, our proposed method can capture different characteristics and more abstract features from data by the serially connection of two different types of autoencoders. More specifically, firstly, the algorithm of Reconstruction Independent Component Analysis (RICA) in sparse autoencoder is trained on patches on all training and test dataset for robust global feature representations learning. Secondly, with the output of RICA, stacked autoencoder with manifold regularization (SAMR) is introduced to ameliorate the quality of multi-label features learning. Comprehensive experiments on several real-world data sets demonstrate the effectiveness of our proposed approach compared with several competing state-of-the-art methods.", "pages": "98939-98947", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Zhu, Yi", "Yang, Yang", "Li, Yun", "Qiang, Jipeng", "Yuan, Yunhao", "Zhang, Runmei"]}]["8576500", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2018.2884913", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Predictive models", "Computational modeling", "Data models", "Videos", "Training", "Base stations", "Mobile edge computing", "deep learning", "proactive caching", "prediction model searching"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "DeepMEC: Mobile Edge Caching Using Deep Learning", "url": "", "volume": "6", "year": "2018", "abstract": "Caching popular contents at edge nodes such as base stations is a crucial solution for improving users' quality of services in next-generation networks. However, it is very challenging to correctly predict the future popularity of contents and decide which contents should be stored in the base station cache. Recently, with the advances in big data and high computing power, deep learning models have achieved high prediction accuracy. Hence, in this paper, deep learning is used to learn and predict the future popularity of contents to support cache decision. First, deep learning models are trained and utilized in the cloud data center to make an efficient cache decision. Then, the final cache decision is sent to each base station to store the popular contents proactively. The proposed caching scheme involves three distinct parts: 1) predicting the future class label of each content; 2) predicting the future popularity score of contents based on the predicted class label; and 3) caching the predicted contents with high popularity scores. The prediction models using the Keras and Tensorflow libraries are implemented in this paper. Finally, the performance of the caching schemes is tested with a Python-based simulator. In terms of a cache hit, simulation results show that the proposed scheme outperforms 38%, convolutional recurrent neural network-based scheme outperforms 33%, and convolutional neural network-based scheme outperforms 25% compared to the baseline scheme.", "pages": "78260-78275", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Thar, Kyi", "Tran, Nguyen", "Oo, Thant", "Hong, Choong"]}]["8653381", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2019.2901864", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Feature extraction", "Twitter", "Real-time systems", "Security", "IEEE Senior Members", "Big Data", "Online social network", "social bots", "user behavior", "semi-supervised clustering"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "Detecting Malicious Social Bots Based on Clickstream Sequences", "url": "", "volume": "7", "year": "2019", "abstract": "With the significant increase in the volume, velocity, and variety of user data (e.g., user-generated data) in online social networks, there have been attempted to design new ways of collecting and analyzing such big data. For example, social bots have been used to perform automated analytical services and provide users with improved quality of service. However, malicious social bots have also been used to disseminate false information (e.g., fake news), and this can result in real-world consequences. Therefore, detecting and removing malicious social bots in online social networks is crucial. The most existing detection methods of malicious social bots analyze the quantitative features of their behavior. These features are easily imitated by social bots; thereby resulting in low accuracy of the analysis. A novel method of detecting malicious social bots, including both features selection based on the transition probability of clickstream sequences and semi-supervised clustering, is presented in this paper. This method not only analyzes transition probability of user behavior clickstreams but also considers the time feature of behavior. Findings from our experiments on real online social network platforms demonstrate that the detection accuracy for different types of malicious social bots by the detection method of malicious social bots based on transition probability of user behavior clickstreams increases by an average of 12.8%, in comparison to the detection method based on quantitative analysis of user behavior.", "pages": "28855-28862", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Shi, Peining", "Zhang, Zhiyong", "Choo, Kim-Kwang"]}]["9420732", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2021.3077028", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Support vector machines", "Petroleum", "Prediction algorithms", "FCC", "Principal component analysis", "Predictive models", "Spectroscopy", "Gasoline octane number", "kernel principal component analysis", "support vector regression", "particle swarm optimization"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "Predictive Analytics for Octane Number: A Novel Hybrid Approach of KPCA and GS-PSO-SVR Model", "url": "", "volume": "9", "year": "2021", "abstract": "Octane number is the most important indicator of reflecting the combustion performance, and a great deal of research has been devoted to improving it. In this paper, a new analytical framework is proposed to predict octane number, kernel principal component analysis (KPCA) is used to reduce the dimension of the variables in the process of Fluid Catalytic Cracking (FCC), support vector regression (SVR) is used to construct the gasoline octane number prediction model and the particle swarm optimization algorithm (PSO) is used to select the optimal combination of parameters for the model. The experiments show that the octane number can be improved under a given production environment with a guaranteed desulfurization effect of gasoline products. Furthermore, several key attributes that have a significantly positive or negative correlation with the improvement of gasoline product quality are identified through computing the feature score. The findings can help engineers adjust operational variables to obtain a series of high-quality products.", "pages": "66531-66541", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Li, Baosheng", "Qin, Chuandong"]}]["9399408", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2021.3072280", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Predictive models", "Atmospheric modeling", "Genetic algorithms", "Feature extraction", "Air quality", "Decoding", "Statistics", "PM 25", "genetic algorithm", "feature selection", "long short-term memory", "encoder-decoder model"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "PM2.5 Prediction Using Genetic Algorithm-Based Feature Selection and Encoder-Decoder Model", "url": "", "volume": "9", "year": "2021", "abstract": "The concentration of fine particulate matter (PM2.5), which represents inhalable particles with diameters of 2.5 micrometers and smaller, is a vital air quality index. Such particles can penetrate deep into the human lungs and severely affect human health. This paper studies accurate PM2.5 prediction, which can potentially contribute to reducing or avoiding the negative consequences. Our approach\u2019s novelty is to utilize the genetic algorithm (GA) and an encoder-decoder (E-D) model for PM2.5 prediction. The GA benefits feature selection and remove outliers to enhance the prediction accuracy. The encoder-decoder model with long short-term memory (LSTM), which relaxes the restrictions between the input and output of the model, can be used to effectively predict the PM2.5 concentration. We evaluate the proposed model on air quality datasets from Hanoi and Taiwan. The evaluation results show that our model achieves excellent performance. By merely using the E-D model, we can obtain more accurate (up to 53.7%) predictions than those of previous works. Moreover, the GA in our model has the advantage of obtaining the optimal feature combination for predicting the PM2.5 concentration. By combining the GA-based feature selection algorithm and the E-D model, our proposed approach further improves the accuracy by at least 13.7%.", "pages": "57338-57350", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Nguyen, Minh", "Le, Phi", "Nguyen, Kien", "Le, Van", "Nguyen, Thanh-Hung", "Ji, Yusheng"]}]["9218922", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2020.3029826", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Anomaly detection", "Machine learning", "Measurement", "Product design", "Inspection", "Manufacturing industries", "Anomaly detection", "assembly lines", "big data", "machine learning", "manufacturing industries", "root cause analysis", "unsupervised learning"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "Assembly Line Anomaly Detection and Root Cause Analysis Using Machine Learning", "url": "", "volume": "8", "year": "2020", "abstract": "Anomaly detection is becoming widely used in Manufacturing Industry to enhance product quality. At the same time, it plays a great role in several other domains due to the fact that anomaly may reveal rare but represent an important phenomenon. The objective of this paper is to detect anomalies and identify the possible variables that caused these anomalies on historical assembly data for two series of products. Multiple anomaly detection techniques were performed; HBOS, IForest, KNN, CBLOF, OCSVM, LOF, and ABOD. Moreover, we used AUROC and Rank Power as performance metrics, followed by Boosting ensemble learning method to ensure the best anomaly detectors robustness. The techniques that gave the highest performance are KNN, ABOD for both product series datasets with 0.95 and 0.99 AUROC respectively. Finally, we applied a statistical root cause analysis on the detected anomalies with the use of Pareto chart to visualize the frequency of the possible causes and its cumulative occurrence. The results showed that there are seven rejection causes for both product series, whereas the first three causes are responsible for 85% of the rejection rates. Besides, assembly machines engineers reported a significant reduction in the rejection rates in both assembly machines after tuning the specification limits of the rejection causes identified by this research results.", "pages": "189661-189672", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Abdelrahman, Osama", "Keikhosrokiani, Pantea"]}]["8502208", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2018.2876755", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Hidden Markov models", "Task analysis", "Machine learning", "Tools", "Education", "Discussion forums", "Bibliographies", "Machine learning", "massive open online courses", "statistical analysis", "big data"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "Analyzing Learners Behavior in MOOCs: An Examination of Performance and Motivation Using a Data-Driven Approach", "url": "", "volume": "6", "year": "2018", "abstract": "Massive open online courses (MOOCs) have been experiencing increasing use and popularity in highly ranked universities in recent years. The opportunity of accessing high quality courseware content within such platforms, while eliminating the burden of educational, financial, and geographical obstacles has led to a rapid growth in participant numbers. The increasing number and diversity of participating learners has opened up new horizons to the research community for the investigation of effective learning environments. Learning Analytics has been used to investigate the impact of engagement on student performance. However, the extensive literature review indicates that there is little research on the impact of MOOCs, particularly in analyzing the link between behavioral engagement and motivation as predictors of learning outcomes. In this paper, we consider a dataset, which originates from online courses provided by Harvard University and the Massachusetts Institute of Technology, delivered through the edX platform. Two sets of empirical experiments are conducted using both statistical and machine learning techniques. Statistical methods are used to examine the association between engagement level and performance, including the consideration of learner educational backgrounds. The results indicate a significant gap between success and failure outcome learner groups, where successful learners are found to read and watch course material to a higher degree. Machine learning algorithms are used to automatically detect learners who are lacking in motivation at an early time in the course, thus providing instructors with insight in regards to student withdrawal.", "pages": "73669-73685", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Al-Shabandar, Raghad", "Hussain, Abir", "Liatsis, Panos", "Keight, Robert"]}]["9732997", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2022.3158743", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Quality of service", "Containers", "Cloud computing", "Monitoring", "Measurement", "Microservice architectures", "Computer architecture", "Cloud", "microservices", "Kubernetes", "SLO", "QoS"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "Autoscaling Pods on an On-Premise Kubernetes Infrastructure QoS-Aware", "url": "", "volume": "10", "year": "2022", "abstract": "Cloud systems and microservices are becoming powerful tools for businesses. The evidence of the advantages of offering infrastructure, hardware or software as a service (IaaS, PaaS, SaaS) is overwhelming. Microservices and decoupled applications are increasingly popular. These architectures, based on containers, have facilitated the efficient development of complex SaaS applications. A big challenge is to manage and design microservices with a massive range of different facilities, from processing and data storage to computing predictive and prescriptive analytics. Computing providers are mainly based on data centers formed of massive and heterogeneous virtualized systems, which are continuously growing and diversifying over time. Moreover, these systems require integrating into current systems while meeting the Quality of Service (QoS) constraints. The primary purpose of this work is to present an on-premise architecture based on Kubernetes and Docker containers aimed at improving QoS regarding resource usage and service level objectives (SLOs). The main contribution of this proposal is its dynamic autoscaling capabilities to adjust system resources to the current workload while improving QoS.", "pages": "33083-33094", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Ru\u00edz, Llu\u00eds", "Pueyo, Pere", "Mateo-Forn\u00e9s, Jordi", "Mayoral, Jordi", "Teh\u00e0s, Francesc"]}]["6522590", {"address": "", "articleno": "", "doi": "10.1109/TST.2013.6522590", "issn": "1007-0214", "issue_date": "", "journal": "Tsinghua Science and Technology", "keywords": ["cloud computing", "recommender systems", "big data", "collaborative filtering", "data mining", "similaritytransitivity", "machine learning", "mapReduce", "android applications"], "month": "June", "number": "3", "numpages": "", "publisher": "", "title": "TST: Threshold based similarity transitivity method in collaborative filtering with cloud computing", "url": "", "volume": "18", "year": "2013", "abstract": "Collaborative filtering solves information overload problem by presenting personalized content to individual users based on their interests, which has been extensively applied in real-world recommender systems. As a class of simple but efficient collaborative filtering method, similarity based approaches make predictions by finding users with similar taste or items that have been similarly chosen. However, as the number of users or items grows rapidly, the traditional approach is suffering from the data sparsity problem. Inaccurate similarities derived from the sparse user-item associations would generate the inaccurate neighborhood for each user or item. Consequently, its poor recommendation drives us to propose a Threshold based Similarity Transitivity (TST) method in this paper. TST firstly filters out those inaccurate similarities by setting an intersection threshold and then replaces them with the transitivity similarity. Besides, the TST method is designed to be scalable with MapReduce framework based on cloud computing platform. We evaluate our algorithm on the public data set MovieLens and a real-world data set from AppChina (an Android application market) with several well-known metrics including precision, recall, coverage, and popularity. The experimental results demonstrate that TST copes well with the tradeoff between quality and quantity of similarity by setting an appropriate threshold. Moreover, we can experimentally find the optimal threshold which will be smaller as the data set becomes sparser. The experimental results also show that TST significantly outperforms the traditional approach even when the data becomes sparser.", "pages": "318-327", "note": "", "ISSN": "1007-0214", "publicationtype": "article", "author": ["Xie, Feng", "Chen, Zhen", "Xu, Hongfeng", "Feng, Xiwei", "Hou, Qi"]}]["8998286", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2020.2973703", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Technological innovation", "Economics", "Green products", "Entropy", "Government policies", "Air pollution", "Climate change", "Regional innovation quality", "configuration of factors", "entropy weight method", "fuzzy-set qualitative comparative analysis", "china", "green development perspective"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "How to Improve Regional Innovation Quality From the Perspective of Green Development? Findings From Entropy Weight Method and Fuzzy-Set Qualitative Comparative Analysis", "url": "", "volume": "8", "year": "2020", "abstract": "Innovation is increasingly becoming the most crucial and effective way to tackle climate change and environmental pollution. Recently, innovation quality has received continuous attention from scholars, entrepreneurs and policymakers. It is necessary to measure innovation quality from the perspective of green development. Based on the statistics data of 30 provincial-level innovation systems in China, this study comprehensively evaluates regional innovation quality and identifies the configuration of factors which could lead to high regional innovation quality. Firstly, this study establishes an indicator system of measuring regional innovation quality which includes seven indicators from three aspects of technological, economic and ecological benefit of innovative activities, and entropy weight method is used to comprehensively evaluate regional innovation quality. The result reveals there are big differences between different regions in China in innovation quality. In terms of three economic regions in China, innovation quality in the eastern region is the highest, followed by the central region and the lowest in the western region. Then, fuzzy-set qualitative comparative analysis method is applied, which uncovers three configurations of factors that could lead to high regional innovation quality. Finally, the conclusion of this study provides policy pathways for improving regional innovation quality.", "pages": "32575-32586", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Zhang, Meili", "Li, Baizhou"]}]["9681298", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2022.3143033", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Feature extraction", "Image recognition", "Bibliographies", "Task analysis", "Deep learning", "Decoding", "Media", "Barcode", "barcode recognition", "barcode detection", "barcode localization", "deep learning", "literature review"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "Deep Learning in Barcode Recognition: A Systematic Literature Review", "url": "", "volume": "10", "year": "2022", "abstract": "The use of deep learning (DL) for barcode recognition and analysis has achieved remarkable success and has attracted great attention in various domains. Unlike other barcode recognition methods, DL-based approaches can significantly improve the speed and accuracy of both barcode detection and decoding. However, after almost a decade of progress, the current status of DL-based barcode recognition has yet to be thoroughly explored. Specifically, summaries of key insights and gaps remain unavailable in the literature. Therefore, this study aims to comprehensively review recent applications of DL methods in barcode recognition. We mainly conducted a well-constructed systematic literature review (SLR) approach to collect relevant articles and evaluate and summarize the state of the art. This study\u2019s contributions are threefold. First, the paper highlights new DL approaches\u2019 applicability to barcode localization and decoding processes and their potential to either reduce the time required or provide higher quality. Second, another main finding of this study signifies an increasing demand for public and specific barcode datasets that allow DL methods to learn more efficiently in the big data era. Finally, we conclude with a discussion on the crucial challenges of DL with respect to barcode recognition, incorporating promising directions for future research development.", "pages": "8049-8072", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Wudhikarn, Ratapol", "Charoenkwan, Phasit", "Malang, Kanokwan"]}]["9146141", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2020.3011112", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Cloud computing", "Servers", "Face recognition", "Machine learning", "Training", "Cryptography", "Privacy", "Information security", "edge and cloud networks", "face recognition", "sparse representation"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "A Privacy-Preserving Learning Framework for Face Recognition in Edge and Cloud Networks", "url": "", "volume": "8", "year": "2020", "abstract": "Offloading the computationally intensive workloads to the edge and cloud not only improves the quality of computation, but also creates an extra degree of diversity by collecting information from devices in service. Nevertheless, significant concerns on privacy are raised as the aggregated information could be misused without the permission by the third party. Sparse coding, which has been successful in computer vision, is finding application in this new domain. In this paper, we develop a secured face recognition framework to orchestrate sparse coding in edge and cloud networks. Specifically, 1). To protect the privacy, a low-complexity encrypting algorithm is developed based on random unitary transform, where its influence on dictionary learning and sparse representation is analysed. Furthermore, it is proved that such influence will not affect the accuracy of face recognition. 2). To fully utilize the multi-device diversity and avoid big data transmission between edge and cloud, a distributed learning framework is established, which extracts deeper features in an intermediate space, expanded according to the dictionaries from each device. Classification is performed in this new feature space to combat the noise and modeling error. Finally, the efficiency and effectiveness of the proposed framework is demonstrated through simulation results.", "pages": "136056-136070", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Wang, Yitu", "Nakachi, Takayuki"]}]["8654618", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2019.2902243", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Clustering algorithms", "Wireless sensor networks", "Routing", "Kernel", "Estimation", "Energy consumption", "Fuzzy logic", "Unequal clustering", "fuzzy logic", "kernel density estimation", "wireless sensor networks"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "An Energy Aware Adaptive Kernel Density Estimation Approach to Unequal Clustering in Wireless Sensor Networks", "url": "", "volume": "7", "year": "2019", "abstract": "Energy conservation is one of the most important challenges in wireless sensor networks (WSNs). Therefore, compared with the traditional networks, the WSNs not only need high-quality services with high throughput or low transmission delay, but also pay greater attention to energy utilization to extend network lifetime. The clustering routing algorithm is considered to be among the effective ways to collect and transmit data in WSNs. Cluster head (CH) plays a vital role in the cluster which is in charge of data aggregation and data transmission, so their energy consumption is higher than non-CH nodes. The traditional clustering algorithm tends to have the same size in each cluster. However, due to the randomness of the node distribution, the equal clustering mechanism obviously cannot reduce energy consumption. In order to solve this problem, this paper contributes a new unequal clustering algorithm, an energy-aware adaptive kernel density estimation algorithm (EAKDE), which aims to balance the energy dissipation among the CHs. EAKDE utilizes fuzzy logic to determine the priority of nodes competing for CH. In order to adapt the dynamic change of node conditions, adaptive kernel density estimation algorithm is utilized to assign the appropriate unequal cluster radius to sensor nodes. The simulation results demonstrate that, in different scenarios, EAKDE outperforms the other well-known algorithms in terms of network stability, network lifetime, and energy efficiency.", "pages": "40569-40580", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Liu, Fagui", "Chang, Yufei"]}]["9133078", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2020.3007002", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Edge computing", "Collaboration", "Resource management", "Cloud computing", "Task analysis", "Reinforcement learning", "Internet of Things", "Collaborative computing", "edge computing", "optimization strategy"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "Collaborative Edge Computing and Caching With Deep Reinforcement Learning Decision Agents", "url": "", "volume": "8", "year": "2020", "abstract": "Large amounts of data will be generated due to the rapid development of the Internet of Things (IoT) technologies and 5th generation mobile networks (5G), the processing and analysis requirements of big data will challenge existing networks and processing platforms. As the most promising technology in 5G networks, edge computing will greatly ease the pressure on network and data processing analysis on the edge. In this paper, we considered the coordination between compute and cache resources between multi-level edge computing nodes (ENs), users under this system can offload computing tasks to ENs to improve quality of service (QoS). We aimed to maximize the long-term profit on the edge, while satisfying the low-latency computing of the users, and jointly optimize the edge-side node offloading strategy and resource allocation. However, it is challenging to obtain an optimal strategy in such a dynamic and complex system. To solve the complex resource allocation problem on the edge and make edge have certain adaptation and cooperation, we used double deep Q-learning (DDQN) to make decisions, ability to maximize long-term gains while making quick decisions. The simulation results prove the effectiveness of DDQN in maximizing revenue when allocation resources on the edge.", "pages": "120604-120612", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Ren, Jianji", "Wang, Haichao", "Hou, Tingting", "Zheng, Shuai", "Tang, Chaosheng"]}]["9430132", {"address": "", "articleno": "", "doi": "10.26599/BDMA.2020.9020029", "issn": "2096-0654", "issue_date": "", "journal": "Big Data Mining and Analytics", "keywords": ["Smart cities", "Urban areas", "Cloud computing", "Task analysis", "Edge computing", "Statistics", "Sociology", "cyber physical systems", "Internet of Things (IoT)", "intelligent computing algorithm", "Quality of Service(QoS)", "smart city"], "month": "Sep.", "number": "3", "numpages": "", "publisher": "", "title": "A survey on algorithms for intelligent computing and smart city applications", "url": "", "volume": "4", "year": "2021", "abstract": "With the rapid development of human society, the urbanization of the world's population is also progressing rapidly. Urbanization has brought many challenges and problems to the development of cities. For example, the urban population is under excessive pressure, various natural resources and energy are increasingly scarce, and environmental pollution is increasing, etc. However, the original urban model has to be changed to enable people to live in greener and more sustainable cities, thus providing them with a more convenient and comfortable living environment. The new urban framework, the smart city, provides excellent opportunities to meet these challenges, while solving urban problems at the same time. At this stage, many countries are actively responding to calls for smart city development plans. This paper investigates the current stage of the smart city. First, it introduces the background of smart city development and gives a brief definition of the concept of the smart city. Second, it describes the framework of a smart city in accordance with the given definition. Finally, various intelligent algorithms to make cities smarter, along with specific examples, are discussed and analyzed.", "pages": "155-172", "note": "", "ISSN": "2096-0654", "publicationtype": "article", "author": ["Tong, Zhao", "Ye, Feng", "Yan, Ming", "Liu, Hong", "Basodi, Sunitha"]}]["9826662", {"address": "", "articleno": "", "doi": "10.1108/IJCS-11-2019-0031", "issn": "2398-7294", "issue_date": "", "journal": "International Journal of Crowd Science", "keywords": ["Crowdsourcing", "Correlation", "Focusing", "Estimation", "Regression analysis", "Resource management", "Reliability", "Human resource", "Quality evaluation", "Work performance", "Crowdsourcing", "Big five", "Task assignment"], "month": "June", "number": "2", "numpages": "", "publisher": "", "title": "Utilizing short version big five traits on crowdsouring", "url": "", "volume": "4", "year": "2020", "abstract": "Purpose \u2013 The purpose of this paper is to examine the efficacy of the Japanese ten-item personality inventory (TIPI-J), a short version of the big five (BF) questionnaire, on crowdsourcing. The BF traits are indicators of personality and are said to be an effective predictor of study performance in various occupations. BF can be used in crowdsourcing to predict crowd workers' performance; however, it will be difficult to use in practice for two reasons like the time-and-effort issue and the bias issue. In this study, an empirical analysis is conducted on crowdsourcing to examine if TIPI-J can solve those issues. Design/methodology/approach \u2013 To investigate the issues, two tasks are posted on a crowdsourcing provider. Both TIPI-J and full version BF are conducted before and after selecting crowd workers. Structural validity and convergence validity are tested with correlation analysis between before (TIPI-J) and after (full version BF) data to examine the bias issue. Additionally, those correlations are compared with previous study and significances are examined. Findings \u2013 The correlations in \u201cconscientiousness\u201d is 0.45-0.50, respectively, compared with a previous study, those two correlations did not show significance. This indicates that no clear bias exists. Originality/value \u2013 This is the first research to investigate the efficacy of TIPI-J on crowdsourcing and showed that TIPI-J can be a useful tool for predicting crowd workers' performance and thus it can help to select appropriate crowd workers.", "pages": "117-132", "note": "", "ISSN": "2398-7294", "publicationtype": "article", "author": ["Igawa, Kousaku", "Higa, Kunihiko", "Takamiya, Tsutomu"]}]["9097181", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2020.2995849", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Load management", "Quality of service", "Switches", "Systematics", "Measurement", "Software", "Artificial intelligence", "conventional", "load balancing", "review", "SDN", "software-defined networking", "systematic"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "A Systematic Review of Load Balancing Techniques in Software-Defined Networking", "url": "", "volume": "8", "year": "2020", "abstract": "The traditional networks are facing difficulties in managing the services offered by cloud computing, big data, and the Internet of Things as the users have become more dependent on their services. Software-Defined Networking (SDN) has pulled enthusiasm in the integration process of technologies and function as per the user's requirements for both academia and industry, and it has begun to be embraced in actual framework usage. The emergence of SDN has given another idea to empower the focal programmability of the system. Because of the increasing demand and the scarcity of resources, the load balancing issue needs to be addressed efficiently to manage the incoming traffic and resources and to improve network performance. One of the most critical issues is the role of the controller in SDN to balance the load for having a better Quality of Service (QoS). Though there are few survey articles written on load balancing, there is no detail and systematic review conducted in load balancing in SDN. Hence, this paper extends and reviews the discussion with a taxonomy of current emerging load balancing techniques in SDN systematically by categorizing the techniques as conventional and artificial intelligence-based techniques to improve the service quality. The review also includes the study of metrics and parameters which have been used to measure the performance. This review would allow gaining more information on load balancing approaches in SDN and enables the researchers to fill the current research gaps.", "pages": "98612-98636", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Belgaum, Mohammad", "Musa, Shahrulniza", "Alam, Muhammad", "Su\u2019ud, Mazliham"]}]["8710242", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2019.2915573", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Optimization", "Throughput", "Resource management", "Quality of service", "Energy efficiency", "Wireless communication", "Convex functions", "MU-MIMO", "rank deficient", "double-objective optimization", "QoS guarantee", "resource allocation"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "Resource Allocation Algorithm for MU-MIMO Systems With Double-Objective Optimization Under the Existence of the Rank Deficient Channel Matrix", "url": "", "volume": "7", "year": "2019", "abstract": "This paper proposes a double-objective optimization resource allocation algorithm for the multi-user multiple-input/multiple-output (MU-MIMO) system in the general wireless environment and demonstrates the maximum number of simultaneously supportable users and the achievable bit rates of users in the general wireless environment with full rank and rank-deficient channels. The double-objective joint optimization algorithm proposed in this paper simultaneously optimizes energy efficiency and system throughput by user selection and power allocation. On this basis, the proposed algorithm guarantees the different QoS requirements of various services, including rate requirements and delay requirements.", "pages": "61307-61319", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Pan, Su", "Yan, Yan", "Bonsu, Kusi", "Zhou, Weiwei"]}]["9091940", {"address": "", "articleno": "", "doi": "10.1109/TGRS.2020.2973363", "issn": "1558-0644", "issue_date": "", "journal": "IEEE Transactions on Geoscience and Remote Sensing", "keywords": ["Hyperspectral imaging", "Convolution", "Task analysis", "Recurrent neural networks", "Semisupervised learning", "Support vector machines", "Graph convolutional network (GCN)", "hyperspectral image classification", "nonlocal graph", "semisupervised learning"], "month": "Dec", "number": "12", "numpages": "", "publisher": "", "title": "Nonlocal Graph Convolutional Networks for Hyperspectral Image Classification", "url": "", "volume": "58", "year": "2020", "abstract": "Over the past few years making use of deep networks, including convolutional neural networks (CNNs) and recurrent neural networks (RNNs), classifying hyperspectral images has progressed significantly and gained increasing attention. In spite of being successful, these networks need an adequate supply of labeled training instances for supervised learning, which, however, is quite costly to collect. On the other hand, unlabeled data can be accessed in almost arbitrary amounts. Hence it would be conceptually of great interest to explore networks that are able to exploit labeled and unlabeled data simultaneously for hyperspectral image classification. In this article, we propose a novel graph-based semisupervised network called nonlocal graph convolutional network (nonlocal GCN). Unlike existing CNNs and RNNs that receive pixels or patches of a hyperspectral image as inputs, this network takes the whole image (including both labeled and unlabeled data) in. More specifically, a nonlocal graph is first calculated. Given this graph representation, a couple of graph convolutional layers are used to extract features. Finally, the semisupervised learning of the network is done by using a cross-entropy error over all labeled instances. Note that the nonlocal GCN is end-to-end trainable. We demonstrate in extensive experiments that compared with state-of-the-art spectral classifiers and spectral-spatial classification networks, the nonlocal GCN is able to offer competitive results and high-quality classification maps (with fine boundaries and without noisy scattered points of misclassification).", "pages": "8246-8257", "note": "", "ISSN": "1558-0644", "publicationtype": "article", "author": ["Mou, Lichao", "Lu, Xiaoqiang", "Li, Xuelong", "Zhu, Xiao"]}]["9085905", {"address": "", "articleno": "", "doi": "10.1109/TDSC.2020.2991872", "issn": "1941-0018", "issue_date": "", "journal": "IEEE Transactions on Dependable and Secure Computing", "keywords": ["Data models", "Feature extraction", "Predictive models", "Heterogeneous networks", "Computer crime", "Semantics", "Online payment services", "fraud detection", "network embedding", "user behavioral modeling"], "month": "Jan", "number": "1", "numpages": "", "publisher": "", "title": "Representing Fine-Grained Co-Occurrences for Behavior-Based Fraud Detection in Online Payment Services", "url": "", "volume": "19", "year": "2022", "abstract": "The vigorous development of e-commerce breeds cybercrime. Online payment fraud detection, a challenge faced by online service, plays an important role in rapidly evolving e-commerce. Behavior-based methods are recognized as a promising method for online payment fraud detection. However, it is a big challenge to build high-resolution behavioral models by using low-quality behavioral data. In this work, we mainly address this problem from data enhancement for behavioral modeling. We extract fine-grained co-occurrence relationships of transactional attributes by using a knowledge graph. Furthermore, we adopt the heterogeneous network embedding to learn and improve representing comprehensive relationships. Particularly, we explore customized network embedding schemes for different types of behavioral models, such as the population-level models, individual-level models, and generalized-agent-based models. The performance gain of our method is validated by the experiments over the real dataset from a commercial bank. It can help representative behavioral models improve significantly the performance of online banking payment fraud detection. To the best of our knowledge, this is the first work to realize data enhancement for diversified behavior models by implementing network embedding algorithms on attribute-level co-occurrence relationships.", "pages": "301-315", "note": "", "ISSN": "1941-0018", "publicationtype": "article", "author": ["Wang, Cheng", "Zhu, Hangyu"]}]["9112172", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2020.3001237", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Agriculture", "Diseases", "Deep learning", "Production", "Training", "Cameras", "IoT", "multiple crops", "fine-grained disease recognition", "ResNet", "singular value decomposition"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "MDFC\u2013ResNet: An Agricultural IoT System to Accurately Recognize Crop Diseases", "url": "", "volume": "8", "year": "2020", "abstract": "Crop disease diagnosis is an essential step in crop disease treatment and is a hot issue in agricultural research. However, in agricultural production, identifying only coarse-grained diseases of crops is insufficient because treatment methods are different in different grades of even the same disease. Inappropriate treatments are not only ineffective in treating diseases but also affect crop yield and food safety. We combine IoT technology with deep learning to build an IoT system for crop fine-grained disease identification. This system can automatically detect crop diseases and send diagnostic results to farmers. We propose a multidimensional feature compensation residual neural network (MDFC-ResNet) model for fine-grained disease identification in the system. MDFC-ResNet identifies from three dimensions, namely, species, coarse-grained disease, and fine-grained disease and sets up a compensation layer that uses a compensation algorithm to fuse multidimensional recognition results. Experiments show that the MDFC-ResNet neural network has better recognition effect and is more instructive in actual agricultural production activities than other popular deep learning models.", "pages": "115287-115298", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Hu, Wei-Jian", "Fan, Jie", "Du, Yong-Xing", "Li, Bao-Shan", "Xiong, Naixue", "Bekkering, Ernst"]}]["8082505", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2017.2766438", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Convolution", "Computed tomography", "Three-dimensional displays", "Training", "Two dimensional displays", "Kernel", "Solid modeling", "Low-dose CT", "convolution neural network", "residual learning", "3D convolution"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "Improving Low-Dose CT Image Using Residual Convolutional Network", "url": "", "volume": "5", "year": "2017", "abstract": "Low-dose CT is an effective solution to alleviate radiation risk to patients, it also introduces additional noise and streak artifacts. In order to maintain a high image quality for low-dose scanned CT data, we propose a post-processing method based on deep learning and using 2-D and 3-D residual convolutional networks. Experimental results and comparisons with other competing methods show that the proposed approach can effectively reduce the low-dose noise and artifacts while preserving tissue details. It is also pointed out that the 3-D model can achieve better performance in both edge-preservation and noise-artifact suppression. Factors that may influence the model performance, such as model width, depth, and dropout, are also examined.", "pages": "24698-24705", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Yang, Wei", "Zhang, Huijuan", "Yang, Jian", "Wu, Jiasong", "Yin, Xiangrui", "Chen, Yang", "Shu, Huazhong", "Luo, Limin", "Coatrieux, Gouenou", "Gui, Zhiguo", "Feng, Qianjin"]}]["8746164", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2019.2924907", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Three-dimensional displays", "Noise reduction", "Rician channels", "Noise measurement", "Matrix decomposition", "Correlation", "3D image denoising", "magnetic resonance image", "low-rank tensor approximation", "nonlocal similarity", "higher-order singular value decomposition"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "Denoising 3D Magnetic Resonance Images Based on Low-Rank Tensor Approximation With Adaptive Multirank Estimation", "url": "", "volume": "7", "year": "2019", "abstract": "The magnetic resonance (MR) imaging technique is widely used in clinical diagnosis. Unfortunately, in practice, the MR images inevitably suffer from noise, which severely degrades image quality and accordingly impacts on the accuracy of clinical diagnosis. By exploiting both the nonlocal similarity over space and the inherent correlation across the slices of the 3D MR images, in this paper, we present a novel Rician noise reduction method for the 3D MR images. Specifically, the 3D nonlocal similar patches are first extracted from the input noisy 3D MR data and then grouped to form a noisy fourth-order tensor. Since 3D patches used to construct the fourth-order tensor share similar structures, a latent noise-free tensor can be approximated by a low-rank tensor. To this end, the higher-order singular value decomposition (HOSVD) is adopted to recover the latent noise-free tensor. Furthermore, the rank of each mode of the tensor is adaptively determined by an enhanced low-rank method. The experimental results on synthetic and real 3D MR images show that the proposed method outperforms several state-of-the-art denoising methods in terms of objective metrics and visual inspection.", "pages": "85995-86003", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Lv, Hongli", "Wang, Renfang"]}]["9625754", {"address": "", "articleno": "", "doi": "10.1109/JSTARS.2021.3129842", "issn": "2151-1535", "issue_date": "", "journal": "IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing", "keywords": ["Land surface temperature", "Land surface", "Surface topography", "Remote sensing", "Vegetation mapping", "Statistics", "Sociology", "Beijing", "direct and indirect effects", "land surface temperature (LST)", "spatial durbin model (SDM)"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "Understanding the Drivers of Land Surface Temperature Based on Multisource Data: A Spatial Econometric Perspective", "url": "", "volume": "14", "year": "2021", "abstract": "Urban thermal condition has seriously affected the quality of residents\u2019 daily life and triggered some environmental issues. Understanding spatial patterns of land surface temperature (LST) and its driving mechanism is important for the sustainable development of cities. Taking Beijing as an example, this study employed spatial econometric models to investigate spatial and temporal heterogeneity of LST from 2014 to 2018 based on multisource remote sensing and statistical data. The global autocorrelation Moran's I index showed the existence of significant positive correlations of LST among regions, indicating the regions with high thermal environments are spatially adjacent. The temperature of a region would increase by more than 0.6% for every 1% increase in LST of surrounding areas based on the spatial Durbin model. In terms of spatial interactions of influencing factors, elevation, normalized difference vegetation index, modified normalized difference water index, nighttime light, and fossil energy consumption of neighbors exhibited significantly positive spatial agglomeration effects on local LST, whereas albedo, GDP, and population density of adjacent areas had negative effects on LST in local areas. Particularly, the indirect effects of drivers were greater than their direct effects, indicating urban thermal condition was an interregional issue and joint control measures should be adopted to mitigate the urban heat island effects as a whole.", "pages": "12263-12272", "note": "", "ISSN": "2151-1535", "publicationtype": "article", "author": ["Liu, Menghang", "Ma, Haitao", "Bai, Yu"]}]["8736742", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2019.2923047", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Interpolation", "Numerical models", "Quality of service", "Wireless communication", "Adaptation models", "Cellular networks", "Data visualization", "Wireless cellular communication", "coverage map", "triangulation method", "linear interpolation"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "Multi-Criteria Coverage Map Construction Based on Adaptive Triangulation-Induced Interpolation for Cellular Networks", "url": "", "volume": "7", "year": "2019", "abstract": "Wireless cellular communications lead to huge demands for estimating and visualizing the data about Quality of Service (QoS) for mobile network operators in the 5th Generation (5G) networks. Constructing a coverage map is an important step to visualize global information about QoS. Inspired by the characteristic of the base station, we present an adaptive triangulation method to divide the region of interest into triangles. Then, we propose a novel area-wise Multi-criteria Triangulation-induced Interpolation (MTI) algorithm which utilizes the linear interpolation to estimate the key performance indicators of the QoS inside a triangle with the known values of its three vertexes, to construct the coverage maps and provide the closed-form solution of the covered region for the multi-criteria problems. We check the accuracy and the efficiency of the MTI algorithm both in 19-cells network scenario and in real big city scenario. The experiment results manifest that the MTI algorithm shows a good performance in constructing the coverage maps and it is significantly lower-cost and higher-efficiency than the traditional point-wise algorithms.", "pages": "80767-80777", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Liu, Yaxi", "Huangfu, Wei", "Zhang, Haijun", "Long, Keping"]}]["8606940", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2019.2891956", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Clustering algorithms", "Qubit", "Partitioning algorithms", "Proteins", "Computer science", "Information technology", "Clustering", "quantum computing", "evolutionary algorithm", "fuzzy set theory", "bioinformatics"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "A Generalized Enhanced Quantum Fuzzy Approach for Efficient Data Clustering", "url": "", "volume": "7", "year": "2019", "abstract": "Data clustering is a challenging task to gain insights into data in various fields. In this paper, an Enhanced Quantum-Inspired Evolutionary Fuzzy C-Means (EQIE-FCM) algorithm is proposed for data clustering. In the EQIE-FCM, quantum computing concept is utilized in combination with the FCM algorithm to improve the clustering process by evolving the clustering parameters. The improvement in the clustering process leads to improvement in the quality of clustering results. To validate the quality of clustering results achieved by the proposed EQIE-FCM approach, its performance is compared with the other quantum-based fuzzy clustering approaches and also with other evolutionary clustering approaches. To evaluate the performance of these approaches, extensive experiments are being carried out on various benchmark datasets and on the protein database that comprises of four superfamilies. The results indicate that the proposed EQIE-FCM approach finds the optimal value of fitness function and the fuzzifier parameter for the reported datasets. In addition to this, the proposed EQIE-FCM approach also finds the optimal number of clusters and more accurate location of initial cluster centers for these benchmark datasets. Thus, it can be regarded as a more efficient approach for data clustering.", "pages": "50347-50361", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Bharill, Neha", "Patel, Om", "Tiwari, Aruna", "Mu, Lifeng", "Li, Dong-Lin", "Mohanty, Manoranjan", "Kaiwartya, Omprakash", "Prasad, Mukesh"]}]["9099843", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2020.2997675", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Task analysis", "Bit error rate", "Context modeling", "Sun", "Sentiment analysis", "Neural networks", "Training data", "Aspect-based sentiment analysis", "natural language processing", "text analysis", "deep learning", "neural network"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "Enhancing Aspect-Based Sentiment Analysis With Capsule Network", "url": "", "volume": "8", "year": "2020", "abstract": "Existing feature-based neural approaches for aspect-based sentiment analysis (ABSA) try to improve their performance with pre-trained word embeddings and by modeling the relations between the text sequence and the aspect (or category), thus heavily depending on the quality of word embeddings and task-specific architectures. Although the recently pre-trained language models, i.e., BERT and XLNet, have achieved state-of-the-art performance in a variety of natural language processing (NLP) tasks, they still subject to the aspect-specific, local feature-aware and task-agnostic challenges. To address these challenges, this paper proposes a XLNet and capsule network based model XLNetCN for ABSA. XLNetCN firstly constructs auxiliary sentence to model the sequence-aspect relation and generate global aspect-specific representations, which enables to enhance aspect-awareness and ensure the full pre-training of XLNet for improving task-agnostic capability. After that, XLNetCN also employs a capsule network with the dynamic routing algorithm to extract the local and spatial hierarchical relations of the text sequence, and yield its local feature representations, which are then merged with the global aspect-related representations for downstream classification via a softmax classifier. Experimental results show that XLNetCN outperforms significantly than the classical BERT, XLNet and traditional feature-based approaches on the two benchmark datasets of SemEval 2014, Laptop and Restaurant, and achieves new state-of-the-art results.", "pages": "100551-100561", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Su, Jindian", "Yu, Shanshan", "Luo, Da"]}]["8902108", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2019.2953835", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Sleep", "Biomedical monitoring", "Monitoring", "Manuals", "Psychology", "Sociology", "Sleep quality", "in-home sensing", "sleep diaries", "polysomnography", "sleep devices", "sleep monitoring", "physiological parameters", "smart health"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "The Meaning of Sleep Quality: A Survey of Available Technologies", "url": "", "volume": "7", "year": "2019", "abstract": "Sleep is an important part of the human daily routine. Restoring sleep is strongly related to a better physical, cognitive, and psychological well-being. By contrast, poor or disordered sleep leads to possible impairments of cognitive and psychological functioning and to a worsened general physical health. In this context, understanding changes in sleep quality becomes a research imperative that leads to the need for the definition of what restoring or quality sleep means. This understanding of what \u201csleep quality\u201d means requires a cross-domain investigation. It arises the need for a comprehensive study that offers a complete taxonomy of sleep monitoring systems, with a focus on sleep quality, and that gives useful insights about which combination of metrics, signals, and sleep variables is the best in relation to different categories of users. The proposed study is focused on systematically categorizing the methods and approaches for sleep quality understanding, with an emphasis on technological approaches, including wearable, on-bed, and actigraphy devices. It offers a systematic review for researchers who are interested in sleep quality identification tasks, and highlights strengths and weaknesses of state-of-the-art metrics and solutions in order to suggest the best choice for new potential research challenges in the field. Another important outcome of the proposed work is the study of the impact on the identified signal metrics and solutions of the different target user populations with their specific user requirements.", "pages": "167374-167390", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Crivello, Antonino", "Barsocchi, Paolo", "Girolami, Michele", "Palumbo, Filippo"]}]["8090525", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2017.2767703", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Servers", "Quality of service", "Clustering algorithms", "Energy consumption", "Heuristic algorithms", "Power demand", "IoT", "energy efficient", "energy saving", "storage cluster", "capacity demand estimation", "capacity provisioning", "QoS", "SLA"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "On-Demand Capacity Provisioning in Storage Clusters Through Workload Pattern Modeling", "url": "", "volume": "5", "year": "2017", "abstract": "Internet of Things (IoT), which is the inter-networking of a wide variety of physical devices, is widely used in our daily life. The exponential increase in the number of diverse devices has resulted in a significant increase in the volume, variety, velocity, and veracity of data (i.e., big data). These data present a large requirement on modern storage systems both for capacity and scale, and energy cost has become a critical problem. For storage clusters, much research effort has been invested in alleviating this problem by providing suitable resource capacity (i.e., on-demand providing). However, it is challenging to match the offered resource capacity with the real system workloads, thus resulting in a violation of service level agreement. By considering a storage cluster as a queueing system, this paper proposes a QoS-oriented capacity provisioning mechanism. Based on workload features, the mechanism models the pattern of current workloads as a suitable queueing model. In accordance with the model, our mechanism can well forecast the actual resource capacity demand without violating the service level agreement, and then offer the required resource capacity in terms of the real workloads. Experimental results demonstrate that the proposed mechanism significantly reduces the energy consumption of a typical storage cluster, while meeting the QoS requirements. It also significantly outperforms two classic and two state-of-the-art capacity provisioning mechanisms.", "pages": "24830-24841", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Hu, Cheng", "Deng, Yuhui", "Yang, Laurence"]}]["9906043", {"address": "", "articleno": "", "doi": "10.26599/TST.2021.9010090", "issn": "1007-0214", "issue_date": "", "journal": "Tsinghua Science and Technology", "keywords": ["Measurement", "Matched filters", "Upper bound", "Event detection", "Heuristic algorithms", "Scalability", "Filtering algorithms", "temporal networks", "graph association rule", "subgraph pattern matching", "graph mining", "big graphs"], "month": "April", "number": "2", "numpages": "", "publisher": "", "title": "Discovering Association Rules with Graph Patterns in Temporal Networks", "url": "", "volume": "28", "year": "2023", "abstract": "Discovering regularities between entities in temporal graphs is vital for many real-world applications (e.g., social recommendation, emergency event detection, and cyberattack event detection). This paper proposes temporal graph association rules (TGARs) that extend traditional graph-pattern association rules in a static graph by incorporating the unique temporal information and constraints. We introduce quality measures (e.g., support, confidence, and diversification) to characterize meaningful TGARs that are useful and diversified. In addition, the proposed support metric is an upper bound for alternative metrics, allowing us to guarantee a superset of patterns. We extend conventional confidence measures in terms of maximal occurrences of TGARs. The diversification score strikes a balance between interestingness and diversity. Although the problem is NP-hard, we develop an effective discovery algorithm for TGARs that integrates TGARs generation and TGARs selection and shows that mining TGARs is feasible over a temporal graph. We propose pruning strategies to filter TGARs that have low support or cannot make top-$k$ as early as possible. Moreover, we design an auxiliary data structure to prune the TGARs that do not meet the constraints during the TGARs generation process to avoid conducting repeated subgraph matching for each extension in the search space. We experimentally verify the effectiveness, efficiency, and scalability of our algorithms in discovering diversified top-$k$ TGARs from temporal graphs in real-life applications.", "pages": "344-359", "note": "", "ISSN": "1007-0214", "publicationtype": "article", "author": ["Huang, Chu", "Zhang, Qianzhen", "Guo, Deke", "Zhao, Xiang", "Wang, Xi"]}]["9772633", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2022.3174351", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Streaming media", "Servers", "Point cloud compression", "Legged locomotion", "Head", "Bit rate", "Quality of experience", "Immersive media", "interactive virtual reality", "video streaming", "omni-directional video", "low-latency", "caching"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "Low Latency Streaming for Path-Walking VR Systems", "url": "", "volume": "10", "year": "2022", "abstract": "Highly immersive content in the form of extended reality (XR) is attracting attention as an alternative to conventional video services such as YouTube and Facebook. Many galleries and museums already offer online virtual reality (VR) tours where users are free to choose the spot they want to move to, beyond merely looking around. Although the ease of implementation, this key-spot hopping is still far from giving the real feeling of walking. Meanwhile, in recent volumetric or light-field-based studies, view rendering that supports free and continuous viewpoint movements has been attempted. With online services in mind, however, the high data volume and computational complexity are a big obstacle to practical applications. Path-walking VR, the target system of this paper, can be a good compromise, where the viewer can enjoy the virtual space while walking along the route. The interactive path-walking VR service is entry-level immersive video, but streaming over the network is still challenging. One of the main problems to be tackled is that the movement patterns of viewers need to be reflected in the streaming strategy to improve the quality of experience. Unlike unidirectional video, the movement of the viewer determines which images and how many images should be transmitted. This paper proposes schemes to reduce streaming delays by reflecting the viewer\u2019s movement characteristics. It is differentiated from existing studies for omnidirectional video in that the proposed schemes control not only image quality but also view update rate. The first is a caching strategy which takes advantage of the geometrical locality of the virtual space that the viewer will soon reach a position close to the current position. This not only reduces the communication delay from the server, but also decreases the burden of server-side request handling. The second scheme uses the relationship between the viewer\u2019s speed and the field of vision. The image quality is adjusted according to the viewer\u2019s speed and head direction. Experimental results show that the proposed schemes achieve stable viewer\u2019s experience by considering walking characteristics in virtual space. It is expected that the results of this paper will provide insight to those who design interactive streaming systems for immersive media applications.", "pages": "50702-50714", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Seo, Won-Ki", "Rhee, Chae"]}]["9828495", {"address": "", "articleno": "", "doi": "10.1109/TCSVT.2022.3190273", "issn": "1558-2205", "issue_date": "", "journal": "IEEE Transactions on Circuits and Systems for Video Technology", "keywords": ["Feature extraction", "Image color analysis", "Distortion", "Robustness", "Transforms", "Image coding", "Visualization", "Image quality assessment", "image hashing", "complementary color wavelet transform (CCWT)", "compressed sensing (CS)", "Sobel operator"], "month": "Nov", "number": "11", "numpages": "", "publisher": "", "title": "Perceptual Hashing With Complementary Color Wavelet Transform and Compressed Sensing for Reduced-Reference Image Quality Assessment", "url": "", "volume": "32", "year": "2022", "abstract": "Image quality assessment (IQA) is an important task of image processing and has diverse applications, such as image super-resolution reconstruction, image transmission and monitoring systems. This paper proposes a perceptual hashing algorithm with complementary color wavelet transform (CCWT) and compressed sensing (CS) for reduced-reference (RR) IQA. The CCWT is exploited to decompose input color image into different sub-bands. Since the calculation of CCWT uses all color channels without discarding any information, the distortions introduced by digital operations on color channels are preserved in the CCWT sub-bands. The block-based CS is used to extract features from the CCWT sub-bands. As the Euclidean distance between the block-based CS features is slightly influenced by content-preserving operations, perceptual features constructed by Euclidean distances are robust, discriminative and compact. Hash sequence is finally determined by quantifying the perceptual features. Effectiveness of the proposed hashing is verified by various experiments on four open image databases. Experimental results demonstrate that the proposed hashing is superior to some state-of-the-art algorithms in terms of classification and RR IQA application.", "pages": "7559-7574", "note": "", "ISSN": "1558-2205", "publicationtype": "article", "author": ["Yu, Mengzhu", "Tang, Zhenjun", "Zhang, Xianquan", "Zhong, Bineng", "Zhang, Xinpeng"]}]["7914634", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2017.2699198", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Array signal processing", "Energy harvesting", "Quality of service", "Bandwidth", "Load management", "Radio access networks", "Wireless communication", "Cloud radio access network (C-RAN)", "energy harvesting", "fronthaul", "decentralization"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "Fronthaul Load Balancing in Energy Harvesting Powered Cloud Radio Access Networks", "url": "", "volume": "5", "year": "2017", "abstract": "Enhanced with wireless power transfer capability, cloud radio access network (C-RAN) enables energy-restrained mobile devices to function uninterruptedly. Beamforming of C-RAN has potential to improve the efficiency of wireless power transfer, in addition to transmission data rates. In this paper, we design the beamforming jointly for data transmission and energy transfer, under finite fronthaul capacity of C-RAN. A non-convex problem is formulated to balance the fronthaul requirements of different remote radio heads (RRHs). Norm approximations and relaxations are carried out to convexify the problem to second-order cone programming (SOCP). To improve the scalability of the design to large networks, we further decentralize the SOCP problem using the alternating direction multiplier method (ADMM). A series of reformulations and transformations are conducted, such that the resultant problem conforms to the state-of-the-art ADMM solver and can be efficiently solved in real time. Simulation results show that the distributed algorithm can remarkably reduce the time complexity without compromising the fronthaul load balancing of its centralized counterpart. The proposed algorithms can also reduce the fronthaul bandwidth requirements by 25% to 50%, compared with the prior art.", "pages": "7762-7775", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Qin, Cheng", "Ni, Wei", "Tian, Hui", "Liu, Ren"]}]["9455417", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2021.3089443", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Economics", "Companies", "Meters", "Meter reading", "Energy consumption", "Supply chains", "Energy demand", "customer profiling", "data engineering", "big data applications", "statistical learning", "pattern analysis"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "Monthly Electricity Demand Patterns and Their Relationship With the Economic Sector and Geographic Location", "url": "", "volume": "9", "year": "2021", "abstract": "In a highly competitive and liberalized energy market, where the retail of electricity is open to many potential companies, it is essential to have tools that help make decisions and guide the design of marketing strategies. In this sense, it is essential for retailers to know the behavior of their customers to correctly define their commercial strategies. One of the most commonly used methods for this is the characterization of their consumption profiles. Fortunately, for regulatory reasons, in some countries, the monthly electricity demand of each customer is openly available to any competitor. This paper explores whether this information, especially the economic sector and geographic location of a client, is useful for determining the client\u2019s demand profile. Specifically, data on electricity demand in Spain from more than 27 million users and for a period of 3 years are analyzed. For this purpose, the electricity consumption of every client is grouped by month and normalized. The resulting demand profiles are later clustered according to different criteria. The main finding of the research is that the combined information on economic activity and location definitely enables prediction of the demand profile. Additionally, profile quality metrics are defined and obtained for the entire dataset. The resulting profiles have a mean dispersion of 10% and a confidence interval of \u00b117%. To clarify the use of these metrics, several examples are detailed, showing how this profile information can be used to improve the marketing decision-making process for electricity retailers.", "pages": "86254-86267", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Luque, Joaquin", "Personal, Enrique", "Garcia-Delgado, Antonio", "Leon, Carlos"]}]["9527089", {"address": "", "articleno": "", "doi": "10.1109/JPHOT.2021.3109016", "issn": "1943-0655", "issue_date": "", "journal": "IEEE Photonics Journal", "keywords": ["Scattering", "Optical scattering", "Optical imaging", "Adaptive optics", "Liquid crystal displays", "Optical reflection", "Optical distortion", "Optical computing", "machine learning", "random media", "feedforward neural networks"], "month": "Oct", "number": "5", "numpages": "", "publisher": "", "title": "Improvement of Image Classification by Multiple Optical Scattering", "url": "", "volume": "13", "year": "2021", "abstract": "Multiple optical scattering occurs when light propagates in a non-uniform medium. During the multiple scattering, images were distorted and the spatial information they carried became scrambled. However, the image information is not lost but presents in the form of speckle patterns (SPs). In this study, we built up an optical random scattering system based on an liquid crystal display (LCD) and an RGB laser source. We found that the image classification can be improved by the help of random scattering which is considered as a feedforward neural network to extracts features from image. Along with the ridge classification deployed on computer, we achieved excellent classification accuracy higher than 94%, for a variety of data sets covering medical, agricultural, environmental protection and other fields. In addition, the proposed optical scattering system has the advantages of high speed, low power consumption, and miniaturization, which is suitable for deploying in edge computing applications.", "pages": "1-5", "note": "", "ISSN": "1943-0655", "publicationtype": "article", "author": ["Gao, Xinyu", "Li, Yi", "Qiu, Yanqing", "Mao, Bangning", "Chen, Miaogen", "Meng, Yanlong", "Zhao, Chunliu", "Kang, Juan", "Guo, Yong", "Shen, Changyu"]}]["9729745", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2022.3157289", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Internet", "Online services", "Encyclopedias", "Training", "Task analysis", "Machine translation", "Buildings", "Dataset", "deep learning", "natural language processing", "Persian", "question answering", "machine reading comprehension"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "PersianQuAD: The Native Question Answering Dataset for the Persian Language", "url": "", "volume": "10", "year": "2022", "abstract": "Developing Question Answering systems (QA) is one of the main goals in Artificial Intelligence. With the advent of Deep Learning (DL) techniques, QA systems have witnessed significant advances. Although DL performs very well on QA, it requires a considerable amount of annotated data for training. Many annotated datasets have been built for the QA task; most of them are exclusively in English. In order to address the need for a high-quality QA dataset in the Persian language, we present PersianQuAD, the native QA dataset for the Persian language. We create PersianQuAD in four steps: 1) Wikipedia article selection, 2) question-answer collection, 3) three-candidates test set preparation, and 4) Data Quality Monitoring. PersianQuAD consists of approximately 20,000 questions and answers made by native annotators on a set of Persian Wikipedia articles. The answer to each question is a segment of the corresponding article text. To better understand PersianQuAD and ensure its representativeness, we analyze PersianQuAD and show it contains questions of varying types and difficulties. We also present three versions of a deep learning-based QA system trained with PersianQuAD. Our best system achieves an F1 score of 82.97% which is comparable to that of QA systems on English SQuAD, made by the Stanford University. This shows that PersianQuAD performs well for training deep-learning-based QA systems. Human performance on PersianQuAD is significantly better (96.49%), demonstrating that PersianQuAD is challenging enough and there is still plenty of room for future improvement. PersianQuAD and all QA models implemented in this paper are freely available.", "pages": "26045-26057", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Kazemi, Arefeh", "Mozafari, Jamshid", "Nematbakhsh, Mohammad"]}]["9761192", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2022.3169174", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Wireless communication", "Data models", "Transmitters", "Propagation losses", "Predictive models", "Mathematical models", "Buildings", "Bi-LSTM", "deep learning", "feature extraction", "fully connected layer", "wireless propagation"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "A Novel Wireless Propagation Model Based on Bi-LSTM Algorithm", "url": "", "volume": "10", "year": "2022", "abstract": "Establishing accurate wireless propagation models is essential for high-quality communications. Aiming at the low accuracy and complexity of the traditional wireless propagation model, a novel accurate wireless propagation model is proposed based on the bi-directional long short-term memory (Bi-LSTM) algorithm of machine learning. The model uses machine learning technology driven by big data and can achieve high real-time performance with low complexity. Also, it can accurately predict the wireless signal coverage intensity in a new environment. To allow the model to accommodate the actual environment of target areas, the propagation model can be dynamically corrected by deep learning and training. The Bi-LSTM is used to describe the relationship between features themselves and the relationship between features and target values of reference signal receiving power (RSRP). The Bi-LSTM is also used to represent the relationship through a full-connection layer to obtain the results so that sufficient parameter space can be provided for the model. The propagation model parameters are searched and fitted through a full-connection optimization. After training and tuning, the model\u2019s predicted value of poor coverage recognition rate (PCRR) can reach 0.2371, while the predicted value of root mean squared error (RMSE) can be 10.4855, which demonstrates the better accuracy of the proposed model.", "pages": "43837-43847", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Yang, Yu", "Wan, Guo", "Tong, Mei"]}]["9320517", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2021.3051051", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Quality of service", "Sports", "Medical services", "Wireless communication", "Communication system security", "Wireless sensor networks", "Privacy", "Sport location", "user clustering", "privacy", "healthcare service", "simhash", "wireless network"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "Sport Location-Based User Clustering With Privacy-Preservation in Wireless IoT-Driven Healthcare", "url": "", "volume": "9", "year": "2021", "abstract": "The gradual prevalence of Internet of Things (IoT) and wireless communication technologies has enabled the wide adoption of various smart devices (e.g., smart watches) in provisioning the healthcare services to massive users. Besides monitoring the real-time health signals or conditions of users, smart devices can also record a series of sport-related user information such as user location information at a certain time point. The location sequence information is valuable to cluster the users who share the similar sport preferences or habits and therefore, is also playing a key role in providing wireless healthcare services to these users. However, the user location information is often sensitive to certain wireless users as they decline to reveal their daily sport behavior patterns to others. In this situation, a natural challenge is raised in securing the sensitive user location information while mining the users\u2019 daily sport behavior patterns and provisioning better healthcare services to the users. Considering this challenge, we take advantage of the well-known SimHash technique to protect users\u2019 location privacy while clustering the users who share similar sport preferences or habits for better healthcare services. At last, we validate the feasibility of the proposal through a set of simulated experiments conducted on a real-world dataset. Reported results demonstrate that our solution performs better than the other two competitive ones while securing user location information.", "pages": "12906-12913", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Zhang, Qiyun", "Zhang, Yuan", "Li, Caizhong", "Yan, Chao", "Duan, Yucong", "Wang, Hao"]}]["9040423", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2020.2981647", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Video surveillance", "Unmanned aerial vehicles", "Smart cities", "Optimization", "Data communication", "Scheduling", "Smart city", "video surveillance", "unmanned aerial vehicle (UAV) cluster", "scheduler", "bin packing", "heterogeneous communication"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "UAV Cluster-Based Video Surveillance System Optimization in Heterogeneous Communication of Smart Cities", "url": "", "volume": "8", "year": "2020", "abstract": "Video surveillance system is the integration of computers, networks, communications, and video CODEC, etc. Because of its distributed architecture, parallel image processing and ease of installation and expansion, it is widely used in many fields such as education, transportation and industry. However, there are some challenges of video surveillance applications in smart cities such as large scale of video events, low quality and big delay of video data transmission, and the loss of video surveillance data integrity. In order to solve the above problems, this paper designs a series of optimization algorithms and scheduling strategies based on Unmanned Aerial Vehicle (UAV) cluster. Firstly, we construct a full device coverage network with UAV cluster in heterogeneous communication environment of smart cities. Secondly, we formulate the scheduling problem of UAV cluster as bi-objective fragile bin packing problem, and design an optimal scheduling algorithm with constant approximation performance ratio. The simulation experimental results fully demonstrate the effectiveness, feasibility and robustness of the proposed solution in terms of system life cycle, video decodable frame rate, the ratio of UAV flight time to system life cycle, throughput and delay.", "pages": "55654-55664", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Jin, Yong", "Qian, Zhenjiang", "Yang, Weiyong"]}]["9351910", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2021.3058582", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Diseases", "Agriculture", "Insects", "Production", "Machine learning algorithms", "Knowledge based systems", "Genetics", "Agricultural DSS", "artificial intelligence", "agricultural knowledge management", "classification of crop diseases", "machine learning", "wheat crop diseases"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "A Generic Approach for Wheat Disease Classification and Verification Using Expert Opinion for Knowledge-Based Decisions", "url": "", "volume": "9", "year": "2021", "abstract": "Crop diseases have mainly affected crop production due to the lack of modern approaches for disease identification. For many years, farmers have identified various crop diseases and have local knowledge about disease management. However, the local knowledge of one agricultural region is not utilized in other regions due to the unavailability of knowledge sharing platforms. Agricultural research also suggests that crop production has mainly decreased due to diseases, methods of cultivation, irrigation, and lack of local agricultural knowledge. In this research, the experience of agricultural experts, farmers, and cultivators is gathered through a crowd-sourced platform. The data is then processed for various disease identification. Hence, timely identification of various crop diseases can benefit farmers to apply relevant management methods. In literature, researchers have proposed various methods for disease management, mostly based on the classification of crop diseases using Machine Learning (ML) algorithms. However, these algorithms are unable to give trustful results due to static data provisioning and the dynamic nature of various diseases in different agricultural regions. Further, the agricultural expert's experience is also not considered in verifying the classification results. To identify the dynamic nature of wheat diseases, we acquired high-quality images and symptoms-based text data from farmers, domain experts, and users using a crowd-sourced platform. Different augmentation techniques were also used to enhance the size of training data. In this paper, a modern generic approach has been proposed for the identification and classification of wheat diseases using Decision Trees (DT) and different deep learning models. Also, results of both algorithms were then verified by domain experts that improved the decision trees accuracy by 28.5%, CNN accuracy by 4.3% (leading to 97.2%), and resulted in decision rules for wheat diseases in a knowledge-based system.", "pages": "31104-31129", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Haider, Waleej", "Rehman, Aqeel-Ur", "Durrani, Nouman", "Rehman, Sadiq"]}]["9931128", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2022.3217480", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Machine learning", "Deep learning", "Measurement", "Data models", "Predictive models", "Codes", "Testing", "Defect density prediction", "deep learning", "data sparsity", "machine learning"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "Software Defect Density Prediction Using Deep Learning", "url": "", "volume": "10", "year": "2022", "abstract": "Delivering a reliable and high-quality software system to client is a big challenge in software development and evolution process. One of the software measures that confirm the quality of the system is the defect density. Practitioners usually need this measure during software development process or during a period of operation to indicate the reliability of software system. However, since predicting defect density before testing the modules is time consuming, managers need to build a prediction model that can help in detecting the defective modules. This process can reduce the testing cost and improve testing resources utilizations. The most intrinsic feature of software defect datasets is the data sparsity in the defect density which might bias the final prediction. Therefore, we use deep learning to build defect density prediction models and handle the inherit challenge of data sparsity in defect density. Deep learning has shown to be effective with sparse data. The constructed model has been evaluated against well-known machine learning methods over 28 public datasets. The obtained results confirmed that the deep learning model is generally more adequate than other machine models over the datasets with high and very high sparsity ratios, and competitive choice when the sparsity ratio is either medium or low.", "pages": "114629-114641", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Alghanim, Firas", "Azzeh, Mohammad", "El-Hassan, Ammar", "Qattous, Hazem"]}]["9792246", {"address": "", "articleno": "", "doi": "10.1109/JIOT.2022.3181665", "issn": "2327-4662", "issue_date": "", "journal": "IEEE Internet of Things Journal", "keywords": ["Deep learning", "Servers", "Training", "Privacy", "Industrial Internet of Things", "Production", "Homomorphic encryption", "Asynchronous deep learning", "homomorphic encryption", "privacy preserving", "proxy re-encryption"], "month": "Nov", "number": "21", "numpages": "", "publisher": "", "title": "Cryptanalysis and Improvement of DeepPAR: Privacy-Preserving and Asynchronous Deep Learning for Industrial IoT", "url": "", "volume": "9", "year": "2022", "abstract": "Industrial Internet of Things (IIoT) is gradually changing the mode of traditional industries with the rapid development of big data. Besides, thanks to the development of deep learning, it can be used to extract useful knowledge from the large amount of data in the IIoT to help improve production and service quality. However, the lack of large-scale data sets will lead to low performance and overfitting of learning models. Therefore, federated deep learning with distributed data sets has been proposed. Nevertheless, the research has shown that federated learning can also leak the private data of participants. In IIoT, once the privacy of participants in some special application scenarios is leaked, it will directly affect national security and people\u2019s lives, such as smart power grid and smart medical care. At present, several privacy-preserving federated learning schemes have been proposed to preserve data privacy of participants, but security issues prevent them from being fully applied. In this article, we analyze the security of the DeepPAR scheme proposed by Zhang et al., and point out that the scheme is insecure in the re-encryption key generation process, which will cause the leakage of the secret key of participants or the proxy server. In addition, the scheme is not resistant to collusion attacks between the parameter server and participants. Based on this, we propose an improved scheme. The security proof shows that the improved scheme solves the security problem of the original scheme and is resistant to collusion attacks. Finally, the security and accuracy of the scheme is illustrated by performance analysis.", "pages": "21958-21970", "note": "", "ISSN": "2327-4662", "publicationtype": "article", "author": ["Chen, Yange", "He, Suyu", "Wang, Baocang", "Duan, Pu", "Zhang, Benyu", "Hong, Zhiyong", "Ping, Yuan"]}]["8327574", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2018.2820164", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Predictive models", "Atmospheric modeling", "Monitoring", "Computational modeling", "Air quality", "Forecasting", "Neural networks", "Internet of Things", "forecasting", "smart cities", "neural networks"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "Improving the Accuracy and Efficiency of PM2.5 Forecast Service Using Cluster-Based Hybrid Neural Network Model", "url": "", "volume": "6", "year": "2018", "abstract": "Information and communication technologies have been widely used to achieve the objective of smart city development. A smart air quality sensing and forecasting system is an important part of a smart city. One of the major challenges in designing such a forecast system is ensuring high accuracy and an acceptable computation time. In this paper, we show that it is possible to accurately forecast fine particulate matter (PM2.5) concentrations with low computation time by using different clustering techniques. An Internet of Things framework comprising of Airbox devices for PM2.5 monitoring has been used to acquire the data. Our main focus is to achieve high forecasting accuracy with reduced computation time. We use a hybrid model to do the forecast and a grid based system to cluster the monitoring stations based on the geographical distance. The experiments and evaluation is done using Airbox devices data from 557 stations deployed all over Taiwan. We are able to demonstrate that a proper clustering based on geographical distance can reduce the forecasting error rate and also the computation time. Also, in order to further evaluate our system, we have applied wavelet-based clustering to group the monitoring stations. A final comparative analysis is done for different clustering schemes with respect to accuracy and computational time.", "pages": "19193-19204", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Mahajan, Sachit", "Liu, Hao-Min", "Tsai, Tzu-Chieh", "Chen, Ling-Jyh"]}]["9773114", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2022.3174595", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Imaging", "Terahertz wave imaging", "Laser beams", "Machine learning", "Ultrafast optics", "Spectroscopy", "Probes", "Terahertz spectrum", "terahertz imaging", "machine learning", "agricultural products", "detection application"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "Machine Learning and Application in Terahertz Technology: A Review on Achievements and Future Challenges", "url": "", "volume": "10", "year": "2022", "abstract": "Terahertz (THz) radiation ( $0.1\\sim 10$ THz) shows great potential in agricultural products detection, biomedical, and security inspection in recent years. Machine learning methods are widely used to support the user demand of higher efficiency and high prediction accuracy. The technological and key challenges of machine learning methods are for THz spectroscopy and image data preprocessing, reconstruction algorithms, and qualitative and quantitative analysis. In this paper, an exhaustive review of recent related works of THz detection and imaging techniques and machine learning methods are presented. The application of machine learning methods combined with THz technology in quality inspection of agricultural products, biomedical, security inspection, and materials science are highlighted. Challenges of machine learning methods for these applications are addressed. The development trend and future perspectives of THz technology are also discussed.", "pages": "53761-53776", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Jiang, Yuying", "Li, Guangming", "Ge, Hongyi", "Wang, Faye", "Li, Li", "Chen, Xinyu", "Lu, Ming", "Zhang, Yuan"]}]["9104973", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2020.2999128", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Sentiment analysis", "Semantics", "Analytical models", "Correlation", "Kernel", "Training", "Visualization", "Image sentiment analysis", "discriminant correlation analysis", "sample-refinement", "cross-modal sentimental semantics", "multidimensional extra evidence mining"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "Multidimensional Extra Evidence Mining for Image Sentiment Analysis", "url": "", "volume": "8", "year": "2020", "abstract": "Image sentiment analysis is a hot research topic in the field of computer vision. However, two key issues need to be addressed. First, high-quality training samples are scarce. There are numerous ambiguous images in the original datasets owing to diverse subjective cognitions from different annotators. Second, the cross-modal sentimental semantics among heterogeneous image features has not been fully explored. To alleviate these problems, we propose a novel model called multidimensional extra evidence mining (ME2M) for image sentiment analysis, it involves sample-refinement and cross-modal sentimental semantics mining. A new soft voting-based sample-refinement strategy is designed to address the former problem, whereas the state-of-the-art discriminant correlation analysis (DCA) model is used to completely mine the cross-modal sentimental semantics among diverse image features. Image sentiment analysis is conducted based on the cross-modal sentimental semantics and a general classifier. The experimental results verify that the ME2M model is effective and robust and that it outperforms the most competitive baselines on two well-known datasets. Furthermore, it is versatile owing to its flexible structure.", "pages": "103619-103634", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Zhang, Hongbin", "Wu, Jinpeng", "Shi, Haowei", "Jiang, Ziliang", "Ji, Donghong", "Yuan, Tian", "Li, Guangli"]}]["9064538", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2020.2987469", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Nanopositioning", "Hysteresis", "Bandwidth", "Robust control", "Uncertainty", "Estimation error", "Nanopositioning", "active disturbance rejection control", "time-varying extended state observer", "variable bandwidth control", "noise attenuation"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "On the Disturbance Rejection of a Piezoelectric Driven Nanopositioning System", "url": "", "volume": "8", "year": "2020", "abstract": "Nanopositioning systems are very popular and playing an increasingly vital role in micro and nano-scale positioning industry due to their unique ability to achieve high-precision and high-speed operation. However, hysteresis, commonly existing in piezoelectric actuators, degrades the precision seriously. Uncertain dynamics and sensor noises also greatly affect the accuracy. To address those challenges, a variable bandwidth active disturbance rejection control (VBADRC) is proposed and realized on a nanopositioning stage. All undesired issues are estimated by a time-varying extended state observer (TESO), and cancelled out by a variable bandwidth controller. Convergence of the TESO, advantages of a TESO over a linear extended state observer (LESO), and the closed-loop stability of the VBADRC are proven theoretically. Improvements of the VBADRC versus the linear active disturbance rejection control (LADRC) are validated by simulations and experiments. Both numerical and experimental results demonstrate that the VBADRC is not only able to provide the same disturbance estimation ability as the LADRC, but also more powerful in noise attenuation and reference tracking.", "pages": "74771-74781", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Wei, Wei", "Xia, Pengfei", "Xue, Wenchao", "Zuo, Min"]}]["9284428", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2020.3042775", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Collaboration", "Art", "Text mining", "Correlation", "Psychology", "Predictive models", "Online education", "preferred learning material", "preference cognotive diagnosis", "knowledge states", "student performance prediction"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "Preference Cognitive Diagnosis for Student Performance Prediction", "url": "", "volume": "8", "year": "2020", "abstract": "Knowledge states modeling is a fundamental issue in online education. One of its tasks is to discover the potential knowledge capacity of students for predicting their performance (i.e., scores on exercises). Current studies either depend on cognitive diagnosis approaches or apply collaborative filtering. However, the prediction accuracy of traditional cognitive diagnosis is insufficient, and collaborative filtering has difficulty ensuring the interpretability of prediction. Actually, students usually read some auxiliary text learning materials that they are interested in, namely, preferred learning material, to consolidate what they have learned. Preference cognitive diagnosis means that the preferred learning materials can reflect the students\u2019 knowledge states (i.e., proficiency for knowledge concepts) to some extent, which is beneficial for predicting students\u2019 performance. Therefore, we propose a preference cognitive diagnosis method (PreferenceCD) to model students\u2019 knowledge states. Specifically, we first design the Direct-Indirect method to acquire students\u2019 preferred learning materials. This method mines important information from students\u2019 reading content that can reflect their preference for learning materials to acquire those preferred learning materials directly. Moreover, it discovers preferred learning materials indirectly by analyzing the similarity of students\u2019 learning behaviors during the reading process. Subsequently, we calculate students\u2019 preference degree for knowledge concepts based on the acquired preferred learning materials and diagnose their proficiency for knowledge concepts by applying a cognitive diagnosis model. After that, we combine the above two aspects to model students\u2019 knowledge states and further predict their scores on exercises. The experimental results on a real-world dataset demonstrate the effectiveness of PreferenceCD with both accuracy and interpretability. The accuracy, root mean square error (RMSE), and mean absolute error (MAE) of PreferenceCD are 0.7614, 0.4805, and 0.2386, respectively, which outperforms related works by about 2-12% in terms of these evaluation metrics.", "pages": "219775-219787", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Jiang, Peichao", "Wang, Xiaodong"]}]["9336017", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2021.3054652", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Wireless communication", "Base stations", "System performance", "Simulation", "Massive MIMO", "Unmanned aerial vehicles", "User experience", "Cell-free massive MIMO", "mobile base station", "trajectory optimization", "power allocation"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "Trajectory Optimization and Power Allocation Algorithm in MBS-Assisted Cell-Free Massive MIMO Systems", "url": "", "volume": "9", "year": "2021", "abstract": "As the mobile networks become even denser and the traffic demand is increasing drastically, it is becoming more heterogeneous. Recently, it is intensive to investigate the Cell-Free Massive multiple-input multiple-output Massive (MIMO) system, where a large number of access points (APs) simultaneously serve a much smaller number of users, and the APs and users are clustered to provide good service for all users. This paper designs a mobile base station (MBS)-assisted Cell-Free Massive MIMO system: Utilizing an MBS deployed on the unmanned aerial vehicle (UAV) to form an air-ground heterogeneous system with the Cell-Free Massive MIMO system and offload part of traffic to the MBS to further improve the system performance. To provide better service to all users, this paper design an MBS flight trajectory optimal method to improve the wireless coverage performance and user experience, and proposed a joint power allocation algorithm based on the consideration of the fairness of the service quality. The simulation results indicate that the performance of the system designed in this paper has a significant improvement compared with the normal Cell-Free Massive MIMO system.", "pages": "30417-30425", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["An, Jue", "Zhao, Feng"]}]["8935233", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2019.2960374", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Tools", "Support vector machines", "Zinc", "Ions", "Protein sequence", "Ensemble", "imbalanced classification", "sample-weighted", "probabilistic neural network", "support vector machine", "zinc-binding sites"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "A Novel Prediction Method for Zinc-Binding Sites in Proteins by an Ensemble of SVM and Sample-Weighted Probabilistic Neural Network", "url": "", "volume": "7", "year": "2019", "abstract": "In the prediction of zinc-binding sites in proteins, there are few real binding-site residues, whereas most residues are non-binding-site residues, resulting in a typical imbalanced classification problem. This paper proposes a novel method, SSWPNN (an ensemble of support vector machine and sample-weighted probabilistic neural network), based on downsampling and an ensemble of different classifiers, in view of the imbalance of zinc-binding sites in proteins. Multiple random downsampling techniques without replacement are performed on the whole set, and the support vector machine is trained as the base classifier on each subset to calculate the weights of samples, while the sample-weighted probabilistic neural network is constructed as a strong classifier for prediction. The experimental results showed that our method is superior to other methods not only in the overall prediction performance for the four types of residues but also in the prediction performance for any type of residue. The results of experimental testing on an independent test set collected by the authors in recent years showed that our method achieved better prediction performance than others not only for the four types of residues overall but also for any one type of residue. In addition, the importance of the features selected by the method is analyzed by reducing certain feature to calculate the scores of the performance index. The source code and datasets are available at http://net.jitsec.cn:88/UploadedImages/SSWPNN.rar.", "pages": "186147-186157", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Li, Hui", "Pi, Dechang", "Chen, Chuanming", "Li, Hongyi"]}]["9878326", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2022.3204812", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Transformers", "Image edge detection", "Quality of service", "Denial-of-service attack", "Cloud computing", "Computer crime", "Throughput", "Reinforcement learning", "Reinforced transformer learning", "VSI-DDoS", "edge clouds", "QoS/QoE", "cloud applications"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "Reinforced Transformer Learning for VSI-DDoS Detection in Edge Clouds", "url": "", "volume": "10", "year": "2022", "abstract": "Edge-driven software applications often deployed as online services in the cloud-to-edge continuum lack significant protection for services and infrastructures against emerging cyberattacks. Very-Short Intermittent Distributed Denial of Service (VSI-DDoS) attack is one of the biggest factors for diminishing the Quality of Services (QoS) and Quality of Experiences (QoE) for users on edge. Unlike conventional DDoS attacks, these attacks live for a very short time (on the order of a few milliseconds) in the traffic to deceive users with a legitimate service experience. To provide protection, we propose a novel and efficient approach for detecting VSI-DDoS attacks using reinforced transformer learning that mitigates the tail latency and service availability problems in edge clouds. In the presence of attacks, the users\u2019 demand for availing ultra-low latency and high throughput services deployed on the edge, can never be met. Moreover, these attacks send very-short intermittent requests towards the target services that enforce longer delays in users\u2019 responses. The assimilation of transformer with deep reinforcement learning accelerates detection performance under adverse conditions by adapting the dynamic and the most discernible patterns of attacks (e.g., multiplicative temporal dependency, attack dynamism). The extensive experiments with testbed and benchmark datasets demonstrate that the proposed approach is suitable, effective, and efficient for detecting VSI-DDoS attacks in edge clouds. The results outperform state-of-the-art methods with $0.9\\%-3.2\\%$ higher accuracy in both datasets.", "pages": "94677-94690", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Bhutto, Adil", "Vu, Xuan", "Elmroth, Erik", "Tay, Wee", "Bhuyan, Monowar"]}]["9051814", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2020.2984720", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Entropy", "Gray-scale", "Optimization methods", "Tools", "Weight measurement", "Directed graphs", "Superpixel", "boundary optimization", "framework", "information measure function", "weighted directed graph"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "A Superpixel Boundary Optimization (SBO) Framework Based on Information Measure Function", "url": "", "volume": "8", "year": "2020", "abstract": "Superpixel is an essential tool for computer vision. In practice, classic superpixel algorithms do not exhibit good boundary adherence with fewer superpixels, which will greatly hamper further analysis. To remedy the defect, a superpixel boundary optimization framework is proposed in this paper. There are three steps in the framework. Firstly, based on the proposed information measure function, the under-segmented superpixels generated by classic superpixel algorithms are screened out. Secondly, with the two invariant centroids method, these under-segmented superpixels are re-segmented to improve the accuracy in boundary adherence. Finally, smaller superpixels are merged to maintain the same number with initial superpixels. Quantitative evaluations on the BSDS500 exhibit that the performance of the classic superpixel algorithms is improved by employing the framework, especially on the condition of fewer superpixels.", "pages": "64783-64798", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Liu, Guoqi", "Li, Xusheng", "Chang, Baofang", "Dong, Yifei"]}]["8315009", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2018.2815578", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Games", "Data centers", "Sensors", "Cloud computing", "Quality of service", "Virtualization", "Heuristic algorithms", "Sensor Cloud", "game theory", "incentive mechanism", "two-stage game approach", "Internet of Everything"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "An Effective Sensor Cloud Control Scheme Based on a Two-Stage Game Approach", "url": "", "volume": "6", "year": "2018", "abstract": "Motivated by complementing the ubiquitous sensor networks and cloud computing technique, a lot of attentions have been drawn to Sensor-Cloud (SC). The idea of SC thrives on the principle of virtualization of physical sensor nodes and has introduced the intermediate processing between physical sensor nodes and end users. This paper proposes an efficient interactive SC control scheme to provide on-demand sensing services for multiple applications. By adopting the game theory, we develop a new two-stage game model, which consists of a judicious mixture of selection and incentive algorithms. In our game model, a user can choose the most adaptable data center to execute its task, and each data center can give appropriate incentives to the participating sensors. The main merit possessed by our two-stage game approach is to shed light on the practical SC control problem while providing excellent adaptability and flexibility to satisfy the different application requirements. To the best of our knowledge, this is the first work to include a novel incentive algorithm in the SC system. Simulation results demonstrate that our approach can outperform existing schemes by about 5%~15% in terms of the normalized user profit, service delay, and SC system throughput. Finally, we discuss future directions for designing SC control frameworks including other issues.", "pages": "20430-20439", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Kim, Sungwook"]}]["8876654", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2019.2948269", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Blockchain", "Internet of Things", "Monitoring", "Supply chains", "Industries", "Security", "Blockchain", "internet of things", "cyber-physical systems", "logistics", "p2p network", "smart contract"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "Estimating Service Quality in Industrial Internet-of-Things Monitoring Applications With Blockchain", "url": "", "volume": "7", "year": "2019", "abstract": "Internet of Things (IoT) plays a big role in automating information generation and consumption in industrial monitoring applications. Blockchain can allow this information to be stored in a manner that is both accessible and reliable for the IoT devices to work with. Blockchain has the capability to collect data from IoT devices and store it in a distributed manner that prevents tampering with the data. This paper discusses the use of blockchain to calculate the Service Quality (SQ) in an Industrial IoT for monitoring application. The proposed framework looks at the blockchain as a finite number of fragmented pieces of data corresponding to a specific industrial process. The SQ is expressed as penalties which is the difference between the expected IoT sensor values and the actual sensor data in reported events from the IoT devices. It also moderates the penalty between similar industrial processes based on each other. The moderation allows better understanding of the system functions and identification of specific problems rather than simply recording the sensor data for a single process. Furthermore, this paper analyzes private blockchains for suitability in IIoT and summarizes some key challenges for IoT to be used with blockchain in context of the proposed framework. The paper uses supply chain as a use case scenario for describing the proposed framework and presents results on its technical feasibility.", "pages": "155489-155503", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Maiti, Ananda", "Raza, Ali", "Kang, Byeong", "Hardy, Lachlan"]}]["9247392", {"address": "", "articleno": "", "doi": "10.1109/JPROC.2020.3032074", "issn": "1558-2256", "issue_date": "", "journal": "Proceedings of the IEEE", "keywords": ["Smart manufacturing", "Digital systems", "Intelligent sensors", "Process control", "Sensor systems", "Service robots", "Computer crime", "Fourth Industrial Revolution", "Robot sensing systems", "Machine components", "Virtual manufacturing", "Digital manufacturing (DM)"], "month": "April", "number": "4", "numpages": "", "publisher": "", "title": "A Survey of Cybersecurity of Digital Manufacturing", "url": "", "volume": "109", "year": "2021", "abstract": "The Industry 4.0 concept promotes a digital manufacturing (DM) paradigm that can enhance quality and productivity, which reduces inventory and the lead time for delivering custom, batch-of-one products based on achieving convergence of additive, subtractive, and hybrid manufacturing machines, automation and robotic systems, sensors, computing, and communication networks, artificial intelligence, and big data. A DM system consists of embedded electronics, sensors, actuators, control software, and interconnectivity to enable the machines and the components within them to exchange data with other machines, components therein, the plant operators, the inventory managers, and customers. This article presents the cybersecurity risks in the emerging DM context, assesses the impact on manufacturing, and identifies approaches to secure DM.", "pages": "495-516", "note": "", "ISSN": "1558-2256", "publicationtype": "article", "author": ["Mahesh, Priyanka", "Tiwari, Akash", "Jin, Chenglu", "Kumar, Panganamala", "Reddy, A.", "Bukkapatanam, Satish", "Gupta, Nikhil", "Karri, Ramesh"]}]["9078126", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2020.2990186", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Photonics", "Resource management", "Telecommunications", "Multiplexing", "Optical fiber networks", "Media", "Quantum key distribution", "spatial division multiplexing", "telecommunications", "communication system security"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "Telecommunication Compatibility Evaluation for Co-existing Quantum Key Distribution in Homogenous Multicore Fiber", "url": "", "volume": "8", "year": "2020", "abstract": "Quantum key distribution (QKD) is regarded as an alternative to traditional cryptography methods for securing data communication by quantum mechanics rather than computational complexity. Towards the massive deployment of QKD, embedding it with the telecommunication system is crucially important. Homogenous optical multi-core fibers (MCFs) compatible with spatial division multiplexing (SDM) are essential components for the next-generation optical communication infrastructure, which provides a big potential for co-existence of optical telecommunication systems and QKD. However, the QKD channel is extremely vulnerable due to the fact that the quantum states can be annihilated by noise during signal propagation. Thus, investigation of telecom compatibility for QKD co-existing with high-speed classical communication in SDM transmission media is needed. In this paper, we present analytical models of the noise sources in QKD links over heterogeneous MCFs. Spontaneous Raman scattering and inter-core crosstalk are experimentally characterized over spans of MCFs with different refractive index profiles, emulating shared telecom traffic conditions. Lower bounds for the secret key rates and quantum bit error rate (QBER) due to different core/wavelength allocation are obtained to validate intra- and inter-core co-existence of QKD and classical telecommunication.", "pages": "78836-78846", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Lin, Rui", "Udalcovs, Aleksejs", "Ozolins, Oskars", "Pang, Xiaodan", "Gan, Lin", "Tang, Ming", "Fu, Songnian", "Popov, Sergei", "Silva, Thiago", "Xavier, Guilherme", "Chen, Jiajia"]}]["8886413", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2019.2950171", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Feature extraction", "Benchmark testing", "Deep learning", "Scalability", "Predictive models", "Software", "Scalable mutation testing", "static analysis", "deep learning", "binary classification"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "Scalable Mutation Testing Using Predictive Analysis of Deep Learning Model", "url": "", "volume": "7", "year": "2019", "abstract": "Software testing plays a crucial role in ensuring the quality of software systems. Mutation testing is designed to measure the adequacy of test suites by detecting artificially induced software faults. Despite their potential, the expensive cost and the scalability of mutation testing with large programs is a big obstacle in its practical use. The selective mutation has been widely investigated and considered to be an effective approach to reduce the cost of mutation testing. In the case of large programs where source code has hundreds of classes and more than 10 KLOC lines of code, the selective mutation can still generate thousands of mutants. Executing each mutant against the test suite is cost-intensive in terms of robustness, resource usage, and computational cost. In this paper, we introduce a new approach to extract features from mutant programs based on mutant killing conditions, i.e. reachability, necessity and sufficiency along with mutant significance and test suite metrics to extract features from mutant programs. A deep learning Keras model is proposed to predict killed and alive mutants from each program. First, the features are extracted using the Eclipse JDT library and program dependency analysis. Second, preprocessing techniques such as Principal Component Analysis and Synthetic Minority Oversampling are used to reduce the high dimensionality of data and to overcome the imbalanced class problem respectively. Lastly, the deep learning model is optimized using fine-tune parameters such as dropout and dense layers, activation function, error and loss rate respectively. The proposed work is analyzed on five opensource programs from GitHub repository consisting of thousands of classes and LOC. The experimental results are appreciable in terms of effectiveness and scalable mutation testing with a slight loss of accuracy.", "pages": "158264-158283", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Naeem, Muhammad", "Lin, Tao", "Naeem, Hamad", "Ullah, Farhan", "Saeed, Saqib"]}]["9913697", {"address": "", "articleno": "", "doi": "10.1109/TVT.2022.3212704", "issn": "1939-9359", "issue_date": "", "journal": "IEEE Transactions on Vehicular Technology", "keywords": ["Shadow mapping", "Safety", "Vehicular ad hoc networks", "Automobiles", "Roads", "Vehicle-to-everything", "Mathematical models", "C-V2X", "LTE Sidelink Mode-4", "5G-NR Sidelink Mode-2a", "Autonomous Mode", "Smart Beamforming", "V2V Communications", "Relaying", "Cooperative V2X Communications", "Big Vehicle Shadowing"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "Cellular V2X Communications in the Presence of Big Vehicle Shadowing: Performance Analysis and Mitigation", "url": "", "volume": "", "year": "2022", "abstract": "In Intelligent Transportation Systems (ITS), vehicular networks are enabling technologies for onboard data services such as traffic safety, user infotainment, etc. Vehicular networks face many challenges when it comes to providing satisfactory quality of service, mainly because of issues that arise from unreliable communication in unfavorable propagation conditions. One prime example is Vehicle-to-Vehicle (V2V) communication in the presence of big vehicles that present obstacles in the communication path of smaller vehicles, where the signal strength decays drastically due to the big vehicle shadowing. As a result, the communication range is shortened, and the safety message dissemination capability is reduced. In this paper, we analyze the impact of big vehicle shadowing on V2V communications, taking into account Cellular Vehicle-to-Everything (C-V2X) networks. A geometric as well as a stochastic approach is employed to analyze the length of shadow regions for conventional cars and big vehicles on the road in different scenarios. This paper analyses the effect of large vehicles shadowing on a V2V communication link as a function of the shadow region length. A beamforming-based signal reception technique is proposed in order to mitigate packet collisions caused by hidden nodes. Considering relaying operation by big vehicles, three relaying schemes are proposed to improve the V2V message dissemination performance. Extensive simulations are conducted to demonstrate the effectiveness of the proposed schemes.", "pages": "1-13", "note": "", "ISSN": "1939-9359", "publicationtype": "article", "author": ["Nguyen, Hieu", "Noor-A-Rahim, Md.", "Guan, Yong", "Pesch, Dirk"]}]["8713985", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2019.2916548", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["NOMA", "Security", "Array signal processing", "Light emitting diodes", "Silicon carbide", "Quality of service", "Visible light communication", "NOMA", "visible light communication", "physical-layer secrecy", "optimal beamforming"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "Secure Transmission for Downlink NOMA Visible Light Communication Networks", "url": "", "volume": "7", "year": "2019", "abstract": "In this paper, we consider the two key problems in the physical-layer security of nonorthogonal multiple access (NOMA) visible light communication (VLC) networks: investigating a closed-form achievable security rate and studying the optimal security beamforming design. Specifically, under the dimming control, practical power, and successive interference cancellation constraints, we derive both the outer and inner bounds of the security capacity region with closed-form expressions, which are evaluated via numerical results. Then, based on the proposed security-rate expression, we investigated the optimal security beamforming design to minimize the total LED power, and to maximize the minimum secrecy rate, respectively. Both the problems are nonconvex. We apply different relaxation techniques to efficiently solve them. The simulation results demonstrate the efficacy of the proposed security beamforming design schemes in the NOMA VLC networks.", "pages": "65332-65341", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Du, Chun", "Zhang, Fan", "Ma, Shuai", "Tang, Yixiao", "Li, Hang", "Wang, Hongmei", "Li, Shiyin"]}]["9138393", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2020.3008525", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Ultrasonic imaging", "Generative adversarial networks", "Feature extraction", "Training", "Gallium nitride", "Extended field of view ultrasound sonography", "gray enhancement", "generative adversarial network", "super-resolution", "image registration"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "Dual-Enhanced Registration for Field of View Ultrasound Sonography", "url": "", "volume": "8", "year": "2020", "abstract": "Extended Field of View Ultrasound Sonography (EFOV-US) uses the existing ultrasound images for image stitching, so as to display the shape and scope of organ occupation and the relationship with surrounding tissues comprehensively. However, there are still some problems in Extended Field of View Ultrasound Sonography, such as matching error and unstable quality of image stitching. In view of these problems, we propose Dual-enhanced EFOV-US method that overcomes the limitation and produces higher quality results. Firstly, the gray enhancement method is used to improve the image contrast and reduce the noise interference. Then the super-resolution method based on the generative adversarial network is used to improve the resolution of the ultrasonic image further and increase the number of feature point matching between stitching images. The high quality ultrasound wide-range image is gotten by stitching and fusing the double enhanced image. The experimental results show that the proposed method is effective and practical.", "pages": "128602-128612", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Fan, Zijia", "Wang, Zhongyang", "Xin, Junchang", "Wang, Zhiqiong", "Liu, Lu", "Zhang, Xia", "Liu, Jiren"]}]["9598922", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2021.3124814", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Strips", "Steel", "Image edge detection", "Feature extraction", "Decoding", "Computational modeling", "Object detection", "Salient object detection", "surface defects", "multi-level feature", "fusion", "edge"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "Edge-Aware Multi-Level Interactive Network for Salient Object Detection of Strip Steel Surface Defects", "url": "", "volume": "9", "year": "2021", "abstract": "The performance of the salient object detection of strip surface defects has been promoted largely by deep learning based models. However, due to the complexity of strip surface defects, the existing models perform poorly in the challenging scenes such as noise disturbance, and low contrast between defect regions and background. Meanwhile, the detection results of existing models often suffer from coarse boundary details. Therefore, we propose a novel saliency model, namely an Edge-aware Multi-level Interactive Network, to detect the defects from the strip steel surface. Concretely, our model adopts the U-shape architecture where the two crucial points are the interactive feature integration and the edge-guided saliency fusion. Firstly, except the skip connection that combines the same stage of encoder and decoder, we deploy another connection, where the features from adjacent levels of encoder are transferred to the same stage of decoder. By this way, we are able to provide an effective fusion of multi-level deep features, yielding a well depiction for defects. Secondly, to give well-defined boundaries for prediction results, we add the edge extraction branch after each decoder block, where the progressive feature aggregation endows the edge with precise details and complete object cues. Meanwhile, together with the edge extraction branches, we deploy the saliency prediction branch at each decoder stage. After that, coupled with the fine edge information, we fuse all outputs of saliency prediction branches into the final saliency map, where the edge cue steers the saliency result to pay more attention to the boundary details. Following this way, we can provide a high-quality saliency map which can accurately locate and segment the defects. Extensive experiments are performed on the public dataset, and the results prove the effectiveness and robustness of our model which consistently outperforms the state-of-the-art models.", "pages": "149465-149476", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Zhou, Xiaofei", "Fang, Hao", "Fei, Xiaobo", "Shi, Ran", "Zhang, Jiyong"]}]["8694988", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2019.2912505", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Quality of service", "Predictive models", "Collaboration", "Data models", "Computational modeling", "Solid modeling", "Service-oriented tensor", "service collaboration", "service recommendation", "QoS prediction", "tensor decomposition"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "Personalized QoS Prediction for Service Recommendation With a Service-Oriented Tensor Model", "url": "", "volume": "7", "year": "2019", "abstract": "Quality of Service (QoS) value is usually unknown in service recommendation practice. There are some matrix factorization approaches for predicting the unknown value with a user-service model, which uses a single collaboration with the user's neighbor when looking for different services. However, the QoS value is highly related to the service provider and participants. The services are considered in various collaboration based on different users. By considering the context of services, this paper proposes a QoS prediction model using tensor decomposition based on service collaboration called Service-oriented Tensor (SOT). The prediction approach analyzes service collaboration from other similar services and relevant users by using a three-order tensor. Compared with the traditional model, the experiment results show that the proposed model achieves better prediction accuracy.", "pages": "55721-55731", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Guo, Lantian", "Mu, Dejun", "Cai, Xiaoyan", "Tian, Gang", "Hao, Fei"]}]["9373354", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2021.3064700", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Internet of Things", "Encryption", "Forward error correction", "Image coding", "Sensors", "Wireless communication", "Wireless sensor networks", "Error robust encryption", "joint compression and error recovery", "projection matrix", "wireless body area network", "resource-constrained", "Industrial Internet of Things"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "A Novel Efficient Secure and Error-Robust Scheme for Internet of Things Using Compressive Sensing", "url": "", "volume": "9", "year": "2021", "abstract": "In most of existing Internet of Things (IoT) applications, data compression, data encryption and error/erasure correction are implemented separately. To achieve reliable communication, in particular, in harsh wireless environment with strong interference, error/erasure correction codes with higher correction capability or Automatic repeat request (ARQ) scheme are desirable but at the cost of increasing complexity and energy consumption. Due to resource-constrained IoT device, it is often challenging to implement all of them. In this paper, we propose a novel lightweight efficient secure error-robust scheme, ENCRUST, which is able to achieve these three functions using simple matrix multiplication. ENCRUST is built on the new theoretical foundation of projection-based encoding presented in this paper, by leveraging the sparsity inherent in the signal. We perform theoretical analysis and experimental study of the proposed scheme in comparison with the conventional schemes. It shows that the proposed scheme can work in low SINR range and the reconstructed signal quality shows graceful degradation. Furthermore, we apply the proposed scheme on real-life electrocardiogram (ECG) dataset and images. The results demonstrate that ENCRUST achieves decent compression, information secrecy as well as strong error recovery in one go.", "pages": "40903-40914", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Kuldeep, Gajraj", "Zhang, Qi"]}]["8794499", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2019.2934633", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Recommender systems", "Semantics", "Motion pictures", "Semantic Web", "Encyclopedias", "Electronic publishing", "Artificial intelligence", "recommender systems", "collaborative filtering", "matrix factorization", "explanations", "semantic web"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "Mining Semantic Knowledge Graphs to Add Explainability to Black Box Recommender Systems", "url": "", "volume": "7", "year": "2019", "abstract": "Recommender systems are being increasingly used to predict the preferences of users on online platforms and recommend relevant options that help them cope with information overload. In particular, modern model-based collaborative filtering algorithms, such as latent factor models, are considered state-of-the-art in recommendation systems. Unfortunately, these black box systems lack transparency, as they provide little information about the reasoning behind their predictions. White box systems, in contrast, can, by nature, easily generate explanations. However, their predictions are less accurate than sophisticated black box models. Recent research has demonstrated that explanations are an essential component in bringing the powerful predictions of big data and machine learning methods to a mass audience without compromising trust. Explanations can take a variety of formats, depending on the recommendation domain and the machine learning model used to make predictions. The objective of this work is to build a recommender system that can generate both accurate predictions and semantically rich explanations that justify the predictions. We propose a novel approach to build an explanation generation mechanism into a latent factor-based black box recommendation model. The designed model is trained to learn to make predictions that are accompanied by explanations that are automatically mined from the semantic web. Our evaluation experiments, which carefully study the trade-offs between the quality of predictions and explanations, show that our proposed approach succeeds in producing explainable predictions without a significant sacrifice in prediction accuracy.", "pages": "110563-110579", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Alshammari, Mohammed", "Nasraoui, Olfa", "Sanders, Scott"]}]["9254094", {"address": "", "articleno": "", "doi": "10.1109/JSTARS.2020.3036896", "issn": "2151-1535", "issue_date": "", "journal": "IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing", "keywords": ["Cloud computing", "Hyperspectral imaging", "Task analysis", "Scheduling", "Scheduling algorithms", "Energy consumption", "Computational modeling", "Energy consumption", "hyperspectral image classification", "makespan", "multiobjective optimization", "task scheduling"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "Multiobjective Task Scheduling for Energy-Efficient Cloud Implementation of Hyperspectral Image Classification", "url": "", "volume": "14", "year": "2021", "abstract": "Cloud computing has become a promising solution to efficient processing of remotely sensed big data, due to its high-performance and scalable computing capabilities. However, existing cloud solutions generally involve the problems of low resource utilization and high energy consumption when processing large-scale remote sensing datasets, affecting the quality-of-service of the cloud system. Aiming at hyperspectral image classification applications, this article proposes an energy-efficient cloud implementation by employing a multiobjective task scheduling algorithm. We first present a parallel computing mechanism for a fusion-based classification method based on Apache Spark. With the general classification flow represented by a workflow model, we formulate a multiobjective scheduling framework that jointly minimizes the total execution time as well as energy consumption. We further develop an effective scheduling algorithm to solve the multiobjective optimization problem and produce a set of Pareto-optimal solutions, providing the tradeoff between computational efficiency and energy efficiency. Experimental results demonstrate that the multiobjective scheduling approach proposed in this work can substantially reduce the execution time and energy consumption for performing large-scale hyperspectral image classification on Spark. In addition, our proposed algorithm can generate better tradeoff solutions to the multiobjective scheduling problem as compared to competing scheduling algorithms.", "pages": "587-600", "note": "", "ISSN": "2151-1535", "publicationtype": "article", "author": ["Sun, Jin", "Li, Heng", "Zhang, Yi", "Xu, Yang", "Zhu, Yaoqin", "Zang, Qitao", "Wu, Zebin", "Wei, Zhihui"]}]["9143071", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2020.3010033", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Twitter", "Feature extraction", "Natural language processing", "Monitoring", "Windows", "COVID-19", "Natural language processing", "topic tracking", "topic detection", "social network analysis", "text mining", "COVID-19", "infodemiology", "infoveillance"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "An Infoveillance System for Detecting and Tracking Relevant Topics From Italian Tweets During the COVID-19 Event", "url": "", "volume": "8", "year": "2020", "abstract": "The year 2020 opened with a dramatic epidemic caused by a new species of coronavirus that soon has been declared a pandemic by the WHO due to the high number of deaths and the critical mass of worldwide hospitalized patients, of order of millions. The COVID-19 pandemic has forced the governments of hundreds of countries to apply several heavy restrictions in the citizens\u2019 socio-economic life. Italy was one of the most affected countries with long-term restrictions, impacting the socio-economic tissue. During this lockdown period, people got informed mostly on Online Social Media, where a heated debate followed all main ongoing events. In this scenario, the following study presents an in-depth analysis of the main emergent topics discussed during the lockdown phase within the Italian Twitter community. The analysis has been conducted through a general purpose methodological framework, grounded on a biological metaphor and on a chain of NLP and graph analysis techniques, in charge of detecting and tracking emerging topics in Online Social Media, e.g. streams of Twitter data. A term-frequency analysis in subsequent time slots is pipelined with nutrition and energy metrics for computing hot terms by also exploiting the tweets quality information, such as the social influence of the users. Finally, a co-occurrence analysis is adopted for building a topic graph where emerging topics are suitably selected. We demonstrate via a careful parameter setting the effectiveness of the topic tracking system, tailored to the current Twitter standard API restrictions, in capturing the main sociopolitical events that occurred during this dramatic phase.", "pages": "132527-132538", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["De, Enrico", "Martino, Alessio", "Rizzi, Antonello"]}]["9335934", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2021.3054822", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Training", "Nash equilibrium", "Generators", "Standards", "Generative adversarial networks", "Games", "Gallium nitride", "Generative adversarial networks", "Nash equilibrium", "Lipschitz constraint"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "Constrained Generative Adversarial Networks", "url": "", "volume": "9", "year": "2021", "abstract": "Generative Adversarial Networks (GANs) are a powerful subclass of generative models. Yet, how to effectively train them to reach Nash equilibrium is a challenge. A number of experiments have indicated that one possible solution is to bound the function space of the discriminator. In practice, when optimizing the standard loss function without limiting the discriminator's output, the discriminator may suffer from lack of convergence. To be able to reach the Nash equilibrium in a faster way during training and obtain better generative data, we propose constrained generative adversarial networks, GAN-C, where a constraint on the discriminator's output is introduced. We theoretically prove that our proposed loss function shares the same Nash equilibrium as the standard one, and our experiments on mixture of Gaussians, MNIST, CIFAR-10, STL-10, FFHQ, and CAT datasets show that our loss function can better stabilize training and yield even better high-quality images.", "pages": "19208-19218", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Chao, Xiaopeng", "Cao, Jiangzhong", "Lu, Yuqin", "Dai, Qingyun", "Liang, Shangsong"]}]["8648375", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2019.2900719", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Electrocardiography", "Image segmentation", "Biomedical monitoring", "Time-frequency analysis", "Continuous wavelet transforms", "Wearable ECG", "signal quality assessment (SQA)", "convolutional neural network (CNN)", "modified frequency slice wavelet transform (MFSWT)"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "Noise Rejection for Wearable ECGs Using Modified Frequency Slice Wavelet Transform and Convolutional Neural Networks", "url": "", "volume": "7", "year": "2019", "abstract": "Progress in wearable techniques makes the long-term daily electrocardiogram (ECG) monitoring possible. However, the long-term wearable ECGs can be significantly contaminated by various noises, which affect the detection and diagnosis of cardiovascular diseases (CVDs). The situation becomes more serious for wearable ECG screening, where the data are huge, and doctors have no way to visually check the signal quality episode-by-episode. Therefore, automatic and accurate noise rejection for the wearable big-data ECGs is craving. This paper addressed this issue and proposed a noise rejection method for wearable ECGs based on the combination of modified frequency slice wavelet transform (MFSWT) and convolutional neural network (CNN). Wearable ECGs were recorded using the newly developed 12-lead Lenovo smart ECG vest with a sample rate of 500 Hz and a resolution of 16 bit. One thousand 10-s ECG segments were picked up and were manually labeled into three quality types: clinically useful segments with good signal quality (type A), clinically useful segments with poor signal quality (type B), and clinically useless segments (pure noises, type C). Each of the 1,000 10-s ECG segments were transformed into a 2-D time-frequency (T-F) image using the MFSWT, with a pixel size of 200\u00d750. Then, the 2-D grayscale images from MFSWT were fed into a 13-layer CNN model for training the classification models. Results from the standard 5-folder cross-validation showed that the proposed combination method of MFSWT and CNN achieved a highest classification accuracy of 86.3%, which was higher than the comparable methods from continuous wavelet transform (CWT) and artificial neural networks (ANN). The combination of MFSWT and CNN also had a good calculation efficiency. This paper indicated that the combination of MFSWT and CNN is a potential method for automatic identification of noisy segments from wearable ECG recordings.", "pages": "34060-34067", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Zhao, Zhongyao", "Liu, Chengyu", "Li, Yaowei", "Li, Yixuan", "Wang, Jingyu", "Lin, Bor-Shyh", "Li, Jianqing"]}]["9130052", {"address": "", "articleno": "", "doi": "10.1109/TGRS.2020.3003421", "issn": "1558-0644", "issue_date": "", "journal": "IEEE Transactions on Geoscience and Remote Sensing", "keywords": ["Strain", "Fading channels", "Time series analysis", "Synthetic aperture radar", "Systematics", "Decorrelation", "Moisture", "Big Data", "deformation estimation", "differential interferometric synthetic aperture radar (SAR) (DInSAR)", "distributed scatterers (DSs)", "error analysis", "near real-time (NRT) processing", "phase inconsistencies", "signal decorrelation", "time-series analysis"], "month": "Feb", "number": "2", "numpages": "", "publisher": "", "title": "Study of Systematic Bias in Measuring Surface Deformation With SAR Interferometry", "url": "", "volume": "59", "year": "2021", "abstract": "This article investigates the presence of a new interferometric signal in multilooked synthetic aperture radar (SAR) interferograms that cannot be attributed to the atmospheric or Earth-surface topography changes. The observed signal is short-lived and decays with the temporal baseline; however, it is distinct from the stochastic noise attributed to temporal decorrelation. The presence of such a fading signal introduces a systematic phase component, particularly in short temporal baseline interferograms. If unattended, it biases the estimation of Earth surface deformation from SAR time series. Here, the contribution of the mentioned phase component is quantitatively assessed. The biasing impact on the deformation-signal retrieval is further evaluated. A quality measure is introduced to allow the prediction of the associated error with the fading signals. Moreover, a practical solution for the mitigation of this physical signal is discussed; special attention is paid to the efficient processing of Big Data from modern SAR missions such as Sentinel-1 and NISAR. Adopting the proposed solution, the deformation bias is shown to decrease significantly. Based on these analyses, we put forward our recommendations for efficient and accurate deformation-signal retrieval from large stacks of multilooked interferograms.", "pages": "1285-1301", "note": "", "ISSN": "1558-0644", "publicationtype": "article", "author": ["Ansari, Homa", "De, Francesco", "Parizzi, Alessandro"]}]["9016229", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2020.2976609", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Computer crime", "Cloud computing", "Big Data", "Floods", "Telecommunication traffic", "Servers", "Low-rate Denial of Service attacks (LDoS)", "detection method", "attack prevention system", "defense mechanism"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "Low-Rate DoS Attacks, Detection, Defense, and Challenges: A Survey", "url": "", "volume": "8", "year": "2020", "abstract": "Low-rate Denial of service (LDoS) attacks has become one of the biggest threats to the Internet, cloud computing platforms, and big data centers. As an evolutionary species of DDoS attack, LDoS attack is essentially different from the DDoS attack. DDoS attacks are the behavior of malicious blocking legitimate network traffic by destroying the targets and the infrastructure around it with huge network traffic. While, LDoS attacks are the behavior of intentional degrading the quality of TCP links by throttling TCP flows to a small fraction of its ideal rate with periodic small pulse sequence. Hence, LDoS attack has a very small flow (around 10%\u201320% of the background traffic), it is easy to eluding the detection of routers and counter-DoS mechanisms. We try to reveal the mechanism of the LDoS attack and attempt to figure out the generation principle of LDoS attack in this paper. We classify the LDoS attacks and existing defense methods according to time domain and frequency domain in which detection and defense are performed. Furthermore, we highlight the filter approach to defense against LDoS attack. The initial purpose of our work is to encourage researchers to study effective ways to detect and defend against LDoS attacks with innovation and aggressiveness.", "pages": "43920-43943", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Zhijun, Wu", "Wenjing, Li", "Liang, Liu", "Meng, Yue"]}]["8758074", {"address": "", "articleno": "", "doi": "10.21629/JSEE.2019.03.06", "issn": "1004-4132", "issue_date": "", "journal": "Journal of Systems Engineering and Electronics", "keywords": ["Wireless fidelity", "Buildings", "Big Data", "Switches", "Throughput", "Markov processes", "cloud detection", "visual image", "satellite image", "variance of local fractal dimension (VLFD)"], "month": "June", "number": "3", "numpages": "", "publisher": "", "title": "Cloud detection from visual band of satellite image based on variance of fractal dimension", "url": "", "volume": "30", "year": "2019", "abstract": "Cover ratio of cloud is a very important factor which affects the quality of a satellite image, therefore cloud detection from satellite images is a necessary step in assessing the image quality. The study on cloud detection from the visual band of a satellite image is developed. Firstly, we consider the differences between the cloud and ground including high grey level, good continuity of grey level, area of cloud region, and the variance of local fractal dimension (VLFD) of the cloud region. A single cloud region detection method is proposed. Secondly, by introducing a reference satellite image and by comparing the variance in the dimensions corresponding to the reference and the tested images, a method that detects multiple cloud regions and determines whether or not the cloud exists in an image is described. By using several Ikonos images, the performance of the proposed method is demonstrated.", "pages": "485-491", "note": "", "ISSN": "1004-4132", "publicationtype": "article", "author": ["Pingfang, Tian", "Qiang, Guang", "Xing, Liu"]}]["9676611", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2022.3141795", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Social networking (online)", "Gaussian processes", "Collaboration", "Predictive models", "Recommender systems", "Deep learning", "Data models", "Social recommendation", "recommender systems", "Gaussian process", "social networks"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "Towards Non-Linear Social Recommendation Using Gaussian Process", "url": "", "volume": "10", "year": "2022", "abstract": "Recent research on recommender systems has proved that by leveraging social network information, the quality of recommendations can be evidently improved. Traditional social recommendation models typically linearly combine social network information. For instance, matrix factorization based models linearly combine latent factors of relevant users and items. However, in practice, the multifaceted social relations are so complex that simple linear combination may not be able to reasonably organize such information for accurate social recommendation. On the other hand, existing deep learning based non-linear methods lack systematic modeling of user-item-friend relations. To handle these issues, we propose a novel, non-linear latent factor model for social recommendations leveraging Gaussian process. By introducing a social-aware covariance function, we organize individual users\u2019 past feedback, as well as the associated social information (e.g., friends\u2019 feedback to the same items) into a covariance matrix, which non-linearly and systematically learns the complex interactions among users, their interacted items and their friends\u2019 opinions. A stochastic gradient descent based optimization algorithm is developed to fit the model. Extensive experiments conducted on three real-world datasets demonstrate that the proposed model outperforms the state-of-the-art social recommendation models and Gaussian process based models.", "pages": "6028-6041", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Zhang, Jiyong", "Liu, Xin", "Zhou, Xiaofei"]}]["9133570", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2020.3007538", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Prediction algorithms", "Public transportation", "Real-time systems", "Meteorology", "Predictive models", "Autoregressive processes", "Neural networks", "Passenger flow prediction", "subway network", "conditional random field", "intelligent transportation"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "MetroEye: A Weather-Aware System for Real-Time Metro Passenger Flow Prediction", "url": "", "volume": "8", "year": "2020", "abstract": "Real-time passenger flow prediction plays an important role in subway network design and management. Most of the existing prediction algorithms only consider the sequence of passenger flow volume, however, ignore the influence of other outer factors, for example, the weather conditions, air quality and temperature. In this paper, a systematic framework, MetroEye, is proposed for weather-aware prediction of real-time passenger flow. The framework contains an offline system and an online system. The offline system adopts a conditional random field (CRF) model to establish the relationship between passenger flow volume and weather factors. Experimental results show the superior prediction accuracy of the model, especially in large stations. The online system provides efficient methods to simulate the real-time passenger flow volume. Due to its high practicality, MetroEye has been adopted by Beijing Urban Rail Transit Control Center to monitor the passenger flow status of the Beijing subway system.", "pages": "129813-129829", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Wang, Jianyuan", "Leng, Biao", "Wu, Junjie", "Du, Heng", "Xiong, Zhang"]}]["9426905", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2021.3078569", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Rails", "Synchronization", "Energy consumption", "Sun", "Rail transportation", "Programming", "Optimization", "Energy consumption", "synchronization time", "time-shift control scheme", "congestion"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "Urban Rail Train Scheduling With Smoothing Energy Consumption Peaks and Synchronization Time Minimization: Using Novel Time-Shift Control Scheme", "url": "", "volume": "9", "year": "2021", "abstract": "Considering operators of the urban rail transit systems are often faced with cost control, passengers required high service quality, which has inter-affection between each other in congestion issues for the peak period. A good cooperative timetable associated with time-based shift radios is proposed to achieve a mutually beneficial win-win situation for operators required low energy consumption (costs), and passengers required short waiting time in high peak level with different time-shift control schemes by shifting passengers' travel times. By seeking the optimal shift radios, we focus on generating a favorable train schedule by taking the optimal decisions in the presence of trade-offs between two conflicting objectives. Subsequently, an improved non-dominated sorting in genetic algorithms (INSGA-II) was presented to solve the multi-objective programming model. Finally, the computational results show that the optimized time-shift control scheme brings a significant effect on reducing congestion during the peak periods.", "pages": "70142-70154", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Zhou, Jin", "Guo, Xin", "Li, Feng"]}]["9220134", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2020.3029968", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Search problems", "Image reconstruction", "Knowledge transfer", "Iterative methods", "Convergence", "Evolutionary computation", "Optimization", "Sparse reconstruction", "multi-objective evolutionary algorithm", "transfer learning", "regularization"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "Multi-Objective Sparse Reconstruction With Transfer Learning and Localized Regularization", "url": "", "volume": "8", "year": "2020", "abstract": "Multi-objective sparse reconstruction methods have shown strong potential in sparse reconstruction. However, most methods are computationally expensive due to the requirement of excessive functional evaluations. Most of these methods adopt arbitrary regularization values for iterative thresholding-based local search, which hardly produces high-precision solutions stably. In this article, we propose a multi-objective sparse reconstruction scheme with novel techniques of transfer learning and localized regularization. Firstly, we design a knowledge transfer operator to reuse the search experience from previously solved homogeneous or heterogeneous sparse reconstruction problems, which can significantly accelerate the convergence and improve the reconstruction quality. Secondly, we develop a localized regularization strategy for iterative thresholding-based local search, which uses systematically designed independent regularization values according to decomposed subproblems. The strategy can lead to improved reconstruction accuracy. Therefore, our proposed scheme is more computationally efficient and accurate, compared to existing multi-objective sparse reconstruction methods. This is validated by extensive experiments on simulated signals and benchmark problems.", "pages": "184920-184933", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Yan, Bai", "Zhao, Qi", "Zhang, J.", "Wang, Zhihai"]}]["9347418", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2021.3057167", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Lighting", "Cameras", "Image restoration", "Dynamic range", "Estimation", "Noise reduction", "Image color analysis", "Multi-view images", "feature matching", "virtual images", "exposure fusion"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "Multiview Ghost-Free Image Enhancement for In-the-Wild Images With Unknown Exposure and Geometry", "url": "", "volume": "9", "year": "2021", "abstract": "The multiview low dynamic range images captured with sparse camera arrangement under ill-lighting conditions contain highlighted and shadow regions due to over-exposed and under-exposed regions. The processing of these images produces contrast distortion, and it is challenging to maintain relative brightness with color consistency. Moreover, the disparity map estimation faces the challenges of holes and artifacts due to a wide baseline and poor visibility, with a shared view of vision. In this article, we propose a multiview ghost-free image enhancement strategy for in-the-wild images with unknown exposure and geometry. We address the complex geometric alignment problem for a wide variational baseline among multiple sparsely arranged cameras. The features among multiple viewpoints are detected and matched for the image restoration. The restored image contains highlighted and shadow regions with a color imbalance problem. We synthesize virtual images following the intensity mapping function, which compensates for the relative brightness and color distortions. Finally, we fuse all the images to obtain high-quality images. The proposed method is more frequent and feasible for future multiview systems with varying baselines without relying on disparity maps. The experimental results demonstrate that the proposed method outperformed the state-of-the-art approaches.", "pages": "24205-24220", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Khan, Rizwan", "Akram, Adeel", "Mehmood, Atif"]}]["9026964", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2020.2978900", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Fabrics", "Image segmentation", "Density functional theory", "Feature extraction", "Iterative methods", "Visualization", "Interference", "Fabric defect detection", "membership", "saliency mapping", "threshold iterative method"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "Fabric Defect Detection Based on Membership Degree of Regions", "url": "", "volume": "8", "year": "2020", "abstract": "The detection of fabric defects is an important part of fabric quality control, and hence is thus a research hotspot in the textile industry. With the aim of effectively detecting fabric defects, this paper describes an improved fabric defect detection method based on the membership degree of each fabric region (TPA). By analyzing the regional features of fabric surface defects, the saliency of defect regions can be determined using the extreme point density map of the image and the features of the membership function region. A threshold iterative method and morphological processing are used to ensure the precise and accurate detection of fabric defects. Experimental results show that compared with two classical fabric defect detection methods, the proposed detection method can detect fabric defects more effectively while also suppressing the interference of noise and background textures. Additionally, numerical results demonstrate the validity and feasibility of the proposed method to satisfy the requirements of online detection.", "pages": "48752-48760", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Song, Liwen", "Li, Ruizhi", "Chen, Shiqiang"]}]["8952710", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2020.2964802", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Ontologies", "Public healthcare", "Semantics", "Cognition", "Adaptation models", "Biological system modeling", "Sociology", "Causal graphs", "intervention evaluation", "logic models", "ontologies", "explainable AI", "public health program evaluation"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "Health Intervention Evaluation Using Semantic Explainability and Causal Reasoning", "url": "", "volume": "8", "year": "2020", "abstract": "As serious public health problems require complex responses, health interventions often involve multiple components implemented by groups including policy experts, social workers, and health practitioners. The success or failure of an intervention depends on many different factors, ranging from available resources to characteristics of the targeted public health issue and community to the complex mechanics relating cause and effects of the actions performed. In this paper, we present a novel formal methodology to evaluate public health interventions, policies, and programs. Our method uses the theory of change (TOC) approach along with logic models that define the intervention under consideration to generate a causal diagram and an ontology-based inference model for causal description. The resulting causal diagram will then be compared to existing knowledge and data to determine whether the intervention is coherent, internally consistent and its goals are achievable in the allotted time with the resources provided. The contextual knowledge and semantics provided by the ontology will generate a more explainable, understandable, and trustworthy approach to compare and assess different interventions based on their shared goals. Depending upon the quality and quantity of data available we perform a mix of qualitative and quantitative evaluation of the interventions. This study uses smoking cessation interventions to showcase the proposed methodology in action.", "pages": "9942-9952", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Brenas, Jon", "Shaban-Nejad, Arash"]}]["8853323", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2019.2944641", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Feature extraction", "Genetic algorithms", "Arrays", "Search problems", "Buildings", "Testing", "Particle swarm optimization", "Classification algorithms", "covering arrays", "random forest", "support vector machines", "genetic algorithms", "particle swarm optimization"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "Wrapper for Building Classification Models Using Covering Arrays", "url": "", "volume": "7", "year": "2019", "abstract": "Wrapper methods are a type of feature selection method that finds a subset of variables to improve the performance of a classifier by removing redundant and irrelevant variables. The use of a wrapper implies that each time a candidate solution is explored, the classifier is evaluated on the quality measures selected (e.g. accuracy or precision). Though robust, this iteration across several candidate solutions can become computationally intensive and time-consuming. In this paper we propose a wrapper, that is based on binary Covering Arrays (CAs), and binary Incremental Covering Arrays (ICAs), that have been widely used for experimental design and fault detection in software and hardware testing. The new wrapper was evaluated with six classifiers on seven data sets. The results show that the CAs and ICAs with strength 6 significantly improve the performance and reduces the number of variables required by the classifier. A comparative analysis of the proposed method against wrappers based on other search approaches such as genetic algorithms (GA) and particle swarm optimization (PSO), shows that the proposed method yields results similar to GA, but not to PSO, with differences to PSO, in accuracy, which in the majority of cases is below 0.04. This lack of accuracy, by which the new wrapper fails to match PSO, is offset by the fact that the user does not need to fine tune algorithm parameters, such as velocity ranges, timing, cognitive coefficient, and social coefficient, while it is also much easier to program in parallel.", "pages": "148297-148312", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Dorado, Hugo", "Cobos, Carlos", "Torres-Jimenez, Jose", "Burra, Dharani", "Mendoza, Martha", "Jim\u00e9nez, Daniel"]}]["7986937", {"address": "", "articleno": "", "doi": "10.23919/TST.2017.7986937", "issn": "1007-0214", "issue_date": "", "journal": "Tsinghua Science and Technology", "keywords": ["Wireless sensor networks", "Computer science", "Special issues and sections", "Internet of Things", "Security", "Delays", "Data privacy"], "month": "Aug", "number": "4", "numpages": "", "publisher": "", "title": "Guest editorial: Special issue on Internet of Things", "url": "", "volume": "22", "year": "2017", "abstract": "Internet of Things (IoT) is a new paradigm that the ubiquitous smart objects, such as devices, vehicles, buildings, etc., interact and exchange data through emerging wireless technology with the intention of improving people's quality of lives in variety areas, such as transportation, manufacturing industry, health care industry, etc. Besides benefits, this envisioned paradigm also poses unprecedented challenges in many respects, including identification of things, privacy and security issues, integration and management of big sensory data, sensing and delivering information from dynamic environments, utilization of knowledge-based decision systems, connectivity issues, etc.", "pages": "343-344", "note": "", "ISSN": "1007-0214", "publicationtype": "article", "author": ["Cai, Zhipeng", "Bourgeois, Anu", "Tong, Weitian"]}]["8004469", {"address": "", "articleno": "", "doi": "10.1109/TGRS.2017.2723896", "issn": "1558-0644", "issue_date": "", "journal": "IEEE Transactions on Geoscience and Remote Sensing", "keywords": ["Earth", "Satellites", "Time series analysis", "Remote sensing", "Australia", "Electric breakdown", "Clouds", "Big data applications", "image analysis", "remote sensing", "time series analysis"], "month": "Nov", "number": "11", "numpages": "", "publisher": "", "title": "High-Dimensional Pixel Composites From Earth Observation Time Series", "url": "", "volume": "55", "year": "2017", "abstract": "High-quality and large-scale image composites are increasingly important for a variety of applications. Yet a number of challenges still exist in the generation of composites with certain desirable qualities such as maintaining the spectral relationship between bands, reduced spatial noise, and consistency across scene boundaries so that large mosaics can be generated. We present a new method for generating pixel-based composite mosaics that achieves these goals. The method, based on a high-dimensional statistic called the `geometric median,' effectively trades a temporal stack of poor quality observations for a single high-quality pixel composite with reduced spatial noise. The method requires no parameters or expert-defined rules. We quantitatively assess its strengths by benchmarking it against two other pixel-based compositing approaches over Tasmania, which is one of the most challenging locations in Australia for obtaining cloud-free imagery.", "pages": "6254-6264", "note": "", "ISSN": "1558-0644", "publicationtype": "article", "author": ["Roberts, Dale", "Mueller, Norman", "Mcintyre, Alexis"]}]["8703749", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2019.2912647", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Three-dimensional displays", "Cameras", "Acceleration", "Feature extraction", "Graphics processing units", "Solid modeling", "Computational modeling", "3D reconstruction", "K nearest neighbor", "feature matching", "structure from motion", "parallel computing"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "Parallel K Nearest Neighbor Matching for 3D Reconstruction", "url": "", "volume": "7", "year": "2019", "abstract": "In recent years, a 3D reconstruction based on structure from motion (SFM) has attracted much attention from the communities of computer vision and graphics. It is well known that the speed and quality of SFM systems largely depend on the technique of feature tracking. If a big volume of image data is inputted for SFM, the speed of this SFM system would become very slow. And, this problem becomes severer for large-scale scenes, which typically needs to capture several thousands of images to recover the point-cloud model of the scene. However, none of the existing methods fully addresses the problem of fast feature tracking. Brute force matching is capable of producing correspondences for small-scale scenes but often getting stuck in repeated features. Hashing matching can only deal with middle-scale scenes and is not capable of large-scale scenes. In this paper, we propose a new feature tacking method working in a parallel manner rather than in a single thread scheme. Our method consists of steps of keypoint detection, descriptor computing, descriptor matching by parallel k -nearest neighbor (Parallel-KNN) search, and outlier rejecting. This method is able to rapidly match a big volume of keypoints and avoids to consume high computation time, then yielding a set of correct correspondences. We demonstrate and evaluate the proposed method on several challenging benchmark datasets, including those with highly repeated features, and compare to the state-of-the-art methods. The experimental results indicate that our method outperforms the compared methods in both efficiency and effectiveness.", "pages": "55248-55260", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Cao, Ming-Wei", "Li, Lin", "Xie, Wen-Jun", "Jia, Wei", "Lv, Zhi-Han", "Zheng, Li-Ping", "Liu, Xiao-Ping"]}]["9345696", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2021.3056931", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Quality of service", "Optimization", "Routing", "Industrial Internet of Things", "Energy consumption", "Production", "Delays", "Industrial Internet of Things (IIoT)", "software-defined networking (SDN)", "multiprogrammability", "traffic engineering", "Quality of Service (QoS)", "energy awareness", "resource optimization"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "Parallel Route Optimization and Service Assurance in Energy-Efficient Software-Defined Industrial IoT Networks", "url": "", "volume": "9", "year": "2021", "abstract": "In recent years, the Industrial world has been embracing new digital technology, including the internet of things (IoT) paradigm that promises revolutionizing-prospects in numerous industrial applications. However, many deployment challenges related to real-time big data analytics, service assurance, resource optimization, energy consumption, and security awareness are raised. In this work, we focus on service assurance and resource optimization, including energy consumption challenges over Industrial Internet of Things (IIoT)-based environments since the existing network routing algorithms cannot meet the strict heterogeneous quality of service (QoS) requirements of industrial communications while optimizing resources. We take advantage of the flexibility and programmability offered by the promising software-defined networking paradigm, and we propose a centralized route optimization and service assurance scheme, named ROSA, over a multi-layer programmable industrial architecture. The proposed solution supports a wide range of heterogeneous flows, such as ultra-reliable low-latency communications (URLLC) and bandwidth-sensitive services. The routing optimization problems are formulated as multi-constrained shortest path problems. The Lagrangian Relaxation approach is used to solve the . Hence, we deploy a pair of parallel routing algorithms run according to the flow type to ensure QoS requirements, efficiently allocate constrained resources, and enhance the overall network energy consumption. We conduct extensive simulations to validate the proposed ROSA scheme. The experimental results show promising performance in terms of reducing bandwidth utilization by up to 22%, end-to-end delay at least by 21%, packet loss by more than 19%, flow violation by about 16%, and energy consumption up to 14% as compared to well-known benchmarks in QoS provisioning and energy-aware routing problem.", "pages": "24682-24696", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Njah, Yosra", "Cheriet, Mohamed"]}]["8409952", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2018.2853985", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Smart cities", "Security", "Privacy", "Computer architecture", "Sensors", "Smart city", "Internet of Things", "security", "privacy"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "Security and Privacy in Smart Cities: Challenges and Opportunities", "url": "", "volume": "6", "year": "2018", "abstract": "Smart cities are expected to improve the quality of daily life, promote sustainable development, and improve the functionality of urban systems. Now that many smart systems have been implemented, security and privacy issues have become a major challenge that requires effective countermeasures. However, traditional cybersecurity protection strategies cannot be applied directly to these intelligent applications because of the heterogeneity, scalability, and dynamic characteristics of smart cities. Furthermore, it is necessary to be aware of security and privacy threats when designing and implementing new mechanisms or systems. Motivated by these factors, we survey the current situations of smart cities with respect to security and privacy to provide an overview of both the academic and industrial fields and to pave the way for further exploration. Specifically, this survey begins with an overview of smart cities to provide an integrated context for readers. Then, we discuss the privacy and security issues in current smart applications along with the corresponding requirements", "pages": "46134-46145", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Cui, Lei", "Xie, Gang", "Qu, Youyang", "Gao, Longxiang", "Yang, Yunyun"]}]["8424161", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2018.2861987", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Voting", "Computer architecture", "Automation", "Media", "Data mining", "Industries", "Artificial intelligence", "automated content generation", "automated storytelling", "natural language processing", "robot journalism"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "No Landslide for the Human Journalist - An Empirical Study of Computer-Generated Election News in Finland", "url": "", "volume": "6", "year": "2018", "abstract": "In an age of struggling news media, automated generation of news via natural language generation (NLG) methods could be of great help, especially in areas where the amount of raw input data is big, and the structure of the data is known in advance. One such news automation system is the Valtteri NLG system, which generates news articles about the Finnish municipal elections of 2017. To evaluate the quality of Valtteri-produced articles and to identify aspects to improve, n = 152 users were asked to evaluate the output of Valtteri. Each evaluator rated six preselected computer-generated articles, four control articles written by journalists, and four computer-generated articles of their own choice. All the articles were evaluated along four dimensions: credibility, liking, quality, and representativeness. As expected, the texts written by Valtteri received lower ratings than those written by journalists, but overall the ratings were satisfactory (average 2.9 versus 4.0 for journalists on a five-point scale). Valtteri's best rating (3.6) was for credibility. The computer-written articles that the evaluators could freely select got slightly better ratings than the preselected computer-written articles. When looking at the results by demographic groups, males aged 55 or more liked the automatic articles best and females aged 34 or less liked them the least. Evaluators mistook 21% of the computer-written articles as written by humans and 10% of the human-written articles as computer-written. The share of users making these mistakes grew with the age. Overall, the male evaluators made less writer-identification mistakes than female evaluators did.", "pages": "43356-43367", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Melin, Magnus", "B\u00e4Ck, Asta", "S\u00f6Derg\u00e5Rd, Caj", "Munezero, Myriam", "Lepp\u00e4Nen, Leo", "Toivonen, Hannu"]}]["9036949", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2020.2980961", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Batteries", "State of charge", "Machine learning", "Maximum likelihood estimation", "Training", "Temperature measurement", "Machine learning", "artificial intelligence", "deep learning", "battery management systems (BMS)", "electric vehicles", "state of charge", "state of health"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "Machine Learning Applied to Electrified Vehicle Battery State of Charge and State of Health Estimation: State-of-the-Art", "url": "", "volume": "8", "year": "2020", "abstract": "The growing interest and recent breakthroughs in artificial intelligence and machine learning (ML) have actively contributed to an increase in research and development of new methods to estimate the states of electrified vehicle batteries. Data-driven approaches, such as ML, are becoming more popular for estimating the state of charge (SOC) and state of health (SOH) due to greater availability of battery data and improved computing power capabilities. This paper provides a survey of battery state estimation methods based on ML approaches such as feedforward neural networks (FNNs), recurrent neural networks (RNNs), support vector machines (SVM), radial basis functions (RBF), and Hamming networks. Comparisons between methods are shown in terms of data quality, inputs and outputs, test conditions, battery types, and stated accuracy to give readers a bigger picture view of the ML landscape for SOC and SOH estimation. Additionally, to provide insight into how to best approach with the comparison of different neural network structures, an FNN and long short-term memory (LSTM) RNN are trained fifty times each for 3000 epochs. The error is somewhat different for each training repetition due to the random initial values of the trainable parameters, demonstrating that it is important to train networks multiple times to achieve the best result. Furthermore, it is recommended that when performing a comparison among estimation techniques such as those presented in this review paper, the compared networks should have a similar number of learnable parameters and be trained and tested with identical data. Otherwise, it is difficult to make a general conclusion regarding the quality of a given estimation technique.", "pages": "52796-52814", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Vidal, Carlos", "Malysz, Pawel", "Kollmeyer, Phillip", "Emadi, Ali"]}]["9036967", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2020.2980982", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Social networking (online)", "Engines", "Smart cities", "History", "Internet", "Wireless sensor networks", "Wireless communication", "Big data", "location based", "point-of-interests", "smart city", "recommendation"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "Exploiting Location-Based Context for POI Recommendation When Traveling to a New Region", "url": "", "volume": "8", "year": "2020", "abstract": "Traveling to a new region has become a very common thing for people, due to work or life requirement. With the development of recommendation engine and the popularity of social media network, people are more and more used to relying on personalized Points-of-Interest (POI) recommendations. However, traditional approaches can fail if users moves to a region where they had little or no active history or even social network friends information before. Under the requirement of smart city construction, the need to give high quality personalized POI recommendation when a user travels to a new region has arisen. Fortunately, with the widespread of wireless Internet, the booming of Internet-of-Things (IoT) and the common-usage of location sensors in mobile phones, the coupling degree between social media networks and location information is ever increasing, which could leads us to a new way to solve this problem in the ear of Big Data. In this research, we presented New Place Recommendation Algorithm (N-PRA) which is designed based on Latent Factor model. Many different types of social media contexts (time-related and location-related), such as a user's interest fluctuation, different types of POIs' popularity fluctuation, types of POIs, the influence of geographical neighborhood on POIs, and user's social network friendship are taken into consideration in this approach. The algorithm presented is verified on Yelp, an open-source real urban data-set, and compared against several other baseline POI recommendation algorithms. Experimental results show that the algorithm presented in this paper could achieve a better accuracy.", "pages": "52404-52412", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Gao, Keyan", "Yang, Xu", "Wu, Chenxia", "Qiao, Tingting", "Chen, Xiaoya", "Yang, Min", "Chen, Ling"]}]["8319426", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2018.2816983", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Virtual machining", "Data centers", "Energy consumption", "Cloud computing", "Production facilities", "Resource management", "Reliability", "Industrial cloud", "particle swarm optimization", "traffic awareness", "virtual machine placement"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "Reliable Virtual Machine Placement Based on Multi-Objective Optimization With Traffic-Aware Algorithm in Industrial Cloud", "url": "", "volume": "6", "year": "2018", "abstract": "In a cloud data center, there is usually a large waste of physical resources and link resources, which leads to increased energy consumption. This paper discusses reducing the loss of link resources from the perspective of connectivity between virtual machines. When the virtual machines of a single user request are concentrated to reduce energy consumption, there will be decreased request reliability. This paper considers a single point of failure to ensure the reliability of the requests. Finally, this paper proposes a multi-objective particle swarm optimization algorithm. It takes the physical resource utilization rate and the link loss rate as the optimization targets and uses service reliability and quality of the tenant as constraint conditions. The simulation results show that the method proposed in this paper reliably satisfies the tenant request, effectively controls the link loss in the data center, and significantly reduces the energy consumption of the data center.", "pages": "23043-23052", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Luo, Juan", "Song, Weiqi", "Yin, Luxiu"]}]["9494369", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2021.3099497", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Annotations", "Proposals", "Training", "Object detection", "Detectors", "Feature extraction", "Streaming media", "Object detection", "weakly-supervised learning", "data augmentation", "mixed-supervision"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "Efficient Weakly-Supervised Object Detection With Pseudo Annotations", "url": "", "volume": "9", "year": "2021", "abstract": "Weakly-supervised object detection (WSOD) has attracted lots of attention in recent years. However, there is still a big gap between WSOD and generic object detection. The main barriers to the efficiency of WSOD are the ineffective data augmentations and inaccurate bounding box predictions. Given only image-level annotations, it is hard for WSOD to effectively utilize variant data augmentations and accurately regress the bounding boxes. Although a fully-supervised object detector can be trained using annotations generated from the weakly-supervised object detector, the performance is still severely limited due to the low quality of mined pseudo annotations. This paper proposes an efficient WSOD method with pseudo annotations (EWPA) to make better use of imperfect annotations. With the assistance of pseudo annotations, EWPA can effectively regress more accurate bounding boxes while the traditional WSOD can only locate the salient parts of an object. Furthermore, pseudo annotations can help design more complex data augmentations, driving the network to learn more discriminative feature representations. Extensive experiments are conducted on PASCAL VOC 2007 and 2012 datasets and validate the effectiveness of EWPA.", "pages": "104356-104366", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Yuan, Qingsheng", "Sun, Gang", "Liang, Jianming", "Leng, Biao"]}]["8194836", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2017.2782778", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Birds", "Noise reduction", "Noise measurement", "Wavelet transforms", "Biomedical acoustics", "Signal to noise ratio", "Noise removal", "bioacoustics", "big data"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "Automatic and Efficient Denoising of Bioacoustics Recordings Using MMSE STSA", "url": "", "volume": "6", "year": "2018", "abstract": "Automatic recording and analysis of bird calls is becoming an important way to understand changes in bird populations and assess environmental health. An issue currently proving problematic with the automatic analysis of bird recordings is interference from noise that can mask vocalizations of interest. As such, noise reduction can greatly increase the accuracy of automatic analyses and reduce processing work for subsequent steps in bioacoustics analyses. However, only limited work has been done in the context of bird recordings. Most semiautomatic methods either manually apply sound enhancement methods available in audio processing systems such as SoX and Audacity or apply preliminary filters such as lowand highpass filters. These methods are insufficient both in terms of how generically they can be applied and their integration with automatic systems that need to process large amounts of data. Some other work applied more sophisticated denoising methods or combinations of different methods such as minimum mean square error short-time spectral amplitude estimator (MMSE STSA) and spectral subtraction for other species such as anurans. However, their effectiveness is not tested on bird recordings. In this paper, we analyze the applicability of the MMSE STSA algorithm to remove noise from environmental recordings containing bird sounds, particularly focusing on its quality and processing time. The experimental evaluation using real data clearly shows that MMSE STSA can reduce noise with similar effectiveness [using objective metrics such as predicted signal quality (SIG)] to a previously recommended wavelet-transform-based denoising technique while executing between approximately 5-300 times faster depending on the audio files tested.", "pages": "5010-5022", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Brown, Alexander", "Garg, Saurabh", "Montgomery, James"]}]["9099840", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2020.2997812", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Genetic algorithms", "Transportation", "Logistics", "Planning", "Sociology", "Statistics", "Companies", "Parallel metaheuristics", "genetic algorithm", "transportation planning", "logistics management"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "A Parallel Genetic Algorithm Framework for Transportation Planning and Logistics Management", "url": "", "volume": "8", "year": "2020", "abstract": "Small to medium sized transportation and logistics companies are usually constrained by limited computing and IT professional resources on implementing an efficient parallel metaheuristic algorithm for planning or management solutions. In this paper we extend the standard meta-description for genetic algorithms (GA) with a simple non-trivial parallel implementation. Our parallel GA framework is chiefly concerned with the development of a straightforward way for engineers to modify existing genetic algorithm implementations for real transportation and logistics problems to make use of commonly available hardware resources without completely reworking complex, useful and usable codes. The framework presented at its parallel base is a modification of the primitive parallelization concept, but if implemented as described it may be gradually extended to fit the qualities of any underlying problem better (via the adaptation of the merging and communications functions).We present our framework and computational results for a classical transportation related combinatorial optimization problem - the traveling salesman problem with a standard sequential genetic algorithm implementation. Our empirical analysis shows that this simple extension can lead to considerable solution improvements. We also tested our assumptions that the framework is easily implemented by an engineer not initially familiar with genetic algorithms to implement the framework for another minimum multiprocessor scheduling problem. These case studies verify that our framework is better than primitive parallelization because it gives empirically better results under equitable conditions. It also outperforms fine grained parallelization as it is easier and faster to implement.", "pages": "106506-106515", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Arkhipov, Dmitri", "Wu, Di", "Wu, Tao", "Regan, Amelia"]}]["8561284", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2018.2885081", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Public transportation", "Logistics", "Urban areas", "Vehicles", "Task analysis", "Planning", "Package distributions", "crowdsourced", "public transportation systems", "quality of passenger experience"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "Planning City-Wide Package Distribution Schemes Using Crowdsourced Public Transportation Systems", "url": "", "volume": "7", "year": "2019", "abstract": "Due to the rapid development of online retailers, there is a great demand for package express shipping services, which causes traffic congestion, resource consumption, and environmental pollution (e.g., carbon emission). However, there is still a large amount of under-utilized capacity in the public transportation systems during off-peak hours. In this paper, we investigate the same-day package distribution using crowdsourced public transportation systems (CPTSs). Specifically, given a number of packages and the timetable of available CPTSs trips, we optimize the schemes of delivering the packages using the under-utilized capacity of the CPTS trips, without impacting the quality of passenger experience. To estimate the amount of under-utilized capacity of each trip across any two adjacent stations, we propose the passenger transit model based on the history data. To assign the under-utilized capacity of each trip to the package deliveries, we develop the minimum limitation delivery (MLD) method, which only utilizes the minimum amount of under-utilized capacity of the whole trip to deliver packages. However, the available capacity is not fully utilized at most stations by MLD. Therefore, we further propose the adaptive limitation delivery (ALD) method, which loads as many packages as possible, until the volume of loaded packages reaches the available capacity in theory. The experimental results and theoretical analysis show that both MLD and ALD could distribute packages efficiently. Moreover, given a set of packages, scheduling of ALD only consumes about 67% time compared to the scheduling of MLD, with a little higher risk of impacting passengers.", "pages": "1234-1246", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Cheng, Geyao", "Guo, Deke", "Shi, Jianmai", "Qin, Yudong"]}]["8606969", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2019.2891759", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Noise reduction", "Matching pursuit algorithms", "Image reconstruction", "Wavelet coefficients", "Compressed sensing", "Compressed sensing", "wavelet denoising", "DNA microarray", "image filtering", "NDOA"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "Wavelet Denoising Algorithm Based on NDOA Compressed Sensing for Fluorescence Image of Microarray", "url": "", "volume": "7", "year": "2019", "abstract": "A microarray can be easily used for quantitatively analyzing the expression levels of DNA genes. Yet, the noises introduced during the application will greatly affect the accuracy of DNA sequence detection. How to reduce the noise constitutes a challenging problem in microarray analysis. Especially, due to the weak fluorescence response, the image of microarray contains difficulties of the low peak-signal-to-noise ratio (PSNR) and luminance contrast. To solve the problem that the wavelet threshold denoising method has poor effective on low PSNR image, a wavelet denoising approach based on compression sensing (CS) optimized by the neural dynamics optimization algorithm (NDOA) is proposed, which preferably solves the denoising difficulties of noise pollution in the microarray image. Under the condition of Gaussian random observation matrix, the effectiveness of NDOA-optimized wavelet denoising based on CS gets better work than the orthogonal matching pursuit and its improved algorithms. The experimental results indicate that the expected wavelet coefficients of the noiseless image have been reconstructed with higher quality. When the compression sampling rate for microarray image is 0.875, the PSNR of the NDOA-optimized wavelet denoising algorithm based on CS is increased about 9 dB, and the root mean squared error is reduced obviously too, in comparison with the wavelet soft-threshold denoising method. It shows that the NDOA-optimized method improves the performance of the classical wavelet threshold denoising.", "pages": "13338-13346", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Gan, Zhenhua", "Zou, Fumin", "Zeng, Nianyin", "Xiong, Baoping", "Liao, Lyuchao", "Li, Han", "Luo, Xin", "Du, Min"]}]["8360973", {"address": "", "articleno": "", "doi": "10.1109/TCYB.2018.2832640", "issn": "2168-2275", "issue_date": "", "journal": "IEEE Transactions on Cybernetics", "keywords": ["Cloud computing", "Task analysis", "Optimization", "Processor scheduling", "Scheduling", "Computational modeling", "Search problems", "Cloud computing", "evolutionary approach", "multiobjective optimization", "workflow scheduling"], "month": "Aug", "number": "8", "numpages": "", "publisher": "", "title": "Multiobjective Cloud Workflow Scheduling: A Multiple Populations Ant Colony System Approach", "url": "", "volume": "49", "year": "2019", "abstract": "Cloud workflow scheduling is significantly challenging due to not only the large scale of workflow but also the elasticity and heterogeneity of cloud resources. Moreover, the pricing model of clouds makes the execution time and execution cost two critical issues in the scheduling. This paper models the cloud workflow scheduling as a multiobjective optimization problem that optimizes both execution time and execution cost. A novel multiobjective ant colony system based on a co-evolutionary multiple populations for multiple objectives framework is proposed, which adopts two colonies to deal with these two objectives, respectively. Moreover, the proposed approach incorporates with the following three novel designs to efficiently deal with the multiobjective challenges: 1) a new pheromone update rule based on a set of nondominated solutions from a global archive to guide each colony to search its optimization objective sufficiently; 2) a complementary heuristic strategy to avoid a colony only focusing on its corresponding single optimization objective, cooperating with the pheromone update rule to balance the search of both objectives; and 3) an elite study strategy to improve the solution quality of the global archive to help further approach the global Pareto front. Experimental simulations are conducted on five types of real-world scientific workflows and consider the properties of Amazon EC2 cloud platform. The experimental results show that the proposed algorithm performs better than both some state-of-the-art multiobjective optimization approaches and the constrained optimization approaches.", "pages": "2912-2926", "note": "", "ISSN": "2168-2275", "publicationtype": "article", "author": ["Chen, Zong-Gan", "Zhan, Zhi-Hui", "Lin, Ying", "Gong, Yue-Jiao", "Gu, Tian-Long", "Zhao, Feng", "Yuan, Hua-Qiang", "Chen, Xiaofeng", "Li, Qing", "Zhang, Jun"]}]["9136648", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2020.3007928", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Image segmentation", "COVID-19", "Optimization methods", "Computed tomography", "Biomedical imaging", "Entropy", "Image segmentation", "multi-level thresholding", "moth-?ame optimization (MFO)", "marine predators algorithm (MPA)", "COVID-19", "swarm intelligence"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "An Improved Marine Predators Algorithm With Fuzzy Entropy for Multi-Level Thresholding: Real World Example of COVID-19 CT Image Segmentation", "url": "", "volume": "8", "year": "2020", "abstract": "Medical imaging techniques play a critical role in diagnosing diseases and patient healthcare. They help in treatment, diagnosis, and early detection. Image segmentation is one of the most important steps in processing medical images, and it has been widely used in many applications. Multi-level thresholding (MLT) is considered as one of the simplest and most effective image segmentation techniques. Traditional approaches apply histogram methods; however, these methods face some challenges. In recent years, swarm intelligence methods have been leveraged in MLT, which is considered an NP-hard problem. One of the main drawbacks of the SI methods is when searching for optimum solutions, and some may get stuck in local optima. This because during the run of SI methods, they create random sequences among different operators. In this study, we propose a hybrid SI based approach that combines the features of two SI methods, marine predators algorithm (MPA) and moth-?ame optimization (MFO). The proposed approach is called MPAMFO, in which, the MFO is utilized as a local search method for MPA to avoid trapping at local optima. The MPAMFO is proposed as an MLT approach for image segmentation, which showed excellent performance in all experiments. To test the performance of MPAMFO, two experiments were carried out. The first one is to segment ten natural gray-scale images. The second experiment tested the MPAMFO for a real-world application, such as CT images of COVID-19. Therefore, thirteen CT images were used to test the performance of MPAMFO. Furthermore, extensive comparisons with several SI methods have been implemented to examine the quality and the performance of the MPAMFO. Overall experimental results confirm that the MPAMFO is an efficient MLT approach that approved its superiority over other existing methods.", "pages": "125306-125330", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Elaziz, Mohamed", "Ewees, Ahmed", "Yousri, Dalia", "Alwerfali, Husein", "Awad, Qamar", "Lu, Songfeng", "Al-Qaness, Mohammed"]}]["8332926", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2018.2823722", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Energy consumption", "Internet", "Resource management", "Routing", "Servers", "Optimization", "Cooperative caching", "Content-centric networking", "energy efficiency", "in-network caching", "neighbor cooperation"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "An In-Network Caching Scheme Based on Energy Efficiency for Content-Centric Networks", "url": "", "volume": "6", "year": "2018", "abstract": "Content-centric networking (CCN) has emerged as a promising architecture for future Internet due to its in-network caching capability and the receiver-driven content retrieval paradigm. Recently, the growing energy consumption driven by explosive increase of network traffic has become a key issue in CCN and caused widespread academic concern. In this paper, we construct a model to analyze the energy consumption of content distribution in CCN, and propose an energy efficiency based in-network caching scheme. In this scheme, a judging condition is designed to reduce the total energy consumption of content dissemination, and then in combination with content popularity and node importance, a cache placement strategy is proposed to optimize the selection of caching nodes. Furthermore, a neighbor cooperation-based cache replacement strategy is also proposed, which uses the cache resource of neighbor nodes to increase the chances of content being cached and improve the quality of caching service and resource utilization. Simulation results demonstrate that our scheme can outperform the existing schemes in terms of the high cache hit rate, the low average response hops, and the low whole energy consumption.", "pages": "20184-20194", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["An, Ying", "Luo, Xi"]}]["8695003", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2019.2912302", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Feature extraction", "Cameras", "Image resolution", "Computer aided instruction", "Measurement", "Benchmark testing", "Task analysis", "Distance learning", "low-resolution video-based pedestrian dataset", "semi-coupled mapping", "person re-identification"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "High-Resolution and Low-Resolution Video Person Re-Identification: A Benchmark", "url": "", "volume": "7", "year": "2019", "abstract": "Person re-identification has recently attracted increasing interest in the computer vision and safety-critical applications. In practice, due to poor quality of cameras or long distance away from person, the captured pedestrian videos usually suffer from low resolution, which will result in the loss of useful information contained in videos and make person re-identification between low-resolution (LR) and high-resolution (HR) videos (PRLHV) be a challenging task. However, the problem of PRLHV has not been well studied. In this paper, we propose a semi-coupled mapping based set-to-set distance learning (SMDL) approach for PRLHV. Specifically, by regarding each video as a set of features extracted from several walking cycles, we learn a discriminative set-to-set distance metric to enhance the separability between videos from different persons. To decrease the influence of low resolution on the distance learning, we design a clustering-based semi-coupled mapping term for our approach, which can reduce the variation between features of low-resolution and high-resolution videos by a semi-coupled mapping matrix. Since there exists no low-resolution video pedestrian re-identification dataset under real-world scenario up to now, we contribute a benchmark dataset for PRLHV, named high-resolution and low-resolution video person re-identification dataset (HLVID). Although this dataset is challenging and difficult for person re-identification, it is a useful attempt for further studies on low-resolution video-based pedestrian re-identification under the real-world scene. The extensive experiments on the newly collected video dataset demonstrate that our approach performs better than the state-of-the-art person re-identification methods in the PRLHV task.", "pages": "63426-63436", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Ma, Fei", "Jing, Xiao-Yuan", "Yao, Yongfang", "Zhu, Xiaoke", "Peng, Zhiping"]}]["9745616", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2022.3163715", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Vehicle routing", "Costs", "Planning", "Load modeling", "Wheelchairs", "Public transportation", "Logistics", "Vehicle routing problem", "multi-parking lot and shelter", "heterogeneous fleet", "split pickup", "Tabu Search"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "Multi-Parking Lot and Shelter Heterogeneous Vehicle Routing Problem With Split Pickup Under Emergencies", "url": "", "volume": "10", "year": "2022", "abstract": "The vehicle rescue process for individuals in residential areas in disaster scenarios is a typical vehicle routing problem (VRP). However, most studies do not consider the factor of individual mobility. In residential areas, there are two types of individuals: individuals with high mobility and individuals with low mobility, such as the elderly. To improve the evacuation efficiency, besides ordinary vehicles, special vehicles equipped with wheelchairs and volunteers are also in great need. Thus, evacuation vehicles should consist of a heterogeneous fleet. Vehicles depart from parking lots, arrive at residential areas to pick up individuals, and then transport them to shelters. In other words, the origin and destination are different, but they are viewed as the same in classical VRP. Each residential area can be served directly by vehicles departing from parking lots or by vehicles that have already served others, which means demands can be split. All these make the VRP in emergency rescue more complicated than classical VRP. Therefore, we propose an integer liner program model \u2013 multi-parking lot and shelter heterogeneous vehicle routing problem with split pickup (MPSHVRPSP) model, which includes matching constraints of individuals and vehicles to satisfy the demands of different types of individuals, and considers the selectivity of parking lots and shelters too. We provide a Tabu Search (TS) algorithm with diversification strategy to solve the model and ensure the high quality of solution. A lot of experiments are carried out on various instances. Our results show that MPSHVRPSP can be applied to efficient evacuation of complicated scenarios that satisfies the demands of all individuals in residential areas. Besides, it is more reasonable compared with classical VRP, and TS can also obtain a satisfactory solution in less time. Furthermore, sensitivity analysis is conducted on factors that may affect the result of objective function.", "pages": "36073-36090", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Xu, Lina", "Wang, Ziyang", "Chen, Xudong", "Lin, Zhengwei"]}]["9475541", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2021.3094972", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Image fusion", "Image segmentation", "Feature extraction", "Medical diagnostic imaging", "Transforms", "Clustering algorithms", "Genetic algorithms", "Multimodal medical image fusion", "superpixel segmentation", "genetic algorithm", "log-gabor filter", "sum modified laplacian"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "A Novel GA-Based Optimized Approach for Regional Multimodal Medical Image Fusion With Superpixel Segmentation", "url": "", "volume": "9", "year": "2021", "abstract": "For multimodal medical image fusion problems, most of the existing fusion approaches are based on pixel-level. However, the pixel-based fusion method tends to lose local and spatial information as the relationships between pixels are not considered appropriately, which has much influence on the quality of the fusion results. To address this issue, a region-based multimodal medical image fusion framework is proposed based on superpixel segmentation and a post-processing optimization method in this paper. In this framework, the average image of the source medical images is firstly obtained by a weighted averaging method. To effectively obtain homogeneous regions and preserve the complete information of image details, the fast linear spectral clustering(LSC) superpixel algorithm is carried out to segment the average image and get superpixel labels. For each region of the medical images, log-gabor filter(LGF) and sum modified laplacian(SML) are adopted to extract texture feature and contrast feature for the measurement of region importance. The most important regions are selected and the decision map is generated by comparison. Moreover, to get a more accurate decision map, a new post-processing optimized method based on genetic algorithm(GA) is given. A weighted strategy is applied to the extracted features and the weighting factor can be adaptively adjusted by GA. The effectiveness of the proposed fusion method is validated by conducting experiments on eight pairs of medical images from diverse modalities. In addition, seven other mainstream medical image fusion methods are adopted for comparing the performance of fusion. Experimental results in terms of qualitative and quantitative evaluation demonstrate that the proposed method can achieve state-of-the-art performance for multimodal medical image fusion problems.", "pages": "96353-96366", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Duan, Junwei", "Mao, Shuqi", "Jin, Junwei", "Zhou, Zhiguo", "Chen, Long", "Chen, C."]}]["8918464", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2019.2957214", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Image edge detection", "Spatial resolution", "Multiresolution analysis", "Degradation", "Optimization", "Pansharpening", "variational model", "spectral and spatial consistency priors"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "Joint Spectral and Spatial Consistency Priors for Variational Pansharpening", "url": "", "volume": "7", "year": "2019", "abstract": "This paper proposes a new variational pansharpening model with joint spectral and spatial consistency priors, which aims to fuse a low resolution (LR) multispectral (MS) image and a high resolution (HR) panchromatic (Pan) image to produce a pan-sharpened HR MS image. Specifically, the proposed model combines three consistency terms into a unified variational framework, which are (1) Local spectral consistency fidelity term, which enforces the degradation relation-based local spectral consistency constraint between the HR MS and LR MS images; (2) Hessian feature-enforced spatial consistency prior term, which particularly models the Hessian feature consistency constraint between the HR MS and Pan images to enforce spatial consistency; and (3) Wavelet-based spectral-spatial consistency prior term, which models the consistency between the HR MS image and the constructed Wavelet-based matching image to enforce spectral-spatial consistency. Moreover, the proposed model is efficiently solved by designing an optimization algorithm under the forward-backward splitting framework. Finally, experiments on the QuickBird, Pleiades and GeoEye-1 satellite datasets systematically illustrate that the proposed method performs better spectral and spatial qualities than various compared methods.", "pages": "174847-174858", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Liu, Pengfei"]}]["9037316", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2020.2981047", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Aircraft", "Aerospace control", "Atmospheric modeling", "Control systems", "Adaptation models", "Force", "Oscillators", "Flight simulation", "human???vehicle system", "human pilot model", "interface", "manual control"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "Mechanism Analysis of Smart Cue on Aircraft for Loss of Control Mitigation", "url": "", "volume": "8", "year": "2020", "abstract": "This paper analyzes the mechanism of the smart inceptor on the aircraft, as a means to mitigate human-vehicle system loss-of-control. We divide the smart inceptor cue into three modes: the smart cue on the human pilot, the smart cue on the flight control system and the smart cue on both of them. The control mechanism of these three modes is developed and analyzed in depth. To evaluate the effect of the three modes, we utilize an intelligent human pilot model to establish the human-vehicle system with the smart inceptor and a scalogram-based pilot induced oscillation metric to predict the handling qualities of the three modes. This paper presents details of the cueing modes and the results of the prediction focused on effectiveness of these modes in preventing the pilots from entering a loss-of-control event. The simulation results indicate that the smart cue on both of them was the most effective method to mitigate the impact of the pilot-aircraft system oscillations for the given failure scenarios. It embodies the function of pilot-aircraft cooperation.", "pages": "58522-58532", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Xu, Shuting", "Zhang, Zhe"]}]["9693896", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2022.3146396", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Streaming media", "Probability", "Power system reliability", "Packet loss", "Relay networks (telecommunication)", "Source coding", "Cognitive radio", "Cognitive radio", "cognitive radio networks", "H264/AVC", "multiple description coding (MDC)", "outage performance", "interweave", "peak signal to noise ratio (PSNR)", "relay"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "Multiple Description Coding for Enhancing Outage and Video Performance Over Relay-Assisted Cognitive Radio Networks", "url": "", "volume": "10", "year": "2022", "abstract": "Multimedia content delivery, such as video transmission over wireless networks, imposes significant challenges include spectrum capacity and packet losses. The cognitive radio (CR) technology is developed to solve the spectrum issue, while multiple description coding (MDC) is one of the promising source coding techniques to alleviate packet loss problems and exploit the benefit of path diversity. The source information was split into several descriptions in MDC, then transmitted over a network with multiple paths. The quality of the received data increases with the number of descriptions received at the receiver. In this paper, the proposed system comprises of relay-assisted cognitive radio network using the MDC technique for video transmission. In the simulations, the outage performance of the MDC scheme over two networks, which were relay-assisted network and non-relay network, were compared. Then, the outage probability was used to estimate the video quality, peak signal to noise ratio (PSNR) of the received video. The results obtained show the benefits of the relay assisted networks by 9% improvement on average outage performance over the non-relay network. Furthermore, the video performance improved by an average of 9% in PSNR compared to the non-relay system.", "pages": "11750-11762", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Ibrahim, N.", "Sali, A.", "Karim, H.", "Ramli, A.", "Ibrahim, N.", "Grace, D."]}]["9625982", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2021.3130280", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Optimization", "Linear programming", "Optical fibers", "Heuristic algorithms", "Bandwidth", "Task analysis", "Packet transport network", "bilevel programming", "multi-objective gray wolf algorithm", "label switching path", "committed information rate"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "Bilevel Multi-Objective Gray Wolf Algorithm Based on Packet Transport Network Optimization", "url": "", "volume": "9", "year": "2021", "abstract": "Packet transport network (PTN), as an efficient transmission network technology in mobile communications in the big data era, is used by more and more communication operators. The existing PTN resource utilization rate is low, the network security is poor, so the existing PTN needs to be optimized in all aspects. For the optimization of the PTN, it is necessary to consider the decision of both the operator user and the service product supplier. Therefore, this paper proposes a bilevel multi-objective gray wolf algorithm based on PTN optimization problem. The operator user is the upper-level decision maker, and the objective function is to pay the product supplier the lowest cost. The product supplier is the lower-level decision maker, it mainly includes two major objective functions. The first objective function is to maximize the Label switching path overlap rate(LSPOR) evaluation score to solve the abnormal Label Switching Path (LSP) problem in the network, and the second is to maximize the committed bandwidth with utilizing rate(CBWUR) evaluation score to solve the problem of excessive Committed Information Rate(CIR) bandwidth usage in the network. According to the three scale network situation in Hubei, China, the improved multi-objective gray wolf algorithm is used to solve the PTN bilevel programming problem. The experimental results show that compared with the initial network, the optimized network size dropped by 125314 hops on average, the LSPOR increased by 13.64%, and the CBWUR increased by 3.7%. This model not only improves the utilization of network resources, but also reduces the cost to be paid by superior decision makers.", "pages": "162792-162804", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Wang, Chunzhi", "Li, Xing", "Wang, Zaoning"]}]["9171438", {"address": "", "articleno": "", "doi": "10.1109/JSTARS.2020.3017934", "issn": "2151-1535", "issue_date": "", "journal": "IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing", "keywords": ["Feature extraction", "Buildings", "Image segmentation", "Semantics", "Remote sensing", "Optimization", "Image edge detection", "Boundary quality", "building extraction", "high resolution", "structural similarity (SSIM)"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "Arbitrary-Shaped Building Boundary-Aware Detection With Pixel Aggregation Network", "url": "", "volume": "14", "year": "2021", "abstract": "Large-scale building extraction is an essential work in the field of a remote sensing image analysis. The high-resolution image extraction methods based on deep learning have achieved state-of-the-art performance. However, most of the previous work has focused on region accuracy rather than boundary quality. Aiming at the low-accuracy problems and incomplete boundary of the building extraction method, we propose a predictive optimization architecture, BAPANet. Notably, the architecture consists of an encoder\u2013decoder network, and residual refinement modules responsible for prediction, and refinement. The objective function optimizes the network in the form of three levels (pixel, feature map, and patch) by fusing three loss functions: binary cross-entropy, intersection over-union, and structural similarity. The five public datasets\u2019 experimental results show that the extraction method in this article has high region accuracy, and the boundary of buildings is clear and complete.", "pages": "2699-2710", "note": "", "ISSN": "2151-1535", "publicationtype": "article", "author": ["Jiang, Xin", "Zhang, Xinchang", "Xin, Qinchuan", "Xi, Xu", "Zhang, Pengcheng"]}]["9475964", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2021.3095335", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Deep learning", "Computational modeling", "Complex networks", "Social networking (online)", "Optimization", "Big Data", "Terminology", "Community detection", "deep learning", "complex networks", "meta-heuristic algorithms", "parallel computing"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "A Review on Community Detection in Large Complex Networks from Conventional to Deep Learning Methods: A Call for the Use of Parallel Meta-Heuristic Algorithms", "url": "", "volume": "9", "year": "2021", "abstract": "Complex networks (CNs) have gained much attention in recent years due to their importance and popularity. The rapid growth in the size of CNs leads to more difficulties in the analysis of CNs tasks. Community Detection (CD) is an important multidisciplinary research area where many machine/deep learning-based methods have been applied to map CNs into a low-dimensional representation for extracting information similarity among members of CNs. Currently, Deep Learning (DL) is one of the promising methods to extract knowledge and learn information from high dimensional space and represent it in low dimensional space. However, designing an accurate and efficient DL-based CD method especially when dealing with large CNs is always an on-going research endeavor to pursue. Meta-Heuristic (MH) algorithms have shown their potentials in improving DL models in terms of solution quality and computational cost. In addition, parallel computing is a feasible solution for building efficient DL models. The algorithmic principle of MH is parallel in nature; however, its computation framework in DL training that is reported in the literature is not really implemented in a parallel computing setup. In this paper, we present a systematic review of CD in CNs from conventional machine learning to DL methods and point out the gap of applying DL-based CD methods in large CNs. In addition, the relevant studies on DL with parallel and MH approaches are reviewed and their implications on DL models are highlighted to prospect effective solutions to overcome the challenges of DL-based CD methods. We also point out research challenges in the field of CD and suggest possible future research directions.", "pages": "96501-96527", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Al-Andoli, Mohammed", "Tan, Shing", "Cheah, Wooi", "Tan, Sin"]}]["7349932", {"address": "", "articleno": "", "doi": "10.1109/TST.2015.7349932", "issn": "1007-0214", "issue_date": "", "journal": "Tsinghua Science and Technology", "keywords": ["Quality of service", "Heuristic algorithms", "Signal processing algorithms", "Particle swarm optimization", "Cloud computing", "Genetic algorithms", "Programming", "MapReduce", " service composition", " Quality of Service (QoS)", " parallel particle swarm optimization"], "month": "December", "number": "6", "numpages": "", "publisher": "", "title": "MR-IDPSO: a novel algorithm for large-scale dynamic service composition", "url": "", "volume": "20", "year": "2015", "abstract": "In the era of big data, data intensive applications have posed new challenges to the field of service composition. How to select the optimal composited service from thousands of functionally equivalent services but different Quality of Service (QoS ) attributes has become a hot research in service computing. As a consequence, in this paper, we propose a novel algorithm MR-IDPSO (MapReduce based on Improved Discrete Particle Swarm Optimization), which makes use of the improved discrete Particle Swarm Optimization (PSO) with the MapReduce to solve large-scale dynamic service composition. Experiments show that our algorithm outperforms the parallel genetic algorithm in terms of solution quality and is efficient for large-scale dynamic service composition. In addition, the experimental results also demonstrate that the performance of MR-IDPSO becomes more better with increasing number of candidate services.", "pages": "602-612", "note": "", "ISSN": "1007-0214", "publicationtype": "article", "author": ["Zhang, Yanping", "Jing, Zihui", "Zhang, Yiwen"]}]["9316706", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2021.3049793", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Buildings", "Tools", "Task analysis", "Social networking (online)", "Crawlers", "Computational modeling", "Vocabulary", "Common crawl", "web crawling", "text corpus", "corpus analysis", "regional languages corpora"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "Corpulyzer: A Novel Framework for Building Low Resource Language Corpora", "url": "", "volume": "9", "year": "2021", "abstract": "The rapid proliferation of artificial intelligence has led to the development of sophisticated cutting-edge systems in natural language processing and computational linguistics domains. These systems heavily rely on high-quality dataset/corpora for the training of deep-learning algorithms to develop precise models. The preparation of a high-quality gold standard corpus for natural language processing on a large scale is a challenging task due to the need of huge computational resources, accurate language identification models, and precise content parsing tools. This task is further exacerbated in case of regional languages due to the scarcity of web content. In this article, we propose a generic framework of Corpus Analyzer - Corpulyzer - a novel framework for building low resource language corpora. Our framework consists of corpus generation and corpus analyzer module. We demonstrate the efficacy of our framework by creating a high-quality large scale corpus for the Urdu language as a case study. Leveraging dataset from Common Crawl Corpus (CCC), first, we prepare a list of seed URLs by filtering the Urdu language webpages. Next, we use Corpulyzer to crawl the World-Wide-Web (WWW) over a period of four years (2016-2020). We build Urdu web corpus \u201cUrduWeb20\u201d that consists of 8.0 million Urdu webpages crawled from 6,590 websites. In addition, we propose Low-Resource Language (LRL) website scoring algorithm and content-size filter for language-focused crawling to achieve optimal use of computational resources. Moreover, we analyze UrduWeb20 using variety of traditional metrics such as web-traffic-rank, URL depth, duplicate documents, and vocabulary distribution along with our newly defined content-richness metrics. Furthermore, we compare different characteristics of our corpus with three datasets of CCC. In general, we observe that contrary to CCC that focuses on crawling the limited number of webpages from highly ranked Urdu websites, Corpulyzer performs an in-depth crawling of Urdu content-rich websites. Finally, we made available Corpulyzer framework for the research community for corpus building.", "pages": "8546-8563", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Tahir, Bilal", "Mehmood, Muhammad"]}]["9095322", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2020.2995346", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Speech enhancement", "Decoding", "Noise measurement", "Spectrogram", "Signal to noise ratio", "Feature extraction", "Convolution", "Speech enhancement", "embedding encoder-decoder", "convolutional neural network", "attention mechanism", "neural network"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "Embedding Encoder-Decoder With Attention Mechanism for Monaural Speech Enhancement", "url": "", "volume": "8", "year": "2020", "abstract": "The auditory selection framework with attention and memory (ASAM), which has an attention mechanism, embedding generator, generated embedding array, and life-long memory, is used to deal with mixed speech. When ASAM is applied to speech enhancement, the discrepancy between the voice and noise feature memories is huge and the separability of noise and voice is increased. However, ASAM cannot achieve desirable performance in terms of speech enhancement because it fails to utilize the time-frequency dependence of the embedding vectors to generate a corresponding mask unit. This work proposes a novel embedding encoder-decoder (EED), and a convolutional neural network (CNN) is used as decoder. The CNN structure is good at detecting local patterns, which can be exploited to extract correlation embedding data from the embedding array to generate the target spectrogram. This work evaluates a similar ASAM, EED with an LSTM encoder and a CNN decoder (RC-EED), RC-EED with an attention mechanism (RC-AEED), other similar EED structures and baseline models. Experiment results show that RC-EED and RC-AEED networks have good performance on speech enhancement task at low signal-to-noise ratio conditions. In addition, RC-AEED exhibits superior speech enhancement performance over ASAM and achieves better speech quality than do deep recurrent network and convolutional recurrent network.", "pages": "96677-96685", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Lan, Tian", "Ye, Wenzheng", "Lyu, Yilan", "Zhang, Junyi", "Liu, Qiao"]}]["9206581", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2020.3027205", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Three-dimensional displays", "Strain", "Inspection", "Surface reconstruction", "Surface treatment", "Cameras", "Image reconstruction", "Aerial inspection", "point cloud change detection", "structural analysis", "structure from motion", "3D reconstruction"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "3D Correspondence and Point Projection Method for Structures Deformation Analysis", "url": "", "volume": "8", "year": "2020", "abstract": "Mining slopes, electrical power generation dams and several big construction enterprises demands continuous inspections. The size and diversity of these structures demands high precision and portable approach. In such environments, 3D reconstruction methodologies are able to capture and analyze the real world in detail. However, the accuracy and precision can affect the ability to process and interpret the acquired data. For instance, laser scanning is a very accurate method and can deliver a higher quality result. Meanwhile, 3D photogrammetry using a single camera and Structure From Motion (SFM) have their performance correlated with the image quality. In a typical application, 3D data from reconstruction is pre-processed by a specialist. Then, it is stored for comparison and analyzed over time. The posterior analysis has several challenges associated with the reconstruction process characteristics. Several techniques have been developed to allow the comparison of point cloud captured at different epochs. Therefore, this research work presents a new methodology to perform alignment and comparison of point clouds, namely 3D-CP2, an acronym for 3D Correspondence and Point Projection. This method intends to analyze the point cloud motion to be applied in terrestrial 3D SFM reconstructions. Besides, the technique can also be used in many other related applications. The methodology developed in this work is applied in controlled experiments and real use cases to show its potential for point cloud displacements analysis. The results showed that the proposed method is efficient and can produce results more accurately than the referenced literature.", "pages": "177823-177836", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Melo, Aurelio", "Pinto, Milena", "Hon\u00f3rio, Leonardo", "Dias, Felipe", "Masson, Juliano"]}]["9433543", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2021.3081601", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["NOMA", "5G mobile communication", "Cooperative communication", "Wireless networks", "Quality of service", "Interference", "Device-to-device communication", "NOMA", "OMA", "uplink", "downlink", "device-to-device", "machine-to-machine"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "A Systematic Review on NOMA Variants for 5G and Beyond", "url": "", "volume": "9", "year": "2021", "abstract": "Over the last few years, interference has been a major hurdle for successfully implementing various end-user applications in the fifth-generation (5G) of wireless networks. During this era, several communication protocols and standards have been developed and used by the community. However, interference persists, keeping given quality of service (QoS) provision to end-users for different 5G applications. To mitigate the issues mentioned above, in this paper, we present an in-depth survey of state-of-the-art non-orthogonal multiple access (NOMA) variants having power and code domains as the backbone for interference mitigation, resource allocations, and QoS management in the 5G environment. These are future smart communication and supported by device-to-device (D2D), cooperative communication (CC), multiple-input and multiple-output (MIMO), and heterogeneous networks (HetNets). From the existing literature, it has been observed that NOMA can resolve most of the issues in the existing proposals to provide contention-based grant-free transmissions between different devices. The key differences between the orthogonal multiple access (OMA) and NOMA in 5G are also discussed in detail. Moreover, several open issues and research challenges of NOMA-based applications are analyzed. Finally, a comparative analysis of different existing proposals is also discussed to provide deep insights to the readers.", "pages": "85573-85644", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Budhiraja, Ishan", "Kumar, Neeraj", "Tyagi, Sudhanshu", "Tanwar, Sudeep", "Han, Zhu", "Piran, Md.", "Suh, Doug"]}]["8976074", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2020.2970063", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Lenses", "Video coding", "Encoding", "Three-dimensional displays", "Bit rate", "Correlation", "Semantics", "3D video coding", "scene detection", "rate control", "image similarity analysis", "bit allocation"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "CTU Layer Rate Control Algorithm in Scene Change Video for Free-Viewpoint Video", "url": "", "volume": "8", "year": "2020", "abstract": "At present, the rate control algorithm for multiview high-efficiency video coding (MV-HEVC) does not have the capability of efficient coding tree unit(CTU) layer bit allocation, and the video quality varies greatly for sequences with sudden scene changes or large motions. To overcome this limitation, this paper proposes a rate control algorithm for MV-HEVC based on scene detection. Firstly, we established \u03c1 domain rate control model based on multi-objective optimization. Then, it uses image similarity to make reasonable bit allocation among viewpoints. If the video scene is switched, the image similarity is recalculated, and then the correlation between the weights of the interview point rates and the correlation between the viewpoints are analyzed. Finally, the frame layer rate control considers the layer B-frame and other factors in allocating the code rate, and the basic unit layer rate control adopts different quantization methods according to the content complexity of the CTU. Experimental results show that the proposed rate control algorithm can maintain good coding efficiency and decrease the average video quality variation by 25.29%.", "pages": "24549-24560", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Yan, Tao", "Ra, In-Ho", "Wen, Hui", "Weng, Min-Hang", "Zhang, Qian", "Che, Yan"]}]["8963698", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2020.2967792", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Object recognition", "Bibliographies", "Data collection", "Quality assessment", "Collaboration", "Computer science", "Recommendations", "news recommendations", "literature review", "recommendation techniques", "news recommendation overview"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "News Recommendation Systems - Accomplishments, Challenges & Future Directions", "url": "", "volume": "8", "year": "2020", "abstract": "News publishers have decreased disseminating news through conventional newspapers and have migrated to the use of digital means like websites and purpose-built mobile applications. It is observed that news recommendation systems can automatically process lengthy articles and identify similar articles for readers considering predefined criteria. The objectives of the current work are to identify and classify the challenges in news recommendation domain, to identify state-of-the-art approaches and classify on the application domain, to identify datasets used for evaluation and their sources, the evaluation approaches used and to highlight the challenges explicitly addressed. The literature is thoroughly studied over the time span of 2001-2019 and shortlisted 81 related studies, broadly classified into six categories and discussed. The analysis showed that 60% of news recommendation system adopted a hybrid approach, 66% studies little talk about datasets, and addresses a few challenges from a long list of challenges in the news domain. This article is the first in the field to draw a comprehensive big picture of news recommendation and explore different dimensions covered in the studies. The last section presents the future research opportunities that lead to improving the recommendation of news articles in the news domain.", "pages": "16702-16725", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Feng, Chong", "Khan, Muzammil", "Rahman, Arif", "Ahmad, Arshad"]}]["8492398", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2018.2876029", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Semantics", "Training", "Ontologies", "Knowledge based systems", "Internet of Things", "Service-oriented architecture", "Event-driven service discovery", "service matching", "semantic similarity assessment", "word embedding"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "Event-Driven Semantic Service Discovery Based on Word Embeddings", "url": "", "volume": "6", "year": "2018", "abstract": "Service discovery is vital to event handling in Internet of Things applications which are based on the event-driven service-oriented architecture. However, in service discovery, the problem of service matching that establishes relationships between services and events has been seldom investigated through a semantic way. In this paper, to facilitate the efficiency of service discovery triggered by events, we propose a novel method of semantic service matching based on word embeddings. In this method, two types of semantic services about events (i.e., event-recognition services and event-handing services) are specified and matched through semantic similarity assessment that is conducted with word embeddings. Besides, to obtain highquality word embeddings, we present a hybrid approach for learning word embedding which treats words in distinct means according to word frequency. Experiments demonstrated on different data sets show that our method of semantic service matching is an effective way to facilitate event-driven service discovery, and the proposed training approach for word embeddings outperforms existing works and is able to improve the accuracy of event-driven service discovery.", "pages": "61030-61038", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Liu, Fagui", "Deng, Dacheng", "Jiang, Jun", "Tang, Quan"]}]["8558534", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2018.2884906", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Manufacturing", "Automation", "Machine learning", "Sensors", "Production", "Control systems", "Cloud computing", "Industrial Internet of Things", "industrial cyber physical systems", "application and service", "control", "networking", "computing", "machine learning", "big data analytics", "survey", "future research directions"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "A Survey on Industrial Internet of Things: A Cyber-Physical Systems Perspective", "url": "", "volume": "6", "year": "2018", "abstract": "The vision of Industry 4.0, otherwise known as the fourth industrial revolution, is the integration of massively deployed smart computing and network technologies in industrial production and manufacturing settings for the purposes of automation, reliability, and control, implicating the development of an Industrial Internet of Things (I-IoT). Specifically, I-IoT is devoted to adopting the IoT to enable the interconnection of anything, anywhere, and at any time in the manufacturing system context to improve the productivity, efficiency, safety, and intelligence. As an emerging technology, I-IoT has distinct properties and requirements that distinguish it from consumer IoT, including the unique types of smart devices incorporated, network technologies and quality-of-service requirements, and strict needs of command and control. To more clearly understand the complexities of I-IoT and its distinct needs and to present a unified assessment of the technology from a systems\u2019 perspective, in this paper, we comprehensively survey the body of existing research on I-IoT. Particularly, we first present the I-IoT architecture, I-IoT applications (i.e., factory automation and process automation), and their characteristics. We then consider existing research efforts from the three key system aspects of control, networking, and computing. Regarding control, we first categorize industrial control systems and then present recent and relevant research efforts. Next, considering networking, we propose a three-dimensional framework to explore the existing research space and investigate the adoption of some representative networking technologies, including 5G, machine-to-machine communication, and software-defined networking. Similarly, concerning computing, we again propose a second three-dimensional framework that explores the problem space of computing in I-IoT and investigate the cloud, edge, and hybrid cloud and edge computing platforms. Finally, we outline particular challenges and future research needs in control, networking, and computing systems, as well as for the adoption of machine learning in an I-IoT context.", "pages": "78238-78259", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Xu, Hansong", "Yu, Wei", "Griffith, David", "Golmie, Nada"]}]["8755846", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2019.2926826", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Predictive models", "Forecasting", "Genetic algorithms", "Computational modeling", "Adaptation models", "Analytical models", "Power generation", "Adaptive model", "big data", "binary genetic algorithm", "distributed PV", "forecasting", "fitness evaluation measure", "predictor subset selection", "renewable energy", "smart grid", "solar energy", "support vector regression"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "Adaptive Predictor Subset Selection Strategy for Enhanced Forecasting of Distributed PV Power Generation", "url": "", "volume": "7", "year": "2019", "abstract": "Distributed photovoltaic (PV) solar power plants are playing an increasing role as a power generation resource in the modern electricity grid. However, PVs pose significant challenges to grid planners, operators, owners, investors, aggregators, and other stakeholders. This is due to the high uncertainty of the PV output power, which is caused by its entire dependence on intermittent environmental factors. This has brought a serious problem to the power industry to integrate and manage power grids containing significant penetration of PVs. Thus, an enhanced PV power forecast is very important to operate these power grids efficiently and reliably. Most previous methodologies have focused on predicting the aggregate amount of potential solar power generation at the national or regional scale and ignored the distributed PVs that are installed primarily for local electric supply. Furthermore, a few research groups have carried out predictor selection before training predictive models. This paper proposes an adaptive hybrid predictor subset selection (PSS) strategy to obtain the most relevant and nonredundant predictors for enhanced short-term forecasting of the power output of distributed PVs. In the proposed strategy, the binary genetic algorithm (BGA) is applied for the feature selection process and support vector regression (SVR) is used for measuring the fitness score of the predictors. In order to validate the effectiveness of the proposed strategy, it is applied to actual distributed PVs located in the Otaniemi area of Espoo, Finland. The findings are compared with those achieved by other PSS techniques. The proposed strategy enhances the quality and efficiency of the predictor subset selection, with minimal chosen predictors to achieve enhanced prediction accuracy. It outperforms the other prediction selection methods. Besides, a configuration of an adaptive forecasting model is introduced and the performance tests are presented to further validate the impact of the PSS results for the PV power prediction accuracy enhancement.", "pages": "90652-90665", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Eseye, Abinet", "Lehtonen, Matti", "Tukia, Toni", "Uimonen, Semen", "Millar, R."]}]["9064504", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2020.2987483", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Smart cities", "Twitter", "Monitoring", "Intelligent sensors", "Early detection of incidents", "smart cities", "citizen sensor", "vehicular communications", "big data analysis", "social networks"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "INRISCO: INcident monitoRing in Smart COmmunities", "url": "", "volume": "8", "year": "2020", "abstract": "Major advances in information and communication technologies (ICTs) make citizens to be considered as sensors in motion. Carrying their mobile devices, moving in their connected vehicles or actively participating in social networks, citizens provide a wealth of information that, after properly processing, can support numerous applications for the benefit of the community. In the context of smart communities, the INRISCO [1] proposal intends for (i) the early detection of abnormal situations in cities (i.e., incidents), (ii) the analysis of whether, according to their impact, those incidents are really adverse for the community; and (iii) the automatic actuation by dissemination of appropriate information to citizens and authorities. Thus, INRISCO will identify and report on incidents in traffic (jam, accident) or public infrastructure (e.g., works, street cut), the occurrence of specific events that affect other citizens' life (e.g., demonstrations, concerts), or environmental problems (e.g., pollution, bad weather). It is of particular interest to this proposal the identification of incidents with a social and economic impact, which affects the quality of life of citizens.", "pages": "72435-72460", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Igartua, M\u00f3nica", "Mendoza, Florina", "Redondo, Rebeca", "Vicente, Manuela", "Forn\u00e9, Jordi", "Campo, Celeste", "Fern\u00e1ndez-Vilas, Ana", "De, Luis", "Garc\u00eda-Rubio, Carlos", "L\u00f3pez, Andr\u00e9s", "Mezher, Ahmad", "D\u00edaz-S\u00e1nchez, Daniel", "Cerezo-Costas, H\u00e9ctor", "Rebollo-Monedero, David", "Arias-Cabarcos, Patricia", "Rico-Novella, Francisco"]}]["7876775", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2017.2681743", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Routing", "Measurement", "Wireless sensor networks", "Cognitive radio", "Algorithm design and analysis", "Routing protocols", "Internet of Things", "cognitive sensor networks", "data forwarding", "spectrum-availability", "retransmission"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "Spectrum-Availability Based Routing for Cognitive Sensor Networks", "url": "", "volume": "5", "year": "2017", "abstract": "With the occurrence of Internet of Things (IoT) era, the proliferation of sensors coupled with the increasing usage of wireless spectrums especially the ISM band makes it difficult to deploy real-life IoT. Currently, the cognitive radio technology enables sensors transmit data packets over the licensed spectrum bands as well as the free ISM bands. The dynamic spectrum access technology enables secondary users (SUs) access wireless channel bands that are originally licensed to primary users. Due to the high dynamic of spectrum availability, it is challenging to design an efficient routing approach for SUs in cognitive sensor networks. We estimate the spectrum availability and spectrum quality from the view of both the global statistical spectrum usage and the local instant spectrum status, and then introduce novel routing metrics to consider the estimation. In our novel routing metrics, one retransmission is allowed to restrict the number of rerouting and then increase the routing performance. Then, the related two routing algorithms according to the proposed routing metrics are designed. Finally, our routing algorithms in extensive simulations are implemented to evaluate the routing performance, and we find that the proposed algorithms achieve a significant performance improvement compared with the reference algorithm.", "pages": "4448-4457", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Zhang, Lichen", "Cai, Zhipeng", "Li, Peng", "Wang, Liang", "Wang, Xiaoming"]}]["8463459", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2018.2865613", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Convolution", "Image resolution", "Training", "Image reconstruction", "Convolutional neural networks", "Convergence", "Image restoration", "Image super-resolution", "residual learning", "dilated convolution"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "Fast Single Image Super-Resolution Via Dilated Residual Networks", "url": "", "volume": "7", "year": "2019", "abstract": "Recently, deep convolutional neural networks (CNNs) have been attracting considerable attention in single image super-resolution. Some CNN-based methods, such as VDSR verified that residual learning can speed up the training and significantly improve the performance of accuracy. However, with very deep networks, convergence speed is still a critical issue in training due to the cost of requiring enormous parameters. In order to deal with this issue, we redesign the residual networks based on dilated networks. In this paper, we propose symmetrical dilated residual convolution networks (FDSR) to tackle image super-resolution problems. Our network is on the basis of the dilated convolutions supported exponential expansion of the receptive field without loss of resolution and coverage. This means that FDSR can speed up the training and improve the performance of accuracy without increasing the model's depth or complexity. Meanwhile, we attempt to combine the image pre-processing approach of VGG-net with mean squared error (MSE) to enhance the performance. The experimental results demonstrate that the training time-consuming proposed model achieves nearly a half with even superior restoration quality. Further, we present a novel network with less layers and parameters can achieve real-time performance on a generic CPU and still maintain superior performance.", "pages": "109729-109738", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Lu, Zhang", "Yu, Zhang", "Yali, Peng", "Shigang, Liu", "Xiaojun, Wu", "Gang, Lu", "Yuan, Rao"]}]["8758410", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2019.2927524", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Binary codes", "Semantics", "Hash functions", "Deep learning", "Training", "Quantization (signal)", "Neural networks", "Hashing", "asymmetric", "deep learning", "similarity"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "Dual Asymmetric Deep Hashing Learning", "url": "", "volume": "7", "year": "2019", "abstract": "Due to the impressive learning power, deep learning has achieved a remarkable performance in supervised hash function learning. In this paper, we propose a novel asymmetric supervised deep hashing method to preserve the semantic structure among different categories and generate the binary codes simultaneously. Specifically, two asymmetric deep networks are constructed to reveal the similarity between each pair of images according to their semantic labels. Furthermore, since the binary codes in the Hamming space also should keep the semantic affinity existing in the original space, another asymmetric pairwise loss is introduced to capture the similarity between the binary codes and real-value features. This asymmetric loss not only improves the retrieval performance, but also contributes to a quick convergence at the training phase. By taking advantage of the two-stream deep structures and two types of asymmetric pairwise functions, an alternative algorithm is designed to efficiently optimize the deep features and high-quality binary codes. Experimental results on three real-world datasets substantiate the effectiveness and superiority of our approach as compared with the state-of-the-art.", "pages": "113372-113384", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Li, Jinxing", "Zhang, Bob", "Lu, Guangming", "Zhang, David"]}]["9229053", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2020.3031973", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Image restoration", "Generative adversarial networks", "Feature extraction", "Biomedical imaging", "Training", "Convolution", "Semantics", "Serial sectioning images", "generative adversarial network", "consecutive context perceive GAN"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "Consecutive Context Perceive Generative Adversarial Networks for Serial Sections Inpainting", "url": "", "volume": "8", "year": "2020", "abstract": "Image inpainting is a hot topic in computer vision research and has been successfully applied to both traditional and digital mediums, such as oil paintings or old photos mending, image or video denoising and super-resolution. With the introduction of artificial intelligence (AI), a series of algorithms, represented by semantic inpainting, have been developed and better results were achieved. Medical image inpainting, as one of the most demanding applications, needs to meet both the visual effects and strict content correctness. 3D reconstruction of microstructures, based on serial sections, could provide more spatial information and help us understand the physiology or pathophysiology mechanism in histology study, in which extremely high-quality continuous images without any defects are required. In this article, we proposed a novel Consecutive Context Perceive Generative Adversarial Networks (CCPGAN) for serial sections inpainting. Our method can learn semantic information from its neighboring image, and restore the damaged parts of serial sectioning images to maximum extent. Validated with 2 sets of serial sectioning images of mouse kidney, qualitative and quantitative results suggested that our method could robustly restore breakage of any size and location while achieving near realtime performance.", "pages": "190417-190430", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Zhang, Siqi", "Wang, Lei", "Zhang, Jie", "Gu, Ling", "Jiang, Xiran", "Zhai, Xiaoyue", "Sha, Xianzheng", "Chang, Shijie"]}]["9181518", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2020.3020685", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["IEEE 802.15 Standard", "Europe", "Reliability", "Smart cities", "Wireless sensor networks", "Bandwidth", "IEEE 802.15.4", "IoT", "LoRa", "LPWAN", "real-world deployment", "smart cities"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "A Comparative Study of LoRa and IEEE 802.15.4-Based IoT Deployments Inside School Buildings", "url": "", "volume": "8", "year": "2020", "abstract": "IoT deployments for smart cities and smart buildings have been multiplying exponentially in recent years, benefiting from a steady rise in the number of new technologies that deal with the underlying networking and application challenges in indoor and outdoor spaces. Due to the overlap in their specifications, we are still trying to figure out which of these technologies fits better to certain application domains, such as building monitoring. In this work, we provide a comparative study between IEEE 802.15.4 and LoRa, based on our experiences from using both wireless networking technologies in the context of indoor deployments aimed at IoT-enabled school buildings in Europe. We provide an apples-to-apples comparison between the two technologies, comparing them in some cases in the same building and application context. Although these two technologies initially might not seem to be competing in the same application space, in practice we found out that both have strengths and weaknesses in the specific application domain we have been using them. Moreover, our LoRa-based networking implementation, on top of Arduino-based hardware, appears to be an option that allows for a robust, reliable and lower overall cost IoT deployment, especially in cases with multi-floor building installations and low bandwidth requirements. We also present a network-level dataset produced from our installations and upon which we based our findings and discussion. We provide data collected from 6 different school buildings, 8 networks and 49 devices, to compare the performance and cost-effectiveness of competing IoT technologies. In that effect, with LoRa we can achieve similar or better link quality to IEEE 802.15.4, with higher data rate and lower costs.", "pages": "160957-160981", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Fraile, Lidia", "Tsampas, Stelios", "Mylonas, Georgios", "Amaxilatis, Dimitrios"]}]["9913473", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2022.3212694", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Degradation", "Predictive models", "Logic gates", "Uncertainty", "Mechanical systems", "Feature extraction", "Data models", "Fusion model", "gated recurrent unit", "prediction intervals", "remaining useful life", "system prognostics", "uncertainty management"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "Remaining Useful Life Interval Prediction for Complex System Based on BiGRU Optimized by Log-Norm", "url": "", "volume": "10", "year": "2022", "abstract": "The task of remaining useful life (RUL) uncertainty management is the major challenge in solving the failure of the complex mechanical system. Primary research methods use statistical models or stochastic processes to fit the distribution of historical degradation data. However, it is difficult to accurately capture the degradation information of monitoring big data through statistics in practice. In this paper, the prediction interval (PI) obtained by the proposed feature attention-log-norm bidirectional gated recurrent unit (FA-LBiGRU) model is adopted to quantify the prediction uncertainty of RUL. Initially, the critical feature vectors are extracted from multi-dimensional, nonlinear, and large-scale sensor signals using the feature attention mechanism. Additionally, the BiGRU network is used to model and learn the time-varying characteristics of the attention-weighted features from the forward and backward directions, and the network parameters are trained by the maximum log-likelihood loss function. Ultimately, the probability density function based on the lognormal distribution is calculated to measure the uncertainty of the equipment RUL. The effectiveness of the proposed method is verified through the well-known benchmark data set of the turbofan engines provided by NASA. The experimental results show that the proposed methods can obtain higher point prediction accuracy for the complex system compared with state-of-the-art approaches and high-quality PIs satisfying real-time requirements.", "pages": "108089-108102", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Yan, Xiaojia", "Liang, Weige", "Xu, Dongxue"]}]["9923925", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2022.3215620", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Supply chains", "Resilience", "Fourth Industrial Revolution", "Quality function deployment", "Consumer electronics", "Decision making", "Business", "Industry 4.0", "supply chain resilience", "ripple effects", "multi-attribute decision making", "quality function deployment"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "Deploying Industry 4.0 Enablers to Strengthen Supply Chain Resilience to Mitigate Ripple Effects: An Empirical Study of Top Relay Manufacturer in China", "url": "", "volume": "10", "year": "2022", "abstract": "In the global marketplace, supply chains are becoming increasingly linked. The risk posed by the ripple effect of uncertainty will damage the normal operation of the entire supply chain, so it is an important issue to enhance the resilience of enterprises that manufacture electronic to reduce the ripple effects of supply chains. Quality function deployment (QFD) has been applied in many areas to solve multi-criteria decision making (MCDM) problems successfully. However, there is still lack of study has addressed the improvement of supply chain resilience through Industry 4.0 to alleviate the ripple effect, and the application of QFD to integrate and analyze the relationship between Industry 4.0 and supply chain resilience, and between supply chain resilience and the ripple effect. Therefore, this study proposes an integrated QFD-MCDM model to identify the key Industry 4.0 enablers (I4Es) to strengthen supply chain resilience indicators (SCRIs) and mitigate the ripple effect risk factors (RERFs), thus providing an effective method for enterprises to develop a resilient supply chain that can quickly respond to changes and uncertainties. The case study considered China\u2019s largest relay manufacturing enterprise as the object and obtained important management insights, as well as practical significance, from implementing the proposed research framework. The study found the following to be the most urgent I4Es required to strengthen SCRIs and reduce the key RERFs: IT information technology structure and level, enterprise strategic management and new technology coordination, supply chain digitization, analysis and management of big Data, IT Infrastructure and use digital technology for new product innovation, intelligent. When these measures are improved, the SCRIs can be improved, such as risk awareness, efficiency, agility, sustainability, coordination and cooperation, supply chain structure and safety assurance. Finally, RERFs, such as the ripple effect caused by economic collapse, epidemic conditions, natural disasters, political factors, supply chain operation capability caused, can be alleviated or eliminated. With limited resources, enterprises can devote their most important resources to the most critical Industry 4.0 improvement strategies. This framework provides an effective method for electronics manufacturers to formulate I4Es and strengthen SCRIs to mitigate the RERFs, and also provides a reference for enterprises in other fields of supply chain management.", "pages": "114829-114855", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Hsu, Chih-Hung", "Zeng, Jun-Yi", "Chang, An-Yuan", "Cai, Shu-Qi"]}]["7782457", {"address": "", "articleno": "", "doi": "10.1109/TSC.2016.2638812", "issn": "1939-1374", "issue_date": "", "journal": "IEEE Transactions on Services Computing", "keywords": ["Feedback", "Measurement", "Reliability", "Security", "Indexes", "Reputation evaluation", "incomplete user feedback", "malicious ratings", "index weights"], "month": "Nov", "number": "6", "numpages": "", "publisher": "", "title": "A High-Reliability Multi-Faceted Reputation Evaluation Mechanism for Online Services", "url": "", "volume": "12", "year": "2019", "abstract": "In today's society, there are plenty of services available, and customers are facing bigger challenge in choosing them than ever before. Therefore, it is important to build a reliable reputation mechanism for selecting a credible service. To address the challenges of reputation evaluation, including the diverse and dynamic natures of services, incompleteness of user feedback, and intricacy of malicious ratings, a High-reliability Multi-faceted Reputation evaluation mechanism for online services (HMRep) is proposed. First, HMRep starts with addressing the incomplete feedback and estimates missing ratings based on both the service quality and a user's rating behavior. Second, HMRep identifies and removes malicious collusive raters and irresponsible raters to improve the accuracy of reputation calculation. Further, the reputation calculation is based on the user credibility and incorporates historical information to reflect the change of the services. Finally, we provide a multi-faceted evaluation method to satisfy some specific needs of customers who are only concerned about a subset of a services features. Experimental results verify the design of HMRep, and reveal HMRep can effectively defend against malicious ratings, and accurately calculate the reputation values of services. HMRep can be applied in lots of sectors for different kinds of services, especially those complex ones.", "pages": "836-850", "note": "", "ISSN": "1939-1374", "publicationtype": "article", "author": ["Wang, Miao", "Wang, Guiling", "Zhang, Yujun", "Li, Zhongcheng"]}]["8620498", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2019.2892864", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["3G mobile communication", "Fading channels", "Long Term Evolution", "Doppler shift", "Coherence", "Bandwidth", "Telecommunications", "Mobile phone", "UMTS", "LTE", "minimization of drive tests", "fading", "multipath", "Doppler", "channel characterization"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "Multipath and Doppler Characterization of an Electromagnetic Environment by Massive MDT Measurements From 3G and 4G Mobile Terminals", "url": "", "volume": "7", "year": "2019", "abstract": "This paper describes an innovative approach to radio channel characterization in UMTS and LTE mobile networks. In place of traditional drive tests (DT), which employ a single test mobile, a massive collection of georeferenced radio measurements is made from a wide population of user equipment (UE). This is possible with new 3GPP features, called \u201cminimization of DTs\u201d (MDT), which are implemented in the last generation UEs and enable the reporting of additional periodical measurements, including GPS position and estimated UE distance (i.e. delay) over the radio path. This opens to new fields of investigation in the mobile radio channel, unreachable with the legacy DT approach, such as multipath and Doppler analysis. The UE MDT data of UMTS and LTE RAN of Telecom Italia Mobile, in the Italian midsized city of Bologna, have been statistically analyzed. The big data elaboration has been performed with the Nokia proprietary system \u201cGeoSynthesis.\u201d The results give a high-resolution geographical view of the above-mentioned channel phenomena affecting the quality and user experience. They are in good accordance with network performance indicators: the higher the multipath time, the worse the decoding performance of radio blocks (block error rate). The estimated Doppler shift also fits the known mobility patterns in the urban environment.", "pages": "13024-13034", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Scaloni, Andrea", "Cirella, Pasquale", "Sgheiz, Mauro", "Diamanti, Riccardo", "Micheli, Davide"]}]["9353046", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2021.3058428", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Feature extraction", "Visualization", "Task analysis", "Satellite broadcasting", "Probability distribution", "Prediction algorithms", "Classification", "dimensionality reduction", "distribution overlap", "feature evaluation", "feature extraction", "feature selection", "filter methods", "machine learning", "overlap coefficient", "pattern recognition"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "Empirical Comparison of the Feature Evaluation Methods Based on Statistical Measures", "url": "", "volume": "9", "year": "2021", "abstract": "One of the most important classification problems is selecting proper features, i.e. features that describe the classified object in the most straightforward way possible. Then, one of the biggest challenges of the feature selection is the evaluation of the feature\u2019s quality. There is a plethora of feature evaluation methods in the literature. This paper presents the results of a comparison between nine selected feature evaluation methods, both existing in literature and newly defined. To make a comparison, features from ten various sets were evaluated by every method. Then, from every feature set, best subset (according to each method) was chosen. Those subsets then were used to train a set of classifiers (including decision trees and forests, linear discriminant analysis, naive Bayes, support vector machines, k nearest neighbors and an artificial neural network). The maximum accuracy of those classifiers, as well as the standard deviation between their accuracies, were used as a quality measures of each particular method. Furthermore, it was determined, which method is the most universal in terms of the data set, i.e. for which method, obtained accuracies were dependent on the feature set the least. Finally, computation time of each method was compared. Results indicated that for applications with limited computational power, method based on the average overlap between feature\u2019s values seem best suited. It led to high accuracies and proved to be fast to compute. However, if the data set is known to be normally distributed, method based on two-sample ${t}$ -test may be preferable.", "pages": "27868-27883", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["\u0141ysiak, Adam", "Szmajda, Miros\u0142aw"]}]["8306964", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2018.2805842", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Load management", "Servers", "Control systems", "Software defined networking", "Protocols", "Systematics", "Routing", "Load balancing", "review", "SDN", "software defined networks", "systematic"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "Load Balancing Mechanisms in the Software Defined Networks: A Systematic and Comprehensive Review of the Literature", "url": "", "volume": "6", "year": "2018", "abstract": "With the expansion of the network and increasing their users, as well as emerging new technologies, such as cloud computing and big data, managing traditional networks is difficult. Therefore, it is necessary to change the traditional network architecture. Lately, to address this issue, a notion named software-defined network (SDN) has been proposed, which makes network management more conformable. Due to limited network resources and to meet the requirements of quality of service, one of the points that must be considered is load balancing issue that serves to distribute data traffic among multiple resources in order to maximize the efficiency and reliability of network resources. Load balancing is established based on the local information of the network in the conventional network. Hence, it is not very precise. However, SDN controllers have a global view of the network and can produce more optimized load balances. Although load balancing mechanisms are important in the SDN, to the best of our knowledge, there exists no precise and systematic review or survey on investigating these issues. Hence, this paper reviews the load balancing mechanisms which have been used in the SDN systematically based on two categories, deterministic and non-deterministic. Also, this paper represents benefits and some weakness regarded of the selected load balancing algorithms and investigates the metrics of their algorithms. In addition, the important challenges of these algorithms have been reviewed, so better load balancing techniques can be applied by the researchers in the future.", "pages": "14159-14178", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Neghabi, Ali", "Jafari, Nima", "Hosseinzadeh, Mehdi", "Rezaee, Ali"]}]["9444358", {"address": "", "articleno": "", "doi": "10.1109/TUFFC.2021.3084898", "issn": "1525-8955", "issue_date": "", "journal": "IEEE Transactions on Ultrasonics, Ferroelectrics, and Frequency Control", "keywords": ["Reliability", "Acoustics", "Monitoring", "Industries", "Standards", "Inspection", "Automotive engineering", "Acousto ultrasonics (AUs)", "automotive industry", "damage detection", "elastodynamic finite integration technique (EFIT)", "reliability assessment"], "month": "Oct", "number": "10", "numpages": "", "publisher": "", "title": "Feasibility of Model-Assisted Probability of Detection Principles for Structural Health Monitoring Systems Based on Guided Waves for Fiber-Reinforced Composites", "url": "", "volume": "68", "year": "2021", "abstract": "In many industrial sectors, structural health monitoring (SHM) is considered as an addition to nondestructive testing (NDT) that can reduce maintenance effort during the lifetime of a technical facility, structural component, or vehicle. A large number of SHM methods are based on ultrasonic waves, whose properties change depending on structural health. However, the wide application of SHM systems is limited due to the lack of suitable methods to assess their reliability. The evaluation of the system performance usually refers to the determination of the probability of detection (POD) of a test procedure. Up until now, only a few limited methods exist to evaluate the POD of SHM systems, which prevents them from being standardized and widely accepted in the industry. The biggest hurdle concerning the POD calculation is the large number of samples needed. A POD analysis requires data from numerous identical structures with integrated SHM systems. Each structure is then damaged at different locations and with various degrees of severity. All of these are connected to high costs. Therefore, one possible way to tackle this problem is to perform computer-aided investigations. In this work, the POD assessment procedure established in NDT according to the Berens model is adapted to guided wave-based SHM systems. The approach implemented here is based on solely computer-aided investigations. After efficient modeling of wave propagation phenomena across an automotive component made of a carbon-fiber-reinforced composite, the POD curves are extracted. Finally, the novel concept of a POD map is introduced to look into the effect of damage position on system reliability.", "pages": "3156-3173", "note": "", "ISSN": "1525-8955", "publicationtype": "article", "author": ["Tsch\u00f6ke, Kilian", "Mueller, Inka", "Memmolo, Vittorio", "Moix-Bonet, Maria", "Moll, Jochen", "Lugovtsova, Yevgeniya", "Golub, Mikhail", "Venkat, Ramanan", "Schubert, Lars"]}]["9406802", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2021.3073915", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Training", "Generators", "Generative adversarial networks", "Deep learning", "Transfer learning", "Image quality", "Computational modeling", "Automated detection", "image synthesis", "sewer defects", "well-known CNN classifiers", "StyleGANs"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "Automated Sewer Defects Detection Using Style-Based Generative Adversarial Networks and Fine-Tuned Well-Known CNN Classifier", "url": "", "volume": "9", "year": "2021", "abstract": "Automated sewer defects detection has become an important trend for better management and maintenance of urban sewer systems. Deep learning technology has developed rapidly and offers an innovative solution for automated detection in engineering applications. However, insufficient data and unbalanced samples have proposed a big challenge to deep learning model training. This study adopts the state-of-the-art Style-based Generative Adversarial Networks (StyleGANs) model and compares the performances of its two variants in producing high-quality synthetic sewer defects images. Seven well-known CNN models are further fine-tuned and trained using the synthetic images for automated sewer defects detection to examine the effects of StyleGANs on augmenting the detection performance. Results show that both StyleGANs are efficient in producing high-quality images with various styles and high-level details for multiple types of sewer defects. Specifically, the StyleGAN2-Adaptive Discriminator Augmentation (StyleGAN2-ADA) with the aid of Freeze Discriminator (Freeze-D) yields the best model performance. Among the adopted CNN classifiers, Inception_v3 achieves the highest detection accuracy. The mean detection accuracy is 94% (with a specific accuracy of 99.7%, 97%, 95.3% and 84% for tree root, residential wall, disjoint and obstacle, respectively) and confirms the reliability of the StyleGANs' performance. The study shows that StyleGANs provide a promising method to alleviate the limited and uneven dataset problem and can improve the deep learning model performance.", "pages": "59498-59507", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Situ, Zuxiang", "Teng, Shuai", "Liu, Hanlin", "Luo, Jinhua", "Zhou, Qianqian"]}]["9787492", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2022.3180033", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Emergency services", "Smart cities", "Crisis management", "Internet of Things", "Sustainable development", "Hazards", "Emergency services", "Resiliency", "Smart cities", "emergencies management", "Internet of Things", "sustainable cities", "resilient cities"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "A Survey of Emergencies Management Systems in Smart Cities", "url": "", "volume": "10", "year": "2022", "abstract": "The rapid urbanization process in the last century has deeply changed the way we live and interact with each other. As most people now live in urban areas, cities are experiencing growing demands for more efficient and sustainable public services that may improve the perceived quality of life, specially with the anticipated impacts of climatic changes. In this already complex scenario with increasingly overcrowded urban areas, different types of emergency situations may happen anywhere and anytime, with unpredictable costs in human lives and economic losses. In order to cope with unexpected and potentially dangerous emergencies, smart cities initiatives have been developed in different cities, addressing multiple aspects of emergencies detection, alerting, and mitigation. In this context, this article surveys recent smart city solutions for crisis management, proposing definitions for emergencies-oriented systems and classifying them according to the employed technologies and provided services. Additionally, recent developments in the domains of Internet of Things, Artificial Intelligence and Big Data are also highlighted when associated to the management of urban emergencies, potentially paving the way for new developments while classifying and organizing them according to different criteria. Finally, open research challenges will be identified, indicating promising trends and research directions for the coming years.", "pages": "61843-61872", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Costa, Daniel", "Peixoto, Jo\u00e3o", "Jesus, Thiago", "Portugal, Paulo", "Vasques, Francisco", "Rangel, Elivelton", "Peixoto, Maycon"]}]["9497114", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2021.3100459", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Load management", "Load modeling", "Social networking (online)", "Optimization", "Heuristic algorithms", "Cellular networks", "Prediction algorithms", "Social events", "mobile communication", "fuzzy control", "load balancing", "social-awareness", "communication system operations and management"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "Social-Aware Load Balancing System for Crowds in Cellular Networks", "url": "", "volume": "9", "year": "2021", "abstract": "Over time, load balancing systems in cellular networks have been key to avoiding overload problems in the network and to maintaining a correct resource allocation and performance. However, the classical approaches were not designed for the dynamism generated by user behavior. The big crowds of users at certain social venues are one of the main concerns of mobile operators due to the load imbalance generated. Additionally, the mobility of users who attend social events (e.g., sports events, concerts, etc.) greatly impacts network performance due to its high correlation with network traffic. The availability to inform about events, particularly regarding venue location (e.g., stadiums, concert halls, convention centers) is exponentially growing thanks to its proliferation in social networks through geolocation databases and other functionalities. Therefore, the present work proposes a novel load balancing system integrating a fuzzy logic controller algorithm with social-awareness, which considers the relative position between cell sites and the social event venue in order to configure the network parameters. This approach is evaluated for different configurations of load balancing methods simulated on an urban macro scenario, mitigating the impact of the number of users per cell without degrading the signal quality. In this way, results show that social event data information plus soft or aggressive transmission power changes in cells can help to maintain the balance in the number of users per cell during mass events.", "pages": "107812-107823", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Torres, Renato", "Fortes, Sergio", "Baena, Eduardo", "Barco, Raquel"]}]["6261335", {"address": "", "articleno": "", "doi": "10.1109/TLT.2012.17", "issn": "1939-1382", "issue_date": "", "journal": "IEEE Transactions on Learning Technologies", "keywords": ["Education courses", "Accreditation", "Computer science education", "Education", "Electronic learning", "Education courses", "Accreditation", "Computer science education", "Education", "Electronic learning", "Curriculum mapping", "Education courses", "Accreditation", "Computer science education", "Education", "Electronic learning", "graduate attributes", "Education courses", "Accreditation", "Computer science education", "Education", "Electronic learning", "accreditation competencies", "Education courses", "Accreditation", "Computer science education", "Education", "Electronic learning", "learner model"], "month": "Jan", "number": "1", "numpages": "", "publisher": "", "title": "Foundations for Modeling University Curricula in Terms of Multiple Learning Goal Sets", "url": "", "volume": "6", "year": "2013", "abstract": "It is important, but very challenging, to design degree programs, so that the sequence of learning activities, topics, and assessments over three to five years give an effective progression in learning of generic skills, discipline-specific learning goals and accreditation competencies. Our CUSP (Course and Unit of Study Portal) system tackles this challenge, by helping subject teachers define the curriculum of their subject, linking it to Faculty and institutional goals. The same information is available to students, enabling them to see how each subject relates to those goals. It then gives additional big-picture views of the degree for the academics responsible for the whole degree, including the ability to easily assess if a degree meets accreditation requirements. CUSP achieves this by exploiting a lightweight semantic mapping approach that gives a highly flexible and scalable way to map learning goals from multiple internal and external accrediting sources across the degree. We report its validation as used in a live university environment, across three diverse faculties, with 277 degrees and 7,810 subject sessions over a period of three years. Data from this evaluation indicates steady improvement in the documentation of the relationships between subjects, assessments, learning outcomes, and program level goals. This is driven by the reporting tools and visualizations provided by CUSP, which enable program designers and lecturers to identify parts of the curriculum that are unclear. This improved documentation of the curriculum enables more accurate and immediate quality reviews. Key contributions of this work are: a validated new approach for curriculum design that helps address the complexity of ensuring learners progressively develop generic skills; and a validated lightweight semantic mapping approach that can flexibly support visualizing the curriculum against multiple sets of learning goal frameworks.", "pages": "25-37", "note": "", "ISSN": "1939-1382", "publicationtype": "article", "author": ["Gluga, Richard", "Kay, Judy", "Lever, Tim"]}]["9696351", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2022.3147488", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Relays", "Logic gates", "Quality of service", "Internet of Things", "Wireless sensor networks", "Optimization", "Base stations", "Coverage", "GIS", "heterogeneous propagation environment", "Internet of Things", "IoT", "RF propagation tool", "RSSI", "sub giga hertz", "sub-GHz"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "Relay Placement Algorithms for IoT Connectivity and Coverage in an Outdoor Heterogeneous Propagation Environment", "url": "", "volume": "10", "year": "2022", "abstract": "A vast majority of the Internet of Things (IoT) devices will be connected in a topology where the edge-devices push data to a local gateway, which forwards the data to a cloud for further processing. In sizeable outdoor deployment regions, the edge-devices may experience poor connectivity due to their distant locations and limited transmission power. Repeaters or relays must be placed at a few locations to ensure reliable connectivity to either a gateway or another node in the network. A big challenge in achieving reliable connectivity and coverage is the outdoor propagation environment being heterogeneous. Engineers often deploy networks based on resource-intensive field visits, detailed surveys, measurements, initial test deployments, followed by fine-tuning. For scalability to large scale IoT deployments, automated network planning tools are essential. Such tools should predict connectivity based on the edge-device locations using available Geographical Information System (GIS) data, identify the need for relays/repeaters, and, if needed, suggest the number of relays needed with their locations. Furthermore, such tools should also be extended to suggest the minimum number and locations of base stations that maximise coverage. In this paper, we propose an automated network deployment framework using a black box received signal strength estimation oracle that provides signal strength estimates between candidate pairs of transceiver locations in a heterogeneous deployment region. Our proposed methodology uses either Ant Colony Optimisation (ACO) or Differential Evolution (DE) to identify the number and locations of relays for meeting specified quality of service constraints. We discuss adaptations of our techniques to handle scenarios with multiple gateways. Further, we show the effectiveness of these algorithms to find suitable candidate base station locations to provide coverage in a heterogeneous propagation environment that meets the specified quality of service constraints. We then demonstrate the effectiveness of our algorithms in two deployment regions.", "pages": "13270-13289", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Rathod, Nihesh", "Sundaresan, Rajesh"]}]["8168250", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2017.2779263", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Cloud computing", "Edge computing", "Taxonomy", "Data structures", "Internet of Things", "Electronic mail", "Blockchain", "decentralized consensus systems", "directed acyclic graph", "edge-centric Internet of Things"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "Decentralized Consensus for Edge-Centric Internet of Things: A Review, Taxonomy, and Research Issues", "url": "", "volume": "6", "year": "2018", "abstract": "With the exponential rise in the number of devices, the Internet of Things (IoT) is geared toward edge-centric computing to offer high bandwidth, low latency, and improved connectivity. In contrast, legacy cloud-centric platforms offer deteriorated bandwidth and connectivity that affect the quality of service. Edge-centric Internet of Things-based technologies, such as fog and mist computing, offer distributed and decentralized solutions to resolve the drawbacks of cloud-centric models. However, to foster distributed edge-centric models, a decentralized consensus system is necessary to incentivize all participants to share their edge resources. This paper is motivated by the shortage of comprehensive reviews on decentralized consensus systems for edge-centric Internet of Things that elucidates myriad of consensus facets, such as data structure, scalable consensus ledgers, and transaction models. Decentralized consensus systems adopt either blockchain or blockchainless directed acyclic graph technologies, which serve as immutable public ledgers for transactions. This paper scrutinizes the pros and cons of state-of-the-art decentralized consensus systems. With an extensive literature review and categorization based on existing decentralized consensus systems, we propose a thematic taxonomy. The pivotal features and characteristics associated with existing decentralized consensus systems are analyzed via a comprehensive qualitative investigation. The commonalities and variances among these systems are analyzed using key criteria derived from the presented literature. Finally, several open research issues on decentralized consensus for edge-centric IoT are presented, which should be highlighted regarding centralization risk and deficiencies in blockchain/blockchainless solutions.", "pages": "1513-1524", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Yeow, Kimchai", "Gani, Abdullah", "Ahmad, Raja", "Rodrigues, Joel", "Ko, Kwangman"]}]["8485356", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2018.2874336", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Image coding", "Compressed sensing", "Encryption", "Image reconstruction", "Sparse matrices", "Chebyshev approximation", "Compressed sensing", "Gauss measurement matrix", "random permutation matrix", "chaos mapping", "chosen plain text attack"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "A Novel Image Compression-Encryption Scheme Based on Chaos and Compression Sensing", "url": "", "volume": "6", "year": "2018", "abstract": "In this paper, a novel image compression\u2013encryption hybrid algorithm is proposed. First, a Gauss random matrix and a random scrambling matrix are generated by using Chebyshev mapping and Logistic mapping, respectively. Then, based on the principle that a scrambling Gauss matrix is still a Gauss matrix, a compression scheme for ciphertext images is designed. It is dependent on the Gauss random matrix and the random scrambling matrix, which mainly consists of three parts: the permutation-based encryption using random scrambling matrix by Alice, the encoding with Gauss random matrix by Charlie, and the joint decryption and decoding by Bob. It has a special application scenario, that is, Alice requires semi integrity Charlie to transmit images to Bob through a channel. Experimental results show that the scheme has strong robustness against noise and chosen-plaintext attack, and further the peak signal-to-noise ratio (PSNR) and subjective visual quality of reconstructed images can be improved by comparing with the similar methods.", "pages": "67095-67107", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Zhu, Shuqin", "Zhu, Congxu", "Wang, Wenhong"]}]["8682040", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2019.2908933", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Feature extraction", "Proposals", "Text recognition", "Recurrent neural networks", "Connectors", "Microsoft Windows", "Gabor filters", "Scene text detection", "multi-orientation", "convolutional neural network", "recurrent neural network", "residual network"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "FTPN: Scene Text Detection With Feature Pyramid Based Text Proposal Network", "url": "", "volume": "7", "year": "2019", "abstract": "Scene text detection is to detect the position of a text in the natural scene, the quality of which will directly affect the subsequent text recognition. It plays an important role in fields such as image retrieval and autopilot. How to perform multi-scale and multi-oriented text detection in the scene still remains as a problem. This paper proposes an effective scene text detection method that combines the convolutional neural network (CNN) and recurrent neural network (RNN). In order to better adapt to texts in different scales, feature pyramid networks (FPN) have been applied in the CNN part to extract multi-scale features of the image. We then utilize bidirectional long-short-term memory (Bi-LSTM) to encode these features to make full use of the text sequence characteristics with the outputs as a series of text proposals. The generated proposals are finally linked into a text line through a well-designed text connector, which can be flexibly adapted to any oriented texts. The proposed method is evaluated on three public datasets: ICDAR2013, ICDAR2015, and USTB-SV1K. For ICDAR2013 and USTB-1K, we have reached 92.5% and 62.6% F-measure, respectively. Our method has reached 72.8% F-measure on the more challenging ICDAR2015 which demonstrates the effectiveness of our method.", "pages": "44219-44228", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Liu, Fagui", "Chen, Cheng", "Gu, Dian", "Zheng, Jingzhong"]}]["8600319", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2018.2889466", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Image edge detection", "Estimation", "Mathematical model", "Image restoration", "Kernel", "Reliability", "Optimization", "Blind image deblurring", "blur kernel", "spatial scale", "L\u2080-regularized intensity prior", "L\u2082-regularized gradient prior"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "Multi-Regularization-Constrained Blur Kernel Estimation Method for Blind Motion Deblurring", "url": "", "volume": "7", "year": "2019", "abstract": "Blur kernel (BK) estimation is the crucial technique to guarantee the success of blind image deblurring. In this paper, we propose a multi-regularization-constrained method to estimate an accurate BK from a single motion-blurred image. First, in order to generate sharp and reliable intermediate latent results, we propose a model which combines the spatial scale, ${L} _{0}$ norm, and the dark channel prior. Second, in order to preserve the continuity and the sparsity, and to remove the flaw in the BK, a dual-constrained regularization model, which combines the ${L} _{0}$ -regularized intensity prior and the ${L} _{2}$ -regularized gradient prior, is proposed for accurate BK estimation. The proposed model can not only preserve the continuity and the sparsity of the BK very well but also can remove the flaw thoroughly. Finally, we propose an efficient optimization strategy which can solve the proposed model efficiently. Extensive experiments compared with the state-of-the-art methods demonstrate that our method estimates more accurate BKs and obtains higher quality deblurring images in terms of both subjective vision and quantitative metrics.", "pages": "5296-5311", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Tang, Shu", "Zheng, Wanpeng", "Xie, Xianzhong", "He, Tao", "Yang, Peng", "Luo, Lei", "Li, Zhixing", "Hu, Yu", "Zhao, Hao"]}]["9036051", {"address": "", "articleno": "", "doi": "10.1109/TCYB.2020.2975530", "issn": "2168-2275", "issue_date": "", "journal": "IEEE Transactions on Cybernetics", "keywords": ["Optimization", "Viruses (medical)", "Resource management", "Computer science", "Genetic algorithms", "Complex networks", "Cooperative coevolution (CC)", "evolutionary algorithm (EA)", "networked system", "resource allocation", "spreading control"], "month": "July", "number": "7", "numpages": "", "publisher": "", "title": "Evolutionary Divide-and-Conquer Algorithm for Virus Spreading Control Over Networks", "url": "", "volume": "51", "year": "2021", "abstract": "The control of virus spreading over complex networks with a limited budget has attracted much attention but remains challenging. This article aims at addressing the combinatorial, discrete resource allocation problems (RAPs) in virus spreading control. To meet the challenges of increasing network scales and improve the solving efficiency, an evolutionary divide-and-conquer algorithm is proposed, namely, a coevolutionary algorithm with network-community-based decomposition (NCD-CEA). It is characterized by the community-based dividing technique and cooperative coevolution conquering thought. First, to reduce the time complexity, NCD-CEA divides a network into multiple communities by a modified community detection method such that the most relevant variables in the solution space are clustered together. The problem and the global swarm are subsequently decomposed into subproblems and subswarms with low-dimensional embeddings. Second, to obtain high-quality solutions, an alternative evolutionary approach is designed by promoting the evolution of subswarms and the global swarm, in turn, with subsolutions evaluated by local fitness functions and global solutions evaluated by a global fitness function. Extensive experiments on different networks show that NCD-CEA has a competitive performance in solving RAPs. This article advances toward controlling virus spreading over large-scale networks.", "pages": "3752-3766", "note": "", "ISSN": "2168-2275", "publicationtype": "article", "author": ["Zhao, Tian-Fang", "Chen, Wei-Neng", "Kwong, Sam", "Gu, Tian-Long", "Yuan, Hua-Qiang", "Zhang, Jie", "Zhang, Jun"]}]["8727877", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2019.2920444", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Optimization", "Microsoft Windows", "Transportation", "Loading", "Logistics", "Heuristic algorithms", "Germanium", "Pickup and delivery problem with time windows", "LIFO loading", "tabu search", "adaptive memory"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "A Fast Decomposition and Reconstruction Framework for the Pickup and Delivery Problem With Time Windows and LIFO Loading", "url": "", "volume": "7", "year": "2019", "abstract": "The pickup and delivery problem with time windows and last-in-first-out (LIFO) loading (PDPTWL) is a combinational optimization problem extended from the well-known vehicle routing problem (VRP), in which the type of customer point is no longer single and the loading order of the requests must meet the LIFO constraint. Due to its NP-hard nature, it is difficult for exact algorithms and heuristics with a linear structure to solve a large-scale problem in a reasonable time. In this paper, we propose a fast decomposition and reconstruction framework (D&R) to solve the PDPTWL with high quality in a relatively short time. An angle-based sweep method is used to decompose a complete solution into multiple sub-solutions, each of which is assigned to a tabu search for optimization. To speed up the whole process, the optimization procedure of sub-solutions is performed by different processors of multi-core CPU in parallel. Three neighborhood operators and three strategies to reduce the number of vehicles are designed to cope with the tabu search for further improvement. Moreover, the adaptive memory mechanism is added to provide a better start when the optimization procedure falls into the local optima. We compare our framework against the best known solutions on 119 instances with up to 300 requests, the results show that our framework is able to improve over 85% (107 out of 119) of the best known solutions. More specifically, the number of vehicles is optimized by about 60% (74 out of 119) and the driving distance by about 50% (59 out of 119). In addition on instances with the largest size of requests, the computational time of our framework can be 1/50 of the comparative results, confirming its efficiency.", "pages": "71813-71826", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Liu, Fagui", "Gui, Mengke", "Yi, Chen", "Lan, Yulin"]}]["9103941", {"address": "", "articleno": "", "doi": "10.1109/JBHI.2020.2990309", "issn": "2168-2208", "issue_date": "", "journal": "IEEE Journal of Biomedical and Health Informatics", "keywords": ["Hospitals", "Training", "Neural networks", "Deep learning", "Optimization", "Machine learning algorithms", "Machine learning algorithms", "multi-layer neural networks", "patient flow", "hospitals"], "month": "Jan", "number": "1", "numpages": "", "publisher": "", "title": "Hospital Admission Location Prediction via Deep Interpretable Networks for the Year-Round Improvement of Emergency Patient Care", "url": "", "volume": "25", "year": "2021", "abstract": "Objective: This paper presents a deep learning method of predicting where in a hospital emergency patients will be admitted after being triaged in the Emergency Department (ED). Such a prediction will allow for the preparation of bed space in the hospital for timely care and admission of the patient as well as allocation of resource to the relevant departments, including during periods of increased demand arising from seasonal peaks in infections. Methods: The problem is posed as a multi-class classification into seven separate ward types. A novel deep learning training strategy was created that combines learning via curriculum and a multi-armed bandit to exploit this curriculum post-initial training. Results: We successfully predict the initial hospital admission location with area-under-receiver-operating-curve (AUROC) ranging between 0.60 to 0.78 for the individual wards and an overall maximum accuracy of 52% where chance corresponds to 14% for this seven-class setting. Our proposed network was able to interpret which features drove the predictions using a `network saliency' term added to the network loss function. Conclusion: We have proven that prediction of location of admission in hospital for emergency patients is possible using information from triage in ED. We have also shown that there are certain tell-tale tests which indicate what space of the hospital a patient will use. Significance: It is hoped that this predictor will be of value to healthcare institutions by allowing for the planning of resource and bed space ahead of the need for it. This in turn should speed up the provision of care for the patient and allow flow of patients out of the ED thereby improving patient flow and the quality of care for the remaining patients within the ED.", "pages": "289-300", "note": "", "ISSN": "2168-2208", "publicationtype": "article", "author": ["El-Bouri, Rasheed", "Eyre, David", "Watkinson, Peter", "Zhu, Tingting", "Clifton, David"]}]["8454706", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2018.2868846", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Rough sets", "Heuristic algorithms", "Search problems", "Aerodynamics", "Optimization", "Graph theory", "Roads", "Vertex cover", "attribute reduction", "rough sets", "hypergraph"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "Dimension Incremental Feature Selection Approach for Vertex Cover of Hypergraph Using Rough Sets", "url": "", "volume": "6", "year": "2018", "abstract": "The minimum vertex cover problem is a well-known optimization problem; it has been used in a wide variety of applications. This paper focuses on rough set-based approach for the minimum vertex cover problem of the dynamic and static hypergraphs. First, we demonstrate the relationship between the attribute reduction of decision table and the minimum vertex cover of hypergraph, and the minimum vertex cover problem is converted to an attribute reduction problem based on this relationship. Then, we discuss the update mechanism of minimum vertex cover from the perspective of attribute reduction, and two types of incremental attribute reduction algorithms are proposed, one is the dynamic increase of single vertex and the other is the dynamic increase of multiple vertices. Our algorithms can quickly update the minimum vertex cover in a dynamic hypergraph and improve the rough sets-based method for the minimum vertex cover problem of a static hypergraph in terms of the computational time and the solution quality. The experimental results show the advantages and limitations of the proposed algorithms compared with the existing algorithms.", "pages": "50142-50153", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Zhou, Qian", "Qin, Xiaolin", "Xie, Xiaojun"]}]["9130666", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2020.3006226", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Force", "Machine tools", "Vibrations", "Harmonic analysis", "Cepstrum", "Tools", "Milling", "Cepstrum editing", "cutting processing", "harmonic elimination", "machine tool", "modal parameters"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "Identification Method of Modal Parameters of Machine Tools Under Periodic Cutting Excitation", "url": "", "volume": "8", "year": "2020", "abstract": "The dynamics of a machine tool have an important influence on the quality and efficiency of the machining process. In this paper, the modal parameters identification method of machine tools under normal cutting excitation is proposed based on the fact that the random components in the cutting force can provide an effective excitation. However, the generated cutting force during machining contains strong periodic components and does not satisfy the white noise assumption. Directly applying the operational modal analysis (OMA) method will face serious harmonic interference. Therefore, it is difficult to ensure the accuracy of the identified parameters. The cepstrum editing method is proposed to eliminate the periodic component. And the modal parameters are extracted from the remaining signal by using the OMA method. The experimental results show that the proposed method works well under normal cutting conditions.", "pages": "120850-120858", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Yin, Ling", "Li, Chunhui", "Qin, Chao", "Peng, Yili", "Gu, Jiarong", "Zhang, Fei", "Li, Shuo", "Song, Zhiqiang"]}]["8307048", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2018.2812208", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Iris recognition", "Convolutional neural networks", "Feature extraction", "Image segmentation", "Training", "Task analysis", "Eye recognition", "iris recognition", "deep learning", "convolutional neural network", "deep residual network", "mixed convolutional and residual network (MiCoReNet)"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "Eye Recognition With Mixed Convolutional and Residual Network (MiCoRe-Net)", "url": "", "volume": "6", "year": "2018", "abstract": "Although iris recognition has achieved big successes on biometric identification in recent years, difficulties in the collection of iris images with high resolution and in the segmentation of valid regions prevent it from applying to large-scale practical applications. In this paper, we present an eye recognition framework based on deep learning, which relaxes the data collection procedure, improves the anti-fake quality, and promotes the performance of biometric identification. Specifically, we propose and train a mixed convolutional and residual network (MiCoRe-Net) for the eye recognition task. Such an architecture inserts a convolutional layer between every two residual layers and takes the advantages from both of convolutional networks and residual networks. Experiment results show that the proposed approach achieves accuracies of 99.08% and 96.12% on the CASIA-Iris-IntervalV4 and the UBIRIS.v2 datasets, respectively, which outperforms other classical classifiers and deep neural networks with other architectures.", "pages": "17905-17912", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Wang, Zi", "Li, Chengcheng", "Shao, Huiru", "Sun, Jiande"]}]["9187621", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2020.3022565", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Batteries", "Routing", "Reverberation", "Relays", "Energy consumption", "Throughput", "Pragmatics", "Packet reverberation", "opportunistic routing", "dearth node", "energy efficiency", "network throughput", "forwarding relay node", "sagacious link"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "Underwater Pragmatic Routing Approach Through Packet Reverberation Mechanism", "url": "", "volume": "8", "year": "2020", "abstract": "The advances in underwater sensor communication has become imperative getting up-to-date information about underwater happenings, especially when world has already faced the calamity like Tsunami. The underwater environment possessed freak and unpredictable movements which becomes more harsh time to time. The sensor nodes deployed under such juncture are the main source of information which in fact, facing numerous challenges. These nodes are mainly energy-constrained and rely on limited battery source. Due to most intricated underwater routing architecture, the biggest detriment is the limited battery lifespan. Therefore, it is imperative to adopt the pragmatic and possible alternate to improve the life expectancy of these sensor nodes. The solution of such shortcomings and identifying the varieties of impingements impelled by forwarding node on battery lifespan during packet transmission course are meticulously explored by developing an Underwater Pragmatic Routing Approach through Packet Reverberation mechanism (UPRA-PR). It is a novel approach and never considered in past. Through Packet Reverberation technique, the use of energy has been confined and the desired outcomes are achieved in four phases. In the first phase, the eligibility criteria for both packet and nodes have been computed by setting the Node Depth Factor ( $\\text{N}_{\\mathrm {df}}$ ). Second phase formulates the forwarding relay node mechanism and rummage out the path failure by complying a Data Rate criterion $D_{0}$ . The selection of the shrewd communication link is established in third phase by considering an Accepted Link Quality (ALQ) factor. The fourth phase where most prominent developments has been made regarding impingements effects on the battery lifespan left by the forwarding node after the packet transmission. The UPRA-PR performance metrics are assessed by staging extensive NS2 simulation with AquaSim 2.0 and compared to state-existing routing protocols i.e., DBR, H2DAB, GEDAR and FBR for Packet dissemination ratio, Path failure, Point-to-point delay estimation, System energy consumption, Network lifespan, Forwarding node impingement and Network throughput. The simulation results have ratified the UPRA-PR performance and justified the statements made in this respect.", "pages": "163091-163114", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Ashraf, Shahzad", "Gao, Mingsheng", "Chen, Zhengming", "Naeem, Hamad", "Ahmad, Arshad", "Ahmed, Tauqeer"]}]["8834787", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2019.2940772", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Pain", "Mobile applications", "Systematics", "Bibliographies", "Databases", "Usability", "Conceptual model", "mobile applications", "m-health", "pain", "self-management", "systematic literature review"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "A Systematic Literature Review of the Pain Management Mobile Applications: Toward Building a Conceptual Model", "url": "", "volume": "7", "year": "2019", "abstract": "In healthcare, mobile-based interventions support the improvement of clinical process and result in a positive behavioral change and improve the patients' health condition. This study aims at reviewing mobile applications documented for pain management in the scientific databases, to identify the key factors that are vital for pain management. In this research, a systematic literature review was conducted on the selected studies collected from five scientific databases: Medline, PubMed, EMBASE, Web of Science and Scopus. After applying the inclusion and exclusion criteria and performing the quality assessment, twenty-five studies were finalized. It has been observed that the apps were not all-inclusive in features to provide an effective pain self-management solution. As found from the review, the general features of the pain management mobile applications are pain information, pain coping strategy, social support, sub-goals and achievements, self-reporting, feedback, and patient report. Some apps involved psychological interventions. A prominent technique found was cognitive behavior therapy. This study has contributed to the body of knowledge by proposing a conceptual model in guiding the development of pain management mobile applications. The conceptual model was evaluated by a panel of experts to evaluate comprehensiveness, accuracy, and dependencies among the elements of the model, and the appropriateness of the proposed model. Experts recognized the importance of pain management and provided positive feedback to the proposed model.", "pages": "131512-131526", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Shah, Umm", "Chiew, Thiam"]}]["9245582", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2020.3034997", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Collaboration", "Image edge detection", "Indexes", "Clustering algorithms", "Measurement", "Semantics", "Object recognition", "Academic social networks", "collaboration intensity", "network science", "subnetwork identification", "subnetwork ranking", "topic modeling"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "TOSNet: A Topic-Based Optimal Subnetwork Identification in Academic Networks", "url": "", "volume": "8", "year": "2020", "abstract": "Subnetwork identification plays a significant role in analyzing, managing, and comprehending the structure and functions in big networks. Numerous approaches have been proposed to solve the problem of subnetwork identification as well as community detection. Most of the methods focus on detecting communities by considering node attributes, edge information, or both. This study focuses on discovering subnetworks containing researchers with similar or related areas of interest or research topics. A topic-aware subnetwork identification is essential to discover potential researchers on particular research topics and provide quality work. Thus, we propose a topic-based optimal subnetwork identification approach (TOSNet). Based on some fundamental characteristics, this paper addresses the following problems: 1)How to discover topic-based subnetworks with a vigorous collaboration intensity? 2) How to rank the discovered subnetworks and single out one optimal subnetwork? We evaluate the performance of the proposed method against baseline methods by adopting the modularity measure, assess the accuracy based on the size of the identified subnetworks, and check the scalability for different sizes of benchmark networks. The experimental findings indicate that our approach shows excellent performance in identifying contextual subnetworks that maintain intensive collaboration amongst researchers for a particular research topic.", "pages": "201015-201027", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Bedru, Hayat", "Zhao, Wenhong", "Alrashoud, Mubarak", "Tolba, Amr", "Guo, He", "Xia, Feng"]}]["9176504", {"address": "", "booktitle": "2020 42nd Annual International Conference of the IEEE Engineering in Medicine & Biology Society (EMBC)", "doi": "10.1109/EMBC44109.2020.9176504", "isbn": "", "keywords": ["Air pollution", "Hospitals", "Time series analysis", "Correlation", "Diseases", "Indexes"], "location": "", "numpages": "", "pages": "5378-5381", "publisher": "", "series": "", "title": "Association Between Consecutive Ambient Air Pollution and Chronic Obstructive Pulmonary Disease Hospitalization: Time Series Study During 2015-2017 in Chengdu China", "url": "", "year": "2020", "abstract": "This paper investigates the association between consecutive ambient air pollution and Chronic Obstructive Pulmonary Disease (COPD) hospitalization in Chengdu China. The three-year (2015-2017) time series data for both ambient air pollutant concentrations and COPD hospitalizations in Chengdu are approved for the study. The big data statistic analysis shows that Air Quality Index (AQI) exceeded the lighted air polluted level in Chengdu region are mainly attributed to particulate matters (i.e., PM2.5 and PM10). The time series study for consecutive ambient air pollutant concentrations reveal that AQI, PM2.5, and PM10 are significantly positive correlated, especially when the number of consecutive polluted days is greater than nine days. The daily COPD hospitalizations for every 10 \u03bcg/m3 increase in PM2.5 and PM10 indicate that consecutive ambient air pollution can lead to an appearance of an elevation of COPD admissions, and also present that dynamic responses before and after the peak admission are different. Support Vector Regression (SVR) is then used to describe the dynamics of COPD hospitalizations to consecutive ambient air pollution. These findings will be further developed for region specific, hospital early notifications of COPD in responses to consecutive ambient air pollution.", "articleno": "", "ISSN": "2694-0604", "month": "July", "number": "", "volume": "", "publicationtype": "inproceedings", "author": ["Zhang, Yi", "Wang, Ziyue", "Xu, Jingjing", "Liu, Yiqi", "Zhou, Bin", "Zhang, Nan", "He, Mingjie", "Fan, Jipeng", "Liu, Xianbo", "Zhao, Jian", "Yang, Qin", "Zhang, Lifu", "Cao, Yu", "Su, Steven"]}]["9516909", {"address": "", "articleno": "", "doi": "10.1109/JSTARS.2021.3105582", "issn": "2151-1535", "issue_date": "", "journal": "IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing", "keywords": ["Land surface temperature", "Urban areas", "Remote sensing", "Earth", "Production", "Artificial satellites", "Temperature", "Geographically weighted regression (GWR)", "human settlements", "land surface temperature (LST)", "Nanjing", "spatial heterogeneity"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "Influencing Factors of Spatial Heterogeneity of Land Surface Temperature in Nanjing, China", "url": "", "volume": "14", "year": "2021", "abstract": "The environment and climate significantly affect the land surface temperature (LST) of a city. Previous studies have revealed that LST exhibits significant spatial heterogeneity primarily caused by a combination of natural factors and human activities. Based on this, the introduction of point of interest data of the \u201cproduction-living-ecological space\u201d divides the influencing pattern into a comprehensive description of human activities supplemented by natural factors, resulting in the precise influencing factors of spatial heterogeneity of LST. Taking Nanjing (Jiangsu Province, China) as a case study, this study uses Landsat-8 remote sensing images, point of interest data, and other data to establish a geographically weighted regression model that combines natural factors and human activities. The main research results are as follows: First, the LST of Nanjing ranged from 19.9 \u00b0C to 47.6 \u00b0C, whereas the distribution trend was \u201clow at both ends and high in the middle.\u201d Second, there is no multicollinearity of the influencing factors, the fitting degree of LST and each influencing factor reached 0.87. The regression coefficients were high and exhibited both positive and negative values, implying that spatial heterogeneity exists among the influencing factors and LST. Finally, the ranking of how all factors influence the LST followed the order of water area > forest and grassland > ecological space > slope > production space > elevation > living space. The research results have practical significance for improving the quality of life of urban residents and providing a critical theoretical basis for optimizing urban human settlements.", "pages": "8341-8349", "note": "", "ISSN": "2151-1535", "publicationtype": "article", "author": ["Fan, Qiang", "Song, Xiaonan", "Shi, Yue", "Gao, Rui"]}]["9093135", {"address": "", "articleno": "", "doi": "10.1109/JSTARS.2020.2990104", "issn": "2151-1535", "issue_date": "", "journal": "IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing", "keywords": ["Agriculture", "Remote sensing", "Earth", "Satellites", "Artificial satellites", "Feature extraction", "Task analysis", "Agricultural remote sensing", "crop mapping", "deep neural network (dnn)", "geoprocessing workflow", "image classification", "Landsat", "North Dakota"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "Deep Learning Classification for Crop Types in North Dakota", "url": "", "volume": "13", "year": "2020", "abstract": "Recently, agricultural remote sensing community has endeavored to utilize the power of artificial intelligence (AI). One important topic is using AI to make the mapping of crops more accurate, automatic, and rapid. This article proposed a classification workflow using deep neural network (DNN) to produce high-quality in-season crop maps from Landsat imageries for North Dakota. We use historical crop maps from the agricultural department and North Dakota ground measurements as training datasets. Processing workflows are created to automate the tedious preprocessing, training, testing, and postprocessing workflows. We tested this hybrid solution on new images and received accurate results on major crops such as corn, soybean, barley, spring wheat, dry beans, sugar beets, and alfalfa. The pixelwise overall accuracy in all three test regions is over 82% for all land types (including noncrop land), which is the same level of accuracy as the U.S. Department of Agriculture Cropland Data Layer. The texture of DNN maps is more consistent with fewer noises, which is more comfortable to read. We find DNN is better on recognizing big farmlands than recognizing the scattered wetlands and suburban regions in North Dakota. The model trained on multiple scenes of multiple years and months yields higher accuracy than any of the models trained only on a single scene, a single month, or a single year. These results reflect that DNN can produce reliable in-season maps for major crops in North Dakota big farms and could provide a relatively accurate reference for the minor crops in scattered wetland fields.", "pages": "2200-2213", "note": "", "ISSN": "2151-1535", "publicationtype": "article", "author": ["Sun, Ziheng", "Di, Liping", "Fang, Hui", "Burgess, Annie"]}]["9359752", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2021.3060749", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Image segmentation", "Optimization", "Linear programming", "Entropy", "Particle swarm optimization", "Histograms", "Heuristic algorithms", "Coyote optimization algorithm", "information entropy", "image segmentation", "multilevel thresholding"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "Fuzzy Multilevel Image Thresholding Based on Improved Coyote Optimization Algorithm", "url": "", "volume": "9", "year": "2021", "abstract": "Due to the computational complexity of multilevel image thresholding, Swarm Intelligence Optimization Algorithm (SIOA) has been widely applied to improve the calculation efficiency. Therefore, more and more attention has been paid to exploring the application of the latest SIOA in multilevel segmentation. This article takes Otsu and fuzzy entropy as the objective functions, using Coyote Optimization Algorithm (COA) for multilevel thresholds optimization selection, through fuzzy median aggregation of local neighborhood information and then forms the Fuzzy Coyote Optimization Algorithm (FCOA), so that the thresholding image segmentation can be achieved in the end. To prevent the COA algorithm from falling into the local optimum, this article follows the differential evolution strategy adopted by the standard COA, using the number of iterations to construct the differential scaling factor to form the Improved Coyote Optimization Algorithm (ICOA). The experimental results show that fuzzy Kapur entropy and fuzzy median value aggregation-based ICOA(FICOA) achieves better image segmentation quality. Compared with Grey Wolf Optimizer (GWO), Fuzzy Modified Quick Artificial Bee Colony and Aggregation Algorithm (FMQABCA) and Fuzzy Modified Discrete Grey Wolf Optimizer and Aggregation Algorithm (FMDGWOA), FCOA and FICOA have certain advantages in visual effects of image segmentation and PSNR, FSIM evaluation indices. Particularly compared with GWO (also a wolf evolutionary algorithm), FICOA shows significant advantages.", "pages": "33595-33607", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Li, Linguo", "Sun, Lijuan", "Xue, Yu", "Li, Shujing", "Huang, Xuwen", "Mansour, Romany"]}]["9505639", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2021.3102423", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Job shop scheduling", "Quality of service", "Queueing analysis", "Markov processes", "Diffserv networks", "Bandwidth", "Videos", "xsSRD traffic", "LRD traffic", "integrated scheduling", "buffer sizing"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "BS-HTIS: Buffer Sizing for Heterogeneous Traffic and Integrated System", "url": "", "volume": "9", "year": "2021", "abstract": "Buffer sizing for switching and routing devices is of significance for guaranteeing the Quality of Service (QoS) of critical services on the Internet of Things (IoT), continuously evolving scheduling mechanisms and complex traffic characteristics pose new challenges for the traditional method of static buffer sizing based on rule-of-thumb. In this paper, the scope of buffer sizing is extended from a basic scheduling system under homogeneous arrival traffic input to an integrated scheduling system under heterogeneous arrival traffic input which is more ubiquitous. In this context, Voices, videos and other heterogeneous data in the IoT are categorized into short-range-dependent (SRD) and long-range-dependent (LRD) traffic, and the integrated scheduling system is decomposed into single-server-single-queue (SSSQ) systems by not only decoupling the complex dependencies among heterogeneous traffic inputs but also taking the impact of SRD and LRD traffic burstiness on the buffer sizing into account. On this basis, expressions for the relation between the minimum buffer size and the maximum overflow probability are presented. The numerical analysis results and simulation analysis results reveal that the average arrival rate, traffic burst level and scheduling priority are positively correlated with the required buffer size, and once the overflow probability is set, the minimum buffer size can be determined correspondingly. The achievements of this paper will provide theoretical guidance for IoT manufacturers and technicians to set buffers more reasonably and use resources more efficiently.", "pages": "115237-115245", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Shi, Huaifeng", "Pan, Chengsheng", "Wang, Yingzhi"]}]["9641843", {"address": "", "articleno": "", "doi": "10.1109/TNSRE.2021.3133616", "issn": "1558-0210", "issue_date": "", "journal": "IEEE Transactions on Neural Systems and Rehabilitation Engineering", "keywords": ["Fall detection", "Sensitivity", "Temperature measurement", "Sea measurements", "Pressure measurement", "Performance evaluation", "Detectors", "Fall detection", "wearable sensors", "machine learning", "elderly health care"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "A Reliable Fall Detection System Based on Analyzing the Physical Activities of Older Adults Living in Long-Term Care Facilities", "url": "", "volume": "29", "year": "2021", "abstract": "Fall detection systems are designed in view to reduce the serious consequences of falls thanks to the early automatic detection that enables a timely medical intervention. The majority of the state-of-the-art fall detection systems are based on machine learning (ML). For training and performance evaluation, they use some datasets that are collected following predefined simulation protocols i.e. subjects are asked to perform different types of activities and to repeat them several times. Apart from the quality of simulating the activities, protocol-based data collection results in big differences between the distribution of the activities of daily living (ADLs) in these datasets in comparison with the actual distribution in real life. In this work, we first show the effects of this problem on the sensitivity of the ML algorithms and on the interpretability of the reported specificity. Then, we propose a reliable design of an ML-based fall detection system that aims at discriminating falls from the ambiguous ADLs. The latter are extracted from 400 days of recorded activities of older adults experiencing their daily life. The proposed system can be used in neck- and wrist-worn fall detectors. In addition, it is invariant to the rotation of the wearable device. The proposed system shows 100% of sensitivity while it generates an average of one false positive every 25 days for the neck-worn device and an average of one false positive every 3 days for the wrist-worn device.", "pages": "2587-2594", "note": "", "ISSN": "1558-0210", "publicationtype": "article", "author": ["Saleh, Majd", "Abbas, Manuel", "Prud\u2019Homm, Joaquim", "Somme, Dominique", "Le, R\u00e9gine"]}]["8845592", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2019.2942586", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Smart cities", "Roads", "Support vector machines", "Machine learning", "Prediction algorithms", "Geospatial analysis", "Climate change", "Urban areas", "Geospatial modeling", "machine learning", "regression", "road traffic", "urban computing", "volunteered geographic information"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "Geospatial Modeling of Road Traffic Using a Semi-Supervised Regression Algorithm", "url": "", "volume": "7", "year": "2019", "abstract": "Nowadays, big cities are facing many challenges with respect to traffic congestion, climate change, air and water pollution, among others. Thus, smart cities are intended to improve the life quality of the citizens, tackling such issues with the integration of information and communication technologies to reduce the impact and achieve a well-being state of citizens. In this work, a model to predict the traffic congestion applying a support vector machine method is proposed. In addition, a crowdsourcing approach based on mining the Twitter social networks collecting events associated with the traffic is also proposed. The main contribution of this research is focused on providing a methodology that characterizes the traffic congestion analyzing crowd-sensed data from a geospatial perspective. This approach was implemented over the Mexico City as a case study, in order to forecast possible future traffic events in the city, in which the citizens share their particular situation to discover alternatives routes for avoiding the traffic congestion. Future works are oriented towards designing mobile applications in order to introduce the proposed approach and integrate information from multiple platforms and navigation systems.", "pages": "177376-177386", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Saldana-Perez, Magdalena", "Torres-Ruiz, Miguel", "Moreno-Ibarra, Marco"]}]["8601324", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2018.2888816", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Semisupervised learning", "Generative adversarial networks", "Gallium nitride", "Support vector machines", "Generators", "Supervised learning", "Medical diagnostic imaging", "Internet of Things", "clinical decision support", "semi-supervised learning", "generative adversarial networks"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "GAN-Based Semi-Supervised Learning Approach for Clinical Decision Support in Health-IoT Platform", "url": "", "volume": "7", "year": "2019", "abstract": "With the development of the Internet of Things (IoT) technology, its application in the medical field becomes more and more extensive. However, with a dramatic increase in medical data obtained from the IoT-based health service system, labeling a large number of medical data requires high cost and relevant domain knowledge. Therefore, how to use a small number of labeled medical data reasonably to build an efficient and high-quality clinical decision support model in the IoT-based platform has been an urgent research topic. In this paper, we propose a novel semi-supervised learning approach in association with generative adversarial networks (GANs) for supporting clinical decision making in the IoT-based health service system. In our approach, GAN is adopted to not only increase the number of labeled data but also to compensate the imbalanced labeled classes with additional artificial data in order to improve the semi-supervised learning performance. Extensive evaluations on a collection of benchmarks and real-world medical datasets show that the proposed technique outperforms the others and provides a potential solution for practical applications.", "pages": "8048-8057", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Yang, Yun", "Nan, Fengtao", "Yang, Po", "Meng, Qiang", "Xie, Yingfu", "Zhang, Dehai", "Muhammad, Khan"]}]["9293355", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2020.3044832", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Power measurement", "Uncertainty", "Generators", "Sports", "Soil measurements", "Photovoltaic systems", "Monitoring", "PV power plants", "on-site I-V curve measurements", "uncertainty method", "translation to STC", "commissioning/quality assurance", "dust and soiling degradation taxes on PV modules"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "Field I-V Curve Measurements Methodology at String Level to Monitor Failures and the Degradation Process: A Case Study of a 1.42 MWp PV Power Plant", "url": "", "volume": "8", "year": "2020", "abstract": "This research shows the methodology results for outdoor characterization by the I-V curves of the PV power generators at the biggest Brazilian rooftop PV power plant mounted at the Mineir\u00e3o Football Stadium. The experimental results of the methodology were obtained by a measurement campaign using two capacitive loads. This work identified a significant difference when these measurements were extrapolated to standard test conditions (STC) and compared to the rated power data shown on the PV modules' label at strings: between 24.12% to 26.19% lower. Results showed a contribution of soiling in a power reduction of about 6.7% on average. Additionally, it was considered an uncertain method, and AC electrical parameters were monitored. The reference PV module's calibration was carried out with great attention - the measurements were made with the PV device under test (DUT), guaranteeing the same real operating conditions for the reference PV module and the PV string as DUT. This measurement method has allowed a better characterization of the uncertainty associated with the measurement process. Finally, this study demonstrates the importance of investigating the actual power of the installed PV generators and how these measurements are essential to guaranteeing energy production following the owner's expectations.", "pages": "226845-226865", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Monteiro, Lu\u00eds", "Mac\u00eado, Wilson", "Cavalcante, Renato", "J\u00fanior, Wilson", "Torres, Pedro", "Brito, Thiago", "Silva, M\u00e1rcio", "Lopes, Bruno", "Fraga, Juliano", "Alves, Danilo", "Chase, Ot\u00e1vio", "Boaventura, Wallace"]}]["8409453", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2018.2854835", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Relays", "Silicon", "Broadcasting", "Cooperative systems", "Network coding", "Receiving antennas", "Wireless communication", "Cognitive radio", "cooperative relay networks", "cooperative quadrature physical-layer network coding", "multi-antennas", "decode-and-forward"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "Physical-Layer Network Coding Based Multi-User Cooperative Relay Transmission With Multi-Antennas in Cognitive Wireless Networks", "url": "", "volume": "6", "year": "2018", "abstract": "A promising way to improve the performance and guarantee the quality of service (QoS) of cognitive wireless cooperative relay networks is to jointly employ physical-layer network coding (PNC) and multi-antenna space-time block coding. This paper proposes a new multi-user transmission coding scheme, cooperative quadrature PNC (CQPNC), for cognitive wireless networks. In CQPNC scheme, two source nodes (users) first use quadrature carriers to transmit signals simultaneously, which are received and processed by a cooperative relay node using the PNC method. The processed signal is then transmitted to the destination node, which makes a combination of the signals from the direct path and relay path to obtain the information transmitted by the source node. Simulation results in difference cases of cognitive wireless networks show that the CQPNC scheme outperforms the traditional cooperation and cooperative network coding transmission schemes on the performance of anti-noise and throughput.", "pages": "40189-40197", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Yang, Hongjuan", "Li, Bo", "Liu, Gongliang", "Ma, Ruofei"]}]["9517271", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2021.3104321", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Spatial resolution", "Feature extraction", "Training", "Deep learning", "Decoding", "Image resolution", "Convolution", "Pan-sharpening", "deep learning", "multispectral image", "panchromatic image", "image colorization", "loss function"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "Pan-Sharpening Based on Panchromatic Colorization Using WorldView-2", "url": "", "volume": "9", "year": "2021", "abstract": "In order to overcome the lack of the multispectral image (MS) and adequately preserve the spatial information of panchromatic (PAN) image and the spectral information of MS image, this study proposes a method which adds the spectral information of the prior MS to the prior PAN during training, and only the posterior PAN is needed for predicting. Firstly, we introduce the autoencoder model based on image colorization and discuss its feasibility in the field of multi-band remote sensing image pan-sharpening. Then, the image quality evaluation functions including spatial and spectral indexes are formed as the loss function to control the image colorization model. Because the loss function contains spatial and spectral evaluation indexes, it could directly calculate the loss between the network output and the label considering characteristics of remote sensing images. Besides, the training data in our model is original PAN, this means that it is not necessary to make the simulated degraded MS and PAN data for training which is a big difference from most existing deep learning pan-sharpening methods. The new loss function including the spectral and spatial quality instead of the general MSE (mean square error), only the original PAN instead of the simulated degraded MS + PAN to be inputted, only the spectral feature instead of the direct fusion result to be learned, these three aspects change the current learning framework and optimization rule of deep learning pan-sharpening. Finally, thousands of remote sensing images from different scenes are adopted to make the training dataset to verify the effectiveness of the proposed method. In addition, we selected seven representative pan-sharpening algorithms and four widely recognized objective fusion metrics to evaluate and compare the performance on the WorldView-2 experimental data. The results show that the proposed method achieves optimal performance in terms of both the subjective visual effect and the object assessment.", "pages": "115523-115534", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Xiong, Zhangxi", "Guo, Qing", "Liu, Mingliang", "Li, An"]}]["9312039", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2020.3048839", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["COVID-19", "Pandemics", "Organizations", "Standards organizations", "Computer hacking", "Buildings", "Social networking (online)", "Multivocal literature review", "social engineering", "COVID-19", "security and privacy", "prospective solutions", "cyber-attacks and threats"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "A Multivocal Literature Review on Growing Social Engineering Based Cyber-Attacks/Threats During the COVID-19 Pandemic: Challenges and Prospective Solutions", "url": "", "volume": "9", "year": "2021", "abstract": "The novel coronavirus (COVID-19) pandemic has caused a considerable and long-lasting social and economic impact on the world. Along with other potential challenges across different domains, it has brought numerous cybersecurity challenges that must be tackled timely to protect victims and critical infrastructure. Social engineering\u2013based cyber-attacks/threats are one of the major methods for creating turmoil, especially by targeting critical infrastructure, such as hospitals and healthcare services. Social engineering\u2013based cyber-attacks are based on the use of psychological and systematic techniques to manipulate the target. The objective of this research study is to explore the state-of-the-art and state-of-the-practice social engineering\u2013based techniques, attack methods, and platforms used for conducting such cybersecurity attacks and threats. We undertake a systematically directed Multivocal Literature Review (MLR) related to the recent upsurge in social engineering\u2013based cyber-attacks/threats since the emergence of the COVID-19 pandemic. A total of 52 primary studies were selected from both formal and grey literature based on the established quality assessment criteria. As an outcome of this research study; we discovered that the major social engineering\u2013based techniques used during the COVID-19 pandemic are phishing, scamming, spamming, smishing, and vishing, in combination with the most used socio-technical method: fake emails, websites, and mobile apps used as weapon platforms for conducting successful cyber-attacks. Three types of malicious software were frequently used for system and resource exploitation are; ransomware, trojans, and bots. We also emphasized the economic impact of cyber-attacks performed on different organizations and critical infrastructure in which hospitals and healthcare were on the top targeted infrastructures during the COVID-19 pandemic. Lastly, we identified the open challenges, general recommendations, and prospective solutions for future work from the researcher and practitioner communities by using the latest technology, such as artificial intelligence, blockchain, and big data analytics.", "pages": "7152-7169", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Hijji, Mohammad", "Alam, Gulzar"]}]["9358144", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2021.3060457", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Blockchain", "Smart contracts", "Predictive models", "Crowdsourcing", "Machine learning", "Peer-to-peer computing", "Energy consumption", "Energy trading", "energy prediction", "predictive analysis", "machine learning", "blockchain"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "Peer-to-Peer Energy Trading Mechanism Based on Blockchain and Machine Learning for Sustainable Electrical Power Supply in Smart Grid", "url": "", "volume": "9", "year": "2021", "abstract": "It is expected that peer to peer energy trading will constitute a significant share of research in upcoming generation power systems due to the rising demand of energy in smart microgrids. However, the on-demand use of energy is considered a big challenge to achieve the optimal cost for households. This paper proposes a blockchain-based predictive energy trading platform to provide real-time support, day-ahead controlling, and generation scheduling of distributed energy resources. The proposed blockchain-based platform consists of two modules; blockchain-based energy trading and smart contract enabled predictive analytics modules. The blockchain module allows peers with real-time energy consumption monitoring, easy energy trading control, reward model, and unchangeable energy trading transaction logs. The smart contract enabled predictive analytics module aims to build a prediction model based on historical energy consumption data to predict short-term energy consumption. This paper uses real energy consumption data acquired from the Jeju province energy department, the Republic of Korea. This study aims to achieve optimal power flow and energy crowdsourcing, supporting energy trading among the consumer and prosumer. Energy trading is based on day-ahead, real-time control, and scheduling of distributed energy resources to meet the smart grid\u2019s load demand. Moreover, we use data mining techniques to perform time-series analysis to extract and analyze underlying patterns from the historical energy consumption data. The time-series analysis supports energy management to devise better future decisions to plan and manage energy resources effectively. To evaluate the proposed predictive model\u2019s performance, we have used several statistical measures, such as mean square error and root mean square error on various machine learning models, namely recurrent neural networks and alike. Moreover, we also evaluate the blockchain platform\u2019s effectiveness through hyperledger calliper in terms of latency, throughput, and resource utilization. Based on the experimental results, the proposed model is effectively used for energy crowdsourcing between the prosumer and consumer to attain service quality.", "pages": "39193-39217", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Jamil, Faisal", "Iqbal, Naeem", [], "Ahmad, Shabir", "Kim, Dohyeun"]}]["9146664", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2020.3011509", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Recycling", "Machine tools", "Reverse logistics", "Production facilities", "Optimization", "Carbon dioxide", "Waste machine tool", "reverse logsitics network", "recycle", "gray wolf algorithm"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "Design of Reverse Logistics Network for Remanufacturing Waste Machine Tools Based on Multi-Objective Gray Wolf Optimization Algorithm", "url": "", "volume": "8", "year": "2020", "abstract": "The high uncertainty of the recovery time, quantity and quality of waste machine tools has led to dynamic changes in the recycling logistics network and is difficult to plan. Considering factors such as recycling efficiency, cost, and carbon emissions, an optimized model for the recycling network of waste machine tool recycling with the goal of minimizing total operating costs and total carbon tax penalties was proposed. The optimization of the combination of recycling efficiency, cost and carbon emissions of waste machine tools has been achieved. For model solving, an optimization model solving algorithm based on the multi-object gray wolf algorithm was proposed. Problems that are difficult to apply due to too slow convergence speed and too many solving parameters were solved. Finally, the recycling process of waste machine tools of a machine tool remanufacturing enterprise was taken as an example, and the proposed model and algorithm were used to optimize the logistics network of waste machine tools recycling. The results show that the optimal scheme of the optimization model of the recycling network of waste machine tools can be obtained from the proposed model. The gray wolf algorithm is superior to the multi-objective non-dominated sorting genetic algorithm in both the convergence speed and the total cost of recovered logistics. Therefore, the validity and feasibility of the model and algorithm in this paper have been verified.", "pages": "141046-141056", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Jiang, Xingyu", "Li, Jiazhen", "Lu, Yitao", "Tian, Guangdong"]}]["8572684", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2018.2886213", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Three-dimensional displays", "Gallium nitride", "Two dimensional displays", "Cameras", "Generators", "Training", "Generative adversarial networks", "Artificial intelligence", "image processing", "sensors", "machine learning", "neural networks"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "Generative Adversarial Network-Based Method for Transforming Single RGB Image Into 3D Point Cloud", "url": "", "volume": "7", "year": "2019", "abstract": "Three-dimensional (3D) point clouds are important for many applications, including object tracking and 3D scene reconstruction. Point clouds are usually obtained from laser scanners, but their high cost impedes the widespread adoption of this technology. We propose a method to generate the 3D point cloud corresponding to a single red\u2013green\u2013blue (RGB) image. The method retrieves high-quality 3D data from two-dimensional (2D) images captured by conventional cameras, which are generally less expensive. The proposed method comprises two stages. First, a generative adversarial network generates a depth image estimation from a single RGB image. Then, the 3D point cloud is calculated from the depth image. The estimation relies on the parameters of the depth camera employed to generate the training data. The experimental results verify that the proposed method provides high-quality 3D point clouds from single 2D images. Moreover, the method does not require a PC with outstanding computational resources, further reducing implementation costs, as only a moderate-capacity graphics processing unit can efficiently handle the calculations.", "pages": "1021-1029", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Chu, Phuong", "Sung, Yunsick", "Cho, Kyungeun"]}]["8886417", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2019.2950347", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Frequency shift keying", "Harmonic analysis", "Heart beat", "Biomedical monitoring", "Sensors", "Monitoring", "Intermodulation", "nonlinear response", "passive tag", "target localization", "vital signs", "wearable"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "Intermodulation-Based Nonlinear Smart Health Sensing of Human Vital Signs and Location", "url": "", "volume": "7", "year": "2019", "abstract": "This paper discusses the use of a nonlinear sensing technology based on radio frequency (RF) intermodulation response to track both the vital signs and location of human subjects. Smart health sensing was realized through the use of a wearable nonlinear tag and an intermodulation-based nonlinear sensor operating in both Doppler and frequency shift keying (FSK) modes. The Doppler mode was used to detect the heartbeat and breathing of the target subject while human subject localization was achieved in the FSK mode. One of the key advantages of this nonlinear smart sensor system was clutter rejection. This system identified the signal reflected from the wearable nonlinear tag and suppressed undesired signals and interferences that were reflected from other objects. The wearable tags used for the experiments were passive, hence they did not require any battery or power supply for their operation. Since the respiration signal is typically stronger than the heartbeat signal, the nonlinear detection setup was designed such that the respiratory signal receives less gain to avoid its sidelobes and harmonics from interfering heartbeat signal detection. This enhanced the heartbeat signal quality so that the cardiac activity could be easily tracked. Four types of experiments were performed on multiple subjects to demonstrate the advantages of this intermodulation-based nonlinear smart health sensing system. Previously, 2nd order harmonics were utilized for target localization and vital sign monitoring. However, these 2nd order harmonics suffer from high path loss and licensing issues. In this paper, target localization and smart health sensing were realized using 3rd order intermodulation with less path loss and no licensing issues compared with its harmonic counterparts. The experiment performed in nonlinear FSK mode was able to detect and locate the source of motion with high accuracy. Similarly, vital signs were recorded in the nonlinear Doppler mode. The design effectively made the amplitude of the heartbeat signal component more prominent, so that the sidelobes and harmonics of respiration do not suppress heartbeat signal.", "pages": "158284-158295", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Mishra, Ashish", "McDonnell, William", "Wang, Jing", "Rodriguez, Daniel", "Li, Changzhi"]}]["9180261", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2020.3018112", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Image color analysis", "Image recognition", "Consumer behavior", "Customer relationship management", "Psychology", "Feature extraction", "Brand management", "Behavioral sciences", "Image Recognition", "HSI Color System", "Marketing Brand", "Brand Loyalty", "Product Brand Recognition"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "Consumer Marketing Brand Cultivation Path Based on Image Recognition Technology", "url": "", "volume": "", "year": "2020", "abstract": "In recent years, with the improvement of national income levels, people's consumption concept has begun to transition from enjoyment to quality, and people pay more and more attention to food safety and health. The research of intelligent brand image recognition technology is the most important to prevent the spread of counterfeit and shoddy products and brand cultivation. This article mainly studies the cultivation method of consumer marketing brand based on image recognition technology. This article is based on image recognition technology. Consumers can obtain relevant information and brand culture through the mobile phone camera function anytime, anywhere, establish a bridge between consumers and company brands, broaden the company's channels for obtaining consumer information big data, and deeply tap consumer potential. Demand, further enrich the brand cultivation methods, and achieve a new terminal service experience of \"efficient, convenient and satisfactory\" for consumers. In the experiment of this paper, the shape of the product packaging is relatively irregular, so there are fewer feature points generated in this part, which leads to the inaccurate extraction and distribution of visual words, which reduces the accuracy of single product extraction slightly, but the overall result is good. The maximum number of iterations in this experiment is 40, which can meet the requirements of most pictures. From the perspective of consumer perception, this paper explores and proposes a new path for consumer marketing brand cultivation of image recognition technology, promotes the deep integration of new artificial intelligence technology and brand industry, and fills the gaps in industry-related technologies and applications.", "pages": "1-1", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Liu, Yishu", "Lyu, Pin", "Gao, Wei"]}]["9775692", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2022.3175573", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Communications technology", "Diseases", "5G mobile communication", "Key performance indicator", "Drugs", "6G mobile communication", "Communication networks", "Precision medicine", "Fourth industrial revolution (4IR) technologies", "Communication technologies", "5G", "6G"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "Enabling Precision Medicine via Contemporary and Future Communication Technologies: A Survey", "url": "", "volume": "", "year": "2022", "abstract": "Precision medicine (PM) is an innovative medical approach that considers differences in the individuals\u2019 omics, medical histories, lifestyles, and environmental information in treating diseases. To fully achieve the envisaged gains of PM, various contemporary and future technologies have to be employed, among which are nanotechnology, sensor network, big data, and artificial intelligence. These technologies and other applications require a communication network that will enable them to work in tandem for the benefit of PM. Hence, communication technology serves as the nervous system of PM, without which the entire system collapses. Therefore, it is essential to explore and determine the candidate communication technology requirements that can guarantee the envisioned gains of PM. To the best of our knowledge, no work exploring how communication technology directly impacts the development and deployment of PM solutions exists. This survey paper is designed to stimulate discussions on PM from the communication engineering perspective. We introduce the fundamentals of PM and the demands in terms of quality of service that each of the enabling technologies of PM places on the communication network. We explore the information in the literature to suggest the ideal metric values of the key performance indicators for the implementation of the different components of PM. The comparative analysis of the suitability of the contemporary and future communication technologies for PM implementation is discussed. Finally, some open research challenges for the candidate communication technologies that will enable the full implementation of PM solutions are highlighted.", "pages": "1-1", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Chude-Okonkwo, Uche", "Paul, Babu", "Vasilakos, Athanasios"]}]["8292789", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2018.2806391", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Mobile communication", "Cloud computing", "Maintenance engineering", "Edge computing", "Heuristic algorithms", "Algorithm design and analysis", "Mobile handsets", "Mobile edge computing", "edge service", "service maintenance", "skyline", "skyline graph"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "Efficient Dynamic Service Maintenance for Edge Services", "url": "", "volume": "6", "year": "2018", "abstract": "The emergence of many new computing applications, such as Internet of Vehicles (IoV) and smart homes, has been made possible by the large pool of cloud resources and services. However, the cloud computing paradigm is unable to meet the requirements of delay-sensitive business applications, such as low latency, mobility support, and location awareness. In this context, Mobile Edge Computing (MEC) is introduced to improve the quality of experience (QoE) by bringing cloud resources and services closer to the user by leveraging available resources in the edge networks. However, the performance of MEC is dynamic in nature due to its location awareness, mobility and proximity. As a result, an effective mechanism is needed for providing efficient dynamic service maintenance for edge services. In this paper, we propose applying the Skyline Graph Model and employing the Directed Acyclic Graph theory to store and update mobile edge services. Specifically, the Skyline Graph (SG) algorithm is designed to solve the insertion, deletion, updating and searching of mobile edge services to achieve efficient maintenance for edge services. Comprehensive experiments are conducted on both real-world web services and simulated datasets to evaluate the effectiveness and efficiency of our approaches. The results show that our algorithms can achieve significantly better performance and robustness than the baseline algorithm.", "pages": "8829-8840", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Zhang, Yiwen", "Li, Jin", "Zhou, Zhangbing", "Liu, Xiao"]}]["9435345", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2021.3081794", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Protocols", "Machine learning", "Wireless communication", "Peer-to-peer computing", "Logic gates", "Batteries", "Zigbee", "Wireless communications", "edge computing", "Internet of Things", "machine learning", "random forest", "sustainability"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "Autonomous Configuration of Communication Systems for IoT Smart Nodes Supported by Machine Learning", "url": "", "volume": "9", "year": "2021", "abstract": "Machine Learning brings intelligence services to IoT systems, with Edge Computing contributing for edge nodes to be part of these services, allowing data to be processed directly in the nodes in real time. This paper introduces a new way of creating a self-configurable IoT node, in terms of communications, supported by machine learning and edge computing, in order to achieve a better efficiency in terms of power consumption, as well as a comparison between regression models and between deploying them in edge or cloud fashions, with a real case implementation. The correct choice of protocol and configuration parameters can make the difference between a device battery lasting 100 times more. The proposed method predicts the energy consumption and quality of signal using regressions based on node location, distance and obstacles and the transmission power used. With an accuracy of 99.88% and a margin of error of 1.504 mA for energy consumption and 98.68% and a margin of error of 1.9558 dBm for link quality, allowing the node to use the best transmission power values for reliability and energy efficiency. With this it is possible to achieve a network that can reduce up to 68% the energy consumption of nodes while only compromising in 7% the quality of the network. Besides that, edge computing proves to be a better solution when energy efficient nodes are needed, as less messages are exchanged, and the reduced latency allows nodes to be configured in less time.", "pages": "75021-75034", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Gl\u00f3ria, Andr\u00e9", "Sebasti\u00e3o, Pedro"]}]["7886281", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2017.2686986", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Psychology", "Brain modeling", "Electroencephalography", "Physiology", "Real-time systems", "Computer architecture", "Analytical models", "Conceptual design", "CPS", "EEG", "Kano model", "SampEn", "SVM"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "A Cyber-Physical System for Product Conceptual Design Based on an Intelligent Psycho-Physiological Approach", "url": "", "volume": "5", "year": "2017", "abstract": "Conceptual design plays an important role in a product development process. Significant development of technologies in the cyber-physical system (CPS) provides innovative approaches for product conceptual design. This paper proposes an architecture of CPS for conceptual design that realizes the acquisition of real-time physiological data from the physical world and the feedback of psychological states from the cyber world. As understanding and meeting the needs of customers have been recognized as significant aspects for conceptual design, an intelligent psycho-physiological approach that incorporates electroencephalogram (EEG) into the Kano model is adopted in this CPS for real-time customer needs analysis. The sample entropy (SampEn) extracted from EEG data is an endogenous neural indicator for customers' psychological states. A support vector machine using this SampEn as input is trained for classifying different categories of quality attributes defined in the Kano model. A case study is conducted to testify the feasibility of the approach proposed in this paper.", "pages": "5378-5387", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Lou, Shanhe", "Feng, Yixiong", "Tian, Guangdong", "Lv, Zhihan", "Li, Zhiwu", "Tan, Jianrong"]}]["9871297", {"address": "", "booktitle": "2022 44th Annual International Conference of the IEEE Engineering in Medicine & Biology Society (EMBC)", "doi": "10.1109/EMBC48229.2022.9871297", "isbn": "", "keywords": ["Ultrasonic imaging", "Thresholding (Imaging)", "Tracking", "Annotations", "Imaging", "Manuals", "Muscles"], "location": "", "numpages": "", "pages": "1520-1526", "publisher": "", "series": "", "title": "AEPUS: a tool for the Automated Extraction of Pennation angles in Ultrasound images with low Signal-to-noise ratio for plane-wave imaging", "url": "", "year": "2022", "abstract": "The penetrating ability of ultrasound (US) com-bined with its real-time operation make it the perfect tool for investigating muscle contraction mechanics during complex functional tasks, e.g., locomotion. Changes in fascicle lengths and pennation angles of muscle fascicles strongly correlate with the capacity of skeletal muscles to produce forces, thereby represent fundamental parameters to be tracked. While the gold standard for extracting these features from US images is still based on manual annotation, the availability of recording devices capable of generating big data of muscle dynamics makes such manual approach unfeasible, setting the need for automated muscle images annotation tools. Existing approaches, however, are seriously limited, also in view of the continuous developments and technology ad-vancements for ultrafast US and plane-wave imaging. In fact, they rely on conventional (slow) B-mode imaging, make use of point tracking approaches (which often fail due to out-of-plane motion), or can only operate on very high quality images. To overcome all these limitations, we present AEPUS, an automated image labeling tool capable of extracting pennation angles from low quality images using a very small number of plane waves, therefore making it capable of exploiting all the benefits of ultrafast US. Clinical Relevance - Ultrasound is a standard research tool to investigate alterations of spastic muscles in children with Cerebral Palsy. We propose a reliable and time-efficient method to track muscle features in ultrasound images and support clinical biomechanists in their analyses.", "articleno": "", "ISSN": "2694-0604", "month": "July", "number": "", "volume": "", "publicationtype": "inproceedings", "author": ["Vostrikov, S.", "Cossettini, A.", "Leitner, C.", "Baumgartner, C.", "Benini, L."]}]["8517127", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2018.2878879", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Edge computing", "Resource management", "Cloud computing", "Social network services", "Computational modeling", "Task analysis", "Computer architecture", "Edge computing", "network utility", "social activity", "vehicular network"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "A Novel Utility Based Resource Management Scheme in Vehicular Social Edge Computing", "url": "", "volume": "6", "year": "2018", "abstract": "Vehicular network aims at providing intelligent transportation and ubiquitous network access. Edge computing is able to reduce the consumption of core network bandwidth and serving latency by processing the generated data at the network edge, and social network is able to provide precise services by analyzing user\u2019s personal behaviors. In this paper, we propose a new network system referred to as vehicular social edge computing (VSEC) that inherits the advantages of both edge computing and social network. VSEC is capable of improving the drivers\u2019 quality of experience while enhancing the service providers\u2019 quality of service. In order to further improve the performance of VSEC, the network utility is modeled and maximized by optimally managing the available network resources via two steps. First, the total processing time is minimized to achieve the optimal payment of the user to each edge device for each kind of the required resource. Second, a utility model is proposed, and the available resources are optimally allocated based on the results from the first step. The two optimization problems are solved by the Lagrangian theory, and the closed-form expressions are obtained. Numerical simulations show different capacities in different scenarios, which may provide some useful insights for VSEC design.", "pages": "66673-66684", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Lin, Fuhong", "L\u00fc, Xing", "You, Ilsun", "Zhou, Xianwei"]}]["8611333", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2019.2893024", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Prediction algorithms", "Social network services", "Recommender systems", "Predictive models", "Computer science", "Collaboration", "Recommender systems", "collaborative filtering", "matrix factorization", "PageRank algorithm", "trust networks"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "A Novel Social Recommendation Method Fusing User\u2019s Social Status and Homophily Based on Matrix Factorization Techniques", "url": "", "volume": "7", "year": "2019", "abstract": "As one of the most successful recommendation techniques, collaborative filtering provides a useful recommendation by associating an active user with a crowd of users who share the same interests. Although some achievements have been achieved both in theory and practice, the efficiency of recommender systems has been negatively affected by the problems of cold start and data sparsity recently. To solve the above problems, the trust relationship among users is employed into recommender systems to build a learning model to further promote the prediction quality and users' satisfaction. However, most of the existing social networks-based recommendation algorithms fail to take into account the fact that users with different levels of trust and backgrounds, that is, user's social status and homophily have different degrees of influence on their friends. In this paper, a novel social matrix factorization-based recommendation method is proposed to improve the recommendation quality by fusing user's social status and homophily. User's social status and homophily play important roles in improving the performance of recommender systems. We first build a user's trust relationship network based on user social relationships and the rating information. Then, the degree of trust is calculated through the trust propagation method and the PageRank algorithm. Finally, the trust relationship is integrated into the matrix factorization model to accurately predict unknown ratings. The proposed method is evaluated using real-life datasets including the Epinions and Douban datasets. The experimental results and comparisons demonstrate that the proposed approach is superior to the existing social networks-based recommendation algorithms.", "pages": "18783-18798", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Chen, Rui", "Hua, Qingyi", "Wang, Bo", "Zheng, Min", "Guan, Weili", "Ji, Xiang", "Gao, Quanli", "Kong, Xiangjie"]}]["9854096", {"address": "", "articleno": "", "doi": "10.1109/JSTARS.2022.3197748", "issn": "2151-1535", "issue_date": "", "journal": "IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing", "keywords": ["Remote sensing", "Generative adversarial networks", "Feature extraction", "Satellites", "Generators", "Image matching", "Training", "Heterogeneous remote sensing images", "image style transfer", "intelligent matching"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "Intelligent Matching Method for Heterogeneous Remote Sensing Images Based on Style Transfer", "url": "", "volume": "15", "year": "2022", "abstract": "Intelligent matching of heterogeneous remote sensing images is a common basic problem in the field of intelligent remote sensing image processing. Aiming at the difficulty of matching satellite-aerial remote sensing images, this article proposes an intelligent matching method for heterogeneous remote sensing images based on style transfer. First, based on the idea of image style transfer of a generative adversarial networks, this method improves the conversion effect of the model on heterogeneous images by constructing a new generative network loss function and converts satellite images into aerial images. Then, the advanced deep learning-based matching algorithms D2-Net and LoFTR are used to achieve matching between the generated aerial image and the original aerial image. Finally, this transformation relationship is mapped to the corresponding satellite\u2013aerial image pair to obtain the final matching result. The image style transfer experiments and the matching experiments we carry out under different test datasets show that the smooth cycle-consistent generative adversarial networks proposed in this article can effectively reduce the complexity of the algorithm and improve the quality of image generation. In addition, combining it with deep learning-based feature-matching methods can effectively improve the accuracy and robustness of the matching algorithm. Our code and data can be found at: https://gitee.com/AZQZ/intelligent-matching.", "pages": "6723-6731", "note": "", "ISSN": "2151-1535", "publicationtype": "article", "author": ["Zhao, Jiawei", "Yang, Dongfang", "Li, Yongfei", "Xiao, Peng", "Yang, Jinglan"]}]["9139962", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2020.3009298", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Agriculture", "Internet of Things", "Sociology", "Statistics", "Wireless sensor networks", "Monitoring", "Diseases", "Artificial intelligence", "cloud computing", "Internet of Things", "precision agriculture", "wireless sensor networks"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "Recent Developments of the Internet of Things in Agriculture: A Survey", "url": "", "volume": "8", "year": "2020", "abstract": "A rise in the population has immensely increased the pressure on the agriculture sector. With the advent of technology, this decade is witnessing a shift from conventional approaches to the most advanced ones. The Internet of Things (IoT) has transformed both the quality and quantity of the agriculture sector. Hybridization of species along with the real-time monitoring of the farms paved a way for resource optimization. Scientists, research institutions, academicians, and most nations across the globe are moving towards the practice and execution of collaborative projects to explore the horizon of this field for serving mankind. The tech industry is racing to provide more optimal solutions. Inclusion of IoT, along with cloud computing, big data analytics, and wireless sensor networks can provide sufficient scope to predict, process, and analyze the situations and improve the activities in the real-time scenario. The concept of heterogeneity and interoperability of the devices by providing flexible, scalable, and durable methods, models are also opening new domains in this field. Therefore, this paper contributes towards the recent IoT technologies in the agriculture sector, along with the development of hardware and software systems. The public and private sector projects and startup's started all over the globe to provide smart and sustainable solutions in precision agriculture are also discussed. The current scenario, applications, research potential, limitations, and future aspects are briefly discussed. Based on the concepts of IoT a precision farming framework is also proposed in this article.", "pages": "129924-129957", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Kour, Vippon", "Arora, Sakshi"]}]["8949497", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2020.2963939", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Surgery", "Real-time systems", "Cancer detection", "Tumors", "Graphics processing units", "Hyperspectral imaging", "Cancer", "Hyperspectral imaging", "high performance computing", "graphic processing unit", "parallel processing", "parallel architectures", "image processing", "biomedical engineering", "medical diagnostic imaging", "cancer detection", "supervised classification", "support vector machines", "K-Nearest neighbors", "principal component analysis", "K-means", "majority voting"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "Towards Real-Time Computing of Intraoperative Hyperspectral Imaging for Brain Cancer Detection Using Multi-GPU Platforms", "url": "", "volume": "8", "year": "2020", "abstract": "Several causes make brain cancer identification a challenging task for neurosurgeons during the surgical procedure. The surgeons' naked eye sometimes is not enough to accurately delineate the brain tumor location and extension due to its diffuse nature that infiltrates in the surrounding healthy tissue. For this reason, a support system that provides accurate cancer delimitation is essential in order to improve the surgery outcomes and hence the patient's quality of life. The brain cancer detection system developed as part of the \u201cHypErspectraL Imaging Cancer Detection\u201d (HELICoiD) European project meets this requirement exploiting a non-invasive technique suitable for medical diagnosis: the hyperspectral imaging (HSI). A crucial constraint that this system has to satisfy is providing a real-time response in order to not prolong the surgery. The large amount of data that characterizes the hyperspectral images, and the complex elaborations performed by the classification system make the High Performance Computing (HPC) systems essential to provide real-time processing. The most efficient implementation developed in this work, which exploits the Graphic Processing Unit (GPU) technology, is able to classify the biggest image of the database (worst case) in less than three seconds, largely satisfying the real-time constraint set to 1 minute for surgical procedures, becoming a potential solution to implement hyperspectral video processing in the near future.", "pages": "8485-8501", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Florimbi, Giordana", "Fabelo, Himar", "Torti, Emanuele", "Ortega, Samuel", "Marrero-Martin, Margarita", "Callico, Gustavo", "Danese, Giovanni", "Leporati, Francesco"]}]["9467347", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2021.3093340", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Sports", "Robots", "Trajectory", "Sports equipment", "Motion control", "Estimation", "Atmospheric modeling", "Ball motion control", "reinforcement learning", "spin velocity estimation", "table tennis robot"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "Ball Motion Control in the Table Tennis Robot System Using Time-Series Deep Reinforcement Learning", "url": "", "volume": "9", "year": "2021", "abstract": "One of the biggest challenges hindering a table tennis robot to play as well as a professional player is the ball\u2019s accurate motion control, which depends on various factors such as the incoming ball\u2019s position, linear, spin velocity and so forth. Unfortunately, some factors are almost impossible to be directly measured in real practice, such as the ball\u2019s spin velocity, which is difficult to be estimated from vision due to the little texture on the ball\u2019s surface. To perform accurate motion control in table tennis, this study proposes to learn a ball stroke strategy to guarantee desirable \u201ctarget landing location\u201d and the \u201cover-net height\u201d which are two key indicators to evaluate the quality of a stroke. To overcome the spin velocity challenge, a deep reinforcement learning (DRL) based stroke approach is developed with the spin velocity estimation capability, through which the system can predict the relative spin velocity of the ball and stroke it back accurately by iteratively learning from the robot-environment interactions. To pre-train the DRL-based strategy effectively, this paper develops a virtual table tennis playing environment, through which various simulated data can be collected. For the real table tennis robot implementation, experimental results demonstrate the superior performance of the proposed control strategy compared to that of the traditional aerodynamics-based method with an average landing error around 80mm and the landing-within-table probability higher than 70%.", "pages": "99816-99827", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Yang, Luo", "Zhang, Haibo", "Zhu, Xiangyang", "Sheng, Xinjun"]}]["9917483", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2022.3212120", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Artificial intelligence", "Sleep apnea", "Sleep", "Cardiovascular system", "Biomedical signal processing", "Heart rate variability", "Electrocardiography", "Random forests", "Hybrid power systems", "Linear discriminant analysis", "Classification algorithms", "Sleep disorder", "cardiovascular syndromes", "ECG sleep signals", "AI-based insomnia detection", "27 machine learning", "CAP sleep database", "hybrid classification scenarios"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "Ensemble Computational Intelligent for Insomnia Sleep Stage Detection via the Sleep ECG Signal", "url": "", "volume": "10", "year": "2022", "abstract": "Insomnia is a common sleep disorder in which patients cannot sleep properly. Accurate detection of insomnia disorder is a crucial step for mental disease analysis in the early stages. The disruption in getting quality sleep is one of the big sources of cardiovascular syndromes such as blood pressure and stroke. The traditional insomnia detection methods are time-consuming, cumbersome, and more expensive because they demand a long time from a trained neurophysiologist, and they are prone to human error, hence, the accuracy of diagnosis gets compromised. Therefore, the automatic insomnia diagnosis from the electrocardiogram (ECG) records is vital for timely detection and cure. In this paper, a novel hybrid artificial intelligence (AI) approach is proposed based on the power spectral density (PSD) of the heart rate variability (HRV) to detect insomnia in three classification scenarios: (1) subject-based classification scenario (normal Vs. insomnia), (2) sleep stage-based classification (REM Vs. W. stage), and (3) the combined classification scenario using both subject-based and sleep stage-based deep features. The ensemble learning of random forest (RF) and decision tree (DT) classifiers are used to perform the first and second classification scenarios, while the linear discriminant analysis (LDA) classifier is used to perform the third combined scenario. The proposed framework includes data collection, investigation of the ECG signals, extraction of the signal HRV, estimation of the PSD, and AI-based classification via hybrid machine learning classifiers. The proposed framework is fine-tuned and evaluated using the free public PhysioNet dataset over fivefold trails cross-validation. For the subject-based classification scenario, the detection performance in terms of sensitivity, specificity, and accuracy is recorded to be 96.0%, 94.0%, and 96.0%, respectively. For the sleep stage-based classification scenario, the detection evaluation results are recorded equally with 96.0% for ceiling level accuracy, sensitivity, and specificity. For the combined classification scenario, the LDA classifier has achieved the best insomnia detection accuracy with 99.0%. In the future, the proposed hybrid AI approach could be applicable for mobile observation schemes to automatically detect insomnia disorders.", "pages": "108710-108721", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Tripathi, Pragati", "Ansari, M.", "Gandhi, Tapan", "Mehrotra, Rajat", "Heyat, Md", "Akhtar, Faijan", "Ukwuoma, Chiagoziem", "Muaad, Abdullah", "Kadah, Yasser", "Al-Antari, Mugahed", "Li, Jian"]}]["7355287", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2015.2508940", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Scheduling", "Optimization", "Processor scheduling", "Cloud computing", "Ant colony optimization", "Memory management", "Resource management", "Cloud computing", "Ant colony", "Task scheduling", "Cloud computing", "ant colony", "task scheduling", "deadline", "cost constraint"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "A Multi-Objective Optimization Scheduling Method Based on the Ant Colony Algorithm in Cloud Computing", "url": "", "volume": "3", "year": "2015", "abstract": "For task-scheduling problems in cloud computing, a multi-objective optimization method is proposed here. First, with an aim toward the biodiversity of resources and tasks in cloud computing, we propose a resource cost model that defines the demand of tasks on resources with more details. This model reflects the relationship between the user's resource costs and the budget costs. A multi-objective optimization scheduling method has been proposed based on this resource cost model. This method considers the makespan and the user's budget costs as constraints of the optimization problem, achieving multi-objective optimization of both performance and cost. An improved ant colony algorithm has been proposed to solve this problem. Two constraint functions were used to evaluate and provide feedback regarding the performance and budget cost. These two constraint functions made the algorithm adjust the quality of the solution in a timely manner based on feedback in order to achieve the optimal solution. Some simulation experiments were designed to evaluate this method's performance using four metrics: 1) the makespan; 2) cost; 3) deadline violation rate; and 4) resource utilization. Experimental results show that based on these four metrics, a multi-objective optimization method is better than other similar methods, especially as it increased 56.6% in the best case scenario.", "pages": "2687-2699", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Zuo, Liyun", "Shu, Lei", "Dong, Shoubin", "Zhu, Chunsheng", "Hara, Takahiro"]}]["8585006", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2018.2888885", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Image reconstruction", "Generators", "Feature extraction", "Image edge detection", "Image restoration", "Generative adversarial networks", "Kernel", "Image deblurring", "conditional generative adversarial network", "receptive field recurrent", "coarse-to-fine"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "Recurrent Conditional Generative Adversarial Network for Image Deblurring", "url": "", "volume": "7", "year": "2019", "abstract": "Nowadays, there is an increasing demand for images with high definition and fine textures, but images captured in natural scenes usually suffer from complicated blurry artifacts, caused mostly by object motion or camera shaking. Since these annoying artifacts greatly decrease image visual quality, deblurring algorithms have been proposed from various aspects. However, most energy-optimization-based algorithms rely heavily on blur kernel priors, and some learning-based methods either adopt pixel-wise loss function or ignore global structural information. Therefore, we propose an image deblurring algorithm based on a recurrent conditional generative adversarial network (RCGAN), in which the scale-recurrent generator extracts sequence spatio\u2013temporal features and reconstructs sharp images in a coarse-to-fine scheme. To thoroughly evaluate the global and local generator performance, we further propose a receptive field recurrent discriminator. Besides, the discriminator takes blurry images as conditions, which helps to differentiate reconstructed images from real sharp ones. Last but not least, since the gradients are vanishing when training the generator with the output of the discriminator, a progressive loss function is proposed to enhance the gradients in back propagation and to take full advantage of discriminative features. Extensive experiments prove the superiority of RCGAN over state-of-the-art algorithms both qualitatively and quantitatively.", "pages": "6186-6193", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Liu, Jing", "Sun, Wanning", "Li, Mengjie"]}]["8290962", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2018.2799953", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Algorithm design and analysis", "Heuristic algorithms", "Approximation algorithms", "Search problems", "Diversity reception", "Computational efficiency", "Upper bound", "Heuristic", "local search", "maximum edge weighted clique"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "An Efficient Local Search for the Maximum Edge Weighted Clique Problem", "url": "", "volume": "6", "year": "2018", "abstract": "The maximum edge weighted clique problem (MEWCP), an extension of the classical maximum clique problem, is an important NP-hard combinatorial optimization problem. The problem has been widely used in various areas. The objective of this paper is to design an efficient local search algorithm to solve the MEWCP. First, the proposed scoring strategy is used to evaluate the benefit of adding and swapping operators. Second, the vertex weighting strategy is used to increase the diversity of solutions and the configuration checking strategy is used to avoid the cycling problem. By combining these three strategies, we propose multiple rules to select the added vertex or the swapped vertex pair. Based on the multiple rules, an efficient local search algorithm, namely, local search based on multiple rules (LSMR), is proposed. LSMR is compared with several representative algorithms on massive graph instances. The experimental results indicate that LSMR is superior to competitors in terms of solution quality and computational efficiency in most instances.", "pages": "10743-10753", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Li, Ruizhi", "Wu, Xiaoli", "Liu, Huan", "Wu, Jun", "Yin, Minghao"]}]["8704719", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2019.2914455", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Feature extraction", "Image retrieval", "Task analysis", "Loss measurement", "Forensics", "Cross-domain", "image retrieval", "shoe-print", "scene of crime"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "Shoe-Print Image Retrieval With Multi-Part Weighted CNN", "url": "", "volume": "7", "year": "2019", "abstract": "Identifying shoe-print impressions in the scene of crime (SoC) from database images is a challenging problem in forensic science due to the complicated impressing surface, the partial absence of on-site impressions, and the huge domain gap between the query and the gallery images. The existing approaches pay much attention to feature extraction while ignoring its distinctive characteristics. In this paper, we propose a novel multi-part weighted convolutional neural network (MP-CNN) for shoe-print image retrieval. Specifically, the proposed CNN model processes images in three steps: 1) dividing the input images vertically into two parts and extracting sub-features by a parameter-shared network individually; 2) calculating the importance weight matrix of the sub-features based on the informative pixels they contained and concatenating them as the final feature, and; 3) using the triplet loss function to measure the similarity between the query and the gallery images. In addition to the proposed network, we adopt an effective strategy to enhance the quality of the images and to reduce the domain gap using the U-Net structure. The experimental evaluations demonstrate that our proposed method significantly outperforms other fine-grained cross-domain methods on SPID dataset and obtains comparative results with the state-of-the-art shoe-print retrieval methods on FID300 dataset.", "pages": "59728-59736", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Ma, Zhanyu", "Ding, Yifeng", "Wen, Shaoguo", "Xie, Jiyang", "Jin, Yifeng", "Si, Zhongwei", "Wang, Haining"]}]["8598873", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2018.2889801", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Heuristic algorithms", "Optimization", "Sociology", "Statistics", "Switches", "Convergence", "Social network services", "Flower pollination algorithm", "opposition-based learning strategy", "dynamic switching probability", "t-distribution variation", "user identification"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "Improved Flower Pollination Algorithm and Its Application in User Identification Across Social Networks", "url": "", "volume": "7", "year": "2019", "abstract": "Aiming at the shortages of basic flower pollination algorithm (FPA) with slow convergence speed, low search precision, and easy to fall into local optimum, a new adaptive FPA based on opposition-based learning and t-distribution (OTAFPA) was proposed and be applied to the social networks. First, the opposition-based learning strategy is utilized to increase the diversity and quality of the initial population. Then, the adaptive dynamic switching probability is introduced, which can effectively balance the global and local search according to the current number of iterations. Finally, the t-distribution variation is used to increase the population diversity and to help the algorithm jump out of the local optimum. The simulation experiments on eight classical test functions show that OTAFPA has better global optimization ability, which improves the convergence speed and the solution accuracy of the algorithm. The OTAFPA also shows superior performance in practical applications of user identification across social networks.", "pages": "44359-44371", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Li, Wenjing", "He, Zhiming", "Zheng, Jian", "Hu, Zhongdong"]}]["9138533", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2020.3008443", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Green products", "Logistics", "Air pollution", "Economics", "Government", "Sustainable development", "Green logistics", "influential factors", "DEMATEL method", "fuzzy set theory", "ISM"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "Green Logistics Development Decision-Making: Factor Identification and Hierarchical Framework Construction", "url": "", "volume": "8", "year": "2020", "abstract": "The influential factors of green logistics development have been an important research topic for academics. However, little attention has been devoted to studying the issue of the prioritization and the hierarchical structure of these factors. The purpose of this study is to identify the key factors influencing the development of green logistics, and to explore the prioritization and the hierarchical structure of the influential factors. An indicator system for influential factors of green logistics development is first developed based on the theory of triple bottom line, including sixteen indicators or factors. The comprehensive influences and the prioritization of these sixteen factors are then analyzed using the DEMATEL method and the multi-level hierarchical structure of these factors is constructed by applying the ISM method. The results indicate that government policies, public supports, laws and regulations, green education, green technology innovations, consumer green demands, quantity and quality of green logistics talents are more influential than the other factors. Social factors including laws and regulations and government policies, public supports and green education are the underlying factors affecting the development of green logistics. Economic factors are at the middle levels and environmental factors are at the surface-level of the hierarchical structure. Recommendations are finally given to governmental agencies and business firms. These findings will be beneficial to promoting the sustainable development of green logistics.", "pages": "127897-127912", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Zhang, Meng", "Sun, Minghe", "Bi, Datian", "Liu, Tongzhe"]}]["9001084", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2020.2974774", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Windows", "Indium tin oxide", "Heuristic algorithms", "Vehicle routing", "Adaptation models", "Logistics", "Time-varying traffic flow", "multiple fuzzy time windows", "Ito algorithm", "vehicle routing problem (VRP)", "consumer satisfaction"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "A Vehicle Routing Problem Model With Multiple Fuzzy Windows Based on Time-Varying Traffic Flow", "url": "", "volume": "8", "year": "2020", "abstract": "In actual distribution process, the traffic flow varies with time, and each consumer has multiple fuzzy windows. To minimize the total distribution cost and mean consumer dissatisfaction, this paper sets up a vehicle routing problem (VRP) model with multiple fuzzy time windows, based on time-varying traffic flow. In addition, the Ito algorithm was improved based on time-varying traffic flow. The model and algorithm were verified through example simulation, in comparison with ant colony optimization (ACO). During the simulation, the improved Ito algorithm effectively reduced the distribution cost and consumer dissatisfaction, and outperformed the ACO in solving efficiency and solution quality. The results fully demonstrate the feasibility and effectiveness of the proposed algorithm. The research findings provide a desirable solution to VRPs with multiple fuzzy windows and time-varying traffic flow in the real world.", "pages": "39439-39444", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Zheng, Jun"]}]["9099803", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2020.2997777", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Scheduling", "Optimal scheduling", "Genetic algorithms", "Rail transportation", "Delays", "Heuristic algorithms", "Linear programming", "High-speed railway", "cyclic timetable", "adding additional train paths", "genetic algorithm", "mixed integer programming"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "Scheduling Extra Train Paths Into Cyclic Timetable Based on the Genetic Algorithm", "url": "", "volume": "8", "year": "2020", "abstract": "In order to support the process of scheduling a hybrid cyclic timetable, this paper is devoted to inserting additional non-cyclic train paths into existing cyclic timetable. The adding train paths problem is an integration of timetable scheduling and rescheduling problem. The train dispatcher can not only modify the given timetable to manage the interruptions in existing operations, but also establish schedules for additional trains. A multi-objective model minimizing both the travel time of additional trains and the variation of existing trains is proposed in this paper. In addition, we consider both general constraints and some additional practical constraints, such as the overtaking priority constraint, reasonable adjustment of initial schedules and the scheduled connections. This problem is very difficult and must be solved in practice. A heuristic algorithm is introduced to find high-quality solutions for large-scale cases within reasonable computing time. Based on high-speed railway line in China, the case studies illustrate the methodology and compare the performance of trains. Numerical experiments indicate that the proposed solution approach to the adding train paths problem is promising.", "pages": "102199-102211", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Tan, Yu-Yan", "Li, Ya-Xuan", "Wang, Ru-Xin"]}]["9046938", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2020.2980179", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": ["Business", "Big Data", "Safety", "Education", "Object recognition", "Broadcasting", "Deep learning", "frequency separation", "image super-resolution"], "month": "", "number": "", "numpages": "", "publisher": "", "title": "Correction to \u201cFrequency Separation Network for Image Super-Resolution\u201d", "url": "", "volume": "8", "year": "2020", "abstract": "In the above-named article, the funding information was incorrect. In addition, there was an error in the author affiliation that is corrected here.", "pages": "54698-54698", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Li, Shanshan", "Cai, Qiang", "Li, Haisheng", "Cao, Jian", "Wang, Lei", "Li, Zhuangzi"]}]["8694036", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2019.2908776", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": [""], "month": "", "number": "", "numpages": "", "publisher": "", "title": "IEEE Access Special Section Editorial: Data Mining and Granular Computing in Big Data and Knowledge Processing", "url": "", "volume": "7", "year": "2019", "abstract": "Data mining has actively contributed to solving many real-world problems with a variety of techniques. Traditional approaches in this field are classification, clustering and regression. During the last few years a number of chal-lenges have emerged, such as imbalanced data, multi-label and multi-instance problems, low quality and/or noisy data or semi-supervised learning, among others [item 1) in the Appendix]. When these non-standard scenarios are encountered in the realm of big data, it remains an uncharted research territory, although a growing effort has been made to break the limits. The current trend is to address the classical and newly emerging data mining problems in big data and knowledge processing. Granular computing provides a powerful tool for multiple granularity and multiple-view data analysis at differ-ent granularity levels, which has demonstrated strong capabil-ities and advantages in intelligent data analysis, pattern recog-nition, machine learning and uncertain reasoning [item 2) inthe Appendix]. Big data often contains a significant amount of unstructured, uncertain and imprecise data. There are new challenges regarding the scalability of granular computing when addressing very big data sets [item 3) in the Appendix]. Big data mining relies on distributed computational strate-gies; it is often impossible to store and process data on one single computing node. The exploration of data mining and granular computing in big data and knowledge processing is an emerging field which crosses multiple research disciplines and industry domains, including transportation, communications, social network, medical health, and so on.", "pages": "47682-47686", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Ding, Weiping", "Yen, Gary", "Beliakov, Gleb", "Triguero, Isaac", "Pratama, Mahardhika", "Zhang, Xiangliang", "Li, Hongjun"]}]["8474523", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2018.2870417", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": [""], "month": "", "number": "", "numpages": "", "publisher": "", "title": "IEEE Access Special Section Editorial: Healthcare Big Data", "url": "", "volume": "6", "year": "2018", "abstract": "With the explosion of big data technology, healthcare data is continuously and rapidly growing, with abundant and various values. There are wide varieties of data and heterogeneous healthcare data (images, text, video, raw sensor data, etc.) that are generated and required to be effectively stored, processed, queried, indexed and analyzed. These datasets differ widely in their volume, variety, velocity and value, including patient-oriented data such as electronic medical records (EMR), public-oriented data such as public health data, and knowledge-oriented data such as drug-to-drug, drug-to-disease, and disease-to-disease interaction registries. Big data in healthcare brings great challenges but plays an important role in healthcare transformation. The traditional techniques do not compromise end-users\u2019 Quality of Service (QoS) in terms of data availability, data response delay, etc. It is urgent to develop software tools and techniques that support rapid query processing and speed-up data analytics, which provide awareness and knowledge in real-time.", "pages": "50555-50558", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Zhang, Yin", "Wang, Haiyang", "Chen, Min", "Wan, Jiafu", "Humar, Iztok"]}]["9274543", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2020.3037614", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": [""], "month": "", "number": "", "numpages": "", "publisher": "", "title": "IEEE Access Special Section Editorial: Convergence of Sensor Networks, Cloud Computing, and Big Data in Industrial Internet of Things", "url": "", "volume": "8", "year": "2020", "abstract": "With growing attention from both academia and industry, Industrial Internet of Things (IIoT), comprised of a multitude of connected devices, is supposed to monitor, collect, exchange, analyze, and instantly act on information to intelligently change the industrial device\u2019s behavior or the industrial environment (e.g., autonomous reaction to unexpected changes in production by effectively detecting failures and triggering maintenance processes). Toward this goal, the convergence of sensor networks, cloud computing, and big data in IIoT is recently identified as an essential component. Specifically, to capture various data in IIoT, sensor networks are deployed. Meanwhile, to store and process data powerfully in IIoT, cloud computing platforms are developed. Moreover, to glean valuable findings from the massive data in IIoT, big data analytics tools are utilized. However, to integrate sensor networks, cloud computing, and big data in IIoT in a robust way, there are a lot of tough issues to be solved with respect to various aspects (e.g., framework, greenness, security, quality of service, etc.).", "pages": "210035-210040", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Shu, Lei", "Piuri, Vincenzo", "Zhu, Chunsheng", "Chen, Xuebin", "Mukherjee, Mithun"]}]["9683997", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2020.3036101", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": [""], "month": "", "number": "", "numpages": "", "publisher": "", "title": "IEEE Access Special Section: Privacy Preservation for Large-Scale User Data in Social Networks", "url": "", "volume": "10", "year": "2022", "abstract": "Social networks have become one of the most popular platforms for people to communicate and interact with their friends and share personal information and experiences (e.g., Facebook owns over 1.23 billion monthly active users). The increasing popularity of social networks has generated extremely large-scale user data (e.g., Twitter generates 500 million tweets per day and around 200 billion tweets per year). These data can help improve people\u2019s quality of life as well as benefit various interest groups such as advertisers, application developers, and so on. However, privacy may be compromised if learning algorithms are used to infer unpublished privacy information from published data. Hence, user data privacy preservation has become one of the most urgent research issues in social networks.", "pages": "4374-4379", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Gao, Yuan", "Li, Yi", "Sun, Yunchuan", "Cai, Zhipeng", "Ma, Liran", "Pusti\u0161ek, Matev\u017e", "Hu, Su"]}]["9526279", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2021.3106756", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": [""], "month": "", "number": "", "numpages": "", "publisher": "", "title": "IEEE Access Special Section Editorial: Toward Smart Cities With IoT Based on Crowdsensing", "url": "", "volume": "9", "year": "2021", "abstract": "The proliferation of the Internet of Things (IoT) has paved the way for the future of smart cities. The large volume of data over the IoT can enable decision-making for various applications such as smart transportation, smart parking, and smart lighting. The key to the success of smart cities is data collection and aggregation over the IoT. Recently, crowdsensing has become a new data collection paradigm over the IoT, which can realize large-scale and fine-grained data collection with low cost for various applications. For example, we can leverage the power of the crowd to build a real-time noise map with microphones on smartphones. Despite the advantages of crowdsensing and the IoT, there are many challenges to utilize crowdsensing over the IoT for smart cities, such as how to allocate tasks to appropriate users to provide high-quality sensing data, how to incentivize users to participate in crowdsourcing, how to detect the reliability of the crowdsourced data, and how to protect the privacy of users.", "pages": "118606-118609", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Wang, Kun", "Wang, Zhibo", "Song, Ye-Qiong", "Yang, Dejun", "He, Shibo", "Wang, Wei"]}]["8262687", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2017.2783118", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": [""], "month": "", "number": "", "numpages": "", "publisher": "", "title": "IEEE Access Special Section Editorial: Health Informatics for the Developing World", "url": "", "volume": "5", "year": "2017", "abstract": "We live in a world with growing disparity in the quality of life available to people in the developed and developing countries. Healthcare in the developing world is fraught with numerous problems such as the lack of health infrastructure, and human resources, which results in very limited health coverage. The field of health informatics has made great strides in recent years towards improving public health systems in the developing world by augmenting them with state-of-the-art information and communication technologies (ICT). Through real-world deployment of these technologies, there is real hope that the health industry in the developing world will progress from its current, largely dysfunctional state to one that is more effective, personalized, and cost effective. Health informatics can usher a new era of personalized health analytics, with the potential to transform healthcare in the developing world. In conjunction with mHealth and eHealth, many other important health informatics trends\u2014such as artificial intelligence (AI), machine learning (ML), big data, crowdsourcing, cloud computing\u2014are also emerging. Exponentially growing heterogeneous data, with the help of big data analytics, has the potential to provide descriptive, predictive, and prescriptive health insights as well as enable new applications such as telemedicine and remote diagnostics and surgery. Such systems could enhance the overall process of monitoring, diagnosis, and prognosis of diseases.", "pages": "27818-27823", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Qadir, Junaid", "Mujeeb-U-Rahman, Muhammad", "Rehmani, Mubashir", "Pathan, Al-Sakib", "Imran, Muhammad", "Hussain, Amir", "Rana, Rajib", "Luo, Bin"]}]["8262678", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2017.2783139", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": [""], "month": "", "number": "", "numpages": "", "publisher": "", "title": "A Special Section in IEEE Access: Cooperative and Intelligent Sensing", "url": "", "volume": "5", "year": "2017", "abstract": "With the advances of technologies in Micro-electro-mechanics (MEMS) and wireless communications, it has become feasible to deploy a large-scale wireless sensor network (WSN) with thousands of tiny and inexpensive sensor nodes scattered over a vast field so that information of interest can be obtained, processed, transmitted, and fused automatically via node collaboration and sensing. With the help of cloud computing, Internet of Things (IoTs), device to device (D2D) communications, and big data, cooperative and intelligent sensing will bridge the gap of ubiquitous sensing, intelligent computing, cooperative communication, and mass data management technologies to create novel solutions that improve the urban environment, human life quality, and smart city systems.", "pages": "27824-27826", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Zhu, Rongbo", "Ma, Maode", "Liu, Lu", "Mao, Shiwen"]}]["9551694", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2021.3111669", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": [""], "month": "", "number": "", "numpages": "", "publisher": "", "title": "IEEE Access Special Section Editorial: Urban Computing and Intelligence", "url": "", "volume": "9", "year": "2021", "abstract": "Urban computing utilizes unobtrusive and ubiquitous sensing technologies, advanced data management and analytics models, and novel visualization methods to create win-win-win solutions which intelligently improve people\u2019s lives, urban environments, and city operation systems. With the help of cloud computing, the Internet of Things, device-to-device (D2D) communication, artificial intelligence (AI), big data, and urban computing and intelligence will bridge the gap of ubiquitous sensing, intelligent computing, cooperative communication, and mass data management technologies, to create novel solutions that improve urban environments, human life quality, and smart city systems. Thus, urban computing and intelligence has recently attracted significant attention from industry and academia for building smart cities.", "pages": "130690-130697", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Zhu, Rongbo", "Liu, Lu", "Ma, Maode", "Li, Hongxiang", "Mao, Shiwen"]}]["9527997", {"address": "", "articleno": "", "doi": "10.1109/ACCESS.2021.3106717", "issn": "2169-3536", "issue_date": "", "journal": "IEEE Access", "keywords": [""], "month": "", "number": "", "numpages": "", "publisher": "", "title": "IEEE Access Special Section Editorial: Advanced Artificial Intelligence Technologies for Smart Manufacturing", "url": "", "volume": "9", "year": "2021", "abstract": "Industry 4.0, also known as the fourth industrial revolution, is an area that many scientists and manufacturers are pursuing. Industry 4.0 consists of many topics such as the Internet of things (IoT), big data, cloud computing, smart manufacturing, and so on. Smart manufacturing is a crucial and valuable topic which aims at developing advanced techniques to improve the quality and costs of manufacturing. Through sensors, networks, and high-performance computers, powerful algorithms for smart manufacturing can be developed and implemented. Thanks to an innovative variety of sensors, reliable, and high-resolution information can be collected and utilized. Networks allow signals to be exchanged quickly between sensors, machines, and computers. Artificial intelligence (AI) requires huge computation power. Modern computers provide graphic cards with parallel computing, breaking this restriction. Algorithms related to smart manufacturing will be more complicated than before. As a result, this Special Section aims to speed up the development of smart manufacturing, attract the attention of communities, and disseminate novel research.", "pages": "119232-119234", "note": "", "ISSN": "2169-3536", "publicationtype": "article", "author": ["Yau, Her-Terng", "Prior, Stephen", "Wang, Yang", "Li, Yunhua"]}]["9691303", {"address": "", "articleno": "", "doi": "10.26599/BDMA.2022.9020001", "issn": "2096-0654", "issue_date": "", "journal": "Big Data Mining and Analytics", "keywords": [""], "month": "June", "number": "2", "numpages": "", "publisher": "", "title": "Call for papers: Special issue on AI-enabled Internet of medical things for medical data analytics", "url": "", "volume": "5", "year": "2022", "abstract": "In the modern era of Big Data, there is always an exponential growth in the amount of data generated and stored in various fields like education, energy, environment, healthcare, fraud, detection, and traffic. At the same time, there is a significant paradigm shift in business and society across the world due to huge growth in various fields like artificial intelligence, machine learning, deep learning and data analytics. This poses significant challenges for decision-making and creates a potential transformation in the economy, government, and industries. Artificial Intelligence tools, techniques, technologies and big data improve the predictive power of the systems created, which enables the government, public and private sectors to discover new patterns and trends. It improves public values like accountability, safety, security, and transparency for better decision-making, policies, and governance. They also have numerous capabilities to perform complex tasks that human skills cannot do. They could be used to collect, organize and analyze, large, varied data sets to discover patterns and trends. It addresses several problems related to the development of the economy by identifying new sources of revenue, expanding the customer base for business, product reviews and promotion. It helps to predict and prevent diseases, predict climatic variations, and provide energy solutions. This special issue aims to call for high-quality papers covering the latest data analytic concepts and technologies of big data and artificial intelligence. This special issue serves as a forum for researchers across the globe to discuss their works and recent advances in this field. The best papers from Artificial intelligence and Big Data Analytics in Product, Finance, Health and Environment are invited. The best high-quality papers will be selected based on the innovativeness and relevance of the theme.", "pages": "162-162", "note": "", "ISSN": "2096-0654", "publicationtype": "article", "author": []}]["6574681", {"address": "", "articleno": "", "doi": "10.1109/TST.2013.6574681", "issn": "1007-0214", "issue_date": "", "journal": "Tsinghua Science and Technology", "keywords": [""], "month": "August", "number": "4", "numpages": "", "publisher": "", "title": "Call for papers special issue of Tsinghua Science and Technology on cloud computing and big data", "url": "", "volume": "18", "year": "2013", "abstract": "This special issue on Cloud Computing and Big Data of Tsinghua Science and Technology is devoted to gather and present new research that address the challenges in the broad areas of Cloud Computing and Big Data. Despite being popular topics in both industry and academia, Cloud Computing and Big Data are having more unsolved problems, not fewer. Challenging problems include key enabling technologies like virtualization and software defined network, powerful data process like deep learning and No-SQL, energy efficiency, privacy and policy, new ecosystem and many more. This Special Issue therefore aims to publish high quality, original, unpublished research papers in the broad area of Cloud Computing and Big Data, and thus presents a platform for scientists and scholars to share their observations and research results in the field.", "pages": "428-428", "note": "", "ISSN": "1007-0214", "publicationtype": "article", "author": []}]["6616528", {"address": "", "articleno": "", "doi": "10.1109/TST.2013.6616528", "issn": "1007-0214", "issue_date": "", "journal": "Tsinghua Science and Technology", "keywords": [""], "month": "Oct", "number": "5", "numpages": "", "publisher": "", "title": "Call for papers special section of Tsinghua Science and Technology on smart grid", "url": "", "volume": "18", "year": "2013", "abstract": "This special section on Smart Grid of Tsinghua Science and Technology is devoted to gather and present new research that addresses the challenges in the broad area of Smart Grid. Smart Grid uses information and communications technology, such as Big Data, Internet of Thing (IoT), and new network technology, to transform electrical grid to be more intelligent and active to gather information about the behaviors of suppliers and consumers, in an automated fashion to improve the efficiency, reliability, and sustainability of the generation and distribution of electricity energy. This special section therefore aims to publish high quality, original, unpublished research papers in the broad area of Smart Grid, and thus presents a platform for scientists and scholars to share their observations and research results in the field.", "pages": "542-542", "note": "", "ISSN": "1007-0214", "publicationtype": "article", "author": []}]